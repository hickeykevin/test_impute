USGAN on last questions, ricardo style evaluation
[2024-06-20 17:58:01,413][HYDRA] Launching 560 jobs locally
[2024-06-20 17:58:01,413][HYDRA] 	#0 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=dream_job data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=0
[2024-06-20 17:58:01,735][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 17:58:03,367][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
2024-06-20 17:58:04 [ERROR]: ❌ No module named 'torch_geometric'
Note torch_geometric is missing, please install it with 'pip install torch_geometric torch_scatter torch_sparse' or 'conda install -c pyg pyg pytorch-scatter pytorch-sparse'
2024-06-20 17:58:04 [ERROR]: ❌ name 'MessagePassing' is not defined
Note torch_geometric is missing, please install it with 'pip install torch_geometric torch_scatter torch_sparse' or 'conda install -c pyg pyg pytorch-scatter pytorch-sparse'
[2024-06-20 17:58:04,845][train.py][INFO] - Instantiating callbacks...
[2024-06-20 17:58:04,845][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 17:58:04,872][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 17:58:04,873][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 17:58:04,876][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 17:58:04,876][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 17:58:04,877][train.py][INFO] - Instantiating loggers...
[2024-06-20 17:58:04,877][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 17:58:04,878][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
/home/khickey/test_impute/env/lib/python3.12/site-packages/torch/cuda/__init__.py:628: UserWarning: Can't initialize NVML
  warnings.warn("Can't initialize NVML")
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 17:58:05,394][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/dream_job/0.0/0/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.29it/s v_num: 0.000      
                                                              val/auc: 0.444    
                                                              val/f1: 0.000     
                                                              val/precision:    
                                                              0.000 val/recall: 
                                                              0.000 val/mre: nan
                                                              train/auc: 0.964  
                                                              train/f1: 0.964   
                                                              train/precision:  
                                                              0.964             
                                                              train/recall:     
                                                              0.964 train/mre:  
                                                              nan               
[2024-06-20 17:59:57,505][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/dream_job/0.0/0>
[2024-06-20 17:59:57,506][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 17:59:57,508][HYDRA] 	#1 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=dream_job data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=1
[rank: 0] Seed set to 1
[2024-06-20 17:59:57,684][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 17:59:57,685][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-20 17:59:57,686][train.py][INFO] - Instantiating callbacks...
[2024-06-20 17:59:57,687][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 17:59:57,689][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 17:59:57,689][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 17:59:57,690][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 17:59:57,690][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 17:59:57,691][train.py][INFO] - Instantiating loggers...
[2024-06-20 17:59:57,691][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 17:59:57,692][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 17:59:57,720][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/dream_job/0.0/1/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.19it/s v_num: 0.000      
                                                              val/auc: 0.444    
                                                              val/f1: 0.000     
                                                              val/precision:    
                                                              0.000 val/recall: 
                                                              0.000 val/mre: nan
                                                              train/auc: 0.973  
                                                              train/f1: 0.973   
                                                              train/precision:  
                                                              0.982             
                                                              train/recall:     
                                                              0.964 train/mre:  
                                                              nan               
[2024-06-20 18:01:50,499][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/dream_job/0.0/1>
[2024-06-20 18:01:50,500][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 18:01:50,502][HYDRA] 	#2 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=dream_job data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=2
[rank: 0] Seed set to 2
[2024-06-20 18:01:50,681][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 18:01:50,683][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-20 18:01:50,684][train.py][INFO] - Instantiating callbacks...
[2024-06-20 18:01:50,684][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 18:01:50,687][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 18:01:50,687][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 18:01:50,687][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 18:01:50,688][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 18:01:50,688][train.py][INFO] - Instantiating loggers...
[2024-06-20 18:01:50,689][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 18:01:50,690][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 18:01:50,720][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/dream_job/0.0/2/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.20it/s v_num: 0.000      
                                                              val/auc: 0.500    
                                                              val/f1: 0.000     
                                                              val/precision:    
                                                              0.000 val/recall: 
                                                              0.000 val/mre: nan
                                                              train/auc: 0.946  
                                                              train/f1: 0.946   
                                                              train/precision:  
                                                              0.946             
                                                              train/recall:     
                                                              0.946 train/mre:  
                                                              nan               
[2024-06-20 18:03:43,905][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/dream_job/0.0/2>
[2024-06-20 18:03:43,905][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 18:03:43,908][HYDRA] 	#3 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=dream_job data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=3
[rank: 0] Seed set to 3
[2024-06-20 18:03:44,091][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 18:03:44,092][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-20 18:03:44,094][train.py][INFO] - Instantiating callbacks...
[2024-06-20 18:03:44,094][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 18:03:44,096][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 18:03:44,097][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 18:03:44,097][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 18:03:44,098][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 18:03:44,098][train.py][INFO] - Instantiating loggers...
[2024-06-20 18:03:44,098][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 18:03:44,099][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 18:03:44,130][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/dream_job/0.0/3/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.18it/s v_num: 0.000      
                                                              val/auc: 0.333    
                                                              val/f1: 0.000     
                                                              val/precision:    
                                                              0.000 val/recall: 
                                                              0.000 val/mre: nan
                                                              train/auc: 0.982  
                                                              train/f1: 0.982   
                                                              train/precision:  
                                                              0.982             
                                                              train/recall:     
                                                              0.982 train/mre:  
                                                              nan               
[2024-06-20 18:05:37,303][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/dream_job/0.0/3>
[2024-06-20 18:05:37,303][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 18:05:37,306][HYDRA] 	#4 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=dream_job data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=4
[rank: 0] Seed set to 4
[2024-06-20 18:05:37,483][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 18:05:37,485][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-20 18:05:37,486][train.py][INFO] - Instantiating callbacks...
[2024-06-20 18:05:37,487][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 18:05:37,489][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 18:05:37,489][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 18:05:37,490][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 18:05:37,491][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 18:05:37,491][train.py][INFO] - Instantiating loggers...
[2024-06-20 18:05:37,491][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 18:05:37,492][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 18:05:37,522][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/dream_job/0.0/4/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.20it/s v_num: 0.000      
                                                              val/auc: 0.500    
                                                              val/f1: 0.000     
                                                              val/precision:    
                                                              0.000 val/recall: 
                                                              0.000 val/mre: nan
                                                              train/auc: 1.000  
                                                              train/f1: 1.000   
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              nan               
[2024-06-20 18:07:30,126][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/dream_job/0.0/4>
[2024-06-20 18:07:30,126][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 18:07:30,129][HYDRA] 	#5 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=dream_job data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=5
[rank: 0] Seed set to 5
[2024-06-20 18:07:30,304][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 18:07:30,305][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-20 18:07:30,307][train.py][INFO] - Instantiating callbacks...
[2024-06-20 18:07:30,307][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 18:07:30,309][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 18:07:30,309][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 18:07:30,310][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 18:07:30,311][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 18:07:30,312][train.py][INFO] - Instantiating loggers...
[2024-06-20 18:07:30,312][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 18:07:30,313][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 18:07:30,377][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/dream_job/0.0/5/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.13it/s v_num: 0.000      
                                                              val/auc: 0.333    
                                                              val/f1: 0.000     
                                                              val/precision:    
                                                              0.000 val/recall: 
                                                              0.000 val/mre: nan
                                                              train/auc: 0.973  
                                                              train/f1: 0.973   
                                                              train/precision:  
                                                              0.965             
                                                              train/recall:     
                                                              0.982 train/mre:  
                                                              nan               
[2024-06-20 18:09:24,266][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/dream_job/0.0/5>
[2024-06-20 18:09:24,268][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 18:09:24,274][HYDRA] 	#6 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=dream_job data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=6
[rank: 0] Seed set to 6
[2024-06-20 18:09:24,568][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 18:09:24,569][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-20 18:09:24,571][train.py][INFO] - Instantiating callbacks...
[2024-06-20 18:09:24,571][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 18:09:24,575][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 18:09:24,576][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 18:09:24,576][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 18:09:24,577][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 18:09:24,577][train.py][INFO] - Instantiating loggers...
[2024-06-20 18:09:24,577][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 18:09:24,579][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 18:09:24,680][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/dream_job/0.0/6/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.20it/s v_num: 0.000      
                                                              val/auc: 0.328    
                                                              val/f1: 0.133     
                                                              val/precision:    
                                                              0.200 val/recall: 
                                                              0.100 val/mre: nan
                                                              train/auc: 0.982  
                                                              train/f1: 0.982   
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              0.964 train/mre:  
                                                              nan               
[2024-06-20 18:11:17,924][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/dream_job/0.0/6>
[2024-06-20 18:11:17,924][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 18:11:17,928][HYDRA] 	#7 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=dream_job data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=7
[rank: 0] Seed set to 7
[2024-06-20 18:11:18,106][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 18:11:18,108][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-20 18:11:18,109][train.py][INFO] - Instantiating callbacks...
[2024-06-20 18:11:18,109][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 18:11:18,112][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 18:11:18,112][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 18:11:18,112][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 18:11:18,113][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 18:11:18,114][train.py][INFO] - Instantiating loggers...
[2024-06-20 18:11:18,114][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 18:11:18,115][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 18:11:18,146][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/dream_job/0.0/7/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.26it/s v_num: 0.000      
                                                              val/auc: 0.500    
                                                              val/f1: 0.000     
                                                              val/precision:    
                                                              0.000 val/recall: 
                                                              0.000 val/mre: nan
                                                              train/auc: 0.982  
                                                              train/f1: 0.982   
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              0.964 train/mre:  
                                                              nan               
[2024-06-20 18:13:11,580][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/dream_job/0.0/7>
[2024-06-20 18:13:11,581][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 18:13:11,604][HYDRA] 	#8 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=dream_job data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=8
[rank: 0] Seed set to 8
[2024-06-20 18:13:11,811][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 18:13:11,812][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-20 18:13:11,814][train.py][INFO] - Instantiating callbacks...
[2024-06-20 18:13:11,814][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 18:13:11,816][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 18:13:11,816][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 18:13:11,817][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 18:13:11,817][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 18:13:11,818][train.py][INFO] - Instantiating loggers...
[2024-06-20 18:13:11,818][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 18:13:11,819][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 18:13:11,857][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/dream_job/0.0/8/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.20it/s v_num: 0.000      
                                                              val/auc: 0.328    
                                                              val/f1: 0.133     
                                                              val/precision:    
                                                              0.200 val/recall: 
                                                              0.100 val/mre: nan
                                                              train/auc: 0.982  
                                                              train/f1: 0.982   
                                                              train/precision:  
                                                              0.982             
                                                              train/recall:     
                                                              0.982 train/mre:  
                                                              nan               
[2024-06-20 18:15:05,335][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/dream_job/0.0/8>
[2024-06-20 18:15:05,336][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 18:15:05,338][HYDRA] 	#9 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=dream_job data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=9
[rank: 0] Seed set to 9
[2024-06-20 18:15:05,510][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 18:15:05,511][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-20 18:15:05,513][train.py][INFO] - Instantiating callbacks...
[2024-06-20 18:15:05,513][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 18:15:05,515][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 18:15:05,515][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 18:15:05,516][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 18:15:05,517][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 18:15:05,517][train.py][INFO] - Instantiating loggers...
[2024-06-20 18:15:05,517][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 18:15:05,518][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 18:15:05,549][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/dream_job/0.0/9/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.20it/s v_num: 0.000      
                                                              val/auc: 0.433    
                                                              val/f1: 0.267     
                                                              val/precision:    
                                                              0.400 val/recall: 
                                                              0.200 val/mre: nan
                                                              train/auc: 0.973  
                                                              train/f1: 0.972   
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              0.946 train/mre:  
                                                              nan               
[2024-06-20 18:16:58,238][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/dream_job/0.0/9>
[2024-06-20 18:16:58,239][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 18:16:58,241][HYDRA] 	#10 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=dream_job data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=0
[2024-06-20 18:16:58,425][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 18:16:58,427][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-20 18:16:58,429][train.py][INFO] - Instantiating callbacks...
[2024-06-20 18:16:58,429][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 18:16:58,431][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 18:16:58,432][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 18:16:58,432][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 18:16:58,433][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 18:16:58,433][train.py][INFO] - Instantiating loggers...
[2024-06-20 18:16:58,433][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 18:16:58,435][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 18:16:58,469][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/dream_job/0.05/0/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.18it/s v_num: 0.000      
                                                              val/auc: 0.500    
                                                              val/f1: 0.000     
                                                              val/precision:    
                                                              0.000 val/recall: 
                                                              0.000 val/mre:    
                                                              0.131 train/auc:  
                                                              0.973 train/f1:   
                                                              0.974             
                                                              train/precision:  
                                                              0.949             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.126             
[2024-06-20 18:18:52,738][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/dream_job/0.05/0>
[2024-06-20 18:18:52,738][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 18:18:52,742][HYDRA] 	#11 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=dream_job data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=1
[rank: 0] Seed set to 1
[2024-06-20 18:18:52,932][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 18:18:52,934][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-20 18:18:52,935][train.py][INFO] - Instantiating callbacks...
[2024-06-20 18:18:52,935][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 18:18:52,938][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 18:18:52,938][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 18:18:52,939][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 18:18:52,939][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 18:18:52,940][train.py][INFO] - Instantiating loggers...
[2024-06-20 18:18:52,940][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 18:18:52,941][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 18:18:52,993][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/dream_job/0.05/1/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.19it/s v_num: 0.000      
                                                              val/auc: 0.378    
                                                              val/f1: 0.250     
                                                              val/precision:    
                                                              0.333 val/recall: 
                                                              0.200 val/mre:    
                                                              0.134 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.130             
[2024-06-20 18:20:45,957][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/dream_job/0.05/1>
[2024-06-20 18:20:45,957][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 18:20:45,960][HYDRA] 	#12 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=dream_job data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=2
[rank: 0] Seed set to 2
[2024-06-20 18:20:46,142][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 18:20:46,144][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-20 18:20:46,145][train.py][INFO] - Instantiating callbacks...
[2024-06-20 18:20:46,145][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 18:20:46,147][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 18:20:46,148][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 18:20:46,148][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 18:20:46,150][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 18:20:46,151][train.py][INFO] - Instantiating loggers...
[2024-06-20 18:20:46,151][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 18:20:46,152][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 18:20:46,185][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/dream_job/0.05/2/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.18it/s v_num: 0.000      
                                                              val/auc: 0.333    
                                                              val/f1: 0.000     
                                                              val/precision:    
                                                              0.000 val/recall: 
                                                              0.000 val/mre:    
                                                              0.130 train/auc:  
                                                              0.929 train/f1:   
                                                              0.923             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              0.857 train/mre:  
                                                              0.124             
[2024-06-20 18:22:40,954][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/dream_job/0.05/2>
[2024-06-20 18:22:40,954][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 18:22:40,957][HYDRA] 	#13 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=dream_job data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=3
[rank: 0] Seed set to 3
[2024-06-20 18:22:41,133][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 18:22:41,135][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-20 18:22:41,136][train.py][INFO] - Instantiating callbacks...
[2024-06-20 18:22:41,136][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 18:22:41,139][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 18:22:41,139][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 18:22:41,140][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 18:22:41,140][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 18:22:41,141][train.py][INFO] - Instantiating loggers...
[2024-06-20 18:22:41,141][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 18:22:41,142][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 18:22:41,173][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/dream_job/0.05/3/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.19it/s v_num: 0.000      
                                                              val/auc: 0.444    
                                                              val/f1: 0.000     
                                                              val/precision:    
                                                              0.000 val/recall: 
                                                              0.000 val/mre:    
                                                              0.128 train/auc:  
                                                              0.991 train/f1:   
                                                              0.991             
                                                              train/precision:  
                                                              0.982             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.125             
[2024-06-20 18:24:35,455][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/dream_job/0.05/3>
[2024-06-20 18:24:35,458][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 18:24:35,478][HYDRA] 	#14 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=dream_job data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=4
[rank: 0] Seed set to 4
[2024-06-20 18:24:35,855][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 18:24:35,856][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-20 18:24:35,858][train.py][INFO] - Instantiating callbacks...
[2024-06-20 18:24:35,858][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 18:24:35,860][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 18:24:35,861][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 18:24:35,861][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 18:24:35,863][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 18:24:35,863][train.py][INFO] - Instantiating loggers...
[2024-06-20 18:24:35,864][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 18:24:35,865][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 18:24:35,932][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/dream_job/0.05/4/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.27it/s v_num: 0.000      
                                                              val/auc: 0.383    
                                                              val/f1: 0.143     
                                                              val/precision:    
                                                              0.250 val/recall: 
                                                              0.100 val/mre:    
                                                              0.128 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.125             
[2024-06-20 18:26:28,916][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/dream_job/0.05/4>
[2024-06-20 18:26:28,917][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 18:26:28,920][HYDRA] 	#15 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=dream_job data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=5
[rank: 0] Seed set to 5
[2024-06-20 18:26:29,092][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 18:26:29,094][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-20 18:26:29,095][train.py][INFO] - Instantiating callbacks...
[2024-06-20 18:26:29,095][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 18:26:29,097][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 18:26:29,098][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 18:26:29,098][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 18:26:29,099][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 18:26:29,099][train.py][INFO] - Instantiating loggers...
[2024-06-20 18:26:29,100][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 18:26:29,101][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 18:26:29,128][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/dream_job/0.05/5/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.18it/s v_num: 0.000      
                                                              val/auc: 0.378    
                                                              val/f1: 0.250     
                                                              val/precision:    
                                                              0.333 val/recall: 
                                                              0.200 val/mre:    
                                                              0.132 train/auc:  
                                                              0.982 train/f1:   
                                                              0.982             
                                                              train/precision:  
                                                              0.982             
                                                              train/recall:     
                                                              0.982 train/mre:  
                                                              0.126             
[2024-06-20 18:28:22,332][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/dream_job/0.05/5>
[2024-06-20 18:28:22,333][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 18:28:22,336][HYDRA] 	#16 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=dream_job data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=6
[rank: 0] Seed set to 6
[2024-06-20 18:28:22,515][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 18:28:22,517][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-20 18:28:22,518][train.py][INFO] - Instantiating callbacks...
[2024-06-20 18:28:22,518][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 18:28:22,520][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 18:28:22,521][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 18:28:22,521][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 18:28:22,522][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 18:28:22,522][train.py][INFO] - Instantiating loggers...
[2024-06-20 18:28:22,522][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 18:28:22,524][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 18:28:22,553][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/dream_job/0.05/6/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.18it/s v_num: 0.000      
                                                              val/auc: 0.272    
                                                              val/f1: 0.125     
                                                              val/precision:    
                                                              0.167 val/recall: 
                                                              0.100 val/mre:    
                                                              0.125 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.122             
[2024-06-20 18:30:15,905][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/dream_job/0.05/6>
[2024-06-20 18:30:15,905][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 18:30:15,908][HYDRA] 	#17 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=dream_job data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=7
[rank: 0] Seed set to 7
[2024-06-20 18:30:16,086][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 18:30:16,088][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-20 18:30:16,089][train.py][INFO] - Instantiating callbacks...
[2024-06-20 18:30:16,090][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 18:30:16,092][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 18:30:16,093][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 18:30:16,093][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 18:30:16,094][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 18:30:16,094][train.py][INFO] - Instantiating loggers...
[2024-06-20 18:30:16,094][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 18:30:16,096][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 18:30:16,124][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/dream_job/0.05/7/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.17it/s v_num: 0.000      
                                                              val/auc: 0.439    
                                                              val/f1: 0.154     
                                                              val/precision:    
                                                              0.333 val/recall: 
                                                              0.100 val/mre:    
                                                              0.134 train/auc:  
                                                              0.982 train/f1:   
                                                              0.982             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              0.964 train/mre:  
                                                              0.127             
[2024-06-20 18:32:10,488][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/dream_job/0.05/7>
[2024-06-20 18:32:10,489][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 18:32:10,492][HYDRA] 	#18 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=dream_job data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=8
[rank: 0] Seed set to 8
[2024-06-20 18:32:10,667][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 18:32:10,669][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-20 18:32:10,670][train.py][INFO] - Instantiating callbacks...
[2024-06-20 18:32:10,671][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 18:32:10,674][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 18:32:10,675][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 18:32:10,675][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 18:32:10,676][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 18:32:10,676][train.py][INFO] - Instantiating loggers...
[2024-06-20 18:32:10,676][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 18:32:10,677][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 18:32:10,707][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/dream_job/0.05/8/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.27it/s v_num: 0.000      
                                                              val/auc: 0.422    
                                                              val/f1: 0.421     
                                                              val/precision:    
                                                              0.444 val/recall: 
                                                              0.400 val/mre:    
                                                              0.132 train/auc:  
                                                              0.973 train/f1:   
                                                              0.972             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              0.946 train/mre:  
                                                              0.126             
[2024-06-20 18:34:04,539][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/dream_job/0.05/8>
[2024-06-20 18:34:04,540][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 18:34:04,542][HYDRA] 	#19 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=dream_job data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=9
[rank: 0] Seed set to 9
[2024-06-20 18:34:04,717][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 18:34:04,719][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-20 18:34:04,720][train.py][INFO] - Instantiating callbacks...
[2024-06-20 18:34:04,720][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 18:34:04,722][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 18:34:04,723][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 18:34:04,723][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 18:34:04,724][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 18:34:04,724][train.py][INFO] - Instantiating loggers...
[2024-06-20 18:34:04,725][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 18:34:04,726][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 18:34:04,756][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/dream_job/0.05/9/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.20it/s v_num: 0.000      
                                                              val/auc: 0.428    
                                                              val/f1: 0.353     
                                                              val/precision:    
                                                              0.429 val/recall: 
                                                              0.300 val/mre:    
                                                              0.131 train/auc:  
                                                              0.955 train/f1:   
                                                              0.953             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              0.911 train/mre:  
                                                              0.126             
[2024-06-20 18:35:58,148][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/dream_job/0.05/9>
[2024-06-20 18:35:58,149][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 18:35:58,153][HYDRA] 	#20 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=dream_job data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=0
[2024-06-20 18:35:58,335][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 18:35:58,337][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-20 18:35:58,338][train.py][INFO] - Instantiating callbacks...
[2024-06-20 18:35:58,338][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 18:35:58,341][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 18:35:58,341][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 18:35:58,342][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 18:35:58,342][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 18:35:58,343][train.py][INFO] - Instantiating loggers...
[2024-06-20 18:35:58,343][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 18:35:58,344][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 18:35:58,375][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/dream_job/0.1/0/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.19it/s v_num: 0.000      
                                                              val/auc: 0.422    
                                                              val/f1: 0.421     
                                                              val/precision:    
                                                              0.444 val/recall: 
                                                              0.400 val/mre:    
                                                              0.139 train/auc:  
                                                              0.973 train/f1:   
                                                              0.972             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              0.946 train/mre:  
                                                              0.135             
[2024-06-20 18:37:52,028][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/dream_job/0.1/0>
[2024-06-20 18:37:52,029][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 18:37:52,031][HYDRA] 	#21 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=dream_job data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=1
[rank: 0] Seed set to 1
[2024-06-20 18:37:52,214][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 18:37:52,215][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-20 18:37:52,218][train.py][INFO] - Instantiating callbacks...
[2024-06-20 18:37:52,218][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 18:37:52,239][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 18:37:52,240][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 18:37:52,240][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 18:37:52,241][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 18:37:52,241][train.py][INFO] - Instantiating loggers...
[2024-06-20 18:37:52,241][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 18:37:52,243][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 18:37:52,273][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/dream_job/0.1/1/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.18it/s v_num: 0.000      
                                                              val/auc: 0.367    
                                                              val/f1: 0.400     
                                                              val/precision:    
                                                              0.400 val/recall: 
                                                              0.400 val/mre:    
                                                              0.142 train/auc:  
                                                              0.938 train/f1:   
                                                              0.933             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              0.875 train/mre:  
                                                              0.139             
[2024-06-20 18:39:45,459][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/dream_job/0.1/1>
[2024-06-20 18:39:45,460][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 18:39:45,463][HYDRA] 	#22 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=dream_job data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=2
[rank: 0] Seed set to 2
[2024-06-20 18:39:45,644][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 18:39:45,646][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-20 18:39:45,647][train.py][INFO] - Instantiating callbacks...
[2024-06-20 18:39:45,648][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 18:39:45,650][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 18:39:45,651][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 18:39:45,651][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 18:39:45,652][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 18:39:45,652][train.py][INFO] - Instantiating loggers...
[2024-06-20 18:39:45,652][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 18:39:45,654][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 18:39:45,682][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/dream_job/0.1/2/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.16it/s v_num: 0.000      
                                                              val/auc: 0.494    
                                                              val/f1: 0.167     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.100 val/mre:    
                                                              0.139 train/auc:  
                                                              0.982 train/f1:   
                                                              0.982             
                                                              train/precision:  
                                                              0.966             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.133             
[2024-06-20 18:41:40,421][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/dream_job/0.1/2>
[2024-06-20 18:41:40,422][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 18:41:40,424][HYDRA] 	#23 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=dream_job data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=3
[rank: 0] Seed set to 3
[2024-06-20 18:41:40,606][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 18:41:40,608][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-20 18:41:40,609][train.py][INFO] - Instantiating callbacks...
[2024-06-20 18:41:40,610][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 18:41:40,612][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 18:41:40,613][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 18:41:40,613][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 18:41:40,614][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 18:41:40,615][train.py][INFO] - Instantiating loggers...
[2024-06-20 18:41:40,615][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 18:41:40,616][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 18:41:40,686][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/dream_job/0.1/3/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:02 • 0:00:00 1.88it/s v_num: 0.000      
                                                              val/auc: 0.433    
                                                              val/f1: 0.267     
                                                              val/precision:    
                                                              0.400 val/recall: 
                                                              0.200 val/mre:    
                                                              0.135 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.133             
[2024-06-20 18:43:34,759][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/dream_job/0.1/3>
[2024-06-20 18:43:34,759][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 18:43:34,762][HYDRA] 	#24 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=dream_job data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=4
[rank: 0] Seed set to 4
[2024-06-20 18:43:34,966][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 18:43:34,967][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-20 18:43:34,968][train.py][INFO] - Instantiating callbacks...
[2024-06-20 18:43:34,969][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 18:43:34,971][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 18:43:34,972][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 18:43:34,972][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 18:43:34,973][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 18:43:34,973][train.py][INFO] - Instantiating loggers...
[2024-06-20 18:43:34,973][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 18:43:34,975][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 18:43:35,017][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/dream_job/0.1/4/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.19it/s v_num: 0.000      
                                                              val/auc: 0.272    
                                                              val/f1: 0.125     
                                                              val/precision:    
                                                              0.167 val/recall: 
                                                              0.100 val/mre:    
                                                              0.136 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.133             
[2024-06-20 18:45:29,023][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/dream_job/0.1/4>
[2024-06-20 18:45:29,024][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 18:45:29,026][HYDRA] 	#25 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=dream_job data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=5
[rank: 0] Seed set to 5
[2024-06-20 18:45:29,205][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 18:45:29,206][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-20 18:45:29,208][train.py][INFO] - Instantiating callbacks...
[2024-06-20 18:45:29,208][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 18:45:29,210][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 18:45:29,211][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 18:45:29,211][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 18:45:29,212][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 18:45:29,212][train.py][INFO] - Instantiating loggers...
[2024-06-20 18:45:29,212][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 18:45:29,214][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 18:45:29,244][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/dream_job/0.1/5/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.24it/s v_num: 0.000      
                                                              val/auc: 0.483    
                                                              val/f1: 0.375     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.300 val/mre:    
                                                              0.140 train/auc:  
                                                              0.991 train/f1:   
                                                              0.991             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              0.982 train/mre:  
                                                              0.134             
[2024-06-20 18:47:23,998][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/dream_job/0.1/5>
[2024-06-20 18:47:23,999][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 18:47:24,003][HYDRA] 	#26 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=dream_job data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=6
[rank: 0] Seed set to 6
[2024-06-20 18:47:24,280][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 18:47:24,281][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-20 18:47:24,283][train.py][INFO] - Instantiating callbacks...
[2024-06-20 18:47:24,283][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 18:47:24,288][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 18:47:24,288][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 18:47:24,289][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 18:47:24,291][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 18:47:24,291][train.py][INFO] - Instantiating loggers...
[2024-06-20 18:47:24,291][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 18:47:24,292][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 18:47:24,414][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/dream_job/0.1/6/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.16it/s v_num: 0.000      
                                                              val/auc: 0.217    
                                                              val/f1: 0.118     
                                                              val/precision:    
                                                              0.143 val/recall: 
                                                              0.100 val/mre:    
                                                              0.134 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.130             
[2024-06-20 18:49:18,515][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/dream_job/0.1/6>
[2024-06-20 18:49:18,515][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 18:49:18,520][HYDRA] 	#27 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=dream_job data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=7
[rank: 0] Seed set to 7
[2024-06-20 18:49:18,704][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 18:49:18,705][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-20 18:49:18,707][train.py][INFO] - Instantiating callbacks...
[2024-06-20 18:49:18,707][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 18:49:18,709][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 18:49:18,710][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 18:49:18,710][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 18:49:18,711][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 18:49:18,711][train.py][INFO] - Instantiating loggers...
[2024-06-20 18:49:18,711][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 18:49:18,713][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 18:49:18,743][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/dream_job/0.1/7/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.21it/s v_num: 0.000      
                                                              val/auc: 0.494    
                                                              val/f1: 0.167     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.100 val/mre:    
                                                              0.142 train/auc:  
                                                              0.991 train/f1:   
                                                              0.991             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              0.982 train/mre:  
                                                              0.137             
[2024-06-20 18:51:13,371][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/dream_job/0.1/7>
[2024-06-20 18:51:13,372][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 18:51:13,374][HYDRA] 	#28 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=dream_job data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=8
[rank: 0] Seed set to 8
[2024-06-20 18:51:13,552][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 18:51:13,553][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-20 18:51:13,555][train.py][INFO] - Instantiating callbacks...
[2024-06-20 18:51:13,555][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 18:51:13,557][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 18:51:13,558][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 18:51:13,558][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 18:51:13,559][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 18:51:13,559][train.py][INFO] - Instantiating loggers...
[2024-06-20 18:51:13,559][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 18:51:13,561][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 18:51:13,588][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/dream_job/0.1/8/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.18it/s v_num: 0.000      
                                                              val/auc: 0.333    
                                                              val/f1: 0.000     
                                                              val/precision:    
                                                              0.000 val/recall: 
                                                              0.000 val/mre:    
                                                              0.140 train/auc:  
                                                              0.991 train/f1:   
                                                              0.991             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              0.982 train/mre:  
                                                              0.136             
[2024-06-20 18:53:06,937][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/dream_job/0.1/8>
[2024-06-20 18:53:06,937][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 18:53:06,940][HYDRA] 	#29 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=dream_job data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=9
[rank: 0] Seed set to 9
[2024-06-20 18:53:07,117][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 18:53:07,119][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-20 18:53:07,120][train.py][INFO] - Instantiating callbacks...
[2024-06-20 18:53:07,120][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 18:53:07,123][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 18:53:07,123][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 18:53:07,124][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 18:53:07,124][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 18:53:07,125][train.py][INFO] - Instantiating loggers...
[2024-06-20 18:53:07,125][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 18:53:07,126][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 18:53:07,158][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/dream_job/0.1/9/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.25it/s v_num: 0.000      
                                                              val/auc: 0.333    
                                                              val/f1: 0.000     
                                                              val/precision:    
                                                              0.000 val/recall: 
                                                              0.000 val/mre:    
                                                              0.139 train/auc:  
                                                              0.991 train/f1:   
                                                              0.991             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              0.982 train/mre:  
                                                              0.135             
[2024-06-20 18:55:01,959][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/dream_job/0.1/9>
[2024-06-20 18:55:01,959][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 18:55:01,962][HYDRA] 	#30 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=dream_job data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=0
[2024-06-20 18:55:02,141][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 18:55:02,143][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-20 18:55:02,144][train.py][INFO] - Instantiating callbacks...
[2024-06-20 18:55:02,144][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 18:55:02,146][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 18:55:02,147][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 18:55:02,147][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 18:55:02,148][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 18:55:02,148][train.py][INFO] - Instantiating loggers...
[2024-06-20 18:55:02,148][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 18:55:02,150][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 18:55:02,181][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/dream_job/0.15/0/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.15it/s v_num: 0.000      
                                                              val/auc: 0.467    
                                                              val/f1: 0.545     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.600 val/mre:    
                                                              0.143 train/auc:  
                                                              0.991 train/f1:   
                                                              0.991             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              0.982 train/mre:  
                                                              0.138             
[2024-06-20 18:56:56,391][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/dream_job/0.15/0>
[2024-06-20 18:56:56,392][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 18:56:56,410][HYDRA] 	#31 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=dream_job data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=1
[rank: 0] Seed set to 1
[2024-06-20 18:56:56,614][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 18:56:56,616][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-20 18:56:56,617][train.py][INFO] - Instantiating callbacks...
[2024-06-20 18:56:56,617][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 18:56:56,621][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 18:56:56,621][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 18:56:56,622][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 18:56:56,622][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 18:56:56,623][train.py][INFO] - Instantiating loggers...
[2024-06-20 18:56:56,623][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 18:56:56,624][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 18:56:56,653][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/dream_job/0.15/1/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.18it/s v_num: 0.000      
                                                              val/auc: 0.372    
                                                              val/f1: 0.333     
                                                              val/precision:    
                                                              0.375 val/recall: 
                                                              0.300 val/mre:    
                                                              0.146 train/auc:  
                                                              0.982 train/f1:   
                                                              0.982             
                                                              train/precision:  
                                                              0.982             
                                                              train/recall:     
                                                              0.982 train/mre:  
                                                              0.142             
[2024-06-20 18:58:50,482][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/dream_job/0.15/1>
[2024-06-20 18:58:50,484][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 18:58:50,487][HYDRA] 	#32 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=dream_job data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=2
[rank: 0] Seed set to 2
[2024-06-20 18:58:50,719][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 18:58:50,721][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-20 18:58:50,723][train.py][INFO] - Instantiating callbacks...
[2024-06-20 18:58:50,723][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 18:58:50,726][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 18:58:50,726][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 18:58:50,727][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 18:58:50,727][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 18:58:50,728][train.py][INFO] - Instantiating loggers...
[2024-06-20 18:58:50,728][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 18:58:50,729][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 18:58:50,791][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/dream_job/0.15/2/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.16it/s v_num: 0.000      
                                                              val/auc: 0.589    
                                                              val/f1: 0.500     
                                                              val/precision:    
                                                              0.667 val/recall: 
                                                              0.400 val/mre:    
                                                              0.144 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.141             
[2024-06-20 19:00:45,524][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/dream_job/0.15/2>
[2024-06-20 19:00:45,525][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 19:00:45,528][HYDRA] 	#33 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=dream_job data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=3
[rank: 0] Seed set to 3
[2024-06-20 19:00:45,762][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 19:00:45,763][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-20 19:00:45,765][train.py][INFO] - Instantiating callbacks...
[2024-06-20 19:00:45,765][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 19:00:45,770][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 19:00:45,770][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 19:00:45,771][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 19:00:45,771][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 19:00:45,772][train.py][INFO] - Instantiating loggers...
[2024-06-20 19:00:45,772][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 19:00:45,773][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 19:00:45,848][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/dream_job/0.15/3/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.17it/s v_num: 0.000      
                                                              val/auc: 0.533    
                                                              val/f1: 0.471     
                                                              val/precision:    
                                                              0.571 val/recall: 
                                                              0.400 val/mre:    
                                                              0.142 train/auc:  
                                                              0.991 train/f1:   
                                                              0.991             
                                                              train/precision:  
                                                              0.982             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.140             
[2024-06-20 19:02:39,862][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/dream_job/0.15/3>
[2024-06-20 19:02:39,863][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 19:02:39,865][HYDRA] 	#34 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=dream_job data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=4
[rank: 0] Seed set to 4
[2024-06-20 19:02:40,036][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 19:02:40,038][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-20 19:02:40,039][train.py][INFO] - Instantiating callbacks...
[2024-06-20 19:02:40,039][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 19:02:40,041][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 19:02:40,042][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 19:02:40,042][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 19:02:40,043][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 19:02:40,043][train.py][INFO] - Instantiating loggers...
[2024-06-20 19:02:40,043][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 19:02:40,044][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 19:02:40,073][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/dream_job/0.15/4/csv_artifacts/
[2024-06-20 19:02:42,017][HYDRA] 	#35 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=dream_job data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=5
[rank: 0] Seed set to 5
[2024-06-20 19:02:42,188][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 19:02:42,189][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-20 19:02:42,190][train.py][INFO] - Instantiating callbacks...
[2024-06-20 19:02:42,191][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 19:02:42,193][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 19:02:42,193][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 19:02:42,194][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 19:02:42,194][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 19:02:42,195][train.py][INFO] - Instantiating loggers...
[2024-06-20 19:02:42,195][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 19:02:42,196][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 19:02:42,224][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/dream_job/0.15/5/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.18it/s v_num: 0.000      
                                                              val/auc: 0.428    
                                                              val/f1: 0.353     
                                                              val/precision:    
                                                              0.429 val/recall: 
                                                              0.300 val/mre:    
                                                              0.145 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.140             
[2024-06-20 19:04:37,072][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/dream_job/0.15/5>
[2024-06-20 19:04:37,073][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 19:04:37,079][HYDRA] 	#36 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=dream_job data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=6
[rank: 0] Seed set to 6
[2024-06-20 19:04:37,260][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 19:04:37,262][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-20 19:04:37,263][train.py][INFO] - Instantiating callbacks...
[2024-06-20 19:04:37,264][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 19:04:37,266][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 19:04:37,266][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 19:04:37,267][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 19:04:37,267][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 19:04:37,268][train.py][INFO] - Instantiating loggers...
[2024-06-20 19:04:37,268][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 19:04:37,269][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 19:04:37,305][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/dream_job/0.15/6/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.16it/s v_num: 0.000      
                                                              val/auc: 0.322    
                                                              val/f1: 0.235     
                                                              val/precision:    
                                                              0.286 val/recall: 
                                                              0.200 val/mre:    
                                                              0.142 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.139             
[2024-06-20 19:06:31,359][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/dream_job/0.15/6>
[2024-06-20 19:06:31,360][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 19:06:31,363][HYDRA] 	#37 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=dream_job data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=7
[rank: 0] Seed set to 7
[2024-06-20 19:06:31,541][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 19:06:31,543][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-20 19:06:31,544][train.py][INFO] - Instantiating callbacks...
[2024-06-20 19:06:31,545][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 19:06:31,548][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 19:06:31,548][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 19:06:31,549][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 19:06:31,549][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 19:06:31,550][train.py][INFO] - Instantiating loggers...
[2024-06-20 19:06:31,550][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 19:06:31,551][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 19:06:31,584][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/dream_job/0.15/7/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.19it/s v_num: 0.000      
                                                              val/auc: 0.633    
                                                              val/f1: 0.632     
                                                              val/precision:    
                                                              0.667 val/recall: 
                                                              0.600 val/mre:    
                                                              0.150 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.144             
[2024-06-20 19:08:25,919][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/dream_job/0.15/7>
[2024-06-20 19:08:25,919][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 19:08:25,922][HYDRA] 	#38 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=dream_job data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=8
[rank: 0] Seed set to 8
[2024-06-20 19:08:26,100][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 19:08:26,102][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-20 19:08:26,103][train.py][INFO] - Instantiating callbacks...
[2024-06-20 19:08:26,103][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 19:08:26,105][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 19:08:26,106][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 19:08:26,106][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 19:08:26,107][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 19:08:26,108][train.py][INFO] - Instantiating loggers...
[2024-06-20 19:08:26,108][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 19:08:26,109][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 19:08:26,140][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/dream_job/0.15/8/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.23it/s v_num: 0.000      
                                                              val/auc: 0.444    
                                                              val/f1: 0.000     
                                                              val/precision:    
                                                              0.000 val/recall: 
                                                              0.000 val/mre:    
                                                              0.144 train/auc:  
                                                              0.982 train/f1:   
                                                              0.982             
                                                              train/precision:  
                                                              0.982             
                                                              train/recall:     
                                                              0.982 train/mre:  
                                                              0.138             
[2024-06-20 19:10:19,756][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/dream_job/0.15/8>
[2024-06-20 19:10:19,757][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 19:10:19,759][HYDRA] 	#39 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=dream_job data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=9
[rank: 0] Seed set to 9
[2024-06-20 19:10:19,932][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 19:10:19,934][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-20 19:10:19,935][train.py][INFO] - Instantiating callbacks...
[2024-06-20 19:10:19,935][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 19:10:19,938][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 19:10:19,938][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 19:10:19,939][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 19:10:19,939][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 19:10:19,940][train.py][INFO] - Instantiating loggers...
[2024-06-20 19:10:19,940][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 19:10:19,941][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 19:10:19,969][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/dream_job/0.15/9/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.26it/s v_num: 0.000      
                                                              val/auc: 0.383    
                                                              val/f1: 0.143     
                                                              val/precision:    
                                                              0.250 val/recall: 
                                                              0.100 val/mre:    
                                                              0.146 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.141             
[2024-06-20 19:12:13,655][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/dream_job/0.15/9>
[2024-06-20 19:12:13,656][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 19:12:13,658][HYDRA] 	#40 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=dream_job data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=0
[2024-06-20 19:12:13,841][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 19:12:13,843][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-20 19:12:13,844][train.py][INFO] - Instantiating callbacks...
[2024-06-20 19:12:13,844][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 19:12:13,846][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 19:12:13,847][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 19:12:13,847][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 19:12:13,854][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 19:12:13,854][train.py][INFO] - Instantiating loggers...
[2024-06-20 19:12:13,854][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 19:12:13,855][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 19:12:13,980][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/dream_job/0.2/0/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.17it/s v_num: 0.000      
                                                              val/auc: 0.383    
                                                              val/f1: 0.143     
                                                              val/precision:    
                                                              0.250 val/recall: 
                                                              0.100 val/mre:    
                                                              0.150 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.145             
[2024-06-20 19:14:08,732][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/dream_job/0.2/0>
[2024-06-20 19:14:08,732][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 19:14:08,734][HYDRA] 	#41 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=dream_job data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=1
[rank: 0] Seed set to 1
[2024-06-20 19:14:08,912][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 19:14:08,914][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-20 19:14:08,915][train.py][INFO] - Instantiating callbacks...
[2024-06-20 19:14:08,916][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 19:14:08,918][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 19:14:08,919][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 19:14:08,919][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 19:14:08,919][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 19:14:08,920][train.py][INFO] - Instantiating loggers...
[2024-06-20 19:14:08,920][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 19:14:08,921][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 19:14:08,952][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/dream_job/0.2/1/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.16it/s v_num: 0.000      
                                                              val/auc: 0.422    
                                                              val/f1: 0.421     
                                                              val/precision:    
                                                              0.444 val/recall: 
                                                              0.400 val/mre:    
                                                              0.150 train/auc:  
                                                              0.991 train/f1:   
                                                              0.991             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              0.982 train/mre:  
                                                              0.146             
[2024-06-20 19:16:02,460][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/dream_job/0.2/1>
[2024-06-20 19:16:02,461][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 19:16:02,464][HYDRA] 	#42 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=dream_job data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=2
[rank: 0] Seed set to 2
[2024-06-20 19:16:02,640][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 19:16:02,641][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-20 19:16:02,643][train.py][INFO] - Instantiating callbacks...
[2024-06-20 19:16:02,643][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 19:16:02,645][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 19:16:02,646][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 19:16:02,646][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 19:16:02,647][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 19:16:02,647][train.py][INFO] - Instantiating loggers...
[2024-06-20 19:16:02,647][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 19:16:02,649][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 19:16:02,679][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/dream_job/0.2/2/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.13it/s v_num: 0.000      
                                                              val/auc: 0.522    
                                                              val/f1: 0.571     
                                                              val/precision:    
                                                              0.545 val/recall: 
                                                              0.600 val/mre:    
                                                              0.148 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.144             
[2024-06-20 19:17:57,880][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/dream_job/0.2/2>
[2024-06-20 19:17:57,880][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 19:17:57,884][HYDRA] 	#43 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=dream_job data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=3
[rank: 0] Seed set to 3
[2024-06-20 19:17:58,063][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 19:17:58,064][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-20 19:17:58,066][train.py][INFO] - Instantiating callbacks...
[2024-06-20 19:17:58,066][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 19:17:58,068][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 19:17:58,069][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 19:17:58,069][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 19:17:58,070][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 19:17:58,070][train.py][INFO] - Instantiating loggers...
[2024-06-20 19:17:58,070][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 19:17:58,072][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 19:17:58,103][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/dream_job/0.2/3/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.17it/s v_num: 0.000      
                                                              val/auc: 0.489    
                                                              val/f1: 0.286     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.200 val/mre:    
                                                              0.150 train/auc:  
                                                              0.991 train/f1:   
                                                              0.991             
                                                              train/precision:  
                                                              0.982             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.146             
[2024-06-20 19:19:52,447][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/dream_job/0.2/3>
[2024-06-20 19:19:52,448][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 19:19:52,450][HYDRA] 	#44 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=dream_job data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=4
[rank: 0] Seed set to 4
[2024-06-20 19:19:52,646][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 19:19:52,648][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-20 19:19:52,649][train.py][INFO] - Instantiating callbacks...
[2024-06-20 19:19:52,649][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 19:19:52,651][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 19:19:52,652][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 19:19:52,652][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 19:19:52,654][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 19:19:52,654][train.py][INFO] - Instantiating loggers...
[2024-06-20 19:19:52,654][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 19:19:52,655][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 19:19:52,690][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/dream_job/0.2/4/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.15it/s v_num: 0.000      
                                                              val/auc: 0.428    
                                                              val/f1: 0.353     
                                                              val/precision:    
                                                              0.429 val/recall: 
                                                              0.300 val/mre:    
                                                              0.147 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.141             
[2024-06-20 19:21:48,185][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/dream_job/0.2/4>
[2024-06-20 19:21:48,185][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 19:21:48,188][HYDRA] 	#45 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=dream_job data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=5
[rank: 0] Seed set to 5
[2024-06-20 19:21:48,369][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 19:21:48,371][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-20 19:21:48,372][train.py][INFO] - Instantiating callbacks...
[2024-06-20 19:21:48,373][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 19:21:48,375][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 19:21:48,375][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 19:21:48,376][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 19:21:48,376][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 19:21:48,377][train.py][INFO] - Instantiating loggers...
[2024-06-20 19:21:48,377][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 19:21:48,378][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 19:21:48,422][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/dream_job/0.2/5/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.16it/s v_num: 0.000      
                                                              val/auc: 0.533    
                                                              val/f1: 0.471     
                                                              val/precision:    
                                                              0.571 val/recall: 
                                                              0.400 val/mre:    
                                                              0.150 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.145             
[2024-06-20 19:23:43,128][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/dream_job/0.2/5>
[2024-06-20 19:23:43,129][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 19:23:43,131][HYDRA] 	#46 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=dream_job data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=6
[rank: 0] Seed set to 6
[2024-06-20 19:23:43,309][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 19:23:43,311][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-20 19:23:43,312][train.py][INFO] - Instantiating callbacks...
[2024-06-20 19:23:43,312][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 19:23:43,315][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 19:23:43,315][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 19:23:43,316][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 19:23:43,316][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 19:23:43,317][train.py][INFO] - Instantiating loggers...
[2024-06-20 19:23:43,317][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 19:23:43,318][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 19:23:43,347][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/dream_job/0.2/6/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.14it/s v_num: 0.000      
                                                              val/auc: 0.272    
                                                              val/f1: 0.125     
                                                              val/precision:    
                                                              0.167 val/recall: 
                                                              0.100 val/mre:    
                                                              0.143 train/auc:  
                                                              0.991 train/f1:   
                                                              0.991             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              0.982 train/mre:  
                                                              0.138             
[2024-06-20 19:25:39,133][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/dream_job/0.2/6>
[2024-06-20 19:25:39,134][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 19:25:39,137][HYDRA] 	#47 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=dream_job data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=7
[rank: 0] Seed set to 7
[2024-06-20 19:25:39,323][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 19:25:39,325][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-20 19:25:39,326][train.py][INFO] - Instantiating callbacks...
[2024-06-20 19:25:39,326][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 19:25:39,329][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 19:25:39,329][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 19:25:39,330][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 19:25:39,330][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 19:25:39,331][train.py][INFO] - Instantiating loggers...
[2024-06-20 19:25:39,331][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 19:25:39,332][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 19:25:39,362][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/dream_job/0.2/7/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.19it/s v_num: 0.000      
                                                              val/auc: 0.517    
                                                              val/f1: 0.609     
                                                              val/precision:    
                                                              0.538 val/recall: 
                                                              0.700 val/mre:    
                                                              0.154 train/auc:  
                                                              0.982 train/f1:   
                                                              0.982             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              0.964 train/mre:  
                                                              0.149             
[2024-06-20 19:27:34,063][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/dream_job/0.2/7>
[2024-06-20 19:27:34,063][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 19:27:34,066][HYDRA] 	#48 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=dream_job data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=8
[rank: 0] Seed set to 8
[2024-06-20 19:27:34,463][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 19:27:34,464][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-20 19:27:34,466][train.py][INFO] - Instantiating callbacks...
[2024-06-20 19:27:34,466][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 19:27:34,468][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 19:27:34,468][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 19:27:34,469][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 19:27:34,469][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 19:27:34,470][train.py][INFO] - Instantiating loggers...
[2024-06-20 19:27:34,470][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 19:27:34,471][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 19:27:34,500][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/dream_job/0.2/8/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.27it/s v_num: 0.000      
                                                              val/auc: 0.372    
                                                              val/f1: 0.333     
                                                              val/precision:    
                                                              0.375 val/recall: 
                                                              0.300 val/mre:    
                                                              0.146 train/auc:  
                                                              0.973 train/f1:   
                                                              0.973             
                                                              train/precision:  
                                                              0.965             
                                                              train/recall:     
                                                              0.982 train/mre:  
                                                              0.140             
[2024-06-20 19:29:26,871][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/dream_job/0.2/8>
[2024-06-20 19:29:26,871][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 19:29:26,874][HYDRA] 	#49 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=dream_job data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=9
[rank: 0] Seed set to 9
[2024-06-20 19:29:27,044][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 19:29:27,045][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-20 19:29:27,046][train.py][INFO] - Instantiating callbacks...
[2024-06-20 19:29:27,047][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 19:29:27,048][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 19:29:27,049][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 19:29:27,049][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 19:29:27,050][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 19:29:27,051][train.py][INFO] - Instantiating loggers...
[2024-06-20 19:29:27,051][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 19:29:27,052][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 19:29:27,080][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/dream_job/0.2/9/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.18it/s v_num: 0.000      
                                                              val/auc: 0.489    
                                                              val/f1: 0.286     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.200 val/mre:    
                                                              0.149 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.144             
[2024-06-20 19:31:20,813][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/dream_job/0.2/9>
[2024-06-20 19:31:20,814][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 19:31:20,816][HYDRA] 	#50 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=dream_job data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=0
[2024-06-20 19:31:20,994][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 19:31:20,996][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-20 19:31:20,998][train.py][INFO] - Instantiating callbacks...
[2024-06-20 19:31:20,998][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 19:31:21,000][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 19:31:21,001][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 19:31:21,003][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 19:31:21,003][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 19:31:21,004][train.py][INFO] - Instantiating loggers...
[2024-06-20 19:31:21,004][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 19:31:21,005][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 19:31:21,034][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/dream_job/0.25/0/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.19it/s v_num: 0.000      
                                                              val/auc: 0.589    
                                                              val/f1: 0.500     
                                                              val/precision:    
                                                              0.667 val/recall: 
                                                              0.400 val/mre:    
                                                              0.155 train/auc:  
                                                              0.955 train/f1:   
                                                              0.957             
                                                              train/precision:  
                                                              0.918             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.148             
[2024-06-20 19:33:14,241][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/dream_job/0.25/0>
[2024-06-20 19:33:14,242][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 19:33:14,244][HYDRA] 	#51 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=dream_job data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=1
[rank: 0] Seed set to 1
[2024-06-20 19:33:14,424][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 19:33:14,425][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-20 19:33:14,427][train.py][INFO] - Instantiating callbacks...
[2024-06-20 19:33:14,427][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 19:33:14,429][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 19:33:14,430][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 19:33:14,430][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 19:33:14,431][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 19:33:14,431][train.py][INFO] - Instantiating loggers...
[2024-06-20 19:33:14,432][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 19:33:14,433][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 19:33:14,461][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/dream_job/0.25/1/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.16it/s v_num: 0.000      
                                                              val/auc: 0.594    
                                                              val/f1: 0.429     
                                                              val/precision:    
                                                              0.750 val/recall: 
                                                              0.300 val/mre:    
                                                              0.155 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.151             
[2024-06-20 19:35:09,487][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/dream_job/0.25/1>
[2024-06-20 19:35:09,487][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 19:35:09,495][HYDRA] 	#52 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=dream_job data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=2
[rank: 0] Seed set to 2
[2024-06-20 19:35:09,701][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 19:35:09,703][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-20 19:35:09,704][train.py][INFO] - Instantiating callbacks...
[2024-06-20 19:35:09,704][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 19:35:09,707][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 19:35:09,707][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 19:35:09,708][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 19:35:09,709][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 19:35:09,710][train.py][INFO] - Instantiating loggers...
[2024-06-20 19:35:09,710][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 19:35:09,711][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 19:35:09,742][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/dream_job/0.25/2/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.17it/s v_num: 0.000      
                                                              val/auc: 0.528    
                                                              val/f1: 0.526     
                                                              val/precision:    
                                                              0.556 val/recall: 
                                                              0.500 val/mre:    
                                                              0.148 train/auc:  
                                                              0.991 train/f1:   
                                                              0.991             
                                                              train/precision:  
                                                              0.982             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.145             
[2024-06-20 19:37:03,850][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/dream_job/0.25/2>
[2024-06-20 19:37:03,850][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 19:37:03,853][HYDRA] 	#53 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=dream_job data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=3
[rank: 0] Seed set to 3
[2024-06-20 19:37:04,030][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 19:37:04,031][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-20 19:37:04,033][train.py][INFO] - Instantiating callbacks...
[2024-06-20 19:37:04,033][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 19:37:04,035][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 19:37:04,036][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 19:37:04,036][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 19:37:04,037][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 19:37:04,037][train.py][INFO] - Instantiating loggers...
[2024-06-20 19:37:04,037][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 19:37:04,039][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 19:37:04,069][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/dream_job/0.25/3/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.14it/s v_num: 0.000      
                                                              val/auc: 0.639    
                                                              val/f1: 0.588     
                                                              val/precision:    
                                                              0.714 val/recall: 
                                                              0.500 val/mre:    
                                                              0.153 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.149             
[2024-06-20 19:38:58,758][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/dream_job/0.25/3>
[2024-06-20 19:38:58,759][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 19:38:58,761][HYDRA] 	#54 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=dream_job data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=4
[rank: 0] Seed set to 4
[2024-06-20 19:38:58,942][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 19:38:58,944][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-20 19:38:58,945][train.py][INFO] - Instantiating callbacks...
[2024-06-20 19:38:58,946][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 19:38:58,948][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 19:38:58,948][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 19:38:58,949][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 19:38:58,949][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 19:38:58,950][train.py][INFO] - Instantiating loggers...
[2024-06-20 19:38:58,950][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 19:38:58,951][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 19:38:58,986][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/dream_job/0.25/4/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.09it/s v_num: 0.000      
                                                              val/auc: 0.494    
                                                              val/f1: 0.167     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.100 val/mre:    
                                                              0.148 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.143             
[2024-06-20 19:40:55,046][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/dream_job/0.25/4>
[2024-06-20 19:40:55,047][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 19:40:55,049][HYDRA] 	#55 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=dream_job data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=5
[rank: 0] Seed set to 5
[2024-06-20 19:40:55,227][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 19:40:55,228][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-20 19:40:55,229][train.py][INFO] - Instantiating callbacks...
[2024-06-20 19:40:55,230][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 19:40:55,232][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 19:40:55,232][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 19:40:55,233][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 19:40:55,233][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 19:40:55,234][train.py][INFO] - Instantiating loggers...
[2024-06-20 19:40:55,234][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 19:40:55,235][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 19:40:55,269][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/dream_job/0.25/5/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.14it/s v_num: 0.000      
                                                              val/auc: 0.372    
                                                              val/f1: 0.333     
                                                              val/precision:    
                                                              0.375 val/recall: 
                                                              0.300 val/mre:    
                                                              0.153 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.149             
[2024-06-20 19:42:51,695][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/dream_job/0.25/5>
[2024-06-20 19:42:51,695][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 19:42:51,699][HYDRA] 	#56 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=dream_job data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=6
[rank: 0] Seed set to 6
[2024-06-20 19:42:51,902][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 19:42:51,904][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-20 19:42:51,905][train.py][INFO] - Instantiating callbacks...
[2024-06-20 19:42:51,906][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 19:42:51,908][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 19:42:51,908][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 19:42:51,909][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 19:42:51,912][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 19:42:51,912][train.py][INFO] - Instantiating loggers...
[2024-06-20 19:42:51,912][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 19:42:51,914][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 19:42:51,985][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/dream_job/0.25/6/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.15it/s v_num: 0.000      
                                                              val/auc: 0.317    
                                                              val/f1: 0.316     
                                                              val/precision:    
                                                              0.333 val/recall: 
                                                              0.300 val/mre:    
                                                              0.146 train/auc:  
                                                              0.991 train/f1:   
                                                              0.991             
                                                              train/precision:  
                                                              0.982             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.142             
[2024-06-20 19:44:47,236][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/dream_job/0.25/6>
[2024-06-20 19:44:47,237][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 19:44:47,240][HYDRA] 	#57 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=dream_job data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=7
[rank: 0] Seed set to 7
[2024-06-20 19:44:47,452][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 19:44:47,453][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-20 19:44:47,455][train.py][INFO] - Instantiating callbacks...
[2024-06-20 19:44:47,455][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 19:44:47,457][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 19:44:47,458][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 19:44:47,458][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 19:44:47,462][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 19:44:47,462][train.py][INFO] - Instantiating loggers...
[2024-06-20 19:44:47,462][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 19:44:47,464][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 19:44:47,511][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/dream_job/0.25/7/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.23it/s v_num: 0.000      
                                                              val/auc: 0.528    
                                                              val/f1: 0.526     
                                                              val/precision:    
                                                              0.556 val/recall: 
                                                              0.500 val/mre:    
                                                              0.155 train/auc:  
                                                              0.973 train/f1:   
                                                              0.973             
                                                              train/precision:  
                                                              0.982             
                                                              train/recall:     
                                                              0.964 train/mre:  
                                                              0.150             
[2024-06-20 19:46:43,292][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/dream_job/0.25/7>
[2024-06-20 19:46:43,293][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 19:46:43,295][HYDRA] 	#58 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=dream_job data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=8
[rank: 0] Seed set to 8
[2024-06-20 19:46:43,477][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 19:46:43,478][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-20 19:46:43,480][train.py][INFO] - Instantiating callbacks...
[2024-06-20 19:46:43,480][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 19:46:43,482][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 19:46:43,483][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 19:46:43,483][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 19:46:43,484][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 19:46:43,484][train.py][INFO] - Instantiating loggers...
[2024-06-20 19:46:43,484][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 19:46:43,485][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 19:46:43,521][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/dream_job/0.25/8/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.15it/s v_num: 0.000      
                                                              val/auc: 0.328    
                                                              val/f1: 0.133     
                                                              val/precision:    
                                                              0.200 val/recall: 
                                                              0.100 val/mre:    
                                                              0.145 train/auc:  
                                                              0.982 train/f1:   
                                                              0.982             
                                                              train/precision:  
                                                              0.982             
                                                              train/recall:     
                                                              0.982 train/mre:  
                                                              0.140             
[2024-06-20 19:48:38,388][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/dream_job/0.25/8>
[2024-06-20 19:48:38,388][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 19:48:38,391][HYDRA] 	#59 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=dream_job data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=9
[rank: 0] Seed set to 9
[2024-06-20 19:48:38,571][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 19:48:38,573][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-20 19:48:38,574][train.py][INFO] - Instantiating callbacks...
[2024-06-20 19:48:38,575][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 19:48:38,577][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 19:48:38,577][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 19:48:38,578][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 19:48:38,578][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 19:48:38,579][train.py][INFO] - Instantiating loggers...
[2024-06-20 19:48:38,579][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 19:48:38,580][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 19:48:38,682][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/dream_job/0.25/9/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.13it/s v_num: 0.000      
                                                              val/auc: 0.483    
                                                              val/f1: 0.375     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.300 val/mre:    
                                                              0.155 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.150             
[2024-06-20 19:50:35,179][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/dream_job/0.25/9>
[2024-06-20 19:50:35,180][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 19:50:35,182][HYDRA] 	#60 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=dream_job data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=0
[2024-06-20 19:50:35,363][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 19:50:35,365][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-20 19:50:35,366][train.py][INFO] - Instantiating callbacks...
[2024-06-20 19:50:35,366][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 19:50:35,368][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 19:50:35,369][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 19:50:35,369][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 19:50:35,371][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 19:50:35,371][train.py][INFO] - Instantiating loggers...
[2024-06-20 19:50:35,371][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 19:50:35,372][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 19:50:35,405][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/dream_job/0.3/0/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.14it/s v_num: 0.000      
                                                              val/auc: 0.322    
                                                              val/f1: 0.235     
                                                              val/precision:    
                                                              0.286 val/recall: 
                                                              0.200 val/mre:    
                                                              0.156 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.150             
[2024-06-20 19:52:30,888][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/dream_job/0.3/0>
[2024-06-20 19:52:30,888][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 19:52:30,891][HYDRA] 	#61 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=dream_job data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=1
[rank: 0] Seed set to 1
[2024-06-20 19:52:31,081][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 19:52:31,083][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-20 19:52:31,084][train.py][INFO] - Instantiating callbacks...
[2024-06-20 19:52:31,085][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 19:52:31,087][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 19:52:31,088][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 19:52:31,088][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 19:52:31,089][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 19:52:31,089][train.py][INFO] - Instantiating loggers...
[2024-06-20 19:52:31,089][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 19:52:31,091][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 19:52:31,138][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/dream_job/0.3/1/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.22it/s v_num: 0.000      
                                                              val/auc: 0.589    
                                                              val/f1: 0.500     
                                                              val/precision:    
                                                              0.667 val/recall: 
                                                              0.400 val/mre:    
                                                              0.156 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.152             
[2024-06-20 19:54:24,878][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/dream_job/0.3/1>
[2024-06-20 19:54:24,878][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 19:54:24,881][HYDRA] 	#62 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=dream_job data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=2
[rank: 0] Seed set to 2
[2024-06-20 19:54:25,087][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 19:54:25,089][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-20 19:54:25,090][train.py][INFO] - Instantiating callbacks...
[2024-06-20 19:54:25,090][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 19:54:25,093][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 19:54:25,093][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 19:54:25,093][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 19:54:25,094][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 19:54:25,094][train.py][INFO] - Instantiating loggers...
[2024-06-20 19:54:25,095][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 19:54:25,096][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 19:54:25,125][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/dream_job/0.3/2/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.23it/s v_num: 0.000      
                                                              val/auc: 0.567    
                                                              val/f1: 0.667     
                                                              val/precision:    
                                                              0.571 val/recall: 
                                                              0.800 val/mre:    
                                                              0.154 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.150             
[2024-06-20 19:56:16,753][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/dream_job/0.3/2>
[2024-06-20 19:56:16,753][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 19:56:16,756][HYDRA] 	#63 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=dream_job data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=3
[rank: 0] Seed set to 3
[2024-06-20 19:56:16,933][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 19:56:16,935][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-20 19:56:16,936][train.py][INFO] - Instantiating callbacks...
[2024-06-20 19:56:16,937][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 19:56:16,939][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 19:56:16,939][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 19:56:16,940][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 19:56:16,940][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 19:56:16,941][train.py][INFO] - Instantiating loggers...
[2024-06-20 19:56:16,941][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 19:56:16,942][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 19:56:16,973][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/dream_job/0.3/3/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.22it/s v_num: 0.000      
                                                              val/auc: 0.539    
                                                              val/f1: 0.400     
                                                              val/precision:    
                                                              0.600 val/recall: 
                                                              0.300 val/mre:    
                                                              0.156 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.153             
[2024-06-20 19:58:09,323][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/dream_job/0.3/3>
[2024-06-20 19:58:09,324][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 19:58:09,326][HYDRA] 	#64 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=dream_job data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=4
[rank: 0] Seed set to 4
[2024-06-20 19:58:09,501][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 19:58:09,502][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-20 19:58:09,504][train.py][INFO] - Instantiating callbacks...
[2024-06-20 19:58:09,504][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 19:58:09,506][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 19:58:09,507][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 19:58:09,507][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 19:58:09,508][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 19:58:09,508][train.py][INFO] - Instantiating loggers...
[2024-06-20 19:58:09,508][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 19:58:09,510][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 19:58:09,539][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/dream_job/0.3/4/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.22it/s v_num: 0.000      
                                                              val/auc: 0.539    
                                                              val/f1: 0.400     
                                                              val/precision:    
                                                              0.600 val/recall: 
                                                              0.300 val/mre:    
                                                              0.155 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.150             
[2024-06-20 20:00:00,421][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/dream_job/0.3/4>
[2024-06-20 20:00:00,422][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 20:00:00,424][HYDRA] 	#65 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=dream_job data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=5
[rank: 0] Seed set to 5
[2024-06-20 20:00:00,599][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 20:00:00,601][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-20 20:00:00,602][train.py][INFO] - Instantiating callbacks...
[2024-06-20 20:00:00,602][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 20:00:00,605][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 20:00:00,605][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 20:00:00,606][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 20:00:00,606][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 20:00:00,607][train.py][INFO] - Instantiating loggers...
[2024-06-20 20:00:00,607][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 20:00:00,608][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 20:00:00,637][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/dream_job/0.3/5/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.22it/s v_num: 0.000      
                                                              val/auc: 0.433    
                                                              val/f1: 0.267     
                                                              val/precision:    
                                                              0.400 val/recall: 
                                                              0.200 val/mre:    
                                                              0.157 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.152             
[2024-06-20 20:01:52,502][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/dream_job/0.3/5>
[2024-06-20 20:01:52,503][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 20:01:52,505][HYDRA] 	#66 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=dream_job data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=6
[rank: 0] Seed set to 6
[2024-06-20 20:01:52,681][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 20:01:52,683][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-20 20:01:52,684][train.py][INFO] - Instantiating callbacks...
[2024-06-20 20:01:52,685][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 20:01:52,687][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 20:01:52,687][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 20:01:52,688][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 20:01:52,688][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 20:01:52,689][train.py][INFO] - Instantiating loggers...
[2024-06-20 20:01:52,689][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 20:01:52,690][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 20:01:52,719][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/dream_job/0.3/6/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.23it/s v_num: 0.000      
                                                              val/auc: 0.217    
                                                              val/f1: 0.118     
                                                              val/precision:    
                                                              0.143 val/recall: 
                                                              0.100 val/mre:    
                                                              0.149 train/auc:  
                                                              0.991 train/f1:   
                                                              0.991             
                                                              train/precision:  
                                                              0.982             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.144             
[2024-06-20 20:03:43,750][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/dream_job/0.3/6>
[2024-06-20 20:03:43,751][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 20:03:43,753][HYDRA] 	#67 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=dream_job data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=7
[rank: 0] Seed set to 7
[2024-06-20 20:03:43,930][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 20:03:43,932][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-20 20:03:43,933][train.py][INFO] - Instantiating callbacks...
[2024-06-20 20:03:43,933][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 20:03:43,936][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 20:03:43,936][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 20:03:43,937][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 20:03:43,937][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 20:03:43,938][train.py][INFO] - Instantiating loggers...
[2024-06-20 20:03:43,938][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 20:03:43,939][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 20:03:43,967][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/dream_job/0.3/7/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.22it/s v_num: 0.000      
                                                              val/auc: 0.478    
                                                              val/f1: 0.444     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.400 val/mre:    
                                                              0.157 train/auc:  
                                                              0.973 train/f1:   
                                                              0.973             
                                                              train/precision:  
                                                              0.982             
                                                              train/recall:     
                                                              0.964 train/mre:  
                                                              0.151             
[2024-06-20 20:05:36,582][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/dream_job/0.3/7>
[2024-06-20 20:05:36,583][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 20:05:36,586][HYDRA] 	#68 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=dream_job data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=8
[rank: 0] Seed set to 8
[2024-06-20 20:05:36,775][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 20:05:36,777][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-20 20:05:36,778][train.py][INFO] - Instantiating callbacks...
[2024-06-20 20:05:36,778][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 20:05:36,781][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 20:05:36,781][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 20:05:36,782][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 20:05:36,783][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 20:05:36,783][train.py][INFO] - Instantiating loggers...
[2024-06-20 20:05:36,783][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 20:05:36,785][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 20:05:36,819][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/dream_job/0.3/8/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.24it/s v_num: 0.000      
                                                              val/auc: 0.483    
                                                              val/f1: 0.375     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.300 val/mre:    
                                                              0.150 train/auc:  
                                                              0.982 train/f1:   
                                                              0.982             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              0.964 train/mre:  
                                                              0.144             
[2024-06-20 20:07:27,639][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/dream_job/0.3/8>
[2024-06-20 20:07:27,640][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 20:07:27,642][HYDRA] 	#69 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=dream_job data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=9
[rank: 0] Seed set to 9
[2024-06-20 20:07:27,822][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 20:07:27,824][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-20 20:07:27,825][train.py][INFO] - Instantiating callbacks...
[2024-06-20 20:07:27,825][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 20:07:27,828][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 20:07:27,828][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 20:07:27,829][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 20:07:27,829][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 20:07:27,830][train.py][INFO] - Instantiating loggers...
[2024-06-20 20:07:27,830][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 20:07:27,831][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 20:07:27,860][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/dream_job/0.3/9/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.28it/s v_num: 0.000      
                                                              val/auc: 0.483    
                                                              val/f1: 0.375     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.300 val/mre:    
                                                              0.152 train/auc:  
                                                              0.991 train/f1:   
                                                              0.991             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              0.982 train/mre:  
                                                              0.146             
[2024-06-20 20:09:19,537][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/dream_job/0.3/9>
[2024-06-20 20:09:19,538][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 20:09:19,540][HYDRA] 	#70 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=easy_sleep data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=0
[2024-06-20 20:09:19,716][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 20:09:19,718][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-20 20:09:19,719][train.py][INFO] - Instantiating callbacks...
[2024-06-20 20:09:19,719][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 20:09:19,721][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 20:09:19,722][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 20:09:19,723][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 20:09:19,723][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 20:09:19,724][train.py][INFO] - Instantiating loggers...
[2024-06-20 20:09:19,724][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 20:09:19,725][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 20:09:19,753][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/easy_sleep/0.0/0/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.10it/s v_num: 0.000      
                                                              val/auc: 0.356    
                                                              val/f1: 0.143     
                                                              val/precision:    
                                                              0.200 val/recall: 
                                                              0.111 val/mre: nan
                                                              train/auc: 0.992  
                                                              train/f1: 0.992   
                                                              train/precision:  
                                                              0.984             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              nan               
[2024-06-20 20:11:15,540][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/easy_sleep/0.0/0>
[2024-06-20 20:11:15,540][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 20:11:15,545][HYDRA] 	#71 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=easy_sleep data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=1
[rank: 0] Seed set to 1
[2024-06-20 20:11:15,731][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 20:11:15,732][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-20 20:11:15,733][train.py][INFO] - Instantiating callbacks...
[2024-06-20 20:11:15,734][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 20:11:15,736][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 20:11:15,736][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 20:11:15,737][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 20:11:15,737][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 20:11:15,738][train.py][INFO] - Instantiating loggers...
[2024-06-20 20:11:15,738][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 20:11:15,739][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 20:11:15,768][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/easy_sleep/0.0/1/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.10it/s v_num: 0.000      
                                                              val/auc: 0.417    
                                                              val/f1: 0.353     
                                                              val/precision:    
                                                              0.375 val/recall: 
                                                              0.333 val/mre: nan
                                                              train/auc: 0.933  
                                                              train/f1: 0.930   
                                                              train/precision:  
                                                              0.981             
                                                              train/recall:     
                                                              0.883 train/mre:  
                                                              nan               
[2024-06-20 20:13:11,637][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/easy_sleep/0.0/1>
[2024-06-20 20:13:11,637][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 20:13:11,640][HYDRA] 	#72 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=easy_sleep data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=2
[rank: 0] Seed set to 2
[2024-06-20 20:13:11,820][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 20:13:11,821][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-20 20:13:11,823][train.py][INFO] - Instantiating callbacks...
[2024-06-20 20:13:11,823][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 20:13:11,826][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 20:13:11,827][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 20:13:11,827][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 20:13:11,828][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 20:13:11,828][train.py][INFO] - Instantiating loggers...
[2024-06-20 20:13:11,828][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 20:13:11,830][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 20:13:11,858][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/easy_sleep/0.0/2/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.11it/s v_num: 0.000      
                                                              val/auc: 0.411    
                                                              val/f1: 0.267     
                                                              val/precision:    
                                                              0.333 val/recall: 
                                                              0.222 val/mre: nan
                                                              train/auc: 0.942  
                                                              train/f1: 0.938   
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              0.883 train/mre:  
                                                              nan               
[2024-06-20 20:15:07,398][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/easy_sleep/0.0/2>
[2024-06-20 20:15:07,399][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 20:15:07,401][HYDRA] 	#73 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=easy_sleep data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=3
[rank: 0] Seed set to 3
[2024-06-20 20:15:07,577][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 20:15:07,579][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-20 20:15:07,580][train.py][INFO] - Instantiating callbacks...
[2024-06-20 20:15:07,580][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 20:15:07,582][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 20:15:07,583][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 20:15:07,583][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 20:15:07,584][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 20:15:07,584][train.py][INFO] - Instantiating loggers...
[2024-06-20 20:15:07,585][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 20:15:07,586][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 20:15:07,615][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/easy_sleep/0.0/3/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.11it/s v_num: 0.000      
                                                              val/auc: 0.350    
                                                              val/f1: 0.000     
                                                              val/precision:    
                                                              0.000 val/recall: 
                                                              0.000 val/mre: nan
                                                              train/auc: 0.917  
                                                              train/f1: 0.911   
                                                              train/precision:  
                                                              0.981             
                                                              train/recall:     
                                                              0.850 train/mre:  
                                                              nan               
[2024-06-20 20:17:03,762][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/easy_sleep/0.0/3>
[2024-06-20 20:17:03,762][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 20:17:03,766][HYDRA] 	#74 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=easy_sleep data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=4
[rank: 0] Seed set to 4
[2024-06-20 20:17:03,967][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 20:17:03,968][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-20 20:17:03,970][train.py][INFO] - Instantiating callbacks...
[2024-06-20 20:17:03,970][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 20:17:03,972][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 20:17:03,973][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 20:17:03,973][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 20:17:03,974][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 20:17:03,974][train.py][INFO] - Instantiating loggers...
[2024-06-20 20:17:03,975][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 20:17:03,976][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 20:17:04,015][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/easy_sleep/0.0/4/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.11it/s v_num: 0.000      
                                                              val/auc: 0.356    
                                                              val/f1: 0.143     
                                                              val/precision:    
                                                              0.200 val/recall: 
                                                              0.111 val/mre: nan
                                                              train/auc: 0.925  
                                                              train/f1: 0.919   
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              0.850 train/mre:  
                                                              nan               
[2024-06-20 20:18:59,194][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/easy_sleep/0.0/4>
[2024-06-20 20:18:59,194][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 20:18:59,197][HYDRA] 	#75 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=easy_sleep data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=5
[rank: 0] Seed set to 5
[2024-06-20 20:18:59,570][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 20:18:59,571][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-20 20:18:59,572][train.py][INFO] - Instantiating callbacks...
[2024-06-20 20:18:59,573][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 20:18:59,575][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 20:18:59,575][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 20:18:59,576][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 20:18:59,576][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 20:18:59,577][train.py][INFO] - Instantiating loggers...
[2024-06-20 20:18:59,577][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 20:18:59,578][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 20:18:59,614][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/easy_sleep/0.0/5/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.07it/s v_num: 0.000      
                                                              val/auc: 0.411    
                                                              val/f1: 0.267     
                                                              val/precision:    
                                                              0.333 val/recall: 
                                                              0.222 val/mre: nan
                                                              train/auc: 0.917  
                                                              train/f1: 0.912   
                                                              train/precision:  
                                                              0.963             
                                                              train/recall:     
                                                              0.867 train/mre:  
                                                              nan               
[2024-06-20 20:20:54,927][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/easy_sleep/0.0/5>
[2024-06-20 20:20:54,928][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 20:20:54,930][HYDRA] 	#76 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=easy_sleep data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=6
[rank: 0] Seed set to 6
[2024-06-20 20:20:55,113][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 20:20:55,114][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-20 20:20:55,116][train.py][INFO] - Instantiating callbacks...
[2024-06-20 20:20:55,116][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 20:20:55,118][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 20:20:55,119][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 20:20:55,119][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 20:20:55,122][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 20:20:55,122][train.py][INFO] - Instantiating loggers...
[2024-06-20 20:20:55,122][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 20:20:55,123][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 20:20:55,188][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/easy_sleep/0.0/6/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.17it/s v_num: 0.000      
                                                              val/auc: 0.406    
                                                              val/f1: 0.154     
                                                              val/precision:    
                                                              0.250 val/recall: 
                                                              0.111 val/mre: nan
                                                              train/auc: 0.950  
                                                              train/f1: 0.947   
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              0.900 train/mre:  
                                                              nan               
[2024-06-20 20:22:50,953][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/easy_sleep/0.0/6>
[2024-06-20 20:22:50,953][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 20:22:50,956][HYDRA] 	#77 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=easy_sleep data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=7
[rank: 0] Seed set to 7
[2024-06-20 20:22:51,130][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 20:22:51,131][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-20 20:22:51,132][train.py][INFO] - Instantiating callbacks...
[2024-06-20 20:22:51,133][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 20:22:51,135][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 20:22:51,135][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 20:22:51,136][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 20:22:51,136][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 20:22:51,137][train.py][INFO] - Instantiating loggers...
[2024-06-20 20:22:51,137][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 20:22:51,138][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 20:22:51,167][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/easy_sleep/0.0/7/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.11it/s v_num: 0.000      
                                                              val/auc: 0.411    
                                                              val/f1: 0.267     
                                                              val/precision:    
                                                              0.333 val/recall: 
                                                              0.222 val/mre: nan
                                                              train/auc: 0.967  
                                                              train/f1: 0.966   
                                                              train/precision:  
                                                              0.983             
                                                              train/recall:     
                                                              0.950 train/mre:  
                                                              nan               
[2024-06-20 20:24:47,405][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/easy_sleep/0.0/7>
[2024-06-20 20:24:47,405][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 20:24:47,407][HYDRA] 	#78 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=easy_sleep data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=8
[rank: 0] Seed set to 8
[2024-06-20 20:24:47,582][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 20:24:47,584][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-20 20:24:47,585][train.py][INFO] - Instantiating callbacks...
[2024-06-20 20:24:47,586][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 20:24:47,588][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 20:24:47,588][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 20:24:47,589][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 20:24:47,589][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 20:24:47,590][train.py][INFO] - Instantiating loggers...
[2024-06-20 20:24:47,590][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 20:24:47,591][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 20:24:47,619][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/easy_sleep/0.0/8/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.15it/s v_num: 0.000      
                                                              val/auc: 0.400    
                                                              val/f1: 0.000     
                                                              val/precision:    
                                                              0.000 val/recall: 
                                                              0.000 val/mre: nan
                                                              train/auc: 0.967  
                                                              train/f1: 0.966   
                                                              train/precision:  
                                                              0.983             
                                                              train/recall:     
                                                              0.950 train/mre:  
                                                              nan               
[2024-06-20 20:26:43,924][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/easy_sleep/0.0/8>
[2024-06-20 20:26:43,924][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 20:26:43,929][HYDRA] 	#79 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=easy_sleep data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=9
[rank: 0] Seed set to 9
[2024-06-20 20:26:44,119][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 20:26:44,120][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-20 20:26:44,122][train.py][INFO] - Instantiating callbacks...
[2024-06-20 20:26:44,122][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 20:26:44,124][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 20:26:44,124][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 20:26:44,125][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 20:26:44,125][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 20:26:44,126][train.py][INFO] - Instantiating loggers...
[2024-06-20 20:26:44,126][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 20:26:44,127][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 20:26:44,156][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/easy_sleep/0.0/9/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.10it/s v_num: 0.000      
                                                              val/auc: 0.411    
                                                              val/f1: 0.267     
                                                              val/precision:    
                                                              0.333 val/recall: 
                                                              0.222 val/mre: nan
                                                              train/auc: 0.942  
                                                              train/f1: 0.938   
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              0.883 train/mre:  
                                                              nan               
[2024-06-20 20:28:39,979][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/easy_sleep/0.0/9>
[2024-06-20 20:28:39,979][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 20:28:39,983][HYDRA] 	#80 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=easy_sleep data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=0
[2024-06-20 20:28:40,159][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 20:28:40,160][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-20 20:28:40,162][train.py][INFO] - Instantiating callbacks...
[2024-06-20 20:28:40,162][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 20:28:40,164][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 20:28:40,164][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 20:28:40,165][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 20:28:40,166][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 20:28:40,166][train.py][INFO] - Instantiating loggers...
[2024-06-20 20:28:40,166][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 20:28:40,167][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 20:28:40,198][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/easy_sleep/0.05/0/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.10it/s v_num: 0.000      
                                                              val/auc: 0.406    
                                                              val/f1: 0.154     
                                                              val/precision:    
                                                              0.250 val/recall: 
                                                              0.111 val/mre:    
                                                              0.114 train/auc:  
                                                              0.958 train/f1:   
                                                              0.957             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              0.917 train/mre:  
                                                              0.115             
[2024-06-20 20:30:35,790][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/easy_sleep/0.05/0>
[2024-06-20 20:30:35,790][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 20:30:35,793][HYDRA] 	#81 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=easy_sleep data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=1
[rank: 0] Seed set to 1
[2024-06-20 20:30:36,173][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 20:30:36,175][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-20 20:30:36,176][train.py][INFO] - Instantiating callbacks...
[2024-06-20 20:30:36,177][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 20:30:36,179][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 20:30:36,179][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 20:30:36,180][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 20:30:36,180][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 20:30:36,181][train.py][INFO] - Instantiating loggers...
[2024-06-20 20:30:36,181][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 20:30:36,182][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 20:30:36,211][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/easy_sleep/0.05/1/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.12it/s v_num: 0.000      
                                                              val/auc: 0.411    
                                                              val/f1: 0.267     
                                                              val/precision:    
                                                              0.333 val/recall: 
                                                              0.222 val/mre:    
                                                              0.114 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.117             
[2024-06-20 20:32:32,051][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/easy_sleep/0.05/1>
[2024-06-20 20:32:32,052][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 20:32:32,054][HYDRA] 	#82 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=easy_sleep data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=2
[rank: 0] Seed set to 2
[2024-06-20 20:32:32,230][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 20:32:32,231][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-20 20:32:32,233][train.py][INFO] - Instantiating callbacks...
[2024-06-20 20:32:32,233][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 20:32:32,235][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 20:32:32,236][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 20:32:32,236][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 20:32:32,237][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 20:32:32,237][train.py][INFO] - Instantiating loggers...
[2024-06-20 20:32:32,237][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 20:32:32,239][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 20:32:32,268][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/easy_sleep/0.05/2/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:02 • 0:00:00 1.84it/s v_num: 0.000      
                                                              val/auc: 0.522    
                                                              val/f1: 0.471     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.444 val/mre:    
                                                              0.115 train/auc:  
                                                              0.967 train/f1:   
                                                              0.966             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              0.933 train/mre:  
                                                              0.113             
[2024-06-20 20:34:28,329][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/easy_sleep/0.05/2>
[2024-06-20 20:34:28,329][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 20:34:28,331][HYDRA] 	#83 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=easy_sleep data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=3
[rank: 0] Seed set to 3
[2024-06-20 20:34:28,510][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 20:34:28,511][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-20 20:34:28,513][train.py][INFO] - Instantiating callbacks...
[2024-06-20 20:34:28,513][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 20:34:28,515][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 20:34:28,516][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 20:34:28,516][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 20:34:28,517][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 20:34:28,517][train.py][INFO] - Instantiating loggers...
[2024-06-20 20:34:28,517][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 20:34:28,518][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 20:34:28,549][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/easy_sleep/0.05/3/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.10it/s v_num: 0.000      
                                                              val/auc: 0.411    
                                                              val/f1: 0.267     
                                                              val/precision:    
                                                              0.333 val/recall: 
                                                              0.222 val/mre:    
                                                              0.112 train/auc:  
                                                              0.925 train/f1:   
                                                              0.919             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              0.850 train/mre:  
                                                              0.114             
[2024-06-20 20:36:24,235][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/easy_sleep/0.05/3>
[2024-06-20 20:36:24,236][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 20:36:24,238][HYDRA] 	#84 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=easy_sleep data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=4
[rank: 0] Seed set to 4
[2024-06-20 20:36:24,415][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 20:36:24,417][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-20 20:36:24,418][train.py][INFO] - Instantiating callbacks...
[2024-06-20 20:36:24,418][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 20:36:24,421][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 20:36:24,421][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 20:36:24,422][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 20:36:24,422][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 20:36:24,423][train.py][INFO] - Instantiating loggers...
[2024-06-20 20:36:24,423][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 20:36:24,424][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 20:36:24,452][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/easy_sleep/0.05/4/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.09it/s v_num: 0.000      
                                                              val/auc: 0.511    
                                                              val/f1: 0.308     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.222 val/mre:    
                                                              0.115 train/auc:  
                                                              0.933 train/f1:   
                                                              0.929             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              0.867 train/mre:  
                                                              0.114             
[2024-06-20 20:38:19,959][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/easy_sleep/0.05/4>
[2024-06-20 20:38:19,959][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 20:38:19,961][HYDRA] 	#85 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=easy_sleep data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=5
[rank: 0] Seed set to 5
[2024-06-20 20:38:20,135][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 20:38:20,137][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-20 20:38:20,138][train.py][INFO] - Instantiating callbacks...
[2024-06-20 20:38:20,138][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 20:38:20,140][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 20:38:20,141][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 20:38:20,141][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 20:38:20,142][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 20:38:20,142][train.py][INFO] - Instantiating loggers...
[2024-06-20 20:38:20,143][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 20:38:20,144][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 20:38:20,173][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/easy_sleep/0.05/5/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.16it/s v_num: 0.000      
                                                              val/auc: 0.517    
                                                              val/f1: 0.400     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.333 val/mre:    
                                                              0.117 train/auc:  
                                                              0.933 train/f1:   
                                                              0.929             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              0.867 train/mre:  
                                                              0.117             
[2024-06-20 20:40:15,264][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/easy_sleep/0.05/5>
[2024-06-20 20:40:15,265][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 20:40:15,268][HYDRA] 	#86 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=easy_sleep data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=6
[rank: 0] Seed set to 6
[2024-06-20 20:40:15,439][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 20:40:15,440][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-20 20:40:15,442][train.py][INFO] - Instantiating callbacks...
[2024-06-20 20:40:15,442][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 20:40:15,444][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 20:40:15,444][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 20:40:15,445][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 20:40:15,445][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 20:40:15,446][train.py][INFO] - Instantiating loggers...
[2024-06-20 20:40:15,446][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 20:40:15,447][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 20:40:15,475][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/easy_sleep/0.05/6/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.10it/s v_num: 0.000      
                                                              val/auc: 0.461    
                                                              val/f1: 0.286     
                                                              val/precision:    
                                                              0.400 val/recall: 
                                                              0.222 val/mre:    
                                                              0.116 train/auc:  
                                                              0.942 train/f1:   
                                                              0.938             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              0.883 train/mre:  
                                                              0.114             
[2024-06-20 20:42:11,571][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/easy_sleep/0.05/6>
[2024-06-20 20:42:11,571][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 20:42:11,573][HYDRA] 	#87 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=easy_sleep data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=7
[rank: 0] Seed set to 7
[2024-06-20 20:42:11,745][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 20:42:11,747][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-20 20:42:11,748][train.py][INFO] - Instantiating callbacks...
[2024-06-20 20:42:11,748][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 20:42:11,750][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 20:42:11,751][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 20:42:11,751][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 20:42:11,752][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 20:42:11,752][train.py][INFO] - Instantiating loggers...
[2024-06-20 20:42:11,752][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 20:42:11,754][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 20:42:11,786][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/easy_sleep/0.05/7/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.17it/s v_num: 0.000      
                                                              val/auc: 0.461    
                                                              val/f1: 0.286     
                                                              val/precision:    
                                                              0.400 val/recall: 
                                                              0.222 val/mre:    
                                                              0.116 train/auc:  
                                                              0.983 train/f1:   
                                                              0.983             
                                                              train/precision:  
                                                              0.983             
                                                              train/recall:     
                                                              0.983 train/mre:  
                                                              0.120             
[2024-06-20 20:44:08,492][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/easy_sleep/0.05/7>
[2024-06-20 20:44:08,493][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 20:44:08,496][HYDRA] 	#88 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=easy_sleep data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=8
[rank: 0] Seed set to 8
[2024-06-20 20:44:08,667][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 20:44:08,668][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-20 20:44:08,669][train.py][INFO] - Instantiating callbacks...
[2024-06-20 20:44:08,670][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 20:44:08,672][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 20:44:08,672][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 20:44:08,672][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 20:44:08,673][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 20:44:08,673][train.py][INFO] - Instantiating loggers...
[2024-06-20 20:44:08,674][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 20:44:08,675][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 20:44:08,702][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/easy_sleep/0.05/8/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.08it/s v_num: 0.000      
                                                              val/auc: 0.517    
                                                              val/f1: 0.400     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.333 val/mre:    
                                                              0.119 train/auc:  
                                                              0.925 train/f1:   
                                                              0.919             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              0.850 train/mre:  
                                                              0.115             
[2024-06-20 20:46:05,605][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/easy_sleep/0.05/8>
[2024-06-20 20:46:05,606][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 20:46:05,609][HYDRA] 	#89 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=easy_sleep data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=9
[rank: 0] Seed set to 9
[2024-06-20 20:46:05,791][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 20:46:05,793][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-20 20:46:05,794][train.py][INFO] - Instantiating callbacks...
[2024-06-20 20:46:05,794][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 20:46:05,798][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 20:46:05,798][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 20:46:05,799][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 20:46:05,801][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 20:46:05,802][train.py][INFO] - Instantiating loggers...
[2024-06-20 20:46:05,802][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 20:46:05,803][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 20:46:05,882][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/easy_sleep/0.05/9/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.08it/s v_num: 0.000      
                                                              val/auc: 0.361    
                                                              val/f1: 0.250     
                                                              val/precision:    
                                                              0.286 val/recall: 
                                                              0.222 val/mre:    
                                                              0.113 train/auc:  
                                                              0.950 train/f1:   
                                                              0.947             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              0.900 train/mre:  
                                                              0.114             
[2024-06-20 20:48:03,001][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/easy_sleep/0.05/9>
[2024-06-20 20:48:03,001][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 20:48:03,004][HYDRA] 	#90 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=easy_sleep data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=0
[2024-06-20 20:48:03,185][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 20:48:03,187][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-20 20:48:03,188][train.py][INFO] - Instantiating callbacks...
[2024-06-20 20:48:03,188][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 20:48:03,191][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 20:48:03,191][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 20:48:03,192][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 20:48:03,192][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 20:48:03,193][train.py][INFO] - Instantiating loggers...
[2024-06-20 20:48:03,193][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 20:48:03,194][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 20:48:03,223][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/easy_sleep/0.1/0/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.07it/s v_num: 0.000      
                                                              val/auc: 0.406    
                                                              val/f1: 0.154     
                                                              val/precision:    
                                                              0.250 val/recall: 
                                                              0.111 val/mre:    
                                                              0.122 train/auc:  
                                                              0.992 train/f1:   
                                                              0.992             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              0.983 train/mre:  
                                                              0.123             
[2024-06-20 20:50:00,456][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/easy_sleep/0.1/0>
[2024-06-20 20:50:00,457][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 20:50:00,460][HYDRA] 	#91 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=easy_sleep data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=1
[rank: 0] Seed set to 1
[2024-06-20 20:50:00,855][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 20:50:00,857][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-20 20:50:00,858][train.py][INFO] - Instantiating callbacks...
[2024-06-20 20:50:00,859][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 20:50:00,861][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 20:50:00,861][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 20:50:00,862][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 20:50:00,865][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 20:50:00,865][train.py][INFO] - Instantiating loggers...
[2024-06-20 20:50:00,865][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 20:50:00,866][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 20:50:00,932][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/easy_sleep/0.1/1/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.08it/s v_num: 0.000      
                                                              val/auc: 0.456    
                                                              val/f1: 0.167     
                                                              val/precision:    
                                                              0.333 val/recall: 
                                                              0.111 val/mre:    
                                                              0.122 train/auc:  
                                                              0.975 train/f1:   
                                                              0.974             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              0.950 train/mre:  
                                                              0.125             
[2024-06-20 20:51:58,727][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/easy_sleep/0.1/1>
[2024-06-20 20:51:58,727][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 20:51:58,730][HYDRA] 	#92 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=easy_sleep data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=2
[rank: 0] Seed set to 2
[2024-06-20 20:51:58,914][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 20:51:58,916][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-20 20:51:58,917][train.py][INFO] - Instantiating callbacks...
[2024-06-20 20:51:58,918][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 20:51:58,920][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 20:51:58,921][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 20:51:58,921][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 20:51:58,922][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 20:51:58,922][train.py][INFO] - Instantiating loggers...
[2024-06-20 20:51:58,922][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 20:51:58,923][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 20:51:58,955][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/easy_sleep/0.1/2/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.14it/s v_num: 0.000      
                                                              val/auc: 0.361    
                                                              val/f1: 0.250     
                                                              val/precision:    
                                                              0.286 val/recall: 
                                                              0.222 val/mre:    
                                                              0.123 train/auc:  
                                                              0.992 train/f1:   
                                                              0.992             
                                                              train/precision:  
                                                              0.984             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.122             
[2024-06-20 20:53:56,301][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/easy_sleep/0.1/2>
[2024-06-20 20:53:56,302][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 20:53:56,304][HYDRA] 	#93 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=easy_sleep data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=3
[rank: 0] Seed set to 3
[2024-06-20 20:53:56,477][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 20:53:56,479][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-20 20:53:56,480][train.py][INFO] - Instantiating callbacks...
[2024-06-20 20:53:56,480][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 20:53:56,482][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 20:53:56,483][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 20:53:56,483][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 20:53:56,484][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 20:53:56,484][train.py][INFO] - Instantiating loggers...
[2024-06-20 20:53:56,484][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 20:53:56,485][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 20:53:56,515][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/easy_sleep/0.1/3/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.08it/s v_num: 0.000      
                                                              val/auc: 0.406    
                                                              val/f1: 0.154     
                                                              val/precision:    
                                                              0.250 val/recall: 
                                                              0.111 val/mre:    
                                                              0.120 train/auc:  
                                                              0.942 train/f1:   
                                                              0.938             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              0.883 train/mre:  
                                                              0.121             
[2024-06-20 20:55:54,714][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/easy_sleep/0.1/3>
[2024-06-20 20:55:54,714][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 20:55:54,717][HYDRA] 	#94 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=easy_sleep data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=4
[rank: 0] Seed set to 4
[2024-06-20 20:55:54,901][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 20:55:54,903][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-20 20:55:54,904][train.py][INFO] - Instantiating callbacks...
[2024-06-20 20:55:54,905][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 20:55:54,907][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 20:55:54,907][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 20:55:54,908][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 20:55:54,911][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 20:55:54,911][train.py][INFO] - Instantiating loggers...
[2024-06-20 20:55:54,911][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 20:55:54,913][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 20:55:54,980][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/easy_sleep/0.1/4/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.07it/s v_num: 0.000      
                                                              val/auc: 0.422    
                                                              val/f1: 0.421     
                                                              val/precision:    
                                                              0.400 val/recall: 
                                                              0.444 val/mre:    
                                                              0.120 train/auc:  
                                                              0.925 train/f1:   
                                                              0.919             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              0.850 train/mre:  
                                                              0.119             
[2024-06-20 20:57:52,671][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/easy_sleep/0.1/4>
[2024-06-20 20:57:52,672][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 20:57:52,674][HYDRA] 	#95 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=easy_sleep data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=5
[rank: 0] Seed set to 5
[2024-06-20 20:57:52,854][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 20:57:52,856][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-20 20:57:52,857][train.py][INFO] - Instantiating callbacks...
[2024-06-20 20:57:52,857][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 20:57:52,859][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 20:57:52,860][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 20:57:52,860][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 20:57:52,861][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 20:57:52,861][train.py][INFO] - Instantiating loggers...
[2024-06-20 20:57:52,862][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 20:57:52,863][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 20:57:52,892][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/easy_sleep/0.1/5/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.04it/s v_num: 0.000      
                                                              val/auc: 0.472    
                                                              val/f1: 0.444     
                                                              val/precision:    
                                                              0.444 val/recall: 
                                                              0.444 val/mre:    
                                                              0.127 train/auc:  
                                                              0.950 train/f1:   
                                                              0.948             
                                                              train/precision:  
                                                              0.982             
                                                              train/recall:     
                                                              0.917 train/mre:  
                                                              0.126             
[2024-06-20 20:59:50,869][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/easy_sleep/0.1/5>
[2024-06-20 20:59:50,870][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 20:59:50,872][HYDRA] 	#96 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=easy_sleep data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=6
[rank: 0] Seed set to 6
[2024-06-20 20:59:51,083][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 20:59:51,085][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-20 20:59:51,086][train.py][INFO] - Instantiating callbacks...
[2024-06-20 20:59:51,086][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 20:59:51,089][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 20:59:51,089][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 20:59:51,090][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 20:59:51,095][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 20:59:51,095][train.py][INFO] - Instantiating loggers...
[2024-06-20 20:59:51,096][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 20:59:51,097][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 20:59:51,164][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/easy_sleep/0.1/6/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.07it/s v_num: 0.000      
                                                              val/auc: 0.483    
                                                              val/f1: 0.545     
                                                              val/precision:    
                                                              0.462 val/recall: 
                                                              0.667 val/mre:    
                                                              0.124 train/auc:  
                                                              0.950 train/f1:   
                                                              0.949             
                                                              train/precision:  
                                                              0.966             
                                                              train/recall:     
                                                              0.933 train/mre:  
                                                              0.121             
[2024-06-20 21:01:49,185][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/easy_sleep/0.1/6>
[2024-06-20 21:01:49,186][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 21:01:49,188][HYDRA] 	#97 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=easy_sleep data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=7
[rank: 0] Seed set to 7
[2024-06-20 21:01:49,367][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 21:01:49,369][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-20 21:01:49,370][train.py][INFO] - Instantiating callbacks...
[2024-06-20 21:01:49,371][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 21:01:49,373][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 21:01:49,373][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 21:01:49,374][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 21:01:49,375][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 21:01:49,375][train.py][INFO] - Instantiating loggers...
[2024-06-20 21:01:49,376][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 21:01:49,377][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 21:01:49,405][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/easy_sleep/0.1/7/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.05it/s v_num: 0.000      
                                                              val/auc: 0.467    
                                                              val/f1: 0.375     
                                                              val/precision:    
                                                              0.429 val/recall: 
                                                              0.333 val/mre:    
                                                              0.126 train/auc:  
                                                              0.967 train/f1:   
                                                              0.966             
                                                              train/precision:  
                                                              0.983             
                                                              train/recall:     
                                                              0.950 train/mre:  
                                                              0.127             
[2024-06-20 21:03:47,937][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/easy_sleep/0.1/7>
[2024-06-20 21:03:47,938][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 21:03:47,941][HYDRA] 	#98 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=easy_sleep data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=8
[rank: 0] Seed set to 8
[2024-06-20 21:03:48,121][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 21:03:48,122][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-20 21:03:48,124][train.py][INFO] - Instantiating callbacks...
[2024-06-20 21:03:48,124][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 21:03:48,126][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 21:03:48,127][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 21:03:48,127][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 21:03:48,128][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 21:03:48,128][train.py][INFO] - Instantiating loggers...
[2024-06-20 21:03:48,129][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 21:03:48,130][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 21:03:48,158][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/easy_sleep/0.1/8/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.07it/s v_num: 0.000      
                                                              val/auc: 0.567    
                                                              val/f1: 0.429     
                                                              val/precision:    
                                                              0.600 val/recall: 
                                                              0.333 val/mre:    
                                                              0.121 train/auc:  
                                                              0.925 train/f1:   
                                                              0.919             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              0.850 train/mre:  
                                                              0.121             
[2024-06-20 21:05:45,656][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/easy_sleep/0.1/8>
[2024-06-20 21:05:45,657][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 21:05:45,659][HYDRA] 	#99 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=easy_sleep data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=9
[rank: 0] Seed set to 9
[2024-06-20 21:05:45,844][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 21:05:45,846][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-20 21:05:45,847][train.py][INFO] - Instantiating callbacks...
[2024-06-20 21:05:45,848][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 21:05:45,850][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 21:05:45,850][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 21:05:45,851][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 21:05:45,853][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 21:05:45,854][train.py][INFO] - Instantiating loggers...
[2024-06-20 21:05:45,854][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 21:05:45,855][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 21:05:45,921][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/easy_sleep/0.1/9/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.06it/s v_num: 0.000      
                                                              val/auc: 0.400    
                                                              val/f1: 0.000     
                                                              val/precision:    
                                                              0.000 val/recall: 
                                                              0.000 val/mre:    
                                                              0.120 train/auc:  
                                                              0.950 train/f1:   
                                                              0.951             
                                                              train/precision:  
                                                              0.935             
                                                              train/recall:     
                                                              0.967 train/mre:  
                                                              0.123             
[2024-06-20 21:07:44,146][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/easy_sleep/0.1/9>
[2024-06-20 21:07:44,146][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 21:07:44,149][HYDRA] 	#100 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=easy_sleep data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=0
[2024-06-20 21:07:44,334][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 21:07:44,336][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-20 21:07:44,337][train.py][INFO] - Instantiating callbacks...
[2024-06-20 21:07:44,338][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 21:07:44,340][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 21:07:44,340][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 21:07:44,341][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 21:07:44,341][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 21:07:44,342][train.py][INFO] - Instantiating loggers...
[2024-06-20 21:07:44,342][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 21:07:44,343][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 21:07:44,374][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/easy_sleep/0.15/0/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.13it/s v_num: 0.000      
                                                              val/auc: 0.517    
                                                              val/f1: 0.400     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.333 val/mre:    
                                                              0.125 train/auc:  
                                                              0.975 train/f1:   
                                                              0.974             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              0.950 train/mre:  
                                                              0.127             
[2024-06-20 21:09:41,789][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/easy_sleep/0.15/0>
[2024-06-20 21:09:41,790][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 21:09:41,792][HYDRA] 	#101 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=easy_sleep data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=1
[rank: 0] Seed set to 1
[2024-06-20 21:09:41,966][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 21:09:41,968][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-20 21:09:41,969][train.py][INFO] - Instantiating callbacks...
[2024-06-20 21:09:41,969][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 21:09:41,971][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 21:09:41,972][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 21:09:41,972][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 21:09:41,973][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 21:09:41,973][train.py][INFO] - Instantiating loggers...
[2024-06-20 21:09:41,973][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 21:09:41,974][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 21:09:42,003][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/easy_sleep/0.15/1/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.05it/s v_num: 0.000      
                                                              val/auc: 0.461    
                                                              val/f1: 0.286     
                                                              val/precision:    
                                                              0.400 val/recall: 
                                                              0.222 val/mre:    
                                                              0.128 train/auc:  
                                                              0.992 train/f1:   
                                                              0.992             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              0.983 train/mre:  
                                                              0.131             
[2024-06-20 21:11:41,210][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/easy_sleep/0.15/1>
[2024-06-20 21:11:41,211][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 21:11:41,214][HYDRA] 	#102 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=easy_sleep data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=2
[rank: 0] Seed set to 2
[2024-06-20 21:11:41,401][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 21:11:41,403][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-20 21:11:41,404][train.py][INFO] - Instantiating callbacks...
[2024-06-20 21:11:41,404][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 21:11:41,406][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 21:11:41,407][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 21:11:41,407][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 21:11:41,410][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 21:11:41,410][train.py][INFO] - Instantiating loggers...
[2024-06-20 21:11:41,410][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 21:11:41,412][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 21:11:41,488][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/easy_sleep/0.15/2/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.05it/s v_num: 0.000      
                                                              val/auc: 0.472    
                                                              val/f1: 0.444     
                                                              val/precision:    
                                                              0.444 val/recall: 
                                                              0.444 val/mre:    
                                                              0.130 train/auc:  
                                                              0.992 train/f1:   
                                                              0.992             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              0.983 train/mre:  
                                                              0.130             
[2024-06-20 21:13:39,421][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/easy_sleep/0.15/2>
[2024-06-20 21:13:39,422][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 21:13:39,424][HYDRA] 	#103 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=easy_sleep data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=3
[rank: 0] Seed set to 3
[2024-06-20 21:13:39,607][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 21:13:39,608][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-20 21:13:39,610][train.py][INFO] - Instantiating callbacks...
[2024-06-20 21:13:39,610][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 21:13:39,612][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 21:13:39,613][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 21:13:39,613][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 21:13:39,614][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 21:13:39,614][train.py][INFO] - Instantiating loggers...
[2024-06-20 21:13:39,615][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 21:13:39,616][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 21:13:39,645][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/easy_sleep/0.15/3/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.06it/s v_num: 0.000      
                                                              val/auc: 0.356    
                                                              val/f1: 0.143     
                                                              val/precision:    
                                                              0.200 val/recall: 
                                                              0.111 val/mre:    
                                                              0.124 train/auc:  
                                                              0.942 train/f1:   
                                                              0.938             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              0.883 train/mre:  
                                                              0.126             
[2024-06-20 21:15:39,245][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/easy_sleep/0.15/3>
[2024-06-20 21:15:39,246][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 21:15:39,248][HYDRA] 	#104 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=easy_sleep data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=4
[rank: 0] Seed set to 4
[2024-06-20 21:15:39,436][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 21:15:39,438][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-20 21:15:39,439][train.py][INFO] - Instantiating callbacks...
[2024-06-20 21:15:39,439][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 21:15:39,441][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 21:15:39,442][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 21:15:39,442][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 21:15:39,445][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 21:15:39,445][train.py][INFO] - Instantiating loggers...
[2024-06-20 21:15:39,446][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 21:15:39,447][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 21:15:39,523][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/easy_sleep/0.15/4/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.08it/s v_num: 0.000      
                                                              val/auc: 0.522    
                                                              val/f1: 0.471     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.444 val/mre:    
                                                              0.128 train/auc:  
                                                              0.958 train/f1:   
                                                              0.957             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              0.917 train/mre:  
                                                              0.128             
[2024-06-20 21:17:37,455][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/easy_sleep/0.15/4>
[2024-06-20 21:17:37,455][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 21:17:37,458][HYDRA] 	#105 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=easy_sleep data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=5
[rank: 0] Seed set to 5
[2024-06-20 21:17:37,652][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 21:17:37,653][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-20 21:17:37,655][train.py][INFO] - Instantiating callbacks...
[2024-06-20 21:17:37,655][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 21:17:37,657][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 21:17:37,657][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 21:17:37,658][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 21:17:37,659][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 21:17:37,659][train.py][INFO] - Instantiating loggers...
[2024-06-20 21:17:37,659][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 21:17:37,660][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 21:17:37,689][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/easy_sleep/0.15/5/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.06it/s v_num: 0.000      
                                                              val/auc: 0.467    
                                                              val/f1: 0.375     
                                                              val/precision:    
                                                              0.429 val/recall: 
                                                              0.333 val/mre:    
                                                              0.128 train/auc:  
                                                              0.933 train/f1:   
                                                              0.929             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              0.867 train/mre:  
                                                              0.128             
[2024-06-20 21:19:35,658][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/easy_sleep/0.15/5>
[2024-06-20 21:19:35,659][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 21:19:35,661][HYDRA] 	#106 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=easy_sleep data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=6
[rank: 0] Seed set to 6
[2024-06-20 21:19:35,865][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 21:19:35,867][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-20 21:19:35,868][train.py][INFO] - Instantiating callbacks...
[2024-06-20 21:19:35,868][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 21:19:35,871][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 21:19:35,871][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 21:19:35,871][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 21:19:35,872][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 21:19:35,872][train.py][INFO] - Instantiating loggers...
[2024-06-20 21:19:35,873][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 21:19:35,874][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 21:19:35,911][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/easy_sleep/0.15/6/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.07it/s v_num: 0.000      
                                                              val/auc: 0.367    
                                                              val/f1: 0.333     
                                                              val/precision:    
                                                              0.333 val/recall: 
                                                              0.333 val/mre:    
                                                              0.127 train/auc:  
                                                              0.983 train/f1:   
                                                              0.983             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              0.967 train/mre:  
                                                              0.126             
[2024-06-20 21:21:34,157][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/easy_sleep/0.15/6>
[2024-06-20 21:21:34,157][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 21:21:34,160][HYDRA] 	#107 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=easy_sleep data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=7
[rank: 0] Seed set to 7
[2024-06-20 21:21:34,345][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 21:21:34,347][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-20 21:21:34,348][train.py][INFO] - Instantiating callbacks...
[2024-06-20 21:21:34,349][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 21:21:34,351][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 21:21:34,351][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 21:21:34,352][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 21:21:34,375][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 21:21:34,375][train.py][INFO] - Instantiating loggers...
[2024-06-20 21:21:34,376][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 21:21:34,377][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 21:21:34,452][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/easy_sleep/0.15/7/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.07it/s v_num: 0.000      
                                                              val/auc: 0.461    
                                                              val/f1: 0.286     
                                                              val/precision:    
                                                              0.400 val/recall: 
                                                              0.222 val/mre:    
                                                              0.126 train/auc:  
                                                              0.925 train/f1:   
                                                              0.919             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              0.850 train/mre:  
                                                              0.130             
[2024-06-20 21:23:32,799][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/easy_sleep/0.15/7>
[2024-06-20 21:23:32,800][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 21:23:32,803][HYDRA] 	#108 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=easy_sleep data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=8
[rank: 0] Seed set to 8
[2024-06-20 21:23:32,984][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 21:23:32,986][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-20 21:23:32,987][train.py][INFO] - Instantiating callbacks...
[2024-06-20 21:23:32,987][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 21:23:32,990][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 21:23:32,990][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 21:23:32,991][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 21:23:32,991][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 21:23:32,992][train.py][INFO] - Instantiating loggers...
[2024-06-20 21:23:32,992][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 21:23:32,993][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 21:23:33,024][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/easy_sleep/0.15/8/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.13it/s v_num: 0.000      
                                                              val/auc: 0.467    
                                                              val/f1: 0.375     
                                                              val/precision:    
                                                              0.429 val/recall: 
                                                              0.333 val/mre:    
                                                              0.124 train/auc:  
                                                              0.942 train/f1:   
                                                              0.938             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              0.883 train/mre:  
                                                              0.125             
[2024-06-20 21:25:31,966][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/easy_sleep/0.15/8>
[2024-06-20 21:25:31,967][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 21:25:31,969][HYDRA] 	#109 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=easy_sleep data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=9
[rank: 0] Seed set to 9
[2024-06-20 21:25:32,309][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 21:25:32,310][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-20 21:25:32,311][train.py][INFO] - Instantiating callbacks...
[2024-06-20 21:25:32,312][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 21:25:32,315][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 21:25:32,316][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 21:25:32,317][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 21:25:32,317][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 21:25:32,318][train.py][INFO] - Instantiating loggers...
[2024-06-20 21:25:32,318][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 21:25:32,319][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 21:25:32,421][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/easy_sleep/0.15/9/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.06it/s v_num: 0.000      
                                                              val/auc: 0.361    
                                                              val/f1: 0.250     
                                                              val/precision:    
                                                              0.286 val/recall: 
                                                              0.222 val/mre:    
                                                              0.125 train/auc:  
                                                              0.983 train/f1:   
                                                              0.983             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              0.967 train/mre:  
                                                              0.129             
[2024-06-20 21:27:30,556][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/easy_sleep/0.15/9>
[2024-06-20 21:27:30,556][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 21:27:30,559][HYDRA] 	#110 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=easy_sleep data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=0
[2024-06-20 21:27:30,749][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 21:27:30,751][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-20 21:27:30,752][train.py][INFO] - Instantiating callbacks...
[2024-06-20 21:27:30,753][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 21:27:30,755][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 21:27:30,755][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 21:27:30,756][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 21:27:30,758][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 21:27:30,758][train.py][INFO] - Instantiating loggers...
[2024-06-20 21:27:30,759][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 21:27:30,760][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 21:27:30,826][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/easy_sleep/0.2/0/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.05it/s v_num: 0.000      
                                                              val/auc: 0.467    
                                                              val/f1: 0.375     
                                                              val/precision:    
                                                              0.429 val/recall: 
                                                              0.333 val/mre:    
                                                              0.134 train/auc:  
                                                              0.975 train/f1:   
                                                              0.975             
                                                              train/precision:  
                                                              0.983             
                                                              train/recall:     
                                                              0.967 train/mre:  
                                                              0.134             
[2024-06-20 21:29:28,559][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/easy_sleep/0.2/0>
[2024-06-20 21:29:28,560][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 21:29:28,562][HYDRA] 	#111 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=easy_sleep data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=1
[rank: 0] Seed set to 1
[2024-06-20 21:29:28,744][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 21:29:28,746][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-20 21:29:28,747][train.py][INFO] - Instantiating callbacks...
[2024-06-20 21:29:28,747][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 21:29:28,750][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 21:29:28,750][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 21:29:28,750][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 21:29:28,751][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 21:29:28,752][train.py][INFO] - Instantiating loggers...
[2024-06-20 21:29:28,752][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 21:29:28,753][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 21:29:28,782][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/easy_sleep/0.2/1/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.05it/s v_num: 0.000      
                                                              val/auc: 0.572    
                                                              val/f1: 0.500     
                                                              val/precision:    
                                                              0.571 val/recall: 
                                                              0.444 val/mre:    
                                                              0.129 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.134             
[2024-06-20 21:31:27,254][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/easy_sleep/0.2/1>
[2024-06-20 21:31:27,254][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 21:31:27,257][HYDRA] 	#112 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=easy_sleep data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=2
[rank: 0] Seed set to 2
[2024-06-20 21:31:27,446][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 21:31:27,448][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-20 21:31:27,449][train.py][INFO] - Instantiating callbacks...
[2024-06-20 21:31:27,449][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 21:31:27,451][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 21:31:27,452][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 21:31:27,452][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 21:31:27,454][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 21:31:27,454][train.py][INFO] - Instantiating loggers...
[2024-06-20 21:31:27,455][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 21:31:27,456][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 21:31:27,523][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/easy_sleep/0.2/2/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.07it/s v_num: 0.000      
                                                              val/auc: 0.472    
                                                              val/f1: 0.444     
                                                              val/precision:    
                                                              0.444 val/recall: 
                                                              0.444 val/mre:    
                                                              0.132 train/auc:  
                                                              0.975 train/f1:   
                                                              0.974             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              0.950 train/mre:  
                                                              0.132             
[2024-06-20 21:33:25,549][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/easy_sleep/0.2/2>
[2024-06-20 21:33:25,550][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 21:33:25,552][HYDRA] 	#113 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=easy_sleep data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=3
[rank: 0] Seed set to 3
[2024-06-20 21:33:25,732][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 21:33:25,734][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-20 21:33:25,735][train.py][INFO] - Instantiating callbacks...
[2024-06-20 21:33:25,736][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 21:33:25,738][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 21:33:25,739][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 21:33:25,739][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 21:33:25,740][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 21:33:25,740][train.py][INFO] - Instantiating loggers...
[2024-06-20 21:33:25,740][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 21:33:25,742][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 21:33:25,771][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/easy_sleep/0.2/3/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.06it/s v_num: 0.000      
                                                              val/auc: 0.356    
                                                              val/f1: 0.143     
                                                              val/precision:    
                                                              0.200 val/recall: 
                                                              0.111 val/mre:    
                                                              0.131 train/auc:  
                                                              0.958 train/f1:   
                                                              0.957             
                                                              train/precision:  
                                                              0.982             
                                                              train/recall:     
                                                              0.933 train/mre:  
                                                              0.132             
[2024-06-20 21:35:24,143][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/easy_sleep/0.2/3>
[2024-06-20 21:35:24,144][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 21:35:24,148][HYDRA] 	#114 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=easy_sleep data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=4
[rank: 0] Seed set to 4
[2024-06-20 21:35:24,354][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 21:35:24,355][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-20 21:35:24,357][train.py][INFO] - Instantiating callbacks...
[2024-06-20 21:35:24,357][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 21:35:24,359][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 21:35:24,360][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 21:35:24,360][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 21:35:24,362][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 21:35:24,363][train.py][INFO] - Instantiating loggers...
[2024-06-20 21:35:24,363][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 21:35:24,364][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 21:35:24,398][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/easy_sleep/0.2/4/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.06it/s v_num: 0.000      
                                                              val/auc: 0.306    
                                                              val/f1: 0.133     
                                                              val/precision:    
                                                              0.167 val/recall: 
                                                              0.111 val/mre:    
                                                              0.129 train/auc:  
                                                              0.975 train/f1:   
                                                              0.974             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              0.950 train/mre:  
                                                              0.130             
[2024-06-20 21:37:22,539][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/easy_sleep/0.2/4>
[2024-06-20 21:37:22,540][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 21:37:22,542][HYDRA] 	#115 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=easy_sleep data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=5
[rank: 0] Seed set to 5
[2024-06-20 21:37:22,729][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 21:37:22,730][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-20 21:37:22,732][train.py][INFO] - Instantiating callbacks...
[2024-06-20 21:37:22,732][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 21:37:22,734][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 21:37:22,735][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 21:37:22,735][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 21:37:22,737][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 21:37:22,737][train.py][INFO] - Instantiating loggers...
[2024-06-20 21:37:22,738][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 21:37:22,739][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 21:37:22,805][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/easy_sleep/0.2/5/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.08it/s v_num: 0.000      
                                                              val/auc: 0.528    
                                                              val/f1: 0.526     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.556 val/mre:    
                                                              0.136 train/auc:  
                                                              0.967 train/f1:   
                                                              0.966             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              0.933 train/mre:  
                                                              0.137             
[2024-06-20 21:39:20,434][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/easy_sleep/0.2/5>
[2024-06-20 21:39:20,434][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 21:39:20,436][HYDRA] 	#116 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=easy_sleep data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=6
[rank: 0] Seed set to 6
[2024-06-20 21:39:20,621][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 21:39:20,622][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-20 21:39:20,624][train.py][INFO] - Instantiating callbacks...
[2024-06-20 21:39:20,624][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 21:39:20,626][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 21:39:20,627][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 21:39:20,627][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 21:39:20,643][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 21:39:20,643][train.py][INFO] - Instantiating loggers...
[2024-06-20 21:39:20,644][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 21:39:20,645][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 21:39:20,690][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/easy_sleep/0.2/6/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.15it/s v_num: 0.000      
                                                              val/auc: 0.467    
                                                              val/f1: 0.375     
                                                              val/precision:    
                                                              0.429 val/recall: 
                                                              0.333 val/mre:    
                                                              0.129 train/auc:  
                                                              0.992 train/f1:   
                                                              0.992             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              0.983 train/mre:  
                                                              0.128             
[2024-06-20 21:41:18,727][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/easy_sleep/0.2/6>
[2024-06-20 21:41:18,728][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 21:41:18,732][HYDRA] 	#117 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=easy_sleep data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=7
[rank: 0] Seed set to 7
[2024-06-20 21:41:18,938][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 21:41:18,939][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-20 21:41:18,941][train.py][INFO] - Instantiating callbacks...
[2024-06-20 21:41:18,941][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 21:41:18,943][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 21:41:18,944][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 21:41:18,944][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 21:41:18,945][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 21:41:18,945][train.py][INFO] - Instantiating loggers...
[2024-06-20 21:41:18,945][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 21:41:18,947][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 21:41:18,983][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/easy_sleep/0.2/7/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.10it/s v_num: 0.000      
                                                              val/auc: 0.461    
                                                              val/f1: 0.286     
                                                              val/precision:    
                                                              0.400 val/recall: 
                                                              0.222 val/mre:    
                                                              0.134 train/auc:  
                                                              0.933 train/f1:   
                                                              0.934             
                                                              train/precision:  
                                                              0.919             
                                                              train/recall:     
                                                              0.950 train/mre:  
                                                              0.136             
[2024-06-20 21:43:14,661][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/easy_sleep/0.2/7>
[2024-06-20 21:43:14,662][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 21:43:14,664][HYDRA] 	#118 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=easy_sleep data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=8
[rank: 0] Seed set to 8
[2024-06-20 21:43:14,848][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 21:43:14,850][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-20 21:43:14,851][train.py][INFO] - Instantiating callbacks...
[2024-06-20 21:43:14,851][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 21:43:14,853][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 21:43:14,854][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 21:43:14,854][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 21:43:14,856][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 21:43:14,857][train.py][INFO] - Instantiating loggers...
[2024-06-20 21:43:14,857][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 21:43:14,858][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 21:43:14,925][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/easy_sleep/0.2/8/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.09it/s v_num: 0.000      
                                                              val/auc: 0.461    
                                                              val/f1: 0.286     
                                                              val/precision:    
                                                              0.400 val/recall: 
                                                              0.222 val/mre:    
                                                              0.127 train/auc:  
                                                              0.925 train/f1:   
                                                              0.920             
                                                              train/precision:  
                                                              0.981             
                                                              train/recall:     
                                                              0.867 train/mre:  
                                                              0.129             
[2024-06-20 21:45:11,819][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/easy_sleep/0.2/8>
[2024-06-20 21:45:11,820][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 21:45:11,823][HYDRA] 	#119 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=easy_sleep data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=9
[rank: 0] Seed set to 9
[2024-06-20 21:45:12,226][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 21:45:12,228][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-20 21:45:12,229][train.py][INFO] - Instantiating callbacks...
[2024-06-20 21:45:12,230][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 21:45:12,233][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 21:45:12,234][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 21:45:12,234][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 21:45:12,238][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 21:45:12,238][train.py][INFO] - Instantiating loggers...
[2024-06-20 21:45:12,238][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 21:45:12,239][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 21:45:12,284][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/easy_sleep/0.2/9/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.15it/s v_num: 0.000      
                                                              val/auc: 0.306    
                                                              val/f1: 0.133     
                                                              val/precision:    
                                                              0.167 val/recall: 
                                                              0.111 val/mre:    
                                                              0.127 train/auc:  
                                                              0.992 train/f1:   
                                                              0.992             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              0.983 train/mre:  
                                                              0.132             
[2024-06-20 21:47:08,839][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/easy_sleep/0.2/9>
[2024-06-20 21:47:08,839][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 21:47:08,842][HYDRA] 	#120 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=easy_sleep data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=0
[2024-06-20 21:47:09,066][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 21:47:09,068][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-20 21:47:09,069][train.py][INFO] - Instantiating callbacks...
[2024-06-20 21:47:09,069][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 21:47:09,072][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 21:47:09,072][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 21:47:09,073][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 21:47:09,073][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 21:47:09,074][train.py][INFO] - Instantiating loggers...
[2024-06-20 21:47:09,074][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 21:47:09,075][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 21:47:09,115][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/easy_sleep/0.25/0/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.08it/s v_num: 0.000      
                                                              val/auc: 0.472    
                                                              val/f1: 0.444     
                                                              val/precision:    
                                                              0.444 val/recall: 
                                                              0.444 val/mre:    
                                                              0.134 train/auc:  
                                                              0.967 train/f1:   
                                                              0.966             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              0.933 train/mre:  
                                                              0.135             
[2024-06-20 21:49:06,289][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/easy_sleep/0.25/0>
[2024-06-20 21:49:06,289][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 21:49:06,291][HYDRA] 	#121 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=easy_sleep data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=1
[rank: 0] Seed set to 1
[2024-06-20 21:49:06,471][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 21:49:06,473][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-20 21:49:06,474][train.py][INFO] - Instantiating callbacks...
[2024-06-20 21:49:06,474][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 21:49:06,477][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 21:49:06,477][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 21:49:06,479][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 21:49:06,481][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 21:49:06,481][train.py][INFO] - Instantiating loggers...
[2024-06-20 21:49:06,481][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 21:49:06,483][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 21:49:06,546][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/easy_sleep/0.25/1/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.08it/s v_num: 0.000      
                                                              val/auc: 0.628    
                                                              val/f1: 0.588     
                                                              val/precision:    
                                                              0.625 val/recall: 
                                                              0.556 val/mre:    
                                                              0.136 train/auc:  
                                                              0.983 train/f1:   
                                                              0.983             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              0.967 train/mre:  
                                                              0.136             
[2024-06-20 21:51:03,445][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/easy_sleep/0.25/1>
[2024-06-20 21:51:03,446][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 21:51:03,448][HYDRA] 	#122 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=easy_sleep data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=2
[rank: 0] Seed set to 2
[2024-06-20 21:51:03,637][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 21:51:03,638][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-20 21:51:03,640][train.py][INFO] - Instantiating callbacks...
[2024-06-20 21:51:03,640][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 21:51:03,643][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 21:51:03,643][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 21:51:03,644][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 21:51:03,645][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 21:51:03,646][train.py][INFO] - Instantiating loggers...
[2024-06-20 21:51:03,646][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 21:51:03,648][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 21:51:03,698][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/easy_sleep/0.25/2/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.09it/s v_num: 0.000      
                                                              val/auc: 0.472    
                                                              val/f1: 0.444     
                                                              val/precision:    
                                                              0.444 val/recall: 
                                                              0.444 val/mre:    
                                                              0.137 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.138             
[2024-06-20 21:53:00,345][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/easy_sleep/0.25/2>
[2024-06-20 21:53:00,345][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 21:53:00,348][HYDRA] 	#123 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=easy_sleep data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=3
[rank: 0] Seed set to 3
[2024-06-20 21:53:00,532][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 21:53:00,534][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-20 21:53:00,535][train.py][INFO] - Instantiating callbacks...
[2024-06-20 21:53:00,535][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 21:53:00,537][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 21:53:00,538][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 21:53:00,538][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 21:53:00,552][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 21:53:00,553][train.py][INFO] - Instantiating loggers...
[2024-06-20 21:53:00,553][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 21:53:00,554][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 21:53:00,643][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/easy_sleep/0.25/3/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.05it/s v_num: 0.000      
                                                              val/auc: 0.461    
                                                              val/f1: 0.286     
                                                              val/precision:    
                                                              0.400 val/recall: 
                                                              0.222 val/mre:    
                                                              0.136 train/auc:  
                                                              0.925 train/f1:   
                                                              0.919             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              0.850 train/mre:  
                                                              0.137             
[2024-06-20 21:54:57,014][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/easy_sleep/0.25/3>
[2024-06-20 21:54:57,015][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 21:54:57,017][HYDRA] 	#124 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=easy_sleep data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=4
[rank: 0] Seed set to 4
[2024-06-20 21:54:57,198][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 21:54:57,199][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-20 21:54:57,201][train.py][INFO] - Instantiating callbacks...
[2024-06-20 21:54:57,201][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 21:54:57,203][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 21:54:57,203][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 21:54:57,204][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 21:54:57,205][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 21:54:57,205][train.py][INFO] - Instantiating loggers...
[2024-06-20 21:54:57,205][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 21:54:57,206][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 21:54:57,235][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/easy_sleep/0.25/4/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.10it/s v_num: 0.000      
                                                              val/auc: 0.483    
                                                              val/f1: 0.545     
                                                              val/precision:    
                                                              0.462 val/recall: 
                                                              0.667 val/mre:    
                                                              0.133 train/auc:  
                                                              0.958 train/f1:   
                                                              0.957             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              0.917 train/mre:  
                                                              0.133             
[2024-06-20 21:56:54,535][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/easy_sleep/0.25/4>
[2024-06-20 21:56:54,535][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 21:56:54,538][HYDRA] 	#125 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=easy_sleep data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=5
[rank: 0] Seed set to 5
[2024-06-20 21:56:54,714][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 21:56:54,716][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-20 21:56:54,717][train.py][INFO] - Instantiating callbacks...
[2024-06-20 21:56:54,717][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 21:56:54,720][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 21:56:54,720][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 21:56:54,720][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 21:56:54,721][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 21:56:54,722][train.py][INFO] - Instantiating loggers...
[2024-06-20 21:56:54,722][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 21:56:54,723][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 21:56:54,751][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/easy_sleep/0.25/5/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.10it/s v_num: 0.000      
                                                              val/auc: 0.567    
                                                              val/f1: 0.429     
                                                              val/precision:    
                                                              0.600 val/recall: 
                                                              0.333 val/mre:    
                                                              0.132 train/auc:  
                                                              0.925 train/f1:   
                                                              0.919             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              0.850 train/mre:  
                                                              0.136             
[2024-06-20 21:58:51,364][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/easy_sleep/0.25/5>
[2024-06-20 21:58:51,365][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 21:58:51,368][HYDRA] 	#126 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=easy_sleep data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=6
[rank: 0] Seed set to 6
[2024-06-20 21:58:51,561][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 21:58:51,563][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-20 21:58:51,564][train.py][INFO] - Instantiating callbacks...
[2024-06-20 21:58:51,564][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 21:58:51,567][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 21:58:51,567][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 21:58:51,568][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 21:58:51,570][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 21:58:51,571][train.py][INFO] - Instantiating loggers...
[2024-06-20 21:58:51,571][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 21:58:51,572][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 21:58:51,641][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/easy_sleep/0.25/6/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.11it/s v_num: 0.000      
                                                              val/auc: 0.478    
                                                              val/f1: 0.500     
                                                              val/precision:    
                                                              0.455 val/recall: 
                                                              0.556 val/mre:    
                                                              0.133 train/auc:  
                                                              0.933 train/f1:   
                                                              0.929             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              0.867 train/mre:  
                                                              0.132             
[2024-06-20 22:00:47,636][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/easy_sleep/0.25/6>
[2024-06-20 22:00:47,636][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 22:00:47,639][HYDRA] 	#127 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=easy_sleep data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=7
[rank: 0] Seed set to 7
[2024-06-20 22:00:47,816][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 22:00:47,817][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-20 22:00:47,818][train.py][INFO] - Instantiating callbacks...
[2024-06-20 22:00:47,819][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 22:00:47,821][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 22:00:47,821][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 22:00:47,822][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 22:00:47,822][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 22:00:47,823][train.py][INFO] - Instantiating loggers...
[2024-06-20 22:00:47,823][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 22:00:47,824][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 22:00:47,868][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/easy_sleep/0.25/7/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.14it/s v_num: 0.000      
                                                              val/auc: 0.461    
                                                              val/f1: 0.286     
                                                              val/precision:    
                                                              0.400 val/recall: 
                                                              0.222 val/mre:    
                                                              0.134 train/auc:  
                                                              0.950 train/f1:   
                                                              0.950             
                                                              train/precision:  
                                                              0.950             
                                                              train/recall:     
                                                              0.950 train/mre:  
                                                              0.138             
[2024-06-20 22:02:44,437][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/easy_sleep/0.25/7>
[2024-06-20 22:02:44,437][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 22:02:44,440][HYDRA] 	#128 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=easy_sleep data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=8
[rank: 0] Seed set to 8
[2024-06-20 22:02:44,613][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 22:02:44,614][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-20 22:02:44,615][train.py][INFO] - Instantiating callbacks...
[2024-06-20 22:02:44,616][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 22:02:44,618][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 22:02:44,618][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 22:02:44,619][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 22:02:44,619][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 22:02:44,620][train.py][INFO] - Instantiating loggers...
[2024-06-20 22:02:44,620][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 22:02:44,621][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 22:02:44,649][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/easy_sleep/0.25/8/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.09it/s v_num: 0.000      
                                                              val/auc: 0.417    
                                                              val/f1: 0.353     
                                                              val/precision:    
                                                              0.375 val/recall: 
                                                              0.333 val/mre:    
                                                              0.130 train/auc:  
                                                              0.967 train/f1:   
                                                              0.966             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              0.933 train/mre:  
                                                              0.134             
[2024-06-20 22:04:40,422][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/easy_sleep/0.25/8>
[2024-06-20 22:04:40,422][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 22:04:40,425][HYDRA] 	#129 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=easy_sleep data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=9
[rank: 0] Seed set to 9
[2024-06-20 22:04:40,609][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 22:04:40,610][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-20 22:04:40,612][train.py][INFO] - Instantiating callbacks...
[2024-06-20 22:04:40,612][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 22:04:40,614][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 22:04:40,615][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 22:04:40,615][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 22:04:40,617][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 22:04:40,618][train.py][INFO] - Instantiating loggers...
[2024-06-20 22:04:40,618][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 22:04:40,619][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 22:04:40,684][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/easy_sleep/0.25/9/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.09it/s v_num: 0.000      
                                                              val/auc: 0.456    
                                                              val/f1: 0.167     
                                                              val/precision:    
                                                              0.333 val/recall: 
                                                              0.111 val/mre:    
                                                              0.130 train/auc:  
                                                              0.983 train/f1:   
                                                              0.983             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              0.967 train/mre:  
                                                              0.136             
[2024-06-20 22:06:37,194][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/easy_sleep/0.25/9>
[2024-06-20 22:06:37,195][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 22:06:37,197][HYDRA] 	#130 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=easy_sleep data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=0
[2024-06-20 22:06:37,377][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 22:06:37,378][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-20 22:06:37,380][train.py][INFO] - Instantiating callbacks...
[2024-06-20 22:06:37,380][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 22:06:37,382][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 22:06:37,383][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 22:06:37,383][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 22:06:37,384][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 22:06:37,384][train.py][INFO] - Instantiating loggers...
[2024-06-20 22:06:37,384][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 22:06:37,385][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 22:06:37,415][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/easy_sleep/0.3/0/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.14it/s v_num: 0.000      
                                                              val/auc: 0.461    
                                                              val/f1: 0.286     
                                                              val/precision:    
                                                              0.400 val/recall: 
                                                              0.222 val/mre:    
                                                              0.138 train/auc:  
                                                              0.950 train/f1:   
                                                              0.947             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              0.900 train/mre:  
                                                              0.140             
[2024-06-20 22:08:33,200][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/easy_sleep/0.3/0>
[2024-06-20 22:08:33,200][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 22:08:33,203][HYDRA] 	#131 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=easy_sleep data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=1
[rank: 0] Seed set to 1
[2024-06-20 22:08:33,375][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 22:08:33,377][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-20 22:08:33,378][train.py][INFO] - Instantiating callbacks...
[2024-06-20 22:08:33,378][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 22:08:33,380][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 22:08:33,381][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 22:08:33,381][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 22:08:33,382][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 22:08:33,382][train.py][INFO] - Instantiating loggers...
[2024-06-20 22:08:33,382][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 22:08:33,383][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 22:08:33,411][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/easy_sleep/0.3/1/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.10it/s v_num: 0.000      
                                                              val/auc: 0.411    
                                                              val/f1: 0.267     
                                                              val/precision:    
                                                              0.333 val/recall: 
                                                              0.222 val/mre:    
                                                              0.136 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.140             
[2024-06-20 22:10:29,392][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/easy_sleep/0.3/1>
[2024-06-20 22:10:29,392][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 22:10:29,394][HYDRA] 	#132 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=easy_sleep data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=2
[rank: 0] Seed set to 2
[2024-06-20 22:10:29,578][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 22:10:29,579][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-20 22:10:29,581][train.py][INFO] - Instantiating callbacks...
[2024-06-20 22:10:29,581][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 22:10:29,583][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 22:10:29,583][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 22:10:29,584][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 22:10:29,587][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 22:10:29,587][train.py][INFO] - Instantiating loggers...
[2024-06-20 22:10:29,588][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 22:10:29,589][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 22:10:29,664][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/easy_sleep/0.3/2/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.10it/s v_num: 0.000      
                                                              val/auc: 0.422    
                                                              val/f1: 0.421     
                                                              val/precision:    
                                                              0.400 val/recall: 
                                                              0.444 val/mre:    
                                                              0.137 train/auc:  
                                                              0.933 train/f1:   
                                                              0.929             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              0.867 train/mre:  
                                                              0.140             
[2024-06-20 22:12:25,800][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/easy_sleep/0.3/2>
[2024-06-20 22:12:25,800][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 22:12:25,803][HYDRA] 	#133 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=easy_sleep data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=3
[rank: 0] Seed set to 3
[2024-06-20 22:12:25,979][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 22:12:25,980][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-20 22:12:25,982][train.py][INFO] - Instantiating callbacks...
[2024-06-20 22:12:25,982][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 22:12:25,984][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 22:12:25,985][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 22:12:25,985][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 22:12:25,986][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 22:12:25,986][train.py][INFO] - Instantiating loggers...
[2024-06-20 22:12:25,986][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 22:12:25,987][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 22:12:26,017][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/easy_sleep/0.3/3/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.17it/s v_num: 0.000      
                                                              val/auc: 0.356    
                                                              val/f1: 0.143     
                                                              val/precision:    
                                                              0.200 val/recall: 
                                                              0.111 val/mre:    
                                                              0.137 train/auc:  
                                                              0.925 train/f1:   
                                                              0.919             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              0.850 train/mre:  
                                                              0.138             
[2024-06-20 22:14:21,615][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/easy_sleep/0.3/3>
[2024-06-20 22:14:21,616][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 22:14:21,618][HYDRA] 	#134 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=easy_sleep data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=4
[rank: 0] Seed set to 4
[2024-06-20 22:14:21,789][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 22:14:21,790][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-20 22:14:21,792][train.py][INFO] - Instantiating callbacks...
[2024-06-20 22:14:21,792][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 22:14:21,794][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 22:14:21,794][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 22:14:21,795][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 22:14:21,795][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 22:14:21,796][train.py][INFO] - Instantiating loggers...
[2024-06-20 22:14:21,796][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 22:14:21,797][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 22:14:21,824][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/easy_sleep/0.3/4/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.09it/s v_num: 0.000      
                                                              val/auc: 0.361    
                                                              val/f1: 0.250     
                                                              val/precision:    
                                                              0.286 val/recall: 
                                                              0.222 val/mre:    
                                                              0.138 train/auc:  
                                                              0.983 train/f1:   
                                                              0.983             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              0.967 train/mre:  
                                                              0.140             
[2024-06-20 22:16:18,370][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/easy_sleep/0.3/4>
[2024-06-20 22:16:18,371][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 22:16:18,376][HYDRA] 	#135 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=easy_sleep data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=5
[rank: 0] Seed set to 5
[2024-06-20 22:16:18,570][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 22:16:18,571][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-20 22:16:18,572][train.py][INFO] - Instantiating callbacks...
[2024-06-20 22:16:18,573][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 22:16:18,575][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 22:16:18,576][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 22:16:18,576][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 22:16:18,580][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 22:16:18,580][train.py][INFO] - Instantiating loggers...
[2024-06-20 22:16:18,580][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 22:16:18,582][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 22:16:18,691][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/easy_sleep/0.3/5/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.09it/s v_num: 0.000      
                                                              val/auc: 0.472    
                                                              val/f1: 0.444     
                                                              val/precision:    
                                                              0.444 val/recall: 
                                                              0.444 val/mre:    
                                                              0.142 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.142             
[2024-06-20 22:18:14,562][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/easy_sleep/0.3/5>
[2024-06-20 22:18:14,563][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 22:18:14,566][HYDRA] 	#136 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=easy_sleep data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=6
[rank: 0] Seed set to 6
[2024-06-20 22:18:14,743][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 22:18:14,745][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-20 22:18:14,746][train.py][INFO] - Instantiating callbacks...
[2024-06-20 22:18:14,746][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 22:18:14,748][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 22:18:14,749][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 22:18:14,749][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 22:18:14,750][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 22:18:14,750][train.py][INFO] - Instantiating loggers...
[2024-06-20 22:18:14,751][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 22:18:14,752][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 22:18:14,781][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/easy_sleep/0.3/6/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.10it/s v_num: 0.000      
                                                              val/auc: 0.361    
                                                              val/f1: 0.250     
                                                              val/precision:    
                                                              0.286 val/recall: 
                                                              0.222 val/mre:    
                                                              0.130 train/auc:  
                                                              0.925 train/f1:   
                                                              0.919             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              0.850 train/mre:  
                                                              0.130             
[2024-06-20 22:20:10,715][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/easy_sleep/0.3/6>
[2024-06-20 22:20:10,716][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 22:20:10,718][HYDRA] 	#137 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=easy_sleep data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=7
[rank: 0] Seed set to 7
[2024-06-20 22:20:10,900][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 22:20:10,902][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-20 22:20:10,903][train.py][INFO] - Instantiating callbacks...
[2024-06-20 22:20:10,903][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 22:20:10,905][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 22:20:10,906][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 22:20:10,906][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 22:20:10,909][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 22:20:10,909][train.py][INFO] - Instantiating loggers...
[2024-06-20 22:20:10,909][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 22:20:10,911][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
[2024-06-20 22:20:10,977][train.py][INFO] - Starting training...
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/easy_sleep/0.3/7/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.11it/s v_num: 0.000      
                                                              val/auc: 0.411    
                                                              val/f1: 0.267     
                                                              val/precision:    
                                                              0.333 val/recall: 
                                                              0.222 val/mre:    
                                                              0.137 train/auc:  
                                                              0.950 train/f1:   
                                                              0.947             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              0.900 train/mre:  
                                                              0.142             
[2024-06-20 22:22:07,994][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/easy_sleep/0.3/7>
[2024-06-20 22:22:07,995][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 22:22:07,998][HYDRA] 	#138 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=easy_sleep data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=8
[rank: 0] Seed set to 8
[2024-06-20 22:22:08,171][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 22:22:08,173][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-20 22:22:08,174][train.py][INFO] - Instantiating callbacks...
[2024-06-20 22:22:08,174][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 22:22:08,177][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 22:22:08,177][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 22:22:08,178][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 22:22:08,178][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 22:22:08,179][train.py][INFO] - Instantiating loggers...
[2024-06-20 22:22:08,179][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 22:22:08,180][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 22:22:08,208][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/easy_sleep/0.3/8/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.09it/s v_num: 0.000      
                                                              val/auc: 0.467    
                                                              val/f1: 0.375     
                                                              val/precision:    
                                                              0.429 val/recall: 
                                                              0.333 val/mre:    
                                                              0.135 train/auc:  
                                                              0.967 train/f1:   
                                                              0.966             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              0.933 train/mre:  
                                                              0.140             
[2024-06-20 22:24:05,180][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/easy_sleep/0.3/8>
[2024-06-20 22:24:05,180][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 22:24:05,183][HYDRA] 	#139 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=easy_sleep data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=9
[rank: 0] Seed set to 9
[2024-06-20 22:24:05,360][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 22:24:05,362][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-20 22:24:05,363][train.py][INFO] - Instantiating callbacks...
[2024-06-20 22:24:05,363][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 22:24:05,365][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 22:24:05,366][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 22:24:05,366][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 22:24:05,367][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 22:24:05,367][train.py][INFO] - Instantiating loggers...
[2024-06-20 22:24:05,367][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 22:24:05,369][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 22:24:05,398][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/easy_sleep/0.3/9/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.10it/s v_num: 0.000      
                                                              val/auc: 0.406    
                                                              val/f1: 0.154     
                                                              val/precision:    
                                                              0.250 val/recall: 
                                                              0.111 val/mre:    
                                                              0.132 train/auc:  
                                                              0.975 train/f1:   
                                                              0.974             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              0.950 train/mre:  
                                                              0.138             
[2024-06-20 22:26:01,157][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/easy_sleep/0.3/9>
[2024-06-20 22:26:01,158][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 22:26:01,161][HYDRA] 	#140 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=feeling_lately data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=0
[2024-06-20 22:26:01,345][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 22:26:01,347][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-20 22:26:01,348][train.py][INFO] - Instantiating callbacks...
[2024-06-20 22:26:01,348][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 22:26:01,351][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 22:26:01,351][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 22:26:01,352][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 22:26:01,354][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 22:26:01,355][train.py][INFO] - Instantiating loggers...
[2024-06-20 22:26:01,355][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 22:26:01,356][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 22:26:01,424][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/feeling_lately/0.0/0/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.28it/s v_num: 0.000      
                                                              val/auc: 0.356    
                                                              val/f1: 0.143     
                                                              val/precision:    
                                                              0.200 val/recall: 
                                                              0.111 val/mre: nan
                                                              train/auc: 0.954  
                                                              train/f1: 0.951   
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              0.907 train/mre:  
                                                              nan               
[2024-06-20 22:27:51,553][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/feeling_lately/0.0/0>
[2024-06-20 22:27:51,554][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 22:27:51,556][HYDRA] 	#141 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=feeling_lately data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=1
[rank: 0] Seed set to 1
[2024-06-20 22:27:51,736][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 22:27:51,738][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-20 22:27:51,739][train.py][INFO] - Instantiating callbacks...
[2024-06-20 22:27:51,739][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 22:27:51,741][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 22:27:51,742][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 22:27:51,742][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 22:27:51,743][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 22:27:51,743][train.py][INFO] - Instantiating loggers...
[2024-06-20 22:27:51,743][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 22:27:51,745][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 22:27:51,772][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/feeling_lately/0.0/1/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.31it/s v_num: 0.000      
                                                              val/auc: 0.406    
                                                              val/f1: 0.154     
                                                              val/precision:    
                                                              0.250 val/recall: 
                                                              0.111 val/mre: nan
                                                              train/auc: 0.935  
                                                              train/f1: 0.938   
                                                              train/precision:  
                                                              0.898             
                                                              train/recall:     
                                                              0.981 train/mre:  
                                                              nan               
[2024-06-20 22:29:40,603][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/feeling_lately/0.0/1>
[2024-06-20 22:29:40,604][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 22:29:40,606][HYDRA] 	#142 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=feeling_lately data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=2
[rank: 0] Seed set to 2
[2024-06-20 22:29:40,782][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 22:29:40,783][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-20 22:29:40,784][train.py][INFO] - Instantiating callbacks...
[2024-06-20 22:29:40,785][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 22:29:40,787][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 22:29:40,787][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 22:29:40,788][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 22:29:40,788][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 22:29:40,789][train.py][INFO] - Instantiating loggers...
[2024-06-20 22:29:40,789][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 22:29:40,790][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 22:29:40,819][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/feeling_lately/0.0/2/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.29it/s v_num: 0.000      
                                                              val/auc: 0.406    
                                                              val/f1: 0.154     
                                                              val/precision:    
                                                              0.250 val/recall: 
                                                              0.111 val/mre: nan
                                                              train/auc: 0.991  
                                                              train/f1: 0.991   
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              0.981 train/mre:  
                                                              nan               
[2024-06-20 22:31:30,170][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/feeling_lately/0.0/2>
[2024-06-20 22:31:30,171][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 22:31:30,174][HYDRA] 	#143 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=feeling_lately data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=3
[rank: 0] Seed set to 3
[2024-06-20 22:31:30,347][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 22:31:30,349][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-20 22:31:30,350][train.py][INFO] - Instantiating callbacks...
[2024-06-20 22:31:30,350][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 22:31:30,353][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 22:31:30,353][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 22:31:30,354][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 22:31:30,354][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 22:31:30,355][train.py][INFO] - Instantiating loggers...
[2024-06-20 22:31:30,355][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 22:31:30,356][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 22:31:30,385][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/feeling_lately/0.0/3/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.35it/s v_num: 0.000      
                                                              val/auc: 0.417    
                                                              val/f1: 0.353     
                                                              val/precision:    
                                                              0.375 val/recall: 
                                                              0.333 val/mre: nan
                                                              train/auc: 0.963  
                                                              train/f1: 0.962   
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              0.926 train/mre:  
                                                              nan               
[2024-06-20 22:33:20,347][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/feeling_lately/0.0/3>
[2024-06-20 22:33:20,347][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 22:33:20,350][HYDRA] 	#144 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=feeling_lately data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=4
[rank: 0] Seed set to 4
[2024-06-20 22:33:20,523][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 22:33:20,524][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-20 22:33:20,525][train.py][INFO] - Instantiating callbacks...
[2024-06-20 22:33:20,526][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 22:33:20,528][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 22:33:20,528][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 22:33:20,529][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 22:33:20,529][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 22:33:20,530][train.py][INFO] - Instantiating loggers...
[2024-06-20 22:33:20,530][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 22:33:20,531][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 22:33:20,559][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/feeling_lately/0.0/4/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.28it/s v_num: 0.000      
                                                              val/auc: 0.361    
                                                              val/f1: 0.250     
                                                              val/precision:    
                                                              0.286 val/recall: 
                                                              0.222 val/mre: nan
                                                              train/auc: 0.991  
                                                              train/f1: 0.991   
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              0.981 train/mre:  
                                                              nan               
[2024-06-20 22:35:09,968][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/feeling_lately/0.0/4>
[2024-06-20 22:35:09,969][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 22:35:09,972][HYDRA] 	#145 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=feeling_lately data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=5
[rank: 0] Seed set to 5
[2024-06-20 22:35:10,335][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 22:35:10,337][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-20 22:35:10,339][train.py][INFO] - Instantiating callbacks...
[2024-06-20 22:35:10,339][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 22:35:10,343][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 22:35:10,343][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 22:35:10,343][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 22:35:10,346][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 22:35:10,346][train.py][INFO] - Instantiating loggers...
[2024-06-20 22:35:10,347][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 22:35:10,348][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 22:35:10,583][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/feeling_lately/0.0/5/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.25it/s v_num: 0.000      
                                                              val/auc: 0.361    
                                                              val/f1: 0.250     
                                                              val/precision:    
                                                              0.286 val/recall: 
                                                              0.222 val/mre: nan
                                                              train/auc: 0.991  
                                                              train/f1: 0.991   
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              0.981 train/mre:  
                                                              nan               
[2024-06-20 22:37:00,178][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/feeling_lately/0.0/5>
[2024-06-20 22:37:00,178][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 22:37:00,182][HYDRA] 	#146 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=feeling_lately data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=6
[rank: 0] Seed set to 6
[2024-06-20 22:37:00,386][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 22:37:00,388][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-20 22:37:00,389][train.py][INFO] - Instantiating callbacks...
[2024-06-20 22:37:00,389][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 22:37:00,392][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 22:37:00,392][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 22:37:00,393][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 22:37:00,394][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 22:37:00,394][train.py][INFO] - Instantiating loggers...
[2024-06-20 22:37:00,394][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 22:37:00,396][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 22:37:00,432][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/feeling_lately/0.0/6/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.29it/s v_num: 0.000      
                                                              val/auc: 0.406    
                                                              val/f1: 0.154     
                                                              val/precision:    
                                                              0.250 val/recall: 
                                                              0.111 val/mre: nan
                                                              train/auc: 0.963  
                                                              train/f1: 0.962   
                                                              train/precision:  
                                                              0.981             
                                                              train/recall:     
                                                              0.944 train/mre:  
                                                              nan               
[2024-06-20 22:38:49,701][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/feeling_lately/0.0/6>
[2024-06-20 22:38:49,702][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 22:38:49,704][HYDRA] 	#147 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=feeling_lately data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=7
[rank: 0] Seed set to 7
[2024-06-20 22:38:49,881][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 22:38:49,882][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-20 22:38:49,883][train.py][INFO] - Instantiating callbacks...
[2024-06-20 22:38:49,884][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 22:38:49,886][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 22:38:49,886][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 22:38:49,887][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 22:38:49,887][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 22:38:49,888][train.py][INFO] - Instantiating loggers...
[2024-06-20 22:38:49,888][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 22:38:49,889][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 22:38:49,917][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/feeling_lately/0.0/7/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.30it/s v_num: 0.000      
                                                              val/auc: 0.356    
                                                              val/f1: 0.143     
                                                              val/precision:    
                                                              0.200 val/recall: 
                                                              0.111 val/mre: nan
                                                              train/auc: 0.981  
                                                              train/f1: 0.981   
                                                              train/precision:  
                                                              0.981             
                                                              train/recall:     
                                                              0.981 train/mre:  
                                                              nan               
[2024-06-20 22:40:39,290][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/feeling_lately/0.0/7>
[2024-06-20 22:40:39,291][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 22:40:39,293][HYDRA] 	#148 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=feeling_lately data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=8
[rank: 0] Seed set to 8
[2024-06-20 22:40:39,465][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 22:40:39,467][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-20 22:40:39,468][train.py][INFO] - Instantiating callbacks...
[2024-06-20 22:40:39,468][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 22:40:39,470][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 22:40:39,471][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 22:40:39,471][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 22:40:39,472][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 22:40:39,472][train.py][INFO] - Instantiating loggers...
[2024-06-20 22:40:39,472][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 22:40:39,473][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 22:40:39,502][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/feeling_lately/0.0/8/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.30it/s v_num: 0.000      
                                                              val/auc: 0.372    
                                                              val/f1: 0.400     
                                                              val/precision:    
                                                              0.364 val/recall: 
                                                              0.444 val/mre: nan
                                                              train/auc: 0.889  
                                                              train/f1: 0.880   
                                                              train/precision:  
                                                              0.957             
                                                              train/recall:     
                                                              0.815 train/mre:  
                                                              nan               
[2024-06-20 22:42:29,022][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/feeling_lately/0.0/8>
[2024-06-20 22:42:29,023][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 22:42:29,026][HYDRA] 	#149 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=feeling_lately data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=9
[rank: 0] Seed set to 9
[2024-06-20 22:42:29,227][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 22:42:29,228][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-20 22:42:29,230][train.py][INFO] - Instantiating callbacks...
[2024-06-20 22:42:29,230][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 22:42:29,232][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 22:42:29,233][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 22:42:29,233][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 22:42:29,236][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 22:42:29,237][train.py][INFO] - Instantiating loggers...
[2024-06-20 22:42:29,237][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 22:42:29,238][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 22:42:29,314][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/feeling_lately/0.0/9/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.30it/s v_num: 0.000      
                                                              val/auc: 0.356    
                                                              val/f1: 0.143     
                                                              val/precision:    
                                                              0.200 val/recall: 
                                                              0.111 val/mre: nan
                                                              train/auc: 0.935  
                                                              train/f1: 0.937   
                                                              train/precision:  
                                                              0.912             
                                                              train/recall:     
                                                              0.963 train/mre:  
                                                              nan               
[2024-06-20 22:44:18,441][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/feeling_lately/0.0/9>
[2024-06-20 22:44:18,442][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 22:44:18,445][HYDRA] 	#150 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=feeling_lately data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=0
[2024-06-20 22:44:18,932][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 22:44:18,934][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-20 22:44:18,935][train.py][INFO] - Instantiating callbacks...
[2024-06-20 22:44:18,935][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 22:44:18,939][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 22:44:18,939][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 22:44:18,940][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 22:44:18,940][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 22:44:18,941][train.py][INFO] - Instantiating loggers...
[2024-06-20 22:44:18,941][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 22:44:18,942][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 22:44:19,063][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/feeling_lately/0.05/0/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.30it/s v_num: 0.000      
                                                              val/auc: 0.356    
                                                              val/f1: 0.143     
                                                              val/precision:    
                                                              0.200 val/recall: 
                                                              0.111 val/mre:    
                                                              0.156 train/auc:  
                                                              0.954 train/f1:   
                                                              0.952             
                                                              train/precision:  
                                                              0.980             
                                                              train/recall:     
                                                              0.926 train/mre:  
                                                              0.125             
[2024-06-20 22:46:08,597][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/feeling_lately/0.05/0>
[2024-06-20 22:46:08,598][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 22:46:08,601][HYDRA] 	#151 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=feeling_lately data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=1
[rank: 0] Seed set to 1
[2024-06-20 22:46:08,779][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 22:46:08,781][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-20 22:46:08,782][train.py][INFO] - Instantiating callbacks...
[2024-06-20 22:46:08,783][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 22:46:08,785][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 22:46:08,785][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 22:46:08,786][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 22:46:08,786][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 22:46:08,787][train.py][INFO] - Instantiating loggers...
[2024-06-20 22:46:08,787][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 22:46:08,788][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 22:46:08,816][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/feeling_lately/0.05/1/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.30it/s v_num: 0.000      
                                                              val/auc: 0.411    
                                                              val/f1: 0.267     
                                                              val/precision:    
                                                              0.333 val/recall: 
                                                              0.222 val/mre:    
                                                              0.155 train/auc:  
                                                              0.972 train/f1:   
                                                              0.972             
                                                              train/precision:  
                                                              0.964             
                                                              train/recall:     
                                                              0.981 train/mre:  
                                                              0.125             
[2024-06-20 22:47:58,824][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/feeling_lately/0.05/1>
[2024-06-20 22:47:58,824][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 22:47:58,827][HYDRA] 	#152 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=feeling_lately data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=2
[rank: 0] Seed set to 2
[2024-06-20 22:47:59,003][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 22:47:59,005][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-20 22:47:59,006][train.py][INFO] - Instantiating callbacks...
[2024-06-20 22:47:59,006][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 22:47:59,008][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 22:47:59,009][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 22:47:59,009][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 22:47:59,010][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 22:47:59,010][train.py][INFO] - Instantiating loggers...
[2024-06-20 22:47:59,011][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 22:47:59,012][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 22:47:59,040][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/feeling_lately/0.05/2/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.35it/s v_num: 0.000      
                                                              val/auc: 0.361    
                                                              val/f1: 0.250     
                                                              val/precision:    
                                                              0.286 val/recall: 
                                                              0.222 val/mre:    
                                                              0.154 train/auc:  
                                                              0.981 train/f1:   
                                                              0.982             
                                                              train/precision:  
                                                              0.964             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.124             
[2024-06-20 22:49:49,207][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/feeling_lately/0.05/2>
[2024-06-20 22:49:49,208][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 22:49:49,210][HYDRA] 	#153 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=feeling_lately data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=3
[rank: 0] Seed set to 3
[2024-06-20 22:49:49,384][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 22:49:49,385][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-20 22:49:49,386][train.py][INFO] - Instantiating callbacks...
[2024-06-20 22:49:49,386][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 22:49:49,388][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 22:49:49,389][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 22:49:49,389][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 22:49:49,390][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 22:49:49,390][train.py][INFO] - Instantiating loggers...
[2024-06-20 22:49:49,391][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 22:49:49,392][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 22:49:49,419][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/feeling_lately/0.05/3/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.30it/s v_num: 0.000      
                                                              val/auc: 0.361    
                                                              val/f1: 0.250     
                                                              val/precision:    
                                                              0.286 val/recall: 
                                                              0.222 val/mre:    
                                                              0.152 train/auc:  
                                                              0.972 train/f1:   
                                                              0.972             
                                                              train/precision:  
                                                              0.981             
                                                              train/recall:     
                                                              0.963 train/mre:  
                                                              0.125             
[2024-06-20 22:51:40,085][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/feeling_lately/0.05/3>
[2024-06-20 22:51:40,085][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 22:51:40,088][HYDRA] 	#154 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=feeling_lately data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=4
[rank: 0] Seed set to 4
[2024-06-20 22:51:40,272][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 22:51:40,273][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-20 22:51:40,275][train.py][INFO] - Instantiating callbacks...
[2024-06-20 22:51:40,275][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 22:51:40,277][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 22:51:40,278][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 22:51:40,278][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 22:51:40,280][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 22:51:40,281][train.py][INFO] - Instantiating loggers...
[2024-06-20 22:51:40,281][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 22:51:40,282][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 22:51:40,348][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/feeling_lately/0.05/4/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.28it/s v_num: 0.000      
                                                              val/auc: 0.361    
                                                              val/f1: 0.250     
                                                              val/precision:    
                                                              0.286 val/recall: 
                                                              0.222 val/mre:    
                                                              0.153 train/auc:  
                                                              0.972 train/f1:   
                                                              0.971             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              0.944 train/mre:  
                                                              0.124             
[2024-06-20 22:53:31,285][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/feeling_lately/0.05/4>
[2024-06-20 22:53:31,285][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 22:53:31,288][HYDRA] 	#155 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=feeling_lately data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=5
[rank: 0] Seed set to 5
[2024-06-20 22:53:31,465][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 22:53:31,466][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-20 22:53:31,468][train.py][INFO] - Instantiating callbacks...
[2024-06-20 22:53:31,468][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 22:53:31,470][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 22:53:31,471][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 22:53:31,471][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 22:53:31,472][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 22:53:31,472][train.py][INFO] - Instantiating loggers...
[2024-06-20 22:53:31,472][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 22:53:31,473][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 22:53:31,502][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/feeling_lately/0.05/5/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.31it/s v_num: 0.000      
                                                              val/auc: 0.361    
                                                              val/f1: 0.250     
                                                              val/precision:    
                                                              0.286 val/recall: 
                                                              0.222 val/mre:    
                                                              0.153 train/auc:  
                                                              0.991 train/f1:   
                                                              0.991             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              0.981 train/mre:  
                                                              0.123             
[2024-06-20 22:55:21,822][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/feeling_lately/0.05/5>
[2024-06-20 22:55:21,823][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 22:55:21,825][HYDRA] 	#156 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=feeling_lately data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=6
[rank: 0] Seed set to 6
[2024-06-20 22:55:22,001][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 22:55:22,003][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-20 22:55:22,004][train.py][INFO] - Instantiating callbacks...
[2024-06-20 22:55:22,004][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 22:55:22,007][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 22:55:22,007][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 22:55:22,008][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 22:55:22,008][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 22:55:22,009][train.py][INFO] - Instantiating loggers...
[2024-06-20 22:55:22,009][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 22:55:22,010][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 22:55:22,038][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/feeling_lately/0.05/6/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.29it/s v_num: 0.000      
                                                              val/auc: 0.361    
                                                              val/f1: 0.250     
                                                              val/precision:    
                                                              0.286 val/recall: 
                                                              0.222 val/mre:    
                                                              0.148 train/auc:  
                                                              0.926 train/f1:   
                                                              0.927             
                                                              train/precision:  
                                                              0.911             
                                                              train/recall:     
                                                              0.944 train/mre:  
                                                              0.123             
[2024-06-20 22:57:11,482][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/feeling_lately/0.05/6>
[2024-06-20 22:57:11,482][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 22:57:11,523][HYDRA] 	#157 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=feeling_lately data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=7
[rank: 0] Seed set to 7
[2024-06-20 22:57:11,707][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 22:57:11,708][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-20 22:57:11,710][train.py][INFO] - Instantiating callbacks...
[2024-06-20 22:57:11,710][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 22:57:11,712][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 22:57:11,712][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 22:57:11,713][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 22:57:11,714][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 22:57:11,714][train.py][INFO] - Instantiating loggers...
[2024-06-20 22:57:11,714][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 22:57:11,716][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 22:57:11,755][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/feeling_lately/0.05/7/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.35it/s v_num: 0.000      
                                                              val/auc: 0.561    
                                                              val/f1: 0.333     
                                                              val/precision:    
                                                              0.667 val/recall: 
                                                              0.222 val/mre:    
                                                              0.154 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.126             
[2024-06-20 22:59:01,551][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/feeling_lately/0.05/7>
[2024-06-20 22:59:01,551][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 22:59:01,554][HYDRA] 	#158 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=feeling_lately data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=8
[rank: 0] Seed set to 8
[2024-06-20 22:59:01,762][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 22:59:01,763][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-20 22:59:01,765][train.py][INFO] - Instantiating callbacks...
[2024-06-20 22:59:01,765][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 22:59:01,768][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 22:59:01,768][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 22:59:01,769][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 22:59:01,769][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 22:59:01,770][train.py][INFO] - Instantiating loggers...
[2024-06-20 22:59:01,770][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 22:59:01,771][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 22:59:01,806][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/feeling_lately/0.05/8/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.29it/s v_num: 0.000      
                                                              val/auc: 0.361    
                                                              val/f1: 0.250     
                                                              val/precision:    
                                                              0.286 val/recall: 
                                                              0.222 val/mre:    
                                                              0.153 train/auc:  
                                                              0.963 train/f1:   
                                                              0.962             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              0.926 train/mre:  
                                                              0.128             
[2024-06-20 23:00:50,832][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/feeling_lately/0.05/8>
[2024-06-20 23:00:50,832][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 23:00:50,835][HYDRA] 	#159 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=feeling_lately data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=9
[rank: 0] Seed set to 9
[2024-06-20 23:00:51,022][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 23:00:51,023][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-20 23:00:51,025][train.py][INFO] - Instantiating callbacks...
[2024-06-20 23:00:51,025][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 23:00:51,027][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 23:00:51,028][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 23:00:51,028][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 23:00:51,031][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 23:00:51,031][train.py][INFO] - Instantiating loggers...
[2024-06-20 23:00:51,031][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 23:00:51,033][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 23:00:51,098][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/feeling_lately/0.05/9/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.29it/s v_num: 0.000      
                                                              val/auc: 0.317    
                                                              val/f1: 0.316     
                                                              val/precision:    
                                                              0.300 val/recall: 
                                                              0.333 val/mre:    
                                                              0.150 train/auc:  
                                                              0.935 train/f1:   
                                                              0.936             
                                                              train/precision:  
                                                              0.927             
                                                              train/recall:     
                                                              0.944 train/mre:  
                                                              0.122             
[2024-06-20 23:02:41,772][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/feeling_lately/0.05/9>
[2024-06-20 23:02:41,773][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 23:02:41,775][HYDRA] 	#160 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=feeling_lately data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=0
[2024-06-20 23:02:41,952][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 23:02:41,954][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-20 23:02:41,955][train.py][INFO] - Instantiating callbacks...
[2024-06-20 23:02:41,956][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 23:02:41,958][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 23:02:41,958][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 23:02:41,959][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 23:02:41,960][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 23:02:41,960][train.py][INFO] - Instantiating loggers...
[2024-06-20 23:02:41,960][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 23:02:41,962][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 23:02:41,992][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/feeling_lately/0.1/0/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.30it/s v_num: 0.000      
                                                              val/auc: 0.406    
                                                              val/f1: 0.154     
                                                              val/precision:    
                                                              0.250 val/recall: 
                                                              0.111 val/mre:    
                                                              0.165 train/auc:  
                                                              0.963 train/f1:   
                                                              0.963             
                                                              train/precision:  
                                                              0.963             
                                                              train/recall:     
                                                              0.963 train/mre:  
                                                              0.134             
[2024-06-20 23:04:31,862][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/feeling_lately/0.1/0>
[2024-06-20 23:04:31,862][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 23:04:31,865][HYDRA] 	#161 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=feeling_lately data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=1
[rank: 0] Seed set to 1
[2024-06-20 23:04:32,040][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 23:04:32,041][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-20 23:04:32,043][train.py][INFO] - Instantiating callbacks...
[2024-06-20 23:04:32,043][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 23:04:32,045][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 23:04:32,046][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 23:04:32,046][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 23:04:32,047][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 23:04:32,048][train.py][INFO] - Instantiating loggers...
[2024-06-20 23:04:32,048][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 23:04:32,049][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 23:04:32,077][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/feeling_lately/0.1/1/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.29it/s v_num: 0.000      
                                                              val/auc: 0.406    
                                                              val/f1: 0.154     
                                                              val/precision:    
                                                              0.250 val/recall: 
                                                              0.111 val/mre:    
                                                              0.162 train/auc:  
                                                              0.981 train/f1:   
                                                              0.981             
                                                              train/precision:  
                                                              0.981             
                                                              train/recall:     
                                                              0.981 train/mre:  
                                                              0.134             
[2024-06-20 23:06:22,785][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/feeling_lately/0.1/1>
[2024-06-20 23:06:22,786][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 23:06:22,788][HYDRA] 	#162 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=feeling_lately data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=2
[rank: 0] Seed set to 2
[2024-06-20 23:06:22,965][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 23:06:22,967][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-20 23:06:22,968][train.py][INFO] - Instantiating callbacks...
[2024-06-20 23:06:22,968][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 23:06:22,970][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 23:06:22,971][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 23:06:22,972][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 23:06:22,973][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 23:06:22,973][train.py][INFO] - Instantiating loggers...
[2024-06-20 23:06:22,973][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 23:06:22,975][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 23:06:23,004][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/feeling_lately/0.1/2/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.36it/s v_num: 0.000      
                                                              val/auc: 0.361    
                                                              val/f1: 0.250     
                                                              val/precision:    
                                                              0.286 val/recall: 
                                                              0.222 val/mre:    
                                                              0.160 train/auc:  
                                                              0.991 train/f1:   
                                                              0.991             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              0.981 train/mre:  
                                                              0.132             
[2024-06-20 23:08:11,515][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/feeling_lately/0.1/2>
[2024-06-20 23:08:11,515][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 23:08:11,518][HYDRA] 	#163 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=feeling_lately data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=3
[rank: 0] Seed set to 3
[2024-06-20 23:08:11,689][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 23:08:11,691][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-20 23:08:11,692][train.py][INFO] - Instantiating callbacks...
[2024-06-20 23:08:11,692][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 23:08:11,694][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 23:08:11,695][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 23:08:11,695][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 23:08:11,696][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 23:08:11,696][train.py][INFO] - Instantiating loggers...
[2024-06-20 23:08:11,696][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 23:08:11,698][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 23:08:11,725][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/feeling_lately/0.1/3/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.29it/s v_num: 0.000      
                                                              val/auc: 0.461    
                                                              val/f1: 0.286     
                                                              val/precision:    
                                                              0.400 val/recall: 
                                                              0.222 val/mre:    
                                                              0.159 train/auc:  
                                                              0.972 train/f1:   
                                                              0.972             
                                                              train/precision:  
                                                              0.964             
                                                              train/recall:     
                                                              0.981 train/mre:  
                                                              0.132             
[2024-06-20 23:10:00,733][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/feeling_lately/0.1/3>
[2024-06-20 23:10:00,733][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 23:10:00,736][HYDRA] 	#164 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=feeling_lately data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=4
[rank: 0] Seed set to 4
[2024-06-20 23:10:00,914][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 23:10:00,916][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-20 23:10:00,917][train.py][INFO] - Instantiating callbacks...
[2024-06-20 23:10:00,918][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 23:10:00,920][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 23:10:00,920][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 23:10:00,921][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 23:10:00,923][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 23:10:00,924][train.py][INFO] - Instantiating loggers...
[2024-06-20 23:10:00,924][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 23:10:00,925][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 23:10:00,992][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/feeling_lately/0.1/4/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.29it/s v_num: 0.000      
                                                              val/auc: 0.361    
                                                              val/f1: 0.250     
                                                              val/precision:    
                                                              0.286 val/recall: 
                                                              0.222 val/mre:    
                                                              0.161 train/auc:  
                                                              0.991 train/f1:   
                                                              0.991             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              0.981 train/mre:  
                                                              0.133             
[2024-06-20 23:11:51,042][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/feeling_lately/0.1/4>
[2024-06-20 23:11:51,043][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 23:11:51,045][HYDRA] 	#165 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=feeling_lately data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=5
[rank: 0] Seed set to 5
[2024-06-20 23:11:51,225][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 23:11:51,227][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-20 23:11:51,228][train.py][INFO] - Instantiating callbacks...
[2024-06-20 23:11:51,228][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 23:11:51,231][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 23:11:51,231][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 23:11:51,231][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 23:11:51,232][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 23:11:51,232][train.py][INFO] - Instantiating loggers...
[2024-06-20 23:11:51,233][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 23:11:51,234][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 23:11:51,262][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/feeling_lately/0.1/5/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.27it/s v_num: 0.000      
                                                              val/auc: 0.478    
                                                              val/f1: 0.500     
                                                              val/precision:    
                                                              0.455 val/recall: 
                                                              0.556 val/mre:    
                                                              0.160 train/auc:  
                                                              0.963 train/f1:   
                                                              0.962             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              0.926 train/mre:  
                                                              0.131             
[2024-06-20 23:13:40,642][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/feeling_lately/0.1/5>
[2024-06-20 23:13:40,642][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 23:13:40,645][HYDRA] 	#166 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=feeling_lately data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=6
[rank: 0] Seed set to 6
[2024-06-20 23:13:40,981][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 23:13:40,983][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-20 23:13:40,984][train.py][INFO] - Instantiating callbacks...
[2024-06-20 23:13:40,984][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 23:13:40,989][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 23:13:40,989][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 23:13:40,990][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 23:13:40,990][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 23:13:40,991][train.py][INFO] - Instantiating loggers...
[2024-06-20 23:13:40,991][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 23:13:40,992][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 23:13:41,110][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/feeling_lately/0.1/6/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.26it/s v_num: 0.000      
                                                              val/auc: 0.511    
                                                              val/f1: 0.308     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.222 val/mre:    
                                                              0.156 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.128             
[2024-06-20 23:15:31,130][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/feeling_lately/0.1/6>
[2024-06-20 23:15:31,130][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 23:15:31,133][HYDRA] 	#167 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=feeling_lately data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=7
[rank: 0] Seed set to 7
[2024-06-20 23:15:31,310][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 23:15:31,311][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-20 23:15:31,312][train.py][INFO] - Instantiating callbacks...
[2024-06-20 23:15:31,313][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 23:15:31,315][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 23:15:31,315][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 23:15:31,316][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 23:15:31,316][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 23:15:31,317][train.py][INFO] - Instantiating loggers...
[2024-06-20 23:15:31,317][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 23:15:31,318][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 23:15:31,347][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/feeling_lately/0.1/7/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.36it/s v_num: 0.000      
                                                              val/auc: 0.517    
                                                              val/f1: 0.400     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.333 val/mre:    
                                                              0.164 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.136             
[2024-06-20 23:17:20,267][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/feeling_lately/0.1/7>
[2024-06-20 23:17:20,268][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 23:17:20,270][HYDRA] 	#168 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=feeling_lately data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=8
[rank: 0] Seed set to 8
[2024-06-20 23:17:20,473][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 23:17:20,474][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-20 23:17:20,475][train.py][INFO] - Instantiating callbacks...
[2024-06-20 23:17:20,476][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 23:17:20,478][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 23:17:20,478][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 23:17:20,479][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 23:17:20,479][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 23:17:20,480][train.py][INFO] - Instantiating loggers...
[2024-06-20 23:17:20,480][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 23:17:20,481][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 23:17:20,519][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/feeling_lately/0.1/8/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.28it/s v_num: 0.000      
                                                              val/auc: 0.417    
                                                              val/f1: 0.353     
                                                              val/precision:    
                                                              0.375 val/recall: 
                                                              0.333 val/mre:    
                                                              0.162 train/auc:  
                                                              0.935 train/f1:   
                                                              0.931             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              0.870 train/mre:  
                                                              0.135             
[2024-06-20 23:19:09,826][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/feeling_lately/0.1/8>
[2024-06-20 23:19:09,826][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 23:19:09,828][HYDRA] 	#169 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=feeling_lately data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=9
[rank: 0] Seed set to 9
[2024-06-20 23:19:10,010][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 23:19:10,011][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-20 23:19:10,013][train.py][INFO] - Instantiating callbacks...
[2024-06-20 23:19:10,013][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 23:19:10,015][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 23:19:10,016][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 23:19:10,016][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 23:19:10,019][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 23:19:10,019][train.py][INFO] - Instantiating loggers...
[2024-06-20 23:19:10,019][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 23:19:10,020][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 23:19:10,086][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/feeling_lately/0.1/9/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.29it/s v_num: 0.000      
                                                              val/auc: 0.522    
                                                              val/f1: 0.471     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.444 val/mre:    
                                                              0.157 train/auc:  
                                                              0.963 train/f1:   
                                                              0.964             
                                                              train/precision:  
                                                              0.946             
                                                              train/recall:     
                                                              0.981 train/mre:  
                                                              0.129             
[2024-06-20 23:21:01,295][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/feeling_lately/0.1/9>
[2024-06-20 23:21:01,295][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 23:21:01,298][HYDRA] 	#170 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=feeling_lately data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=0
[2024-06-20 23:21:01,478][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 23:21:01,480][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-20 23:21:01,481][train.py][INFO] - Instantiating callbacks...
[2024-06-20 23:21:01,481][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 23:21:01,484][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 23:21:01,484][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 23:21:01,484][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 23:21:01,486][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 23:21:01,486][train.py][INFO] - Instantiating loggers...
[2024-06-20 23:21:01,486][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 23:21:01,487][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 23:21:01,518][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/feeling_lately/0.15/0/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.29it/s v_num: 0.000      
                                                              val/auc: 0.467    
                                                              val/f1: 0.375     
                                                              val/precision:    
                                                              0.429 val/recall: 
                                                              0.333 val/mre:    
                                                              0.170 train/auc:  
                                                              0.954 train/f1:   
                                                              0.955             
                                                              train/precision:  
                                                              0.930             
                                                              train/recall:     
                                                              0.981 train/mre:  
                                                              0.139             
[2024-06-20 23:22:52,801][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/feeling_lately/0.15/0>
[2024-06-20 23:22:52,801][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 23:22:52,803][HYDRA] 	#171 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=feeling_lately data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=1
[rank: 0] Seed set to 1
[2024-06-20 23:22:52,979][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 23:22:52,981][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-20 23:22:52,982][train.py][INFO] - Instantiating callbacks...
[2024-06-20 23:22:52,983][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 23:22:52,985][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 23:22:52,985][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 23:22:52,986][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 23:22:52,986][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 23:22:52,987][train.py][INFO] - Instantiating loggers...
[2024-06-20 23:22:52,987][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 23:22:52,988][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 23:22:53,016][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/feeling_lately/0.15/1/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.30it/s v_num: 0.000      
                                                              val/auc: 0.456    
                                                              val/f1: 0.167     
                                                              val/precision:    
                                                              0.333 val/recall: 
                                                              0.111 val/mre:    
                                                              0.166 train/auc:  
                                                              0.963 train/f1:   
                                                              0.964             
                                                              train/precision:  
                                                              0.946             
                                                              train/recall:     
                                                              0.981 train/mre:  
                                                              0.140             
[2024-06-20 23:24:44,685][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/feeling_lately/0.15/1>
[2024-06-20 23:24:44,685][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 23:24:44,687][HYDRA] 	#172 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=feeling_lately data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=2
[rank: 0] Seed set to 2
[2024-06-20 23:24:44,860][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 23:24:44,862][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-20 23:24:44,863][train.py][INFO] - Instantiating callbacks...
[2024-06-20 23:24:44,863][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 23:24:44,865][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 23:24:44,866][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 23:24:44,866][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 23:24:44,867][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 23:24:44,867][train.py][INFO] - Instantiating loggers...
[2024-06-20 23:24:44,867][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 23:24:44,868][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 23:24:44,897][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/feeling_lately/0.15/2/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.32it/s v_num: 0.000      
                                                              val/auc: 0.361    
                                                              val/f1: 0.250     
                                                              val/precision:    
                                                              0.286 val/recall: 
                                                              0.222 val/mre:    
                                                              0.166 train/auc:  
                                                              0.935 train/f1:   
                                                              0.931             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              0.870 train/mre:  
                                                              0.136             
[2024-06-20 23:26:34,177][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/feeling_lately/0.15/2>
[2024-06-20 23:26:34,178][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 23:26:34,180][HYDRA] 	#173 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=feeling_lately data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=3
[rank: 0] Seed set to 3
[2024-06-20 23:26:34,362][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 23:26:34,364][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-20 23:26:34,365][train.py][INFO] - Instantiating callbacks...
[2024-06-20 23:26:34,365][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 23:26:34,368][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 23:26:34,368][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 23:26:34,368][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 23:26:34,371][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 23:26:34,372][train.py][INFO] - Instantiating loggers...
[2024-06-20 23:26:34,372][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 23:26:34,373][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 23:26:34,441][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/feeling_lately/0.15/3/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.28it/s v_num: 0.000      
                                                              val/auc: 0.417    
                                                              val/f1: 0.353     
                                                              val/precision:    
                                                              0.375 val/recall: 
                                                              0.333 val/mre:    
                                                              0.166 train/auc:  
                                                              0.991 train/f1:   
                                                              0.991             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              0.981 train/mre:  
                                                              0.139             
[2024-06-20 23:28:24,445][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/feeling_lately/0.15/3>
[2024-06-20 23:28:24,446][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 23:28:24,448][HYDRA] 	#174 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=feeling_lately data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=4
[rank: 0] Seed set to 4
[2024-06-20 23:28:24,628][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 23:28:24,630][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-20 23:28:24,631][train.py][INFO] - Instantiating callbacks...
[2024-06-20 23:28:24,631][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 23:28:24,633][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 23:28:24,634][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 23:28:24,634][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 23:28:24,635][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 23:28:24,635][train.py][INFO] - Instantiating loggers...
[2024-06-20 23:28:24,636][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 23:28:24,637][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 23:28:24,666][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/feeling_lately/0.15/4/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.30it/s v_num: 0.000      
                                                              val/auc: 0.356    
                                                              val/f1: 0.143     
                                                              val/precision:    
                                                              0.200 val/recall: 
                                                              0.111 val/mre:    
                                                              0.167 train/auc:  
                                                              0.991 train/f1:   
                                                              0.991             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              0.981 train/mre:  
                                                              0.141             
[2024-06-20 23:30:14,006][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/feeling_lately/0.15/4>
[2024-06-20 23:30:14,007][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 23:30:14,010][HYDRA] 	#175 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=feeling_lately data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=5
[rank: 0] Seed set to 5
[2024-06-20 23:30:14,187][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 23:30:14,188][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-20 23:30:14,189][train.py][INFO] - Instantiating callbacks...
[2024-06-20 23:30:14,190][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 23:30:14,192][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 23:30:14,192][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 23:30:14,193][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 23:30:14,193][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 23:30:14,194][train.py][INFO] - Instantiating loggers...
[2024-06-20 23:30:14,194][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 23:30:14,195][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 23:30:14,224][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/feeling_lately/0.15/5/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.28it/s v_num: 0.000      
                                                              val/auc: 0.417    
                                                              val/f1: 0.353     
                                                              val/precision:    
                                                              0.375 val/recall: 
                                                              0.333 val/mre:    
                                                              0.166 train/auc:  
                                                              0.954 train/f1:   
                                                              0.951             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              0.907 train/mre:  
                                                              0.137             
[2024-06-20 23:32:03,387][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/feeling_lately/0.15/5>
[2024-06-20 23:32:03,387][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 23:32:03,390][HYDRA] 	#176 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=feeling_lately data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=6
[rank: 0] Seed set to 6
[2024-06-20 23:32:03,562][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 23:32:03,564][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-20 23:32:03,565][train.py][INFO] - Instantiating callbacks...
[2024-06-20 23:32:03,565][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 23:32:03,568][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 23:32:03,568][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 23:32:03,569][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 23:32:03,569][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 23:32:03,570][train.py][INFO] - Instantiating loggers...
[2024-06-20 23:32:03,570][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 23:32:03,571][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 23:32:03,600][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/feeling_lately/0.15/6/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.31it/s v_num: 0.000      
                                                              val/auc: 0.517    
                                                              val/f1: 0.400     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.333 val/mre:    
                                                              0.165 train/auc:  
                                                              0.991 train/f1:   
                                                              0.991             
                                                              train/precision:  
                                                              0.982             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.138             
[2024-06-20 23:33:53,385][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/feeling_lately/0.15/6>
[2024-06-20 23:33:53,385][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 23:33:53,388][HYDRA] 	#177 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=feeling_lately data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=7
[rank: 0] Seed set to 7
[2024-06-20 23:33:53,563][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 23:33:53,564][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-20 23:33:53,565][train.py][INFO] - Instantiating callbacks...
[2024-06-20 23:33:53,566][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 23:33:53,568][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 23:33:53,568][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 23:33:53,569][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 23:33:53,569][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 23:33:53,570][train.py][INFO] - Instantiating loggers...
[2024-06-20 23:33:53,570][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 23:33:53,571][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 23:33:53,600][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/feeling_lately/0.15/7/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.29it/s v_num: 0.000      
                                                              val/auc: 0.461    
                                                              val/f1: 0.286     
                                                              val/precision:    
                                                              0.400 val/recall: 
                                                              0.222 val/mre:    
                                                              0.170 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.142             
[2024-06-20 23:35:43,563][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/feeling_lately/0.15/7>
[2024-06-20 23:35:43,563][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 23:35:43,566][HYDRA] 	#178 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=feeling_lately data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=8
[rank: 0] Seed set to 8
[2024-06-20 23:35:43,748][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 23:35:43,750][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-20 23:35:43,751][train.py][INFO] - Instantiating callbacks...
[2024-06-20 23:35:43,751][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 23:35:43,753][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 23:35:43,754][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 23:35:43,754][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 23:35:43,757][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 23:35:43,758][train.py][INFO] - Instantiating loggers...
[2024-06-20 23:35:43,758][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 23:35:43,759][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 23:35:43,828][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/feeling_lately/0.15/8/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.28it/s v_num: 0.000      
                                                              val/auc: 0.422    
                                                              val/f1: 0.421     
                                                              val/precision:    
                                                              0.400 val/recall: 
                                                              0.444 val/mre:    
                                                              0.167 train/auc:  
                                                              0.963 train/f1:   
                                                              0.962             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              0.926 train/mre:  
                                                              0.140             
[2024-06-20 23:37:33,559][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/feeling_lately/0.15/8>
[2024-06-20 23:37:33,562][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 23:37:33,578][HYDRA] 	#179 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=feeling_lately data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=9
[rank: 0] Seed set to 9
[2024-06-20 23:37:33,867][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 23:37:33,868][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-20 23:37:33,869][train.py][INFO] - Instantiating callbacks...
[2024-06-20 23:37:33,870][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 23:37:33,873][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 23:37:33,873][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 23:37:33,874][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 23:37:33,874][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 23:37:33,875][train.py][INFO] - Instantiating loggers...
[2024-06-20 23:37:33,875][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 23:37:33,876][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 23:37:33,922][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/feeling_lately/0.15/9/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.30it/s v_num: 0.000      
                                                              val/auc: 0.528    
                                                              val/f1: 0.526     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.556 val/mre:    
                                                              0.162 train/auc:  
                                                              0.963 train/f1:   
                                                              0.963             
                                                              train/precision:  
                                                              0.963             
                                                              train/recall:     
                                                              0.963 train/mre:  
                                                              0.134             
[2024-06-20 23:39:22,950][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/feeling_lately/0.15/9>
[2024-06-20 23:39:22,950][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 23:39:22,953][HYDRA] 	#180 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=feeling_lately data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=0
[2024-06-20 23:39:23,130][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 23:39:23,132][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-20 23:39:23,133][train.py][INFO] - Instantiating callbacks...
[2024-06-20 23:39:23,133][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 23:39:23,136][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 23:39:23,136][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 23:39:23,136][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 23:39:23,137][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 23:39:23,137][train.py][INFO] - Instantiating loggers...
[2024-06-20 23:39:23,138][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 23:39:23,139][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 23:39:23,173][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/feeling_lately/0.2/0/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.29it/s v_num: 0.000      
                                                              val/auc: 0.411    
                                                              val/f1: 0.267     
                                                              val/precision:    
                                                              0.333 val/recall: 
                                                              0.222 val/mre:    
                                                              0.172 train/auc:  
                                                              0.935 train/f1:   
                                                              0.936             
                                                              train/precision:  
                                                              0.927             
                                                              train/recall:     
                                                              0.944 train/mre:  
                                                              0.140             
[2024-06-20 23:41:12,106][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/feeling_lately/0.2/0>
[2024-06-20 23:41:12,106][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 23:41:12,109][HYDRA] 	#181 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=feeling_lately data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=1
[rank: 0] Seed set to 1
[2024-06-20 23:41:12,288][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 23:41:12,289][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-20 23:41:12,291][train.py][INFO] - Instantiating callbacks...
[2024-06-20 23:41:12,291][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 23:41:12,293][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 23:41:12,294][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 23:41:12,294][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 23:41:12,295][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 23:41:12,295][train.py][INFO] - Instantiating loggers...
[2024-06-20 23:41:12,296][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 23:41:12,297][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 23:41:12,326][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/feeling_lately/0.2/1/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.34it/s v_num: 0.000      
                                                              val/auc: 0.450    
                                                              val/f1: 0.000     
                                                              val/precision:    
                                                              0.000 val/recall: 
                                                              0.000 val/mre:    
                                                              0.169 train/auc:  
                                                              0.963 train/f1:   
                                                              0.962             
                                                              train/precision:  
                                                              0.981             
                                                              train/recall:     
                                                              0.944 train/mre:  
                                                              0.142             
[2024-06-20 23:43:01,522][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/feeling_lately/0.2/1>
[2024-06-20 23:43:01,523][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 23:43:01,525][HYDRA] 	#182 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=feeling_lately data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=2
[rank: 0] Seed set to 2
[2024-06-20 23:43:01,700][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 23:43:01,701][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-20 23:43:01,703][train.py][INFO] - Instantiating callbacks...
[2024-06-20 23:43:01,703][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 23:43:01,705][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 23:43:01,705][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 23:43:01,706][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 23:43:01,706][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 23:43:01,707][train.py][INFO] - Instantiating loggers...
[2024-06-20 23:43:01,707][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 23:43:01,708][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 23:43:01,741][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/feeling_lately/0.2/2/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.30it/s v_num: 0.000      
                                                              val/auc: 0.356    
                                                              val/f1: 0.143     
                                                              val/precision:    
                                                              0.200 val/recall: 
                                                              0.111 val/mre:    
                                                              0.173 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.146             
[2024-06-20 23:44:50,805][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/feeling_lately/0.2/2>
[2024-06-20 23:44:50,806][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 23:44:50,808][HYDRA] 	#183 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=feeling_lately data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=3
[rank: 0] Seed set to 3
[2024-06-20 23:44:50,998][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 23:44:51,000][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-20 23:44:51,001][train.py][INFO] - Instantiating callbacks...
[2024-06-20 23:44:51,002][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 23:44:51,004][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 23:44:51,004][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 23:44:51,005][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 23:44:51,010][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 23:44:51,010][train.py][INFO] - Instantiating loggers...
[2024-06-20 23:44:51,010][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 23:44:51,012][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 23:44:51,273][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/feeling_lately/0.2/3/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.29it/s v_num: 0.000      
                                                              val/auc: 0.411    
                                                              val/f1: 0.267     
                                                              val/precision:    
                                                              0.333 val/recall: 
                                                              0.222 val/mre:    
                                                              0.171 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.143             
[2024-06-20 23:46:41,669][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/feeling_lately/0.2/3>
[2024-06-20 23:46:41,672][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 23:46:41,690][HYDRA] 	#184 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=feeling_lately data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=4
[rank: 0] Seed set to 4
[2024-06-20 23:46:42,082][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 23:46:42,084][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-20 23:46:42,085][train.py][INFO] - Instantiating callbacks...
[2024-06-20 23:46:42,085][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 23:46:42,090][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 23:46:42,090][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 23:46:42,091][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 23:46:42,092][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 23:46:42,092][train.py][INFO] - Instantiating loggers...
[2024-06-20 23:46:42,092][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 23:46:42,094][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 23:46:42,190][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/feeling_lately/0.2/4/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.29it/s v_num: 0.000      
                                                              val/auc: 0.406    
                                                              val/f1: 0.154     
                                                              val/precision:    
                                                              0.250 val/recall: 
                                                              0.111 val/mre:    
                                                              0.170 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.145             
[2024-06-20 23:48:31,866][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/feeling_lately/0.2/4>
[2024-06-20 23:48:31,866][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 23:48:31,869][HYDRA] 	#185 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=feeling_lately data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=5
[rank: 0] Seed set to 5
[2024-06-20 23:48:32,044][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 23:48:32,046][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-20 23:48:32,047][train.py][INFO] - Instantiating callbacks...
[2024-06-20 23:48:32,048][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 23:48:32,050][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 23:48:32,050][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 23:48:32,051][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 23:48:32,051][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 23:48:32,052][train.py][INFO] - Instantiating loggers...
[2024-06-20 23:48:32,052][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 23:48:32,053][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 23:48:32,081][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/feeling_lately/0.2/5/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.37it/s v_num: 0.000      
                                                              val/auc: 0.478    
                                                              val/f1: 0.500     
                                                              val/precision:    
                                                              0.455 val/recall: 
                                                              0.556 val/mre:    
                                                              0.170 train/auc:  
                                                              0.972 train/f1:   
                                                              0.971             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              0.944 train/mre:  
                                                              0.141             
[2024-06-20 23:50:22,096][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/feeling_lately/0.2/5>
[2024-06-20 23:50:22,097][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 23:50:22,099][HYDRA] 	#186 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=feeling_lately data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=6
[rank: 0] Seed set to 6
[2024-06-20 23:50:22,509][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 23:50:22,510][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-20 23:50:22,511][train.py][INFO] - Instantiating callbacks...
[2024-06-20 23:50:22,511][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 23:50:22,514][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 23:50:22,514][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 23:50:22,515][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 23:50:22,515][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 23:50:22,515][train.py][INFO] - Instantiating loggers...
[2024-06-20 23:50:22,516][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 23:50:22,517][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 23:50:22,546][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/feeling_lately/0.2/6/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.29it/s v_num: 0.000      
                                                              val/auc: 0.361    
                                                              val/f1: 0.250     
                                                              val/precision:    
                                                              0.286 val/recall: 
                                                              0.222 val/mre:    
                                                              0.167 train/auc:  
                                                              0.954 train/f1:   
                                                              0.952             
                                                              train/precision:  
                                                              0.980             
                                                              train/recall:     
                                                              0.926 train/mre:  
                                                              0.140             
[2024-06-20 23:52:11,684][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/feeling_lately/0.2/6>
[2024-06-20 23:52:11,685][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 23:52:11,687][HYDRA] 	#187 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=feeling_lately data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=7
[rank: 0] Seed set to 7
[2024-06-20 23:52:11,880][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 23:52:11,881][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-20 23:52:11,883][train.py][INFO] - Instantiating callbacks...
[2024-06-20 23:52:11,883][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 23:52:11,886][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 23:52:11,887][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 23:52:11,887][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 23:52:11,890][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 23:52:11,891][train.py][INFO] - Instantiating loggers...
[2024-06-20 23:52:11,891][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 23:52:11,892][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 23:52:11,959][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/feeling_lately/0.2/7/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.30it/s v_num: 0.000      
                                                              val/auc: 0.567    
                                                              val/f1: 0.429     
                                                              val/precision:    
                                                              0.600 val/recall: 
                                                              0.333 val/mre:    
                                                              0.174 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.147             
[2024-06-20 23:54:01,546][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/feeling_lately/0.2/7>
[2024-06-20 23:54:01,546][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 23:54:01,549][HYDRA] 	#188 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=feeling_lately data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=8
[rank: 0] Seed set to 8
[2024-06-20 23:54:01,752][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 23:54:01,754][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-20 23:54:01,755][train.py][INFO] - Instantiating callbacks...
[2024-06-20 23:54:01,756][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 23:54:01,758][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 23:54:01,759][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 23:54:01,759][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 23:54:01,760][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 23:54:01,760][train.py][INFO] - Instantiating loggers...
[2024-06-20 23:54:01,760][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 23:54:01,762][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 23:54:01,807][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/feeling_lately/0.2/8/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.33it/s v_num: 0.000      
                                                              val/auc: 0.417    
                                                              val/f1: 0.353     
                                                              val/precision:    
                                                              0.375 val/recall: 
                                                              0.333 val/mre:    
                                                              0.173 train/auc:  
                                                              0.963 train/f1:   
                                                              0.962             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              0.926 train/mre:  
                                                              0.147             
[2024-06-20 23:55:52,148][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/feeling_lately/0.2/8>
[2024-06-20 23:55:52,149][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 23:55:52,151][HYDRA] 	#189 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=feeling_lately data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=9
[rank: 0] Seed set to 9
[2024-06-20 23:55:52,325][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 23:55:52,326][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-20 23:55:52,328][train.py][INFO] - Instantiating callbacks...
[2024-06-20 23:55:52,328][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 23:55:52,330][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 23:55:52,330][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 23:55:52,331][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 23:55:52,331][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 23:55:52,332][train.py][INFO] - Instantiating loggers...
[2024-06-20 23:55:52,332][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 23:55:52,333][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 23:55:52,363][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/feeling_lately/0.2/9/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.30it/s v_num: 0.000      
                                                              val/auc: 0.517    
                                                              val/f1: 0.400     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.333 val/mre:    
                                                              0.168 train/auc:  
                                                              0.963 train/f1:   
                                                              0.964             
                                                              train/precision:  
                                                              0.931             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.140             
[2024-06-20 23:57:41,355][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/feeling_lately/0.2/9>
[2024-06-20 23:57:41,355][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 23:57:41,358][HYDRA] 	#190 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=feeling_lately data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=0
[2024-06-20 23:57:41,540][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 23:57:41,542][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-20 23:57:41,543][train.py][INFO] - Instantiating callbacks...
[2024-06-20 23:57:41,543][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 23:57:41,546][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 23:57:41,546][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 23:57:41,547][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 23:57:41,573][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 23:57:41,573][train.py][INFO] - Instantiating loggers...
[2024-06-20 23:57:41,573][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 23:57:41,575][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 23:57:41,664][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/feeling_lately/0.25/0/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.28it/s v_num: 0.000      
                                                              val/auc: 0.567    
                                                              val/f1: 0.429     
                                                              val/precision:    
                                                              0.600 val/recall: 
                                                              0.333 val/mre:    
                                                              0.176 train/auc:  
                                                              0.963 train/f1:   
                                                              0.963             
                                                              train/precision:  
                                                              0.963             
                                                              train/recall:     
                                                              0.963 train/mre:  
                                                              0.143             
[2024-06-20 23:59:31,288][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/feeling_lately/0.25/0>
[2024-06-20 23:59:31,289][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-20 23:59:31,291][HYDRA] 	#191 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=feeling_lately data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=1
[rank: 0] Seed set to 1
[2024-06-20 23:59:31,475][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-20 23:59:31,477][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-20 23:59:31,478][train.py][INFO] - Instantiating callbacks...
[2024-06-20 23:59:31,479][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-20 23:59:31,481][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-20 23:59:31,481][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-20 23:59:31,482][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-20 23:59:31,482][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-20 23:59:31,483][train.py][INFO] - Instantiating loggers...
[2024-06-20 23:59:31,483][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-20 23:59:31,484][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-20 23:59:31,521][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/feeling_lately/0.25/1/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.27it/s v_num: 0.000      
                                                              val/auc: 0.406    
                                                              val/f1: 0.154     
                                                              val/precision:    
                                                              0.250 val/recall: 
                                                              0.111 val/mre:    
                                                              0.175 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.149             
[2024-06-21 00:01:21,636][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/feeling_lately/0.25/1>
[2024-06-21 00:01:21,637][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 00:01:21,639][HYDRA] 	#192 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=feeling_lately data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=2
[rank: 0] Seed set to 2
[2024-06-21 00:01:21,822][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 00:01:21,824][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-21 00:01:21,825][train.py][INFO] - Instantiating callbacks...
[2024-06-21 00:01:21,825][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 00:01:21,828][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 00:01:21,828][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 00:01:21,829][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 00:01:21,830][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 00:01:21,831][train.py][INFO] - Instantiating loggers...
[2024-06-21 00:01:21,831][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 00:01:21,832][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 00:01:21,862][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/feeling_lately/0.25/2/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.37it/s v_num: 0.000      
                                                              val/auc: 0.511    
                                                              val/f1: 0.308     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.222 val/mre:    
                                                              0.174 train/auc:  
                                                              0.981 train/f1:   
                                                              0.982             
                                                              train/precision:  
                                                              0.964             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.147             
[2024-06-21 00:03:12,965][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/feeling_lately/0.25/2>
[2024-06-21 00:03:12,966][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 00:03:12,971][HYDRA] 	#193 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=feeling_lately data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=3
[rank: 0] Seed set to 3
[2024-06-21 00:03:13,278][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 00:03:13,279][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-21 00:03:13,280][train.py][INFO] - Instantiating callbacks...
[2024-06-21 00:03:13,281][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 00:03:13,284][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 00:03:13,284][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 00:03:13,284][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 00:03:13,285][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 00:03:13,285][train.py][INFO] - Instantiating loggers...
[2024-06-21 00:03:13,286][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 00:03:13,287][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 00:03:13,380][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/feeling_lately/0.25/3/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.28it/s v_num: 0.000      
                                                              val/auc: 0.411    
                                                              val/f1: 0.267     
                                                              val/precision:    
                                                              0.333 val/recall: 
                                                              0.222 val/mre:    
                                                              0.176 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.149             
[2024-06-21 00:05:05,039][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/feeling_lately/0.25/3>
[2024-06-21 00:05:05,040][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 00:05:05,042][HYDRA] 	#194 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=feeling_lately data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=4
[rank: 0] Seed set to 4
[2024-06-21 00:05:05,225][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 00:05:05,227][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-21 00:05:05,228][train.py][INFO] - Instantiating callbacks...
[2024-06-21 00:05:05,228][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 00:05:05,230][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 00:05:05,231][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 00:05:05,231][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 00:05:05,234][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 00:05:05,234][train.py][INFO] - Instantiating loggers...
[2024-06-21 00:05:05,234][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 00:05:05,236][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 00:05:05,302][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/feeling_lately/0.25/4/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.29it/s v_num: 0.000      
                                                              val/auc: 0.461    
                                                              val/f1: 0.286     
                                                              val/precision:    
                                                              0.400 val/recall: 
                                                              0.222 val/mre:    
                                                              0.173 train/auc:  
                                                              0.972 train/f1:   
                                                              0.971             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              0.944 train/mre:  
                                                              0.145             
[2024-06-21 00:06:55,378][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/feeling_lately/0.25/4>
[2024-06-21 00:06:55,379][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 00:06:55,382][HYDRA] 	#195 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=feeling_lately data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=5
[rank: 0] Seed set to 5
[2024-06-21 00:06:55,566][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 00:06:55,567][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-21 00:06:55,568][train.py][INFO] - Instantiating callbacks...
[2024-06-21 00:06:55,569][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 00:06:55,571][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 00:06:55,571][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 00:06:55,572][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 00:06:55,572][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 00:06:55,573][train.py][INFO] - Instantiating loggers...
[2024-06-21 00:06:55,573][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 00:06:55,574][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 00:06:55,603][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/feeling_lately/0.25/5/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.29it/s v_num: 0.000      
                                                              val/auc: 0.467    
                                                              val/f1: 0.375     
                                                              val/precision:    
                                                              0.429 val/recall: 
                                                              0.333 val/mre:    
                                                              0.176 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.149             
[2024-06-21 00:08:44,888][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/feeling_lately/0.25/5>
[2024-06-21 00:08:44,889][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 00:08:44,891][HYDRA] 	#196 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=feeling_lately data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=6
[rank: 0] Seed set to 6
[2024-06-21 00:08:45,499][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 00:08:45,500][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-21 00:08:45,502][train.py][INFO] - Instantiating callbacks...
[2024-06-21 00:08:45,502][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 00:08:45,506][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 00:08:45,507][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 00:08:45,507][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 00:08:45,508][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 00:08:45,508][train.py][INFO] - Instantiating loggers...
[2024-06-21 00:08:45,509][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 00:08:45,510][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 00:08:45,616][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/feeling_lately/0.25/6/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.31it/s v_num: 0.000      
                                                              val/auc: 0.411    
                                                              val/f1: 0.267     
                                                              val/precision:    
                                                              0.333 val/recall: 
                                                              0.222 val/mre:    
                                                              0.170 train/auc:  
                                                              0.981 train/f1:   
                                                              0.981             
                                                              train/precision:  
                                                              0.981             
                                                              train/recall:     
                                                              0.981 train/mre:  
                                                              0.143             
[2024-06-21 00:10:34,063][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/feeling_lately/0.25/6>
[2024-06-21 00:10:34,063][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 00:10:34,066][HYDRA] 	#197 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=feeling_lately data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=7
[rank: 0] Seed set to 7
[2024-06-21 00:10:34,242][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 00:10:34,244][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-21 00:10:34,245][train.py][INFO] - Instantiating callbacks...
[2024-06-21 00:10:34,245][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 00:10:34,247][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 00:10:34,248][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 00:10:34,248][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 00:10:34,249][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 00:10:34,249][train.py][INFO] - Instantiating loggers...
[2024-06-21 00:10:34,250][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 00:10:34,251][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 00:10:34,282][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/feeling_lately/0.25/7/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.31it/s v_num: 0.000      
                                                              val/auc: 0.567    
                                                              val/f1: 0.429     
                                                              val/precision:    
                                                              0.600 val/recall: 
                                                              0.333 val/mre:    
                                                              0.177 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.150             
[2024-06-21 00:12:23,336][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/feeling_lately/0.25/7>
[2024-06-21 00:12:23,337][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 00:12:23,339][HYDRA] 	#198 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=feeling_lately data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=8
[rank: 0] Seed set to 8
[2024-06-21 00:12:23,521][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 00:12:23,522][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-21 00:12:23,524][train.py][INFO] - Instantiating callbacks...
[2024-06-21 00:12:23,524][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 00:12:23,526][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 00:12:23,526][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 00:12:23,527][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 00:12:23,530][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 00:12:23,530][train.py][INFO] - Instantiating loggers...
[2024-06-21 00:12:23,530][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 00:12:23,532][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 00:12:23,598][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/feeling_lately/0.25/8/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.28it/s v_num: 0.000      
                                                              val/auc: 0.350    
                                                              val/f1: 0.000     
                                                              val/precision:    
                                                              0.000 val/recall: 
                                                              0.000 val/mre:    
                                                              0.178 train/auc:  
                                                              0.991 train/f1:   
                                                              0.991             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              0.981 train/mre:  
                                                              0.151             
[2024-06-21 00:14:13,098][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/feeling_lately/0.25/8>
[2024-06-21 00:14:13,099][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 00:14:13,101][HYDRA] 	#199 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=feeling_lately data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=9
[rank: 0] Seed set to 9
[2024-06-21 00:14:13,282][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 00:14:13,284][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-21 00:14:13,285][train.py][INFO] - Instantiating callbacks...
[2024-06-21 00:14:13,285][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 00:14:13,288][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 00:14:13,289][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 00:14:13,289][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 00:14:13,290][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 00:14:13,290][train.py][INFO] - Instantiating loggers...
[2024-06-21 00:14:13,290][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 00:14:13,292][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 00:14:13,321][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/feeling_lately/0.25/9/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.32it/s v_num: 0.000      
                                                              val/auc: 0.411    
                                                              val/f1: 0.267     
                                                              val/precision:    
                                                              0.333 val/recall: 
                                                              0.222 val/mre:    
                                                              0.171 train/auc:  
                                                              0.935 train/f1:   
                                                              0.939             
                                                              train/precision:  
                                                              0.885             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.145             
[2024-06-21 00:16:02,362][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/feeling_lately/0.25/9>
[2024-06-21 00:16:02,362][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 00:16:02,365][HYDRA] 	#200 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=feeling_lately data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=0
[2024-06-21 00:16:02,719][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 00:16:02,721][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-21 00:16:02,722][train.py][INFO] - Instantiating callbacks...
[2024-06-21 00:16:02,722][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 00:16:02,726][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 00:16:02,727][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 00:16:02,727][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 00:16:02,728][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 00:16:02,728][train.py][INFO] - Instantiating loggers...
[2024-06-21 00:16:02,728][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 00:16:02,729][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 00:16:02,830][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/feeling_lately/0.3/0/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.28it/s v_num: 0.000      
                                                              val/auc: 0.411    
                                                              val/f1: 0.267     
                                                              val/precision:    
                                                              0.333 val/recall: 
                                                              0.222 val/mre:    
                                                              0.177 train/auc:  
                                                              0.963 train/f1:   
                                                              0.963             
                                                              train/precision:  
                                                              0.963             
                                                              train/recall:     
                                                              0.963 train/mre:  
                                                              0.144             
[2024-06-21 00:17:52,615][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/feeling_lately/0.3/0>
[2024-06-21 00:17:52,615][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 00:17:52,617][HYDRA] 	#201 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=feeling_lately data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=1
[rank: 0] Seed set to 1
[2024-06-21 00:17:52,801][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 00:17:52,802][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-21 00:17:52,804][train.py][INFO] - Instantiating callbacks...
[2024-06-21 00:17:52,804][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 00:17:52,806][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 00:17:52,806][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 00:17:52,807][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 00:17:52,809][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 00:17:52,810][train.py][INFO] - Instantiating loggers...
[2024-06-21 00:17:52,810][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 00:17:52,811][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 00:17:52,880][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/feeling_lately/0.3/1/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.31it/s v_num: 0.000      
                                                              val/auc: 0.367    
                                                              val/f1: 0.333     
                                                              val/precision:    
                                                              0.333 val/recall: 
                                                              0.333 val/mre:    
                                                              0.173 train/auc:  
                                                              0.935 train/f1:   
                                                              0.931             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              0.870 train/mre:  
                                                              0.149             
[2024-06-21 00:19:41,770][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/feeling_lately/0.3/1>
[2024-06-21 00:19:41,771][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 00:19:41,773][HYDRA] 	#202 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=feeling_lately data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=2
[rank: 0] Seed set to 2
[2024-06-21 00:19:41,950][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 00:19:41,952][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-21 00:19:41,953][train.py][INFO] - Instantiating callbacks...
[2024-06-21 00:19:41,953][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 00:19:41,955][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 00:19:41,956][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 00:19:41,956][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 00:19:41,957][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 00:19:41,957][train.py][INFO] - Instantiating loggers...
[2024-06-21 00:19:41,958][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 00:19:41,959][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 00:19:41,988][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/feeling_lately/0.3/2/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.30it/s v_num: 0.000      
                                                              val/auc: 0.628    
                                                              val/f1: 0.588     
                                                              val/precision:    
                                                              0.625 val/recall: 
                                                              0.556 val/mre:    
                                                              0.179 train/auc:  
                                                              0.981 train/f1:   
                                                              0.982             
                                                              train/precision:  
                                                              0.964             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.152             
[2024-06-21 00:21:31,485][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/feeling_lately/0.3/2>
[2024-06-21 00:21:31,486][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 00:21:31,488][HYDRA] 	#203 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=feeling_lately data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=3
[rank: 0] Seed set to 3
[2024-06-21 00:21:31,664][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 00:21:31,666][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-21 00:21:31,667][train.py][INFO] - Instantiating callbacks...
[2024-06-21 00:21:31,667][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 00:21:31,669][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 00:21:31,670][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 00:21:31,670][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 00:21:31,671][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 00:21:31,671][train.py][INFO] - Instantiating loggers...
[2024-06-21 00:21:31,671][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 00:21:31,673][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 00:21:31,701][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/feeling_lately/0.3/3/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.33it/s v_num: 0.000      
                                                              val/auc: 0.578    
                                                              val/f1: 0.556     
                                                              val/precision:    
                                                              0.556 val/recall: 
                                                              0.556 val/mre:    
                                                              0.177 train/auc:  
                                                              0.991 train/f1:   
                                                              0.991             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              0.981 train/mre:  
                                                              0.150             
[2024-06-21 00:23:22,350][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/feeling_lately/0.3/3>
[2024-06-21 00:23:22,351][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 00:23:22,353][HYDRA] 	#204 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=feeling_lately data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=4
[rank: 0] Seed set to 4
[2024-06-21 00:23:22,529][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 00:23:22,531][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-21 00:23:22,532][train.py][INFO] - Instantiating callbacks...
[2024-06-21 00:23:22,532][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 00:23:22,534][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 00:23:22,535][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 00:23:22,535][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 00:23:22,536][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 00:23:22,536][train.py][INFO] - Instantiating loggers...
[2024-06-21 00:23:22,536][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 00:23:22,538][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 00:23:22,568][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/feeling_lately/0.3/4/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.29it/s v_num: 0.000      
                                                              val/auc: 0.411    
                                                              val/f1: 0.267     
                                                              val/precision:    
                                                              0.333 val/recall: 
                                                              0.222 val/mre:    
                                                              0.175 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.149             
[2024-06-21 00:25:13,102][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/feeling_lately/0.3/4>
[2024-06-21 00:25:13,103][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 00:25:13,118][HYDRA] 	#205 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=feeling_lately data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=5
[rank: 0] Seed set to 5
[2024-06-21 00:25:13,386][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 00:25:13,387][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-21 00:25:13,389][train.py][INFO] - Instantiating callbacks...
[2024-06-21 00:25:13,389][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 00:25:13,392][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 00:25:13,392][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 00:25:13,393][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 00:25:13,408][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 00:25:13,408][train.py][INFO] - Instantiating loggers...
[2024-06-21 00:25:13,409][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 00:25:13,410][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 00:25:13,557][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/feeling_lately/0.3/5/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.30it/s v_num: 0.000      
                                                              val/auc: 0.356    
                                                              val/f1: 0.143     
                                                              val/precision:    
                                                              0.200 val/recall: 
                                                              0.111 val/mre:    
                                                              0.179 train/auc:  
                                                              0.972 train/f1:   
                                                              0.972             
                                                              train/precision:  
                                                              0.981             
                                                              train/recall:     
                                                              0.963 train/mre:  
                                                              0.152             
[2024-06-21 00:27:02,412][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/feeling_lately/0.3/5>
[2024-06-21 00:27:02,413][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 00:27:02,415][HYDRA] 	#206 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=feeling_lately data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=6
[rank: 0] Seed set to 6
[2024-06-21 00:27:02,860][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 00:27:02,862][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-21 00:27:02,863][train.py][INFO] - Instantiating callbacks...
[2024-06-21 00:27:02,863][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 00:27:02,865][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 00:27:02,866][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 00:27:02,866][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 00:27:02,867][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 00:27:02,868][train.py][INFO] - Instantiating loggers...
[2024-06-21 00:27:02,868][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 00:27:02,869][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 00:27:02,899][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/feeling_lately/0.3/6/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.30it/s v_num: 0.000      
                                                              val/auc: 0.461    
                                                              val/f1: 0.286     
                                                              val/precision:    
                                                              0.400 val/recall: 
                                                              0.222 val/mre:    
                                                              0.177 train/auc:  
                                                              0.991 train/f1:   
                                                              0.991             
                                                              train/precision:  
                                                              0.982             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.152             
[2024-06-21 00:28:52,766][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/feeling_lately/0.3/6>
[2024-06-21 00:28:52,766][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 00:28:52,769][HYDRA] 	#207 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=feeling_lately data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=7
[rank: 0] Seed set to 7
[2024-06-21 00:28:52,945][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 00:28:52,946][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-21 00:28:52,948][train.py][INFO] - Instantiating callbacks...
[2024-06-21 00:28:52,948][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 00:28:52,951][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 00:28:52,952][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 00:28:52,952][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 00:28:52,953][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 00:28:52,953][train.py][INFO] - Instantiating loggers...
[2024-06-21 00:28:52,953][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 00:28:52,955][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 00:28:52,983][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/feeling_lately/0.3/7/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.38it/s v_num: 0.000      
                                                              val/auc: 0.561    
                                                              val/f1: 0.333     
                                                              val/precision:    
                                                              0.667 val/recall: 
                                                              0.222 val/mre:    
                                                              0.179 train/auc:  
                                                              0.991 train/f1:   
                                                              0.991             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              0.981 train/mre:  
                                                              0.153             
[2024-06-21 00:30:43,323][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/feeling_lately/0.3/7>
[2024-06-21 00:30:43,324][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 00:30:43,326][HYDRA] 	#208 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=feeling_lately data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=8
[rank: 0] Seed set to 8
[2024-06-21 00:30:43,500][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 00:30:43,501][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-21 00:30:43,503][train.py][INFO] - Instantiating callbacks...
[2024-06-21 00:30:43,503][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 00:30:43,505][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 00:30:43,505][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 00:30:43,506][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 00:30:43,506][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 00:30:43,507][train.py][INFO] - Instantiating loggers...
[2024-06-21 00:30:43,507][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 00:30:43,508][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 00:30:43,536][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/feeling_lately/0.3/8/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.26it/s v_num: 0.000      
                                                              val/auc: 0.367    
                                                              val/f1: 0.333     
                                                              val/precision:    
                                                              0.333 val/recall: 
                                                              0.333 val/mre:    
                                                              0.181 train/auc:  
                                                              0.963 train/f1:   
                                                              0.962             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              0.926 train/mre:  
                                                              0.155             
[2024-06-21 00:32:35,171][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/feeling_lately/0.3/8>
[2024-06-21 00:32:35,172][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 00:32:35,179][HYDRA] 	#209 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=feeling_lately data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=9
[rank: 0] Seed set to 9
[2024-06-21 00:32:35,374][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 00:32:35,376][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-21 00:32:35,377][train.py][INFO] - Instantiating callbacks...
[2024-06-21 00:32:35,378][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 00:32:35,380][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 00:32:35,380][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 00:32:35,381][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 00:32:35,383][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 00:32:35,384][train.py][INFO] - Instantiating loggers...
[2024-06-21 00:32:35,384][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 00:32:35,385][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 00:32:35,456][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/feeling_lately/0.3/9/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.27it/s v_num: 0.000      
                                                              val/auc: 0.472    
                                                              val/f1: 0.444     
                                                              val/precision:    
                                                              0.444 val/recall: 
                                                              0.444 val/mre:    
                                                              0.171 train/auc:  
                                                              0.926 train/f1:   
                                                              0.920             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              0.852 train/mre:  
                                                              0.143             
[2024-06-21 00:34:26,202][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/feeling_lately/0.3/9>
[2024-06-21 00:34:26,203][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 00:34:26,206][HYDRA] 	#210 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=friend_describe data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=0
[2024-06-21 00:34:26,389][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 00:34:26,391][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-21 00:34:26,392][train.py][INFO] - Instantiating callbacks...
[2024-06-21 00:34:26,392][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 00:34:26,394][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 00:34:26,395][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 00:34:26,395][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 00:34:26,396][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 00:34:26,396][train.py][INFO] - Instantiating loggers...
[2024-06-21 00:34:26,396][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 00:34:26,398][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 00:34:26,428][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/friend_describe/0.0/0/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.16it/s v_num: 0.000      
                                                              val/auc: 0.500    
                                                              val/f1: 0.000     
                                                              val/precision:    
                                                              0.000 val/recall: 
                                                              0.000 val/mre: nan
                                                              train/auc: 0.942  
                                                              train/f1: 0.942   
                                                              train/precision:  
                                                              0.934             
                                                              train/recall:     
                                                              0.950 train/mre:  
                                                              nan               
[2024-06-21 00:36:22,233][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/friend_describe/0.0/0>
[2024-06-21 00:36:22,233][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 00:36:22,236][HYDRA] 	#211 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=friend_describe data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=1
[rank: 0] Seed set to 1
[2024-06-21 00:36:22,406][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 00:36:22,408][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-21 00:36:22,409][train.py][INFO] - Instantiating callbacks...
[2024-06-21 00:36:22,409][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 00:36:22,411][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 00:36:22,412][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 00:36:22,412][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 00:36:22,413][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 00:36:22,413][train.py][INFO] - Instantiating loggers...
[2024-06-21 00:36:22,413][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 00:36:22,414][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 00:36:22,442][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/friend_describe/0.0/1/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.09it/s v_num: 0.000      
                                                              val/auc: 0.471    
                                                              val/f1: 0.200     
                                                              val/precision:    
                                                              0.333 val/recall: 
                                                              0.143 val/mre: nan
                                                              train/auc: 1.000  
                                                              train/f1: 1.000   
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              nan               
[2024-06-21 00:38:17,866][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/friend_describe/0.0/1>
[2024-06-21 00:38:17,867][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 00:38:17,870][HYDRA] 	#212 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=friend_describe data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=2
[rank: 0] Seed set to 2
[2024-06-21 00:38:18,185][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 00:38:18,187][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-21 00:38:18,188][train.py][INFO] - Instantiating callbacks...
[2024-06-21 00:38:18,189][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 00:38:18,192][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 00:38:18,192][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 00:38:18,193][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 00:38:18,196][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 00:38:18,196][train.py][INFO] - Instantiating loggers...
[2024-06-21 00:38:18,196][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 00:38:18,197][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 00:38:18,322][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/friend_describe/0.0/2/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.09it/s v_num: 0.000      
                                                              val/auc: 0.400    
                                                              val/f1: 0.000     
                                                              val/precision:    
                                                              0.000 val/recall: 
                                                              0.000 val/mre: nan
                                                              train/auc: 1.000  
                                                              train/f1: 1.000   
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              nan               
[2024-06-21 00:40:13,907][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/friend_describe/0.0/2>
[2024-06-21 00:40:13,908][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 00:40:13,910][HYDRA] 	#213 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=friend_describe data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=3
[rank: 0] Seed set to 3
[2024-06-21 00:40:14,110][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 00:40:14,111][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-21 00:40:14,112][train.py][INFO] - Instantiating callbacks...
[2024-06-21 00:40:14,113][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 00:40:14,115][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 00:40:14,116][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 00:40:14,116][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 00:40:14,117][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 00:40:14,117][train.py][INFO] - Instantiating loggers...
[2024-06-21 00:40:14,117][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 00:40:14,119][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 00:40:14,160][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/friend_describe/0.0/3/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.09it/s v_num: 0.000      
                                                              val/auc: 0.350    
                                                              val/f1: 0.000     
                                                              val/precision:    
                                                              0.000 val/recall: 
                                                              0.000 val/mre: nan
                                                              train/auc: 0.933  
                                                              train/f1: 0.933   
                                                              train/precision:  
                                                              0.933             
                                                              train/recall:     
                                                              0.933 train/mre:  
                                                              nan               
[2024-06-21 00:42:09,780][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/friend_describe/0.0/3>
[2024-06-21 00:42:09,780][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 00:42:09,785][HYDRA] 	#214 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=friend_describe data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=4
[rank: 0] Seed set to 4
[2024-06-21 00:42:10,007][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 00:42:10,009][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-21 00:42:10,010][train.py][INFO] - Instantiating callbacks...
[2024-06-21 00:42:10,010][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 00:42:10,012][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 00:42:10,013][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 00:42:10,013][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 00:42:10,018][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 00:42:10,019][train.py][INFO] - Instantiating loggers...
[2024-06-21 00:42:10,019][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 00:42:10,020][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 00:42:10,095][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/friend_describe/0.0/4/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.10it/s v_num: 0.000      
                                                              val/auc: 0.350    
                                                              val/f1: 0.000     
                                                              val/precision:    
                                                              0.000 val/recall: 
                                                              0.000 val/mre: nan
                                                              train/auc: 1.000  
                                                              train/f1: 1.000   
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              nan               
[2024-06-21 00:44:05,486][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/friend_describe/0.0/4>
[2024-06-21 00:44:05,486][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 00:44:05,489][HYDRA] 	#215 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=friend_describe data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=5
[rank: 0] Seed set to 5
[2024-06-21 00:44:05,666][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 00:44:05,667][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-21 00:44:05,669][train.py][INFO] - Instantiating callbacks...
[2024-06-21 00:44:05,669][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 00:44:05,671][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 00:44:05,672][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 00:44:05,672][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 00:44:05,673][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 00:44:05,673][train.py][INFO] - Instantiating loggers...
[2024-06-21 00:44:05,673][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 00:44:05,675][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 00:44:05,704][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/friend_describe/0.0/5/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.10it/s v_num: 0.000      
                                                              val/auc: 0.300    
                                                              val/f1: 0.000     
                                                              val/precision:    
                                                              0.000 val/recall: 
                                                              0.000 val/mre: nan
                                                              train/auc: 0.983  
                                                              train/f1: 0.983   
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              0.967 train/mre:  
                                                              nan               
[2024-06-21 00:46:01,749][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/friend_describe/0.0/5>
[2024-06-21 00:46:01,750][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 00:46:01,753][HYDRA] 	#216 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=friend_describe data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=6
[rank: 0] Seed set to 6
[2024-06-21 00:46:01,928][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 00:46:01,929][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-21 00:46:01,931][train.py][INFO] - Instantiating callbacks...
[2024-06-21 00:46:01,931][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 00:46:01,933][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 00:46:01,934][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 00:46:01,934][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 00:46:01,935][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 00:46:01,935][train.py][INFO] - Instantiating loggers...
[2024-06-21 00:46:01,935][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 00:46:01,937][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 00:46:01,965][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/friend_describe/0.0/6/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.09it/s v_num: 0.000      
                                                              val/auc: 0.400    
                                                              val/f1: 0.000     
                                                              val/precision:    
                                                              0.000 val/recall: 
                                                              0.000 val/mre: nan
                                                              train/auc: 0.958  
                                                              train/f1: 0.957   
                                                              train/precision:  
                                                              0.982             
                                                              train/recall:     
                                                              0.933 train/mre:  
                                                              nan               
[2024-06-21 00:47:59,290][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/friend_describe/0.0/6>
[2024-06-21 00:47:59,291][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 00:47:59,293][HYDRA] 	#217 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=friend_describe data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=7
[rank: 0] Seed set to 7
[2024-06-21 00:47:59,479][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 00:47:59,480][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-21 00:47:59,481][train.py][INFO] - Instantiating callbacks...
[2024-06-21 00:47:59,482][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 00:47:59,484][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 00:47:59,484][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 00:47:59,485][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 00:47:59,488][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 00:47:59,488][train.py][INFO] - Instantiating loggers...
[2024-06-21 00:47:59,488][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 00:47:59,489][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 00:47:59,556][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/friend_describe/0.0/7/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.10it/s v_num: 0.000      
                                                              val/auc: 0.500    
                                                              val/f1: 0.000     
                                                              val/precision:    
                                                              0.000 val/recall: 
                                                              0.000 val/mre: nan
                                                              train/auc: 0.975  
                                                              train/f1: 0.975   
                                                              train/precision:  
                                                              0.983             
                                                              train/recall:     
                                                              0.967 train/mre:  
                                                              nan               
[2024-06-21 00:49:55,376][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/friend_describe/0.0/7>
[2024-06-21 00:49:55,376][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 00:49:55,381][HYDRA] 	#218 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=friend_describe data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=8
[rank: 0] Seed set to 8
[2024-06-21 00:49:55,565][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 00:49:55,567][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-21 00:49:55,568][train.py][INFO] - Instantiating callbacks...
[2024-06-21 00:49:55,569][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 00:49:55,571][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 00:49:55,571][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 00:49:55,572][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 00:49:55,572][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 00:49:55,573][train.py][INFO] - Instantiating loggers...
[2024-06-21 00:49:55,573][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 00:49:55,574][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 00:49:55,603][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/friend_describe/0.0/8/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.17it/s v_num: 0.000      
                                                              val/auc: 0.500    
                                                              val/f1: 0.000     
                                                              val/precision:    
                                                              0.000 val/recall: 
                                                              0.000 val/mre: nan
                                                              train/auc: 0.950  
                                                              train/f1: 0.947   
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              0.900 train/mre:  
                                                              nan               
[2024-06-21 00:51:51,507][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/friend_describe/0.0/8>
[2024-06-21 00:51:51,508][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 00:51:51,510][HYDRA] 	#219 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=friend_describe data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=9
[rank: 0] Seed set to 9
[2024-06-21 00:51:51,682][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 00:51:51,683][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-21 00:51:51,684][train.py][INFO] - Instantiating callbacks...
[2024-06-21 00:51:51,684][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 00:51:51,686][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 00:51:51,687][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 00:51:51,687][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 00:51:51,688][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 00:51:51,688][train.py][INFO] - Instantiating loggers...
[2024-06-21 00:51:51,689][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 00:51:51,690][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 00:51:51,718][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/friend_describe/0.0/9/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.09it/s v_num: 0.000      
                                                              val/auc: 0.200    
                                                              val/f1: 0.000     
                                                              val/precision:    
                                                              0.000 val/recall: 
                                                              0.000 val/mre: nan
                                                              train/auc: 0.983  
                                                              train/f1: 0.983   
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              0.967 train/mre:  
                                                              nan               
[2024-06-21 00:53:48,087][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/friend_describe/0.0/9>
[2024-06-21 00:53:48,087][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 00:53:48,090][HYDRA] 	#220 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=friend_describe data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=0
[2024-06-21 00:53:48,276][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 00:53:48,278][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-21 00:53:48,279][train.py][INFO] - Instantiating callbacks...
[2024-06-21 00:53:48,280][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 00:53:48,282][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 00:53:48,282][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 00:53:48,283][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 00:53:48,285][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 00:53:48,286][train.py][INFO] - Instantiating loggers...
[2024-06-21 00:53:48,286][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 00:53:48,287][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 00:53:48,352][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/friend_describe/0.05/0/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.10it/s v_num: 0.000      
                                                              val/auc: 0.171    
                                                              val/f1: 0.125     
                                                              val/precision:    
                                                              0.111 val/recall: 
                                                              0.143 val/mre:    
                                                              0.106 train/auc:  
                                                              0.942 train/f1:   
                                                              0.938             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              0.883 train/mre:  
                                                              0.116             
[2024-06-21 00:55:43,939][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/friend_describe/0.05/0>
[2024-06-21 00:55:43,939][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 00:55:43,942][HYDRA] 	#221 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=friend_describe data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=1
[rank: 0] Seed set to 1
[2024-06-21 00:55:44,121][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 00:55:44,123][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-21 00:55:44,124][train.py][INFO] - Instantiating callbacks...
[2024-06-21 00:55:44,124][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 00:55:44,127][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 00:55:44,127][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 00:55:44,128][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 00:55:44,128][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 00:55:44,129][train.py][INFO] - Instantiating loggers...
[2024-06-21 00:55:44,129][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 00:55:44,130][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 00:55:44,159][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/friend_describe/0.05/1/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.13it/s v_num: 0.000      
                                                              val/auc: 0.400    
                                                              val/f1: 0.000     
                                                              val/precision:    
                                                              0.000 val/recall: 
                                                              0.000 val/mre:    
                                                              0.107 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.118             
[2024-06-21 00:57:39,308][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/friend_describe/0.05/1>
[2024-06-21 00:57:39,308][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 00:57:39,311][HYDRA] 	#222 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=friend_describe data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=2
[rank: 0] Seed set to 2
[2024-06-21 00:57:39,488][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 00:57:39,490][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-21 00:57:39,491][train.py][INFO] - Instantiating callbacks...
[2024-06-21 00:57:39,491][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 00:57:39,494][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 00:57:39,494][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 00:57:39,494][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 00:57:39,495][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 00:57:39,495][train.py][INFO] - Instantiating loggers...
[2024-06-21 00:57:39,496][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 00:57:39,497][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 00:57:39,527][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/friend_describe/0.05/2/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.11it/s v_num: 0.000      
                                                              val/auc: 0.471    
                                                              val/f1: 0.200     
                                                              val/precision:    
                                                              0.333 val/recall: 
                                                              0.143 val/mre:    
                                                              0.104 train/auc:  
                                                              0.992 train/f1:   
                                                              0.992             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              0.983 train/mre:  
                                                              0.113             
[2024-06-21 00:59:35,074][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/friend_describe/0.05/2>
[2024-06-21 00:59:35,074][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 00:59:35,077][HYDRA] 	#223 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=friend_describe data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=3
[rank: 0] Seed set to 3
[2024-06-21 00:59:35,259][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 00:59:35,260][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-21 00:59:35,261][train.py][INFO] - Instantiating callbacks...
[2024-06-21 00:59:35,262][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 00:59:35,264][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 00:59:35,264][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 00:59:35,265][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 00:59:35,268][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 00:59:35,268][train.py][INFO] - Instantiating loggers...
[2024-06-21 00:59:35,268][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 00:59:35,269][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 00:59:35,355][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/friend_describe/0.05/3/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.12it/s v_num: 0.000      
                                                              val/auc: 0.300    
                                                              val/f1: 0.000     
                                                              val/precision:    
                                                              0.000 val/recall: 
                                                              0.000 val/mre:    
                                                              0.102 train/auc:  
                                                              0.975 train/f1:   
                                                              0.976             
                                                              train/precision:  
                                                              0.952             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.116             
[2024-06-21 01:01:31,396][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/friend_describe/0.05/3>
[2024-06-21 01:01:31,397][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 01:01:31,406][HYDRA] 	#224 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=friend_describe data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=4
[rank: 0] Seed set to 4
[2024-06-21 01:01:31,631][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 01:01:31,633][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-21 01:01:31,634][train.py][INFO] - Instantiating callbacks...
[2024-06-21 01:01:31,634][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 01:01:31,637][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 01:01:31,637][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 01:01:31,638][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 01:01:31,639][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 01:01:31,639][train.py][INFO] - Instantiating loggers...
[2024-06-21 01:01:31,639][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 01:01:31,641][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 01:01:31,688][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/friend_describe/0.05/4/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.09it/s v_num: 0.000      
                                                              val/auc: 0.350    
                                                              val/f1: 0.000     
                                                              val/precision:    
                                                              0.000 val/recall: 
                                                              0.000 val/mre:    
                                                              0.108 train/auc:  
                                                              0.975 train/f1:   
                                                              0.975             
                                                              train/precision:  
                                                              0.983             
                                                              train/recall:     
                                                              0.967 train/mre:  
                                                              0.115             
[2024-06-21 01:03:28,069][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/friend_describe/0.05/4>
[2024-06-21 01:03:28,070][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 01:03:28,072][HYDRA] 	#225 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=friend_describe data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=5
[rank: 0] Seed set to 5
[2024-06-21 01:03:28,259][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 01:03:28,261][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-21 01:03:28,262][train.py][INFO] - Instantiating callbacks...
[2024-06-21 01:03:28,263][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 01:03:28,265][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 01:03:28,265][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 01:03:28,266][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 01:03:28,268][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 01:03:28,269][train.py][INFO] - Instantiating loggers...
[2024-06-21 01:03:28,269][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 01:03:28,270][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 01:03:28,346][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/friend_describe/0.05/5/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.10it/s v_num: 0.000      
                                                              val/auc: 0.350    
                                                              val/f1: 0.000     
                                                              val/precision:    
                                                              0.000 val/recall: 
                                                              0.000 val/mre:    
                                                              0.108 train/auc:  
                                                              0.983 train/f1:   
                                                              0.983             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              0.967 train/mre:  
                                                              0.120             
[2024-06-21 01:05:24,250][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/friend_describe/0.05/5>
[2024-06-21 01:05:24,251][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 01:05:24,253][HYDRA] 	#226 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=friend_describe data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=6
[rank: 0] Seed set to 6
[2024-06-21 01:05:24,429][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 01:05:24,431][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-21 01:05:24,432][train.py][INFO] - Instantiating callbacks...
[2024-06-21 01:05:24,433][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 01:05:24,435][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 01:05:24,435][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 01:05:24,436][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 01:05:24,436][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 01:05:24,437][train.py][INFO] - Instantiating loggers...
[2024-06-21 01:05:24,437][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 01:05:24,438][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 01:05:24,468][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/friend_describe/0.05/6/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.16it/s v_num: 0.000      
                                                              val/auc: 0.450    
                                                              val/f1: 0.000     
                                                              val/precision:    
                                                              0.000 val/recall: 
                                                              0.000 val/mre:    
                                                              0.102 train/auc:  
                                                              0.958 train/f1:   
                                                              0.957             
                                                              train/precision:  
                                                              0.982             
                                                              train/recall:     
                                                              0.933 train/mre:  
                                                              0.111             
[2024-06-21 01:07:20,542][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/friend_describe/0.05/6>
[2024-06-21 01:07:20,543][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 01:07:20,546][HYDRA] 	#227 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=friend_describe data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=7
[rank: 0] Seed set to 7
[2024-06-21 01:07:20,716][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 01:07:20,718][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-21 01:07:20,719][train.py][INFO] - Instantiating callbacks...
[2024-06-21 01:07:20,719][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 01:07:20,721][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 01:07:20,722][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 01:07:20,722][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 01:07:20,723][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 01:07:20,723][train.py][INFO] - Instantiating loggers...
[2024-06-21 01:07:20,723][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 01:07:20,724][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 01:07:20,753][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/friend_describe/0.05/7/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.10it/s v_num: 0.000      
                                                              val/auc: 0.350    
                                                              val/f1: 0.000     
                                                              val/precision:    
                                                              0.000 val/recall: 
                                                              0.000 val/mre:    
                                                              0.104 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.116             
[2024-06-21 01:09:16,384][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/friend_describe/0.05/7>
[2024-06-21 01:09:16,385][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 01:09:16,387][HYDRA] 	#228 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=friend_describe data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=8
[rank: 0] Seed set to 8
[2024-06-21 01:09:16,576][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 01:09:16,578][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-21 01:09:16,579][train.py][INFO] - Instantiating callbacks...
[2024-06-21 01:09:16,579][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 01:09:16,582][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 01:09:16,582][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 01:09:16,583][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 01:09:16,585][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 01:09:16,586][train.py][INFO] - Instantiating loggers...
[2024-06-21 01:09:16,586][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 01:09:16,587][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 01:09:16,652][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/friend_describe/0.05/8/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.10it/s v_num: 0.000      
                                                              val/auc: 0.350    
                                                              val/f1: 0.000     
                                                              val/precision:    
                                                              0.000 val/recall: 
                                                              0.000 val/mre:    
                                                              0.104 train/auc:  
                                                              0.958 train/f1:   
                                                              0.957             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              0.917 train/mre:  
                                                              0.115             
[2024-06-21 01:11:12,670][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/friend_describe/0.05/8>
[2024-06-21 01:11:12,671][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 01:11:12,674][HYDRA] 	#229 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=friend_describe data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=9
[rank: 0] Seed set to 9
[2024-06-21 01:11:12,862][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 01:11:12,864][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-21 01:11:12,865][train.py][INFO] - Instantiating callbacks...
[2024-06-21 01:11:12,865][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 01:11:12,868][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 01:11:12,869][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 01:11:12,869][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 01:11:12,870][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 01:11:12,870][train.py][INFO] - Instantiating loggers...
[2024-06-21 01:11:12,871][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 01:11:12,872][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 01:11:12,902][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/friend_describe/0.05/9/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.13it/s v_num: 0.000      
                                                              val/auc: 0.200    
                                                              val/f1: 0.000     
                                                              val/precision:    
                                                              0.000 val/recall: 
                                                              0.000 val/mre:    
                                                              0.102 train/auc:  
                                                              0.975 train/f1:   
                                                              0.974             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              0.950 train/mre:  
                                                              0.113             
[2024-06-21 01:13:08,544][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/friend_describe/0.05/9>
[2024-06-21 01:13:08,544][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 01:13:08,547][HYDRA] 	#230 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=friend_describe data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=0
[2024-06-21 01:13:08,741][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 01:13:08,743][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-21 01:13:08,744][train.py][INFO] - Instantiating callbacks...
[2024-06-21 01:13:08,745][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 01:13:08,747][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 01:13:08,748][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 01:13:08,748][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 01:13:08,749][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 01:13:08,749][train.py][INFO] - Instantiating loggers...
[2024-06-21 01:13:08,750][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 01:13:08,751][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 01:13:08,789][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/friend_describe/0.1/0/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.11it/s v_num: 0.000      
                                                              val/auc: 0.350    
                                                              val/f1: 0.000     
                                                              val/precision:    
                                                              0.000 val/recall: 
                                                              0.000 val/mre:    
                                                              0.116 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.127             
[2024-06-21 01:15:04,055][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/friend_describe/0.1/0>
[2024-06-21 01:15:04,055][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 01:15:04,059][HYDRA] 	#231 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=friend_describe data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=1
[rank: 0] Seed set to 1
[2024-06-21 01:15:04,288][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 01:15:04,289][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-21 01:15:04,291][train.py][INFO] - Instantiating callbacks...
[2024-06-21 01:15:04,291][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 01:15:04,294][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 01:15:04,294][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 01:15:04,295][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 01:15:04,297][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 01:15:04,298][train.py][INFO] - Instantiating loggers...
[2024-06-21 01:15:04,298][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 01:15:04,299][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 01:15:04,446][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/friend_describe/0.1/1/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.10it/s v_num: 0.000      
                                                              val/auc: 0.507    
                                                              val/f1: 0.526     
                                                              val/precision:    
                                                              0.417 val/recall: 
                                                              0.714 val/mre:    
                                                              0.117 train/auc:  
                                                              0.992 train/f1:   
                                                              0.992             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              0.983 train/mre:  
                                                              0.127             
[2024-06-21 01:16:59,544][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/friend_describe/0.1/1>
[2024-06-21 01:16:59,545][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 01:16:59,547][HYDRA] 	#232 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=friend_describe data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=2
[rank: 0] Seed set to 2
[2024-06-21 01:16:59,725][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 01:16:59,727][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-21 01:16:59,728][train.py][INFO] - Instantiating callbacks...
[2024-06-21 01:16:59,728][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 01:16:59,730][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 01:16:59,731][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 01:16:59,731][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 01:16:59,732][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 01:16:59,732][train.py][INFO] - Instantiating loggers...
[2024-06-21 01:16:59,732][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 01:16:59,734][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 01:16:59,762][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/friend_describe/0.1/2/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.09it/s v_num: 0.000      
                                                              val/auc: 0.371    
                                                              val/f1: 0.167     
                                                              val/precision:    
                                                              0.200 val/recall: 
                                                              0.143 val/mre:    
                                                              0.111 train/auc:  
                                                              0.983 train/f1:   
                                                              0.983             
                                                              train/precision:  
                                                              0.983             
                                                              train/recall:     
                                                              0.983 train/mre:  
                                                              0.122             
[2024-06-21 01:18:55,362][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/friend_describe/0.1/2>
[2024-06-21 01:18:55,363][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 01:18:55,365][HYDRA] 	#233 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=friend_describe data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=3
[rank: 0] Seed set to 3
[2024-06-21 01:18:55,550][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 01:18:55,551][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-21 01:18:55,553][train.py][INFO] - Instantiating callbacks...
[2024-06-21 01:18:55,553][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 01:18:55,555][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 01:18:55,555][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 01:18:55,556][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 01:18:55,558][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 01:18:55,559][train.py][INFO] - Instantiating loggers...
[2024-06-21 01:18:55,559][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 01:18:55,560][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 01:18:55,629][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/friend_describe/0.1/3/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.11it/s v_num: 0.000      
                                                              val/auc: 0.350    
                                                              val/f1: 0.000     
                                                              val/precision:    
                                                              0.000 val/recall: 
                                                              0.000 val/mre:    
                                                              0.111 train/auc:  
                                                              0.983 train/f1:   
                                                              0.983             
                                                              train/precision:  
                                                              0.983             
                                                              train/recall:     
                                                              0.983 train/mre:  
                                                              0.122             
[2024-06-21 01:20:51,599][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/friend_describe/0.1/3>
[2024-06-21 01:20:51,600][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 01:20:51,603][HYDRA] 	#234 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=friend_describe data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=4
[rank: 0] Seed set to 4
[2024-06-21 01:20:51,782][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 01:20:51,784][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-21 01:20:51,785][train.py][INFO] - Instantiating callbacks...
[2024-06-21 01:20:51,785][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 01:20:51,788][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 01:20:51,788][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 01:20:51,789][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 01:20:51,789][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 01:20:51,790][train.py][INFO] - Instantiating loggers...
[2024-06-21 01:20:51,790][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 01:20:51,791][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 01:20:51,820][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/friend_describe/0.1/4/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.17it/s v_num: 0.000      
                                                              val/auc: 0.450    
                                                              val/f1: 0.000     
                                                              val/precision:    
                                                              0.000 val/recall: 
                                                              0.000 val/mre:    
                                                              0.115 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.122             
[2024-06-21 01:22:48,280][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/friend_describe/0.1/4>
[2024-06-21 01:22:48,280][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 01:22:48,283][HYDRA] 	#235 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=friend_describe data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=5
[rank: 0] Seed set to 5
[2024-06-21 01:22:48,454][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 01:22:48,455][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-21 01:22:48,456][train.py][INFO] - Instantiating callbacks...
[2024-06-21 01:22:48,456][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 01:22:48,458][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 01:22:48,459][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 01:22:48,459][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 01:22:48,460][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 01:22:48,460][train.py][INFO] - Instantiating loggers...
[2024-06-21 01:22:48,461][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 01:22:48,462][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 01:22:48,490][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/friend_describe/0.1/5/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.06it/s v_num: 0.000      
                                                              val/auc: 0.364    
                                                              val/f1: 0.353     
                                                              val/precision:    
                                                              0.300 val/recall: 
                                                              0.429 val/mre:    
                                                              0.114 train/auc:  
                                                              0.992 train/f1:   
                                                              0.992             
                                                              train/precision:  
                                                              0.984             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.126             
[2024-06-21 01:24:44,817][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/friend_describe/0.1/5>
[2024-06-21 01:24:44,817][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 01:24:44,820][HYDRA] 	#236 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=friend_describe data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=6
[rank: 0] Seed set to 6
[2024-06-21 01:24:45,007][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 01:24:45,009][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-21 01:24:45,010][train.py][INFO] - Instantiating callbacks...
[2024-06-21 01:24:45,011][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 01:24:45,013][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 01:24:45,013][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 01:24:45,014][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 01:24:45,037][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 01:24:45,038][train.py][INFO] - Instantiating loggers...
[2024-06-21 01:24:45,038][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 01:24:45,039][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 01:24:45,116][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/friend_describe/0.1/6/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.10it/s v_num: 0.000      
                                                              val/auc: 0.450    
                                                              val/f1: 0.000     
                                                              val/precision:    
                                                              0.000 val/recall: 
                                                              0.000 val/mre:    
                                                              0.110 train/auc:  
                                                              0.950 train/f1:   
                                                              0.949             
                                                              train/precision:  
                                                              0.966             
                                                              train/recall:     
                                                              0.933 train/mre:  
                                                              0.119             
[2024-06-21 01:26:40,673][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/friend_describe/0.1/6>
[2024-06-21 01:26:40,674][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 01:26:40,677][HYDRA] 	#237 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=friend_describe data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=7
[rank: 0] Seed set to 7
[2024-06-21 01:26:40,852][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 01:26:40,853][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-21 01:26:40,854][train.py][INFO] - Instantiating callbacks...
[2024-06-21 01:26:40,855][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 01:26:40,857][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 01:26:40,857][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 01:26:40,858][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 01:26:40,859][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 01:26:40,859][train.py][INFO] - Instantiating loggers...
[2024-06-21 01:26:40,859][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 01:26:40,861][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 01:26:40,893][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/friend_describe/0.1/7/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.10it/s v_num: 0.000      
                                                              val/auc: 0.543    
                                                              val/f1: 0.364     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.286 val/mre:    
                                                              0.116 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.125             
[2024-06-21 01:28:36,442][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/friend_describe/0.1/7>
[2024-06-21 01:28:36,442][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 01:28:36,444][HYDRA] 	#238 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=friend_describe data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=8
[rank: 0] Seed set to 8
[2024-06-21 01:28:36,619][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 01:28:36,620][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-21 01:28:36,621][train.py][INFO] - Instantiating callbacks...
[2024-06-21 01:28:36,622][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 01:28:36,624][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 01:28:36,624][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 01:28:36,625][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 01:28:36,625][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 01:28:36,626][train.py][INFO] - Instantiating loggers...
[2024-06-21 01:28:36,626][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 01:28:36,627][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 01:28:36,657][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/friend_describe/0.1/8/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.09it/s v_num: 0.000      
                                                              val/auc: 0.300    
                                                              val/f1: 0.000     
                                                              val/precision:    
                                                              0.000 val/recall: 
                                                              0.000 val/mre:    
                                                              0.112 train/auc:  
                                                              0.967 train/f1:   
                                                              0.967             
                                                              train/precision:  
                                                              0.967             
                                                              train/recall:     
                                                              0.967 train/mre:  
                                                              0.124             
[2024-06-21 01:30:31,734][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/friend_describe/0.1/8>
[2024-06-21 01:30:31,735][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 01:30:31,737][HYDRA] 	#239 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=friend_describe data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=9
[rank: 0] Seed set to 9
[2024-06-21 01:30:31,915][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 01:30:31,916][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-21 01:30:31,918][train.py][INFO] - Instantiating callbacks...
[2024-06-21 01:30:31,918][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 01:30:31,920][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 01:30:31,921][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 01:30:31,921][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 01:30:31,924][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 01:30:31,924][train.py][INFO] - Instantiating loggers...
[2024-06-21 01:30:31,924][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 01:30:31,925][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 01:30:31,995][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/friend_describe/0.1/9/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.10it/s v_num: 0.000      
                                                              val/auc: 0.350    
                                                              val/f1: 0.000     
                                                              val/precision:    
                                                              0.000 val/recall: 
                                                              0.000 val/mre:    
                                                              0.114 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.123             
[2024-06-21 01:32:28,036][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/friend_describe/0.1/9>
[2024-06-21 01:32:28,037][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 01:32:28,039][HYDRA] 	#240 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=friend_describe data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=0
[2024-06-21 01:32:28,259][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 01:32:28,261][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-21 01:32:28,262][train.py][INFO] - Instantiating callbacks...
[2024-06-21 01:32:28,262][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 01:32:28,266][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 01:32:28,266][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 01:32:28,267][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 01:32:28,267][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 01:32:28,268][train.py][INFO] - Instantiating loggers...
[2024-06-21 01:32:28,268][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 01:32:28,269][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 01:32:28,302][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/friend_describe/0.15/0/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.08it/s v_num: 0.000      
                                                              val/auc: 0.100    
                                                              val/f1: 0.000     
                                                              val/precision:    
                                                              0.000 val/recall: 
                                                              0.000 val/mre:    
                                                              0.118 train/auc:  
                                                              0.992 train/f1:   
                                                              0.992             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              0.983 train/mre:  
                                                              0.127             
[2024-06-21 01:34:24,781][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/friend_describe/0.15/0>
[2024-06-21 01:34:24,782][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 01:34:24,784][HYDRA] 	#241 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=friend_describe data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=1
[rank: 0] Seed set to 1
[2024-06-21 01:34:24,977][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 01:34:24,979][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-21 01:34:24,980][train.py][INFO] - Instantiating callbacks...
[2024-06-21 01:34:24,980][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 01:34:24,982][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 01:34:24,983][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 01:34:24,983][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 01:34:24,986][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 01:34:24,986][train.py][INFO] - Instantiating loggers...
[2024-06-21 01:34:24,987][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 01:34:24,988][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 01:34:25,069][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/friend_describe/0.15/1/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.16it/s v_num: 0.000      
                                                              val/auc: 0.321    
                                                              val/f1: 0.154     
                                                              val/precision:    
                                                              0.167 val/recall: 
                                                              0.143 val/mre:    
                                                              0.124 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.134             
[2024-06-21 01:36:20,245][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/friend_describe/0.15/1>
[2024-06-21 01:36:20,245][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 01:36:20,248][HYDRA] 	#242 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=friend_describe data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=2
[rank: 0] Seed set to 2
[2024-06-21 01:36:20,441][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 01:36:20,443][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-21 01:36:20,444][train.py][INFO] - Instantiating callbacks...
[2024-06-21 01:36:20,444][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 01:36:20,446][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 01:36:20,447][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 01:36:20,447][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 01:36:20,448][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 01:36:20,448][train.py][INFO] - Instantiating loggers...
[2024-06-21 01:36:20,449][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 01:36:20,450][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 01:36:20,546][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/friend_describe/0.15/2/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.17it/s v_num: 0.000      
                                                              val/auc: 0.350    
                                                              val/f1: 0.000     
                                                              val/precision:    
                                                              0.000 val/recall: 
                                                              0.000 val/mre:    
                                                              0.120 train/auc:  
                                                              0.967 train/f1:   
                                                              0.966             
                                                              train/precision:  
                                                              0.983             
                                                              train/recall:     
                                                              0.950 train/mre:  
                                                              0.130             
[2024-06-21 01:38:12,761][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/friend_describe/0.15/2>
[2024-06-21 01:38:12,762][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 01:38:12,764][HYDRA] 	#243 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=friend_describe data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=3
[rank: 0] Seed set to 3
[2024-06-21 01:38:12,982][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 01:38:12,984][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-21 01:38:12,985][train.py][INFO] - Instantiating callbacks...
[2024-06-21 01:38:12,986][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 01:38:12,988][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 01:38:12,988][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 01:38:12,989][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 01:38:12,991][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 01:38:12,991][train.py][INFO] - Instantiating loggers...
[2024-06-21 01:38:12,992][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 01:38:12,993][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 01:38:13,082][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/friend_describe/0.15/3/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.16it/s v_num: 0.000      
                                                              val/auc: 0.400    
                                                              val/f1: 0.000     
                                                              val/precision:    
                                                              0.000 val/recall: 
                                                              0.000 val/mre:    
                                                              0.116 train/auc:  
                                                              0.958 train/f1:   
                                                              0.959             
                                                              train/precision:  
                                                              0.951             
                                                              train/recall:     
                                                              0.967 train/mre:  
                                                              0.127             
[2024-06-21 01:40:06,393][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/friend_describe/0.15/3>
[2024-06-21 01:40:06,394][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 01:40:06,397][HYDRA] 	#244 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=friend_describe data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=4
[rank: 0] Seed set to 4
[2024-06-21 01:40:06,634][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 01:40:06,636][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-21 01:40:06,637][train.py][INFO] - Instantiating callbacks...
[2024-06-21 01:40:06,637][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 01:40:06,640][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 01:40:06,640][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 01:40:06,641][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 01:40:06,643][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 01:40:06,644][train.py][INFO] - Instantiating loggers...
[2024-06-21 01:40:06,644][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 01:40:06,645][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 01:40:06,775][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/friend_describe/0.15/4/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.16it/s v_num: 0.000      
                                                              val/auc: 0.500    
                                                              val/f1: 0.000     
                                                              val/precision:    
                                                              0.000 val/recall: 
                                                              0.000 val/mre:    
                                                              0.119 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.129             
[2024-06-21 01:41:59,951][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/friend_describe/0.15/4>
[2024-06-21 01:41:59,951][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 01:41:59,954][HYDRA] 	#245 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=friend_describe data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=5
[rank: 0] Seed set to 5
[2024-06-21 01:42:00,143][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 01:42:00,144][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-21 01:42:00,145][train.py][INFO] - Instantiating callbacks...
[2024-06-21 01:42:00,146][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 01:42:00,148][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 01:42:00,148][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 01:42:00,149][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 01:42:00,151][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 01:42:00,151][train.py][INFO] - Instantiating loggers...
[2024-06-21 01:42:00,151][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 01:42:00,153][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 01:42:00,229][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/friend_describe/0.15/5/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.18it/s v_num: 0.000      
                                                              val/auc: 0.514    
                                                              val/f1: 0.429     
                                                              val/precision:    
                                                              0.429 val/recall: 
                                                              0.429 val/mre:    
                                                              0.125 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.135             
[2024-06-21 01:43:52,327][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/friend_describe/0.15/5>
[2024-06-21 01:43:52,328][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 01:43:52,331][HYDRA] 	#246 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=friend_describe data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=6
[rank: 0] Seed set to 6
[2024-06-21 01:43:52,580][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 01:43:52,582][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-21 01:43:52,583][train.py][INFO] - Instantiating callbacks...
[2024-06-21 01:43:52,583][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 01:43:52,585][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 01:43:52,586][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 01:43:52,586][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 01:43:52,589][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 01:43:52,590][train.py][INFO] - Instantiating loggers...
[2024-06-21 01:43:52,590][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 01:43:52,591][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 01:43:52,667][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/friend_describe/0.15/6/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.14it/s v_num: 0.000      
                                                              val/auc: 0.400    
                                                              val/f1: 0.000     
                                                              val/precision:    
                                                              0.000 val/recall: 
                                                              0.000 val/mre:    
                                                              0.115 train/auc:  
                                                              0.933 train/f1:   
                                                              0.935             
                                                              train/precision:  
                                                              0.906             
                                                              train/recall:     
                                                              0.967 train/mre:  
                                                              0.126             
[2024-06-21 01:45:46,650][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/friend_describe/0.15/6>
[2024-06-21 01:45:46,651][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 01:45:46,653][HYDRA] 	#247 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=friend_describe data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=7
[rank: 0] Seed set to 7
[2024-06-21 01:45:46,837][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 01:45:46,839][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-21 01:45:46,840][train.py][INFO] - Instantiating callbacks...
[2024-06-21 01:45:46,840][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 01:45:46,842][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 01:45:46,843][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 01:45:46,843][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 01:45:46,846][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 01:45:46,846][train.py][INFO] - Instantiating loggers...
[2024-06-21 01:45:46,846][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 01:45:46,847][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 01:45:46,913][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/friend_describe/0.15/7/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.17it/s v_num: 0.000      
                                                              val/auc: 0.393    
                                                              val/f1: 0.286     
                                                              val/precision:    
                                                              0.286 val/recall: 
                                                              0.286 val/mre:    
                                                              0.121 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.131             
[2024-06-21 01:47:40,748][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/friend_describe/0.15/7>
[2024-06-21 01:47:40,749][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 01:47:40,751][HYDRA] 	#248 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=friend_describe data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=8
[rank: 0] Seed set to 8
[2024-06-21 01:47:40,935][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 01:47:40,937][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-21 01:47:40,938][train.py][INFO] - Instantiating callbacks...
[2024-06-21 01:47:40,938][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 01:47:40,940][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 01:47:40,941][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 01:47:40,941][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 01:47:40,944][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 01:47:40,944][train.py][INFO] - Instantiating loggers...
[2024-06-21 01:47:40,944][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 01:47:40,946][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 01:47:41,012][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/friend_describe/0.15/8/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.18it/s v_num: 0.000      
                                                              val/auc: 0.350    
                                                              val/f1: 0.000     
                                                              val/precision:    
                                                              0.000 val/recall: 
                                                              0.000 val/mre:    
                                                              0.120 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.130             
[2024-06-21 01:49:33,640][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/friend_describe/0.15/8>
[2024-06-21 01:49:33,641][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 01:49:33,643][HYDRA] 	#249 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=friend_describe data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=9
[rank: 0] Seed set to 9
[2024-06-21 01:49:34,086][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 01:49:34,088][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-21 01:49:34,089][train.py][INFO] - Instantiating callbacks...
[2024-06-21 01:49:34,089][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 01:49:34,091][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 01:49:34,091][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 01:49:34,092][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 01:49:34,099][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 01:49:34,100][train.py][INFO] - Instantiating loggers...
[2024-06-21 01:49:34,100][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 01:49:34,101][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 01:49:34,169][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/friend_describe/0.15/9/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.19it/s v_num: 0.000      
                                                              val/auc: 0.400    
                                                              val/f1: 0.000     
                                                              val/precision:    
                                                              0.000 val/recall: 
                                                              0.000 val/mre:    
                                                              0.118 train/auc:  
                                                              0.983 train/f1:   
                                                              0.984             
                                                              train/precision:  
                                                              0.968             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.127             
[2024-06-21 01:51:26,030][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/friend_describe/0.15/9>
[2024-06-21 01:51:26,030][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 01:51:26,033][HYDRA] 	#250 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=friend_describe data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=0
[2024-06-21 01:51:26,215][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 01:51:26,216][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-21 01:51:26,218][train.py][INFO] - Instantiating callbacks...
[2024-06-21 01:51:26,218][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 01:51:26,220][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 01:51:26,220][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 01:51:26,221][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 01:51:26,223][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 01:51:26,224][train.py][INFO] - Instantiating loggers...
[2024-06-21 01:51:26,224][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 01:51:26,225][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 01:51:26,326][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/friend_describe/0.2/0/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.19it/s v_num: 0.000      
                                                              val/auc: 0.250    
                                                              val/f1: 0.000     
                                                              val/precision:    
                                                              0.000 val/recall: 
                                                              0.000 val/mre:    
                                                              0.127 train/auc:  
                                                              0.983 train/f1:   
                                                              0.983             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              0.967 train/mre:  
                                                              0.135             
[2024-06-21 01:53:18,439][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/friend_describe/0.2/0>
[2024-06-21 01:53:18,440][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 01:53:18,442][HYDRA] 	#251 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=friend_describe data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=1
[rank: 0] Seed set to 1
[2024-06-21 01:53:18,625][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 01:53:18,627][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-21 01:53:18,628][train.py][INFO] - Instantiating callbacks...
[2024-06-21 01:53:18,628][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 01:53:18,630][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 01:53:18,631][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 01:53:18,631][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 01:53:18,634][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 01:53:18,634][train.py][INFO] - Instantiating loggers...
[2024-06-21 01:53:18,634][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 01:53:18,635][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 01:53:18,699][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/friend_describe/0.2/1/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.17it/s v_num: 0.000      
                                                              val/auc: 0.421    
                                                              val/f1: 0.182     
                                                              val/precision:    
                                                              0.250 val/recall: 
                                                              0.143 val/mre:    
                                                              0.129 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.139             
[2024-06-21 01:55:10,616][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/friend_describe/0.2/1>
[2024-06-21 01:55:10,617][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 01:55:10,619][HYDRA] 	#252 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=friend_describe data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=2
[rank: 0] Seed set to 2
[2024-06-21 01:55:10,803][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 01:55:10,804][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-21 01:55:10,805][train.py][INFO] - Instantiating callbacks...
[2024-06-21 01:55:10,805][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 01:55:10,807][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 01:55:10,808][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 01:55:10,809][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 01:55:10,812][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 01:55:10,812][train.py][INFO] - Instantiating loggers...
[2024-06-21 01:55:10,812][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 01:55:10,814][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 01:55:10,879][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/friend_describe/0.2/2/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.18it/s v_num: 0.000      
                                                              val/auc: 0.343    
                                                              val/f1: 0.267     
                                                              val/precision:    
                                                              0.250 val/recall: 
                                                              0.286 val/mre:    
                                                              0.128 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.138             
[2024-06-21 01:57:03,146][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/friend_describe/0.2/2>
[2024-06-21 01:57:03,147][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 01:57:03,163][HYDRA] 	#253 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=friend_describe data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=3
[rank: 0] Seed set to 3
[2024-06-21 01:57:03,363][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 01:57:03,364][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-21 01:57:03,365][train.py][INFO] - Instantiating callbacks...
[2024-06-21 01:57:03,366][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 01:57:03,368][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 01:57:03,368][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 01:57:03,368][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 01:57:03,371][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 01:57:03,372][train.py][INFO] - Instantiating loggers...
[2024-06-21 01:57:03,372][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 01:57:03,373][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 01:57:03,444][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/friend_describe/0.2/3/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.19it/s v_num: 0.000      
                                                              val/auc: 0.321    
                                                              val/f1: 0.154     
                                                              val/precision:    
                                                              0.167 val/recall: 
                                                              0.143 val/mre:    
                                                              0.118 train/auc:  
                                                              0.942 train/f1:   
                                                              0.940             
                                                              train/precision:  
                                                              0.965             
                                                              train/recall:     
                                                              0.917 train/mre:  
                                                              0.130             
[2024-06-21 01:58:54,994][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/friend_describe/0.2/3>
[2024-06-21 01:58:54,994][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 01:58:54,996][HYDRA] 	#254 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=friend_describe data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=4
[rank: 0] Seed set to 4
[2024-06-21 01:58:55,179][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 01:58:55,180][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-21 01:58:55,181][train.py][INFO] - Instantiating callbacks...
[2024-06-21 01:58:55,182][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 01:58:55,184][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 01:58:55,184][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 01:58:55,185][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 01:58:55,187][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 01:58:55,188][train.py][INFO] - Instantiating loggers...
[2024-06-21 01:58:55,188][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 01:58:55,189][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 01:58:55,256][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/friend_describe/0.2/4/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.16it/s v_num: 0.000      
                                                              val/auc: 0.250    
                                                              val/f1: 0.000     
                                                              val/precision:    
                                                              0.000 val/recall: 
                                                              0.000 val/mre:    
                                                              0.118 train/auc:  
                                                              0.958 train/f1:   
                                                              0.957             
                                                              train/precision:  
                                                              0.982             
                                                              train/recall:     
                                                              0.933 train/mre:  
                                                              0.129             
[2024-06-21 02:00:47,922][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/friend_describe/0.2/4>
[2024-06-21 02:00:47,922][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 02:00:47,925][HYDRA] 	#255 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=friend_describe data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=5
[rank: 0] Seed set to 5
[2024-06-21 02:00:48,107][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 02:00:48,109][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-21 02:00:48,110][train.py][INFO] - Instantiating callbacks...
[2024-06-21 02:00:48,110][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 02:00:48,112][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 02:00:48,113][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 02:00:48,113][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 02:00:48,116][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 02:00:48,116][train.py][INFO] - Instantiating loggers...
[2024-06-21 02:00:48,116][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 02:00:48,117][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 02:00:48,181][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/friend_describe/0.2/5/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.17it/s v_num: 0.000      
                                                              val/auc: 0.486    
                                                              val/f1: 0.471     
                                                              val/precision:    
                                                              0.400 val/recall: 
                                                              0.571 val/mre:    
                                                              0.126 train/auc:  
                                                              0.958 train/f1:   
                                                              0.957             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              0.917 train/mre:  
                                                              0.137             
[2024-06-21 02:02:40,960][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/friend_describe/0.2/5>
[2024-06-21 02:02:40,961][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 02:02:40,964][HYDRA] 	#256 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=friend_describe data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=6
[rank: 0] Seed set to 6
[2024-06-21 02:02:41,155][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 02:02:41,156][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-21 02:02:41,157][train.py][INFO] - Instantiating callbacks...
[2024-06-21 02:02:41,158][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 02:02:41,160][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 02:02:41,160][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 02:02:41,161][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 02:02:41,163][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 02:02:41,164][train.py][INFO] - Instantiating loggers...
[2024-06-21 02:02:41,164][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 02:02:41,165][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 02:02:41,236][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/friend_describe/0.2/6/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.16it/s v_num: 0.000      
                                                              val/auc: 0.350    
                                                              val/f1: 0.000     
                                                              val/precision:    
                                                              0.000 val/recall: 
                                                              0.000 val/mre:    
                                                              0.119 train/auc:  
                                                              0.992 train/f1:   
                                                              0.992             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              0.983 train/mre:  
                                                              0.129             
[2024-06-21 02:04:33,245][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/friend_describe/0.2/6>
[2024-06-21 02:04:33,245][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 02:04:33,248][HYDRA] 	#257 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=friend_describe data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=7
[rank: 0] Seed set to 7
[2024-06-21 02:04:33,441][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 02:04:33,443][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-21 02:04:33,444][train.py][INFO] - Instantiating callbacks...
[2024-06-21 02:04:33,444][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 02:04:33,447][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 02:04:33,448][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 02:04:33,448][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 02:04:33,453][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 02:04:33,453][train.py][INFO] - Instantiating loggers...
[2024-06-21 02:04:33,454][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 02:04:33,455][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 02:04:33,567][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/friend_describe/0.2/7/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.17it/s v_num: 0.000      
                                                              val/auc: 0.400    
                                                              val/f1: 0.000     
                                                              val/precision:    
                                                              0.000 val/recall: 
                                                              0.000 val/mre:    
                                                              0.125 train/auc:  
                                                              0.933 train/f1:   
                                                              0.933             
                                                              train/precision:  
                                                              0.933             
                                                              train/recall:     
                                                              0.933 train/mre:  
                                                              0.135             
[2024-06-21 02:06:26,434][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/friend_describe/0.2/7>
[2024-06-21 02:06:26,434][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 02:06:26,437][HYDRA] 	#258 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=friend_describe data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=8
[rank: 0] Seed set to 8
[2024-06-21 02:06:26,620][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 02:06:26,621][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-21 02:06:26,623][train.py][INFO] - Instantiating callbacks...
[2024-06-21 02:06:26,623][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 02:06:26,625][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 02:06:26,625][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 02:06:26,626][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 02:06:26,628][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 02:06:26,629][train.py][INFO] - Instantiating loggers...
[2024-06-21 02:06:26,629][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 02:06:26,630][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 02:06:26,696][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/friend_describe/0.2/8/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.17it/s v_num: 0.000      
                                                              val/auc: 0.250    
                                                              val/f1: 0.000     
                                                              val/precision:    
                                                              0.000 val/recall: 
                                                              0.000 val/mre:    
                                                              0.127 train/auc:  
                                                              0.992 train/f1:   
                                                              0.992             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              0.983 train/mre:  
                                                              0.136             
[2024-06-21 02:08:18,481][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/friend_describe/0.2/8>
[2024-06-21 02:08:18,482][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 02:08:18,484][HYDRA] 	#259 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=friend_describe data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=9
[rank: 0] Seed set to 9
[2024-06-21 02:08:18,666][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 02:08:18,667][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-21 02:08:18,669][train.py][INFO] - Instantiating callbacks...
[2024-06-21 02:08:18,669][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 02:08:18,671][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 02:08:18,671][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 02:08:18,672][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 02:08:18,674][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 02:08:18,675][train.py][INFO] - Instantiating loggers...
[2024-06-21 02:08:18,675][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 02:08:18,676][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 02:08:18,739][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/friend_describe/0.2/9/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.18it/s v_num: 0.000      
                                                              val/auc: 0.371    
                                                              val/f1: 0.167     
                                                              val/precision:    
                                                              0.200 val/recall: 
                                                              0.143 val/mre:    
                                                              0.124 train/auc:  
                                                              0.967 train/f1:   
                                                              0.966             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              0.933 train/mre:  
                                                              0.132             
[2024-06-21 02:10:11,170][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/friend_describe/0.2/9>
[2024-06-21 02:10:11,170][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 02:10:11,173][HYDRA] 	#260 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=friend_describe data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=0
[2024-06-21 02:10:11,354][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 02:10:11,356][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-21 02:10:11,357][train.py][INFO] - Instantiating callbacks...
[2024-06-21 02:10:11,357][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 02:10:11,359][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 02:10:11,360][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 02:10:11,360][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 02:10:11,363][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 02:10:11,364][train.py][INFO] - Instantiating loggers...
[2024-06-21 02:10:11,364][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 02:10:11,365][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 02:10:11,435][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/friend_describe/0.25/0/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.17it/s v_num: 0.000      
                                                              val/auc: 0.121    
                                                              val/f1: 0.118     
                                                              val/precision:    
                                                              0.100 val/recall: 
                                                              0.143 val/mre:    
                                                              0.131 train/auc:  
                                                              0.992 train/f1:   
                                                              0.992             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              0.983 train/mre:  
                                                              0.137             
[2024-06-21 02:12:04,121][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/friend_describe/0.25/0>
[2024-06-21 02:12:04,122][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 02:12:04,125][HYDRA] 	#261 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=friend_describe data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=1
[rank: 0] Seed set to 1
[2024-06-21 02:12:04,309][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 02:12:04,310][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-21 02:12:04,312][train.py][INFO] - Instantiating callbacks...
[2024-06-21 02:12:04,312][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 02:12:04,314][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 02:12:04,314][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 02:12:04,315][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 02:12:04,317][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 02:12:04,318][train.py][INFO] - Instantiating loggers...
[2024-06-21 02:12:04,318][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 02:12:04,319][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 02:12:04,383][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/friend_describe/0.25/1/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.18it/s v_num: 0.000      
                                                              val/auc: 0.421    
                                                              val/f1: 0.182     
                                                              val/precision:    
                                                              0.250 val/recall: 
                                                              0.143 val/mre:    
                                                              0.132 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.143             
[2024-06-21 02:13:56,048][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/friend_describe/0.25/1>
[2024-06-21 02:13:56,049][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 02:13:56,051][HYDRA] 	#262 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=friend_describe data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=2
[rank: 0] Seed set to 2
[2024-06-21 02:13:56,233][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 02:13:56,235][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-21 02:13:56,236][train.py][INFO] - Instantiating callbacks...
[2024-06-21 02:13:56,236][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 02:13:56,238][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 02:13:56,239][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 02:13:56,239][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 02:13:56,242][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 02:13:56,242][train.py][INFO] - Instantiating loggers...
[2024-06-21 02:13:56,242][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 02:13:56,244][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 02:13:56,313][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/friend_describe/0.25/2/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.19it/s v_num: 0.000      
                                                              val/auc: 0.371    
                                                              val/f1: 0.167     
                                                              val/precision:    
                                                              0.200 val/recall: 
                                                              0.143 val/mre:    
                                                              0.127 train/auc:  
                                                              0.975 train/f1:   
                                                              0.974             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              0.950 train/mre:  
                                                              0.138             
[2024-06-21 02:15:48,557][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/friend_describe/0.25/2>
[2024-06-21 02:15:48,558][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 02:15:48,561][HYDRA] 	#263 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=friend_describe data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=3
[rank: 0] Seed set to 3
[2024-06-21 02:15:49,018][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 02:15:49,020][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-21 02:15:49,021][train.py][INFO] - Instantiating callbacks...
[2024-06-21 02:15:49,021][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 02:15:49,023][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 02:15:49,024][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 02:15:49,024][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 02:15:49,027][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 02:15:49,027][train.py][INFO] - Instantiating loggers...
[2024-06-21 02:15:49,028][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 02:15:49,029][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 02:15:49,095][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/friend_describe/0.25/3/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.19it/s v_num: 0.000      
                                                              val/auc: 0.350    
                                                              val/f1: 0.000     
                                                              val/precision:    
                                                              0.000 val/recall: 
                                                              0.000 val/mre:    
                                                              0.118 train/auc:  
                                                              0.925 train/f1:   
                                                              0.927             
                                                              train/precision:  
                                                              0.905             
                                                              train/recall:     
                                                              0.950 train/mre:  
                                                              0.131             
[2024-06-21 02:17:41,004][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/friend_describe/0.25/3>
[2024-06-21 02:17:41,005][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 02:17:41,008][HYDRA] 	#264 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=friend_describe data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=4
[rank: 0] Seed set to 4
[2024-06-21 02:17:41,190][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 02:17:41,192][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-21 02:17:41,193][train.py][INFO] - Instantiating callbacks...
[2024-06-21 02:17:41,193][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 02:17:41,195][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 02:17:41,196][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 02:17:41,196][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 02:17:41,199][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 02:17:41,199][train.py][INFO] - Instantiating loggers...
[2024-06-21 02:17:41,200][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 02:17:41,201][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 02:17:41,266][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/friend_describe/0.25/4/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.18it/s v_num: 0.000      
                                                              val/auc: 0.371    
                                                              val/f1: 0.167     
                                                              val/precision:    
                                                              0.200 val/recall: 
                                                              0.143 val/mre:    
                                                              0.125 train/auc:  
                                                              0.975 train/f1:   
                                                              0.974             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              0.950 train/mre:  
                                                              0.136             
[2024-06-21 02:19:32,722][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/friend_describe/0.25/4>
[2024-06-21 02:19:32,723][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 02:19:32,726][HYDRA] 	#265 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=friend_describe data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=5
[rank: 0] Seed set to 5
[2024-06-21 02:19:32,907][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 02:19:32,909][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-21 02:19:32,910][train.py][INFO] - Instantiating callbacks...
[2024-06-21 02:19:32,910][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 02:19:32,912][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 02:19:32,913][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 02:19:32,913][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 02:19:32,916][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 02:19:32,916][train.py][INFO] - Instantiating loggers...
[2024-06-21 02:19:32,917][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 02:19:32,918][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 02:19:32,986][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/friend_describe/0.25/5/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.16it/s v_num: 0.000      
                                                              val/auc: 0.421    
                                                              val/f1: 0.182     
                                                              val/precision:    
                                                              0.250 val/recall: 
                                                              0.143 val/mre:    
                                                              0.125 train/auc:  
                                                              0.992 train/f1:   
                                                              0.992             
                                                              train/precision:  
                                                              0.984             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.137             
[2024-06-21 02:21:25,189][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/friend_describe/0.25/5>
[2024-06-21 02:21:25,190][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 02:21:25,193][HYDRA] 	#266 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=friend_describe data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=6
[rank: 0] Seed set to 6
[2024-06-21 02:21:25,397][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 02:21:25,398][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-21 02:21:25,399][train.py][INFO] - Instantiating callbacks...
[2024-06-21 02:21:25,400][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 02:21:25,403][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 02:21:25,403][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 02:21:25,404][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 02:21:25,407][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 02:21:25,407][train.py][INFO] - Instantiating loggers...
[2024-06-21 02:21:25,407][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 02:21:25,408][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 02:21:25,504][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/friend_describe/0.25/6/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.17it/s v_num: 0.000      
                                                              val/auc: 0.321    
                                                              val/f1: 0.154     
                                                              val/precision:    
                                                              0.167 val/recall: 
                                                              0.143 val/mre:    
                                                              0.119 train/auc:  
                                                              0.958 train/f1:   
                                                              0.958             
                                                              train/precision:  
                                                              0.966             
                                                              train/recall:     
                                                              0.950 train/mre:  
                                                              0.130             
[2024-06-21 02:23:18,749][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/friend_describe/0.25/6>
[2024-06-21 02:23:18,749][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 02:23:18,752][HYDRA] 	#267 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=friend_describe data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=7
[rank: 0] Seed set to 7
[2024-06-21 02:23:18,984][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 02:23:18,986][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-21 02:23:18,987][train.py][INFO] - Instantiating callbacks...
[2024-06-21 02:23:18,987][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 02:23:18,989][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 02:23:18,990][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 02:23:18,990][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 02:23:18,993][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 02:23:18,993][train.py][INFO] - Instantiating loggers...
[2024-06-21 02:23:18,993][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 02:23:18,994][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 02:23:19,091][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/friend_describe/0.25/7/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.17it/s v_num: 0.000      
                                                              val/auc: 0.150    
                                                              val/f1: 0.000     
                                                              val/precision:    
                                                              0.000 val/recall: 
                                                              0.000 val/mre:    
                                                              0.126 train/auc:  
                                                              0.958 train/f1:   
                                                              0.957             
                                                              train/precision:  
                                                              0.982             
                                                              train/recall:     
                                                              0.933 train/mre:  
                                                              0.135             
[2024-06-21 02:25:11,600][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/friend_describe/0.25/7>
[2024-06-21 02:25:11,601][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 02:25:11,604][HYDRA] 	#268 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=friend_describe data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=8
[rank: 0] Seed set to 8
[2024-06-21 02:25:11,793][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 02:25:11,795][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-21 02:25:11,796][train.py][INFO] - Instantiating callbacks...
[2024-06-21 02:25:11,796][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 02:25:11,798][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 02:25:11,799][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 02:25:11,799][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 02:25:11,802][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 02:25:11,802][train.py][INFO] - Instantiating loggers...
[2024-06-21 02:25:11,802][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 02:25:11,804][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 02:25:11,881][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/friend_describe/0.25/8/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.16it/s v_num: 0.000      
                                                              val/auc: 0.343    
                                                              val/f1: 0.267     
                                                              val/precision:    
                                                              0.250 val/recall: 
                                                              0.286 val/mre:    
                                                              0.128 train/auc:  
                                                              0.967 train/f1:   
                                                              0.966             
                                                              train/precision:  
                                                              0.983             
                                                              train/recall:     
                                                              0.950 train/mre:  
                                                              0.139             
[2024-06-21 02:27:07,236][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/friend_describe/0.25/8>
[2024-06-21 02:27:07,237][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 02:27:07,239][HYDRA] 	#269 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=friend_describe data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=9
[rank: 0] Seed set to 9
[2024-06-21 02:27:07,435][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 02:27:07,436][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-21 02:27:07,437][train.py][INFO] - Instantiating callbacks...
[2024-06-21 02:27:07,438][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 02:27:07,440][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 02:27:07,440][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 02:27:07,441][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 02:27:07,444][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 02:27:07,445][train.py][INFO] - Instantiating loggers...
[2024-06-21 02:27:07,445][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 02:27:07,446][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 02:27:07,526][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/friend_describe/0.25/9/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.18it/s v_num: 0.000      
                                                              val/auc: 0.521    
                                                              val/f1: 0.222     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.143 val/mre:    
                                                              0.124 train/auc:  
                                                              0.967 train/f1:   
                                                              0.966             
                                                              train/precision:  
                                                              0.983             
                                                              train/recall:     
                                                              0.950 train/mre:  
                                                              0.133             
[2024-06-21 02:29:00,360][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/friend_describe/0.25/9>
[2024-06-21 02:29:00,360][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 02:29:00,363][HYDRA] 	#270 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=friend_describe data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=0
[2024-06-21 02:29:00,542][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 02:29:00,543][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-21 02:29:00,544][train.py][INFO] - Instantiating callbacks...
[2024-06-21 02:29:00,545][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 02:29:00,547][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 02:29:00,547][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 02:29:00,547][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 02:29:00,550][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 02:29:00,550][train.py][INFO] - Instantiating loggers...
[2024-06-21 02:29:00,551][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 02:29:00,552][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 02:29:00,618][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/friend_describe/0.3/0/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.18it/s v_num: 0.000      
                                                              val/auc: 0.350    
                                                              val/f1: 0.000     
                                                              val/precision:    
                                                              0.000 val/recall: 
                                                              0.000 val/mre:    
                                                              0.134 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.144             
[2024-06-21 02:30:53,429][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/friend_describe/0.3/0>
[2024-06-21 02:30:53,430][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 02:30:53,433][HYDRA] 	#271 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=friend_describe data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=1
[rank: 0] Seed set to 1
[2024-06-21 02:30:53,615][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 02:30:53,616][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-21 02:30:53,617][train.py][INFO] - Instantiating callbacks...
[2024-06-21 02:30:53,618][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 02:30:53,620][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 02:30:53,620][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 02:30:53,621][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 02:30:53,623][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 02:30:53,624][train.py][INFO] - Instantiating loggers...
[2024-06-21 02:30:53,624][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 02:30:53,625][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 02:30:53,691][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/friend_describe/0.3/1/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.16it/s v_num: 0.000      
                                                              val/auc: 0.250    
                                                              val/f1: 0.000     
                                                              val/precision:    
                                                              0.000 val/recall: 
                                                              0.000 val/mre:    
                                                              0.133 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.144             
[2024-06-21 02:32:46,980][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/friend_describe/0.3/1>
[2024-06-21 02:32:46,980][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 02:32:46,983][HYDRA] 	#272 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=friend_describe data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=2
[rank: 0] Seed set to 2
[2024-06-21 02:32:47,165][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 02:32:47,167][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-21 02:32:47,168][train.py][INFO] - Instantiating callbacks...
[2024-06-21 02:32:47,168][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 02:32:47,170][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 02:32:47,171][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 02:32:47,171][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 02:32:47,174][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 02:32:47,174][train.py][INFO] - Instantiating loggers...
[2024-06-21 02:32:47,174][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 02:32:47,176][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 02:32:47,243][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/friend_describe/0.3/2/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.17it/s v_num: 0.000      
                                                              val/auc: 0.321    
                                                              val/f1: 0.154     
                                                              val/precision:    
                                                              0.167 val/recall: 
                                                              0.143 val/mre:    
                                                              0.132 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.143             
[2024-06-21 02:34:39,948][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/friend_describe/0.3/2>
[2024-06-21 02:34:39,948][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 02:34:39,951][HYDRA] 	#273 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=friend_describe data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=3
[rank: 0] Seed set to 3
[2024-06-21 02:34:40,143][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 02:34:40,144][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-21 02:34:40,146][train.py][INFO] - Instantiating callbacks...
[2024-06-21 02:34:40,146][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 02:34:40,148][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 02:34:40,148][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 02:34:40,149][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 02:34:40,151][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 02:34:40,152][train.py][INFO] - Instantiating loggers...
[2024-06-21 02:34:40,152][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 02:34:40,153][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 02:34:40,217][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/friend_describe/0.3/3/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.19it/s v_num: 0.000      
                                                              val/auc: 0.221    
                                                              val/f1: 0.133     
                                                              val/precision:    
                                                              0.125 val/recall: 
                                                              0.143 val/mre:    
                                                              0.116 train/auc:  
                                                              0.892 train/f1:   
                                                              0.894             
                                                              train/precision:  
                                                              0.873             
                                                              train/recall:     
                                                              0.917 train/mre:  
                                                              0.131             
[2024-06-21 02:36:32,444][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/friend_describe/0.3/3>
[2024-06-21 02:36:32,445][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 02:36:32,447][HYDRA] 	#274 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=friend_describe data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=4
[rank: 0] Seed set to 4
[2024-06-21 02:36:32,627][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 02:36:32,629][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-21 02:36:32,630][train.py][INFO] - Instantiating callbacks...
[2024-06-21 02:36:32,630][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 02:36:32,632][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 02:36:32,633][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 02:36:32,633][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 02:36:32,636][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 02:36:32,636][train.py][INFO] - Instantiating loggers...
[2024-06-21 02:36:32,636][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 02:36:32,638][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 02:36:32,701][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/friend_describe/0.3/4/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.17it/s v_num: 0.000      
                                                              val/auc: 0.250    
                                                              val/f1: 0.000     
                                                              val/precision:    
                                                              0.000 val/recall: 
                                                              0.000 val/mre:    
                                                              0.128 train/auc:  
                                                              0.975 train/f1:   
                                                              0.974             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              0.950 train/mre:  
                                                              0.141             
[2024-06-21 02:38:24,937][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/friend_describe/0.3/4>
[2024-06-21 02:38:24,937][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 02:38:24,939][HYDRA] 	#275 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=friend_describe data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=5
[rank: 0] Seed set to 5
[2024-06-21 02:38:25,120][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 02:38:25,122][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-21 02:38:25,123][train.py][INFO] - Instantiating callbacks...
[2024-06-21 02:38:25,123][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 02:38:25,125][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 02:38:25,126][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 02:38:25,126][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 02:38:25,129][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 02:38:25,130][train.py][INFO] - Instantiating loggers...
[2024-06-21 02:38:25,130][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 02:38:25,131][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 02:38:25,200][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/friend_describe/0.3/5/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.15it/s v_num: 0.000      
                                                              val/auc: 0.243    
                                                              val/f1: 0.235     
                                                              val/precision:    
                                                              0.200 val/recall: 
                                                              0.286 val/mre:    
                                                              0.140 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.148             
[2024-06-21 02:40:17,178][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/friend_describe/0.3/5>
[2024-06-21 02:40:17,178][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 02:40:17,181][HYDRA] 	#276 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=friend_describe data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=6
[rank: 0] Seed set to 6
[2024-06-21 02:40:17,390][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 02:40:17,392][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-21 02:40:17,393][train.py][INFO] - Instantiating callbacks...
[2024-06-21 02:40:17,393][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 02:40:17,396][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 02:40:17,396][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 02:40:17,397][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 02:40:17,399][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 02:40:17,399][train.py][INFO] - Instantiating loggers...
[2024-06-21 02:40:17,399][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 02:40:17,401][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 02:40:17,512][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/friend_describe/0.3/6/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.17it/s v_num: 0.000      
                                                              val/auc: 0.421    
                                                              val/f1: 0.182     
                                                              val/precision:    
                                                              0.250 val/recall: 
                                                              0.143 val/mre:    
                                                              0.125 train/auc:  
                                                              0.975 train/f1:   
                                                              0.974             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              0.950 train/mre:  
                                                              0.135             
[2024-06-21 02:42:09,578][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/friend_describe/0.3/6>
[2024-06-21 02:42:09,578][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 02:42:09,581][HYDRA] 	#277 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=friend_describe data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=7
[rank: 0] Seed set to 7
[2024-06-21 02:42:09,763][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 02:42:09,764][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-21 02:42:09,765][train.py][INFO] - Instantiating callbacks...
[2024-06-21 02:42:09,766][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 02:42:09,768][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 02:42:09,768][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 02:42:09,769][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 02:42:09,771][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 02:42:09,771][train.py][INFO] - Instantiating loggers...
[2024-06-21 02:42:09,772][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 02:42:09,773][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 02:42:09,837][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/friend_describe/0.3/7/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.17it/s v_num: 0.000      
                                                              val/auc: 0.200    
                                                              val/f1: 0.000     
                                                              val/precision:    
                                                              0.000 val/recall: 
                                                              0.000 val/mre:    
                                                              0.124 train/auc:  
                                                              0.967 train/f1:   
                                                              0.966             
                                                              train/precision:  
                                                              0.983             
                                                              train/recall:     
                                                              0.950 train/mre:  
                                                              0.135             
[2024-06-21 02:44:02,110][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/friend_describe/0.3/7>
[2024-06-21 02:44:02,110][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 02:44:02,113][HYDRA] 	#278 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=friend_describe data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=8
[rank: 0] Seed set to 8
[2024-06-21 02:44:02,296][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 02:44:02,298][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-21 02:44:02,299][train.py][INFO] - Instantiating callbacks...
[2024-06-21 02:44:02,299][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 02:44:02,301][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 02:44:02,302][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 02:44:02,302][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 02:44:02,305][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 02:44:02,305][train.py][INFO] - Instantiating loggers...
[2024-06-21 02:44:02,305][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 02:44:02,306][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 02:44:02,372][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/friend_describe/0.3/8/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.18it/s v_num: 0.000      
                                                              val/auc: 0.271    
                                                              val/f1: 0.143     
                                                              val/precision:    
                                                              0.143 val/recall: 
                                                              0.143 val/mre:    
                                                              0.125 train/auc:  
                                                              0.958 train/f1:   
                                                              0.958             
                                                              train/precision:  
                                                              0.966             
                                                              train/recall:     
                                                              0.950 train/mre:  
                                                              0.136             
[2024-06-21 02:45:54,503][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/friend_describe/0.3/8>
[2024-06-21 02:45:54,503][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 02:45:54,506][HYDRA] 	#279 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=friend_describe data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=9
[rank: 0] Seed set to 9
[2024-06-21 02:45:54,688][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 02:45:54,689][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-21 02:45:54,690][train.py][INFO] - Instantiating callbacks...
[2024-06-21 02:45:54,691][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 02:45:54,693][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 02:45:54,693][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 02:45:54,694][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 02:45:54,713][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 02:45:54,714][train.py][INFO] - Instantiating loggers...
[2024-06-21 02:45:54,714][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 02:45:54,715][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 02:45:54,801][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/friend_describe/0.3/9/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.18it/s v_num: 0.000      
                                                              val/auc: 0.371    
                                                              val/f1: 0.167     
                                                              val/precision:    
                                                              0.200 val/recall: 
                                                              0.143 val/mre:    
                                                              0.129 train/auc:  
                                                              0.992 train/f1:   
                                                              0.992             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              0.983 train/mre:  
                                                              0.138             
[2024-06-21 02:47:46,772][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/friend_describe/0.3/9>
[2024-06-21 02:47:46,773][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 02:47:46,775][HYDRA] 	#280 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=last_happy data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=0
[2024-06-21 02:47:46,956][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 02:47:46,958][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-21 02:47:46,959][train.py][INFO] - Instantiating callbacks...
[2024-06-21 02:47:46,959][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 02:47:46,961][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 02:47:46,962][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 02:47:46,962][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 02:47:46,965][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 02:47:46,965][train.py][INFO] - Instantiating loggers...
[2024-06-21 02:47:46,965][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 02:47:46,967][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 02:47:47,032][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/last_happy/0.0/0/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.17it/s v_num: 0.000      
                                                              val/auc: 0.400    
                                                              val/f1: 0.000     
                                                              val/precision:    
                                                              0.000 val/recall: 
                                                              0.000 val/mre: nan
                                                              train/auc: 0.992  
                                                              train/f1: 0.992   
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              0.983 train/mre:  
                                                              nan               
[2024-06-21 02:49:39,615][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/last_happy/0.0/0>
[2024-06-21 02:49:39,615][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 02:49:39,618][HYDRA] 	#281 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=last_happy data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=1
[rank: 0] Seed set to 1
[2024-06-21 02:49:39,884][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 02:49:39,886][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-21 02:49:39,887][train.py][INFO] - Instantiating callbacks...
[2024-06-21 02:49:39,887][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 02:49:39,889][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 02:49:39,890][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 02:49:39,890][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 02:49:39,893][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 02:49:39,893][train.py][INFO] - Instantiating loggers...
[2024-06-21 02:49:39,893][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 02:49:39,895][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 02:49:39,962][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/last_happy/0.0/1/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.17it/s v_num: 0.000      
                                                              val/auc: 0.525    
                                                              val/f1: 0.333     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.250 val/mre: nan
                                                              train/auc: 0.992  
                                                              train/f1: 0.992   
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              0.983 train/mre:  
                                                              nan               
[2024-06-21 02:51:32,659][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/last_happy/0.0/1>
[2024-06-21 02:51:32,660][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 02:51:32,662][HYDRA] 	#282 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=last_happy data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=2
[rank: 0] Seed set to 2
[2024-06-21 02:51:32,846][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 02:51:32,848][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-21 02:51:32,849][train.py][INFO] - Instantiating callbacks...
[2024-06-21 02:51:32,849][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 02:51:32,851][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 02:51:32,852][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 02:51:32,852][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 02:51:32,855][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 02:51:32,855][train.py][INFO] - Instantiating loggers...
[2024-06-21 02:51:32,855][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 02:51:32,856][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 02:51:32,975][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/last_happy/0.0/2/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.18it/s v_num: 0.000      
                                                              val/auc: 0.350    
                                                              val/f1: 0.000     
                                                              val/precision:    
                                                              0.000 val/recall: 
                                                              0.000 val/mre: nan
                                                              train/auc: 0.983  
                                                              train/f1: 0.983   
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              0.967 train/mre:  
                                                              nan               
[2024-06-21 02:53:25,373][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/last_happy/0.0/2>
[2024-06-21 02:53:25,373][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 02:53:25,376][HYDRA] 	#283 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=last_happy data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=3
[rank: 0] Seed set to 3
[2024-06-21 02:53:25,557][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 02:53:25,558][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-21 02:53:25,559][train.py][INFO] - Instantiating callbacks...
[2024-06-21 02:53:25,560][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 02:53:25,562][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 02:53:25,562][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 02:53:25,562][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 02:53:25,565][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 02:53:25,566][train.py][INFO] - Instantiating loggers...
[2024-06-21 02:53:25,566][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 02:53:25,567][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 02:53:25,631][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/last_happy/0.0/3/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.17it/s v_num: 0.000      
                                                              val/auc: 0.525    
                                                              val/f1: 0.333     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.250 val/mre: nan
                                                              train/auc: 0.992  
                                                              train/f1: 0.992   
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              0.983 train/mre:  
                                                              nan               
[2024-06-21 02:55:18,446][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/last_happy/0.0/3>
[2024-06-21 02:55:18,447][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 02:55:18,449][HYDRA] 	#284 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=last_happy data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=4
[rank: 0] Seed set to 4
[2024-06-21 02:55:18,634][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 02:55:18,635][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-21 02:55:18,637][train.py][INFO] - Instantiating callbacks...
[2024-06-21 02:55:18,637][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 02:55:18,639][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 02:55:18,639][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 02:55:18,640][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 02:55:18,642][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 02:55:18,643][train.py][INFO] - Instantiating loggers...
[2024-06-21 02:55:18,643][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 02:55:18,644][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 02:55:18,713][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/last_happy/0.0/4/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.17it/s v_num: 0.000      
                                                              val/auc: 0.350    
                                                              val/f1: 0.000     
                                                              val/precision:    
                                                              0.000 val/recall: 
                                                              0.000 val/mre: nan
                                                              train/auc: 0.992  
                                                              train/f1: 0.992   
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              0.983 train/mre:  
                                                              nan               
[2024-06-21 02:57:10,927][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/last_happy/0.0/4>
[2024-06-21 02:57:10,928][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 02:57:10,930][HYDRA] 	#285 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=last_happy data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=5
[rank: 0] Seed set to 5
[2024-06-21 02:57:11,116][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 02:57:11,117][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-21 02:57:11,118][train.py][INFO] - Instantiating callbacks...
[2024-06-21 02:57:11,118][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 02:57:11,120][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 02:57:11,121][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 02:57:11,121][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 02:57:11,124][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 02:57:11,124][train.py][INFO] - Instantiating loggers...
[2024-06-21 02:57:11,125][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 02:57:11,126][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 02:57:11,191][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/last_happy/0.0/5/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.17it/s v_num: 0.000      
                                                              val/auc: 0.363    
                                                              val/f1: 0.154     
                                                              val/precision:    
                                                              0.200 val/recall: 
                                                              0.125 val/mre: nan
                                                              train/auc: 0.992  
                                                              train/f1: 0.992   
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              0.983 train/mre:  
                                                              nan               
[2024-06-21 02:59:03,195][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/last_happy/0.0/5>
[2024-06-21 02:59:03,195][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 02:59:03,197][HYDRA] 	#286 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=last_happy data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=6
[rank: 0] Seed set to 6
[2024-06-21 02:59:03,378][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 02:59:03,380][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-21 02:59:03,381][train.py][INFO] - Instantiating callbacks...
[2024-06-21 02:59:03,381][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 02:59:03,383][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 02:59:03,384][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 02:59:03,384][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 02:59:03,388][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 02:59:03,388][train.py][INFO] - Instantiating loggers...
[2024-06-21 02:59:03,388][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 02:59:03,389][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 02:59:03,458][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/last_happy/0.0/6/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.17it/s v_num: 0.000      
                                                              val/auc: 0.512    
                                                              val/f1: 0.200     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.125 val/mre: nan
                                                              train/auc: 0.983  
                                                              train/f1: 0.984   
                                                              train/precision:  
                                                              0.968             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              nan               
[2024-06-21 03:00:57,058][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/last_happy/0.0/6>
[2024-06-21 03:00:57,059][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 03:00:57,062][HYDRA] 	#287 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=last_happy data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=7
[rank: 0] Seed set to 7
[2024-06-21 03:00:57,249][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 03:00:57,250][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-21 03:00:57,251][train.py][INFO] - Instantiating callbacks...
[2024-06-21 03:00:57,252][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 03:00:57,254][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 03:00:57,254][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 03:00:57,255][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 03:00:57,258][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 03:00:57,258][train.py][INFO] - Instantiating loggers...
[2024-06-21 03:00:57,258][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 03:00:57,259][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 03:00:57,326][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/last_happy/0.0/7/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.18it/s v_num: 0.000      
                                                              val/auc: 0.412    
                                                              val/f1: 0.167     
                                                              val/precision:    
                                                              0.250 val/recall: 
                                                              0.125 val/mre: nan
                                                              train/auc: 0.992  
                                                              train/f1: 0.992   
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              0.983 train/mre:  
                                                              nan               
[2024-06-21 03:02:50,010][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/last_happy/0.0/7>
[2024-06-21 03:02:50,010][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 03:02:50,013][HYDRA] 	#288 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=last_happy data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=8
[rank: 0] Seed set to 8
[2024-06-21 03:02:50,195][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 03:02:50,197][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-21 03:02:50,198][train.py][INFO] - Instantiating callbacks...
[2024-06-21 03:02:50,198][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 03:02:50,200][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 03:02:50,201][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 03:02:50,201][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 03:02:50,204][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 03:02:50,204][train.py][INFO] - Instantiating loggers...
[2024-06-21 03:02:50,204][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 03:02:50,206][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 03:02:50,272][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/last_happy/0.0/8/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.18it/s v_num: 0.000      
                                                              val/auc: 0.412    
                                                              val/f1: 0.476     
                                                              val/precision:    
                                                              0.385 val/recall: 
                                                              0.625 val/mre: nan
                                                              train/auc: 0.983  
                                                              train/f1: 0.983   
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              0.967 train/mre:  
                                                              nan               
[2024-06-21 03:04:42,181][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/last_happy/0.0/8>
[2024-06-21 03:04:42,181][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 03:04:42,184][HYDRA] 	#289 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=last_happy data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=9
[rank: 0] Seed set to 9
[2024-06-21 03:04:42,372][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 03:04:42,374][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-21 03:04:42,375][train.py][INFO] - Instantiating callbacks...
[2024-06-21 03:04:42,375][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 03:04:42,377][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 03:04:42,378][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 03:04:42,378][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 03:04:42,381][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 03:04:42,381][train.py][INFO] - Instantiating loggers...
[2024-06-21 03:04:42,381][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 03:04:42,382][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 03:04:42,448][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/last_happy/0.0/9/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.17it/s v_num: 0.000      
                                                              val/auc: 0.400    
                                                              val/f1: 0.421     
                                                              val/precision:    
                                                              0.364 val/recall: 
                                                              0.500 val/mre: nan
                                                              train/auc: 0.975  
                                                              train/f1: 0.974   
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              0.950 train/mre:  
                                                              nan               
[2024-06-21 03:06:35,958][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/last_happy/0.0/9>
[2024-06-21 03:06:35,959][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 03:06:35,962][HYDRA] 	#290 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=last_happy data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=0
[2024-06-21 03:06:36,144][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 03:06:36,145][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-21 03:06:36,147][train.py][INFO] - Instantiating callbacks...
[2024-06-21 03:06:36,147][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 03:06:36,149][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 03:06:36,149][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 03:06:36,150][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 03:06:36,152][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 03:06:36,153][train.py][INFO] - Instantiating loggers...
[2024-06-21 03:06:36,153][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 03:06:36,154][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 03:06:36,219][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/last_happy/0.05/0/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.17it/s v_num: 0.000      
                                                              val/auc: 0.425    
                                                              val/f1: 0.286     
                                                              val/precision:    
                                                              0.333 val/recall: 
                                                              0.250 val/mre:    
                                                              0.130 train/auc:  
                                                              0.992 train/f1:   
                                                              0.992             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              0.983 train/mre:  
                                                              0.135             
[2024-06-21 03:08:28,834][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/last_happy/0.05/0>
[2024-06-21 03:08:28,835][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 03:08:28,837][HYDRA] 	#291 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=last_happy data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=1
[rank: 0] Seed set to 1
[2024-06-21 03:08:29,019][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 03:08:29,021][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-21 03:08:29,022][train.py][INFO] - Instantiating callbacks...
[2024-06-21 03:08:29,022][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 03:08:29,024][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 03:08:29,025][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 03:08:29,025][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 03:08:29,028][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 03:08:29,028][train.py][INFO] - Instantiating loggers...
[2024-06-21 03:08:29,028][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 03:08:29,029][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 03:08:29,093][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/last_happy/0.05/1/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.19it/s v_num: 0.000      
                                                              val/auc: 0.412    
                                                              val/f1: 0.167     
                                                              val/precision:    
                                                              0.250 val/recall: 
                                                              0.125 val/mre:    
                                                              0.134 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.141             
[2024-06-21 03:10:21,044][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/last_happy/0.05/1>
[2024-06-21 03:10:21,045][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 03:10:21,047][HYDRA] 	#292 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=last_happy data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=2
[rank: 0] Seed set to 2
[2024-06-21 03:10:21,234][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 03:10:21,236][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-21 03:10:21,237][train.py][INFO] - Instantiating callbacks...
[2024-06-21 03:10:21,237][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 03:10:21,239][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 03:10:21,240][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 03:10:21,240][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 03:10:21,242][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 03:10:21,243][train.py][INFO] - Instantiating loggers...
[2024-06-21 03:10:21,243][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 03:10:21,244][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 03:10:21,321][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/last_happy/0.05/2/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.14it/s v_num: 0.000      
                                                              val/auc: 0.512    
                                                              val/f1: 0.200     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.125 val/mre:    
                                                              0.135 train/auc:  
                                                              0.983 train/f1:   
                                                              0.983             
                                                              train/precision:  
                                                              0.983             
                                                              train/recall:     
                                                              0.983 train/mre:  
                                                              0.137             
[2024-06-21 03:12:14,468][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/last_happy/0.05/2>
[2024-06-21 03:12:14,469][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 03:12:14,471][HYDRA] 	#293 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=last_happy data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=3
[rank: 0] Seed set to 3
[2024-06-21 03:12:14,654][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 03:12:14,656][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-21 03:12:14,657][train.py][INFO] - Instantiating callbacks...
[2024-06-21 03:12:14,657][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 03:12:14,659][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 03:12:14,660][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 03:12:14,660][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 03:12:14,663][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 03:12:14,663][train.py][INFO] - Instantiating loggers...
[2024-06-21 03:12:14,663][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 03:12:14,665][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 03:12:14,728][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/last_happy/0.05/3/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.17it/s v_num: 0.000      
                                                              val/auc: 0.213    
                                                              val/f1: 0.125     
                                                              val/precision:    
                                                              0.125 val/recall: 
                                                              0.125 val/mre:    
                                                              0.127 train/auc:  
                                                              0.992 train/f1:   
                                                              0.992             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              0.983 train/mre:  
                                                              0.135             
[2024-06-21 03:14:07,373][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/last_happy/0.05/3>
[2024-06-21 03:14:07,373][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 03:14:07,376][HYDRA] 	#294 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=last_happy data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=4
[rank: 0] Seed set to 4
[2024-06-21 03:14:07,559][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 03:14:07,561][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-21 03:14:07,562][train.py][INFO] - Instantiating callbacks...
[2024-06-21 03:14:07,562][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 03:14:07,564][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 03:14:07,565][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 03:14:07,565][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 03:14:07,568][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 03:14:07,568][train.py][INFO] - Instantiating loggers...
[2024-06-21 03:14:07,568][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 03:14:07,570][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 03:14:07,633][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/last_happy/0.05/4/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.19it/s v_num: 0.000      
                                                              val/auc: 0.300    
                                                              val/f1: 0.000     
                                                              val/precision:    
                                                              0.000 val/recall: 
                                                              0.000 val/mre:    
                                                              0.129 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.134             
[2024-06-21 03:15:59,789][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/last_happy/0.05/4>
[2024-06-21 03:15:59,790][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 03:15:59,795][HYDRA] 	#295 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=last_happy data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=5
[rank: 0] Seed set to 5
[2024-06-21 03:16:00,042][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 03:16:00,044][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-21 03:16:00,045][train.py][INFO] - Instantiating callbacks...
[2024-06-21 03:16:00,045][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 03:16:00,050][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 03:16:00,050][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 03:16:00,051][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 03:16:00,057][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 03:16:00,058][train.py][INFO] - Instantiating loggers...
[2024-06-21 03:16:00,058][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 03:16:00,059][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 03:16:00,177][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/last_happy/0.05/5/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.17it/s v_num: 0.000      
                                                              val/auc: 0.400    
                                                              val/f1: 0.000     
                                                              val/precision:    
                                                              0.000 val/recall: 
                                                              0.000 val/mre:    
                                                              0.138 train/auc:  
                                                              0.967 train/f1:   
                                                              0.968             
                                                              train/precision:  
                                                              0.938             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.143             
[2024-06-21 03:17:52,647][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/last_happy/0.05/5>
[2024-06-21 03:17:52,647][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 03:17:52,651][HYDRA] 	#296 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=last_happy data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=6
[rank: 0] Seed set to 6
[2024-06-21 03:17:52,837][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 03:17:52,838][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-21 03:17:52,840][train.py][INFO] - Instantiating callbacks...
[2024-06-21 03:17:52,840][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 03:17:52,842][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 03:17:52,842][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 03:17:52,843][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 03:17:52,846][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 03:17:52,846][train.py][INFO] - Instantiating loggers...
[2024-06-21 03:17:52,846][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 03:17:52,847][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 03:17:52,914][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/last_happy/0.05/6/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.18it/s v_num: 0.000      
                                                              val/auc: 0.375    
                                                              val/f1: 0.267     
                                                              val/precision:    
                                                              0.286 val/recall: 
                                                              0.250 val/mre:    
                                                              0.129 train/auc:  
                                                              0.992 train/f1:   
                                                              0.992             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              0.983 train/mre:  
                                                              0.136             
[2024-06-21 03:19:45,643][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/last_happy/0.05/6>
[2024-06-21 03:19:45,643][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 03:19:45,646][HYDRA] 	#297 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=last_happy data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=7
[rank: 0] Seed set to 7
[2024-06-21 03:19:45,831][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 03:19:45,833][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-21 03:19:45,834][train.py][INFO] - Instantiating callbacks...
[2024-06-21 03:19:45,834][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 03:19:45,836][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 03:19:45,837][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 03:19:45,837][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 03:19:45,840][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 03:19:45,840][train.py][INFO] - Instantiating loggers...
[2024-06-21 03:19:45,840][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 03:19:45,842][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 03:19:45,908][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/last_happy/0.05/7/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.19it/s v_num: 0.000      
                                                              val/auc: 0.425    
                                                              val/f1: 0.286     
                                                              val/precision:    
                                                              0.333 val/recall: 
                                                              0.250 val/mre:    
                                                              0.134 train/auc:  
                                                              0.992 train/f1:   
                                                              0.992             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              0.983 train/mre:  
                                                              0.140             
[2024-06-21 03:21:38,810][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/last_happy/0.05/7>
[2024-06-21 03:21:38,811][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 03:21:38,813][HYDRA] 	#298 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=last_happy data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=8
[rank: 0] Seed set to 8
[2024-06-21 03:21:39,004][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 03:21:39,006][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-21 03:21:39,007][train.py][INFO] - Instantiating callbacks...
[2024-06-21 03:21:39,007][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 03:21:39,010][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 03:21:39,010][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 03:21:39,011][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 03:21:39,020][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 03:21:39,020][train.py][INFO] - Instantiating loggers...
[2024-06-21 03:21:39,020][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 03:21:39,022][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 03:21:39,165][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/last_happy/0.05/8/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.17it/s v_num: 0.000      
                                                              val/auc: 0.438    
                                                              val/f1: 0.375     
                                                              val/precision:    
                                                              0.375 val/recall: 
                                                              0.375 val/mre:    
                                                              0.131 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.137             
[2024-06-21 03:23:32,687][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/last_happy/0.05/8>
[2024-06-21 03:23:32,687][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 03:23:32,690][HYDRA] 	#299 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=last_happy data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=9
[rank: 0] Seed set to 9
[2024-06-21 03:23:32,877][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 03:23:32,878][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-21 03:23:32,879][train.py][INFO] - Instantiating callbacks...
[2024-06-21 03:23:32,880][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 03:23:32,882][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 03:23:32,882][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 03:23:32,883][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 03:23:32,886][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 03:23:32,886][train.py][INFO] - Instantiating loggers...
[2024-06-21 03:23:32,886][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 03:23:32,887][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 03:23:32,955][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/last_happy/0.05/9/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.18it/s v_num: 0.000      
                                                              val/auc: 0.488    
                                                              val/f1: 0.400     
                                                              val/precision:    
                                                              0.429 val/recall: 
                                                              0.375 val/mre:    
                                                              0.129 train/auc:  
                                                              0.992 train/f1:   
                                                              0.992             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              0.983 train/mre:  
                                                              0.134             
[2024-06-21 03:25:25,553][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/last_happy/0.05/9>
[2024-06-21 03:25:25,553][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 03:25:25,556][HYDRA] 	#300 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=last_happy data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=0
[2024-06-21 03:25:25,739][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 03:25:25,741][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-21 03:25:25,742][train.py][INFO] - Instantiating callbacks...
[2024-06-21 03:25:25,742][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 03:25:25,745][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 03:25:25,745][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 03:25:25,746][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 03:25:25,748][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 03:25:25,749][train.py][INFO] - Instantiating loggers...
[2024-06-21 03:25:25,749][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 03:25:25,750][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 03:25:25,822][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/last_happy/0.1/0/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.19it/s v_num: 0.000      
                                                              val/auc: 0.512    
                                                              val/f1: 0.200     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.125 val/mre:    
                                                              0.135 train/auc:  
                                                              0.967 train/f1:   
                                                              0.966             
                                                              train/precision:  
                                                              0.983             
                                                              train/recall:     
                                                              0.950 train/mre:  
                                                              0.142             
[2024-06-21 03:27:17,488][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/last_happy/0.1/0>
[2024-06-21 03:27:17,488][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 03:27:17,491][HYDRA] 	#301 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=last_happy data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=1
[rank: 0] Seed set to 1
[2024-06-21 03:27:17,673][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 03:27:17,674][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-21 03:27:17,675][train.py][INFO] - Instantiating callbacks...
[2024-06-21 03:27:17,675][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 03:27:17,678][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 03:27:17,678][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 03:27:17,678][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 03:27:17,681][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 03:27:17,681][train.py][INFO] - Instantiating loggers...
[2024-06-21 03:27:17,682][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 03:27:17,683][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 03:27:17,748][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/last_happy/0.1/1/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.14it/s v_num: 0.000      
                                                              val/auc: 0.363    
                                                              val/f1: 0.154     
                                                              val/precision:    
                                                              0.200 val/recall: 
                                                              0.125 val/mre:    
                                                              0.144 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.150             
[2024-06-21 03:29:10,521][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/last_happy/0.1/1>
[2024-06-21 03:29:10,522][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 03:29:10,524][HYDRA] 	#302 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=last_happy data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=2
[rank: 0] Seed set to 2
[2024-06-21 03:29:10,708][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 03:29:10,710][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-21 03:29:10,711][train.py][INFO] - Instantiating callbacks...
[2024-06-21 03:29:10,711][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 03:29:10,713][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 03:29:10,714][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 03:29:10,714][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 03:29:10,717][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 03:29:10,717][train.py][INFO] - Instantiating loggers...
[2024-06-21 03:29:10,718][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 03:29:10,719][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 03:29:10,787][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/last_happy/0.1/2/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.19it/s v_num: 0.000      
                                                              val/auc: 0.475    
                                                              val/f1: 0.308     
                                                              val/precision:    
                                                              0.400 val/recall: 
                                                              0.250 val/mre:    
                                                              0.140 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.146             
[2024-06-21 03:31:03,042][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/last_happy/0.1/2>
[2024-06-21 03:31:03,043][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 03:31:03,045][HYDRA] 	#303 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=last_happy data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=3
[rank: 0] Seed set to 3
[2024-06-21 03:31:03,228][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 03:31:03,230][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-21 03:31:03,231][train.py][INFO] - Instantiating callbacks...
[2024-06-21 03:31:03,231][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 03:31:03,233][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 03:31:03,234][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 03:31:03,234][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 03:31:03,240][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 03:31:03,241][train.py][INFO] - Instantiating loggers...
[2024-06-21 03:31:03,241][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 03:31:03,242][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 03:31:03,309][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/last_happy/0.1/3/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.19it/s v_num: 0.000      
                                                              val/auc: 0.375    
                                                              val/f1: 0.267     
                                                              val/precision:    
                                                              0.286 val/recall: 
                                                              0.250 val/mre:    
                                                              0.136 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.143             
[2024-06-21 03:32:55,322][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/last_happy/0.1/3>
[2024-06-21 03:32:55,323][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 03:32:55,326][HYDRA] 	#304 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=last_happy data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=4
[rank: 0] Seed set to 4
[2024-06-21 03:32:55,516][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 03:32:55,517][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-21 03:32:55,518][train.py][INFO] - Instantiating callbacks...
[2024-06-21 03:32:55,519][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 03:32:55,521][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 03:32:55,521][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 03:32:55,522][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 03:32:55,524][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 03:32:55,525][train.py][INFO] - Instantiating loggers...
[2024-06-21 03:32:55,525][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 03:32:55,526][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 03:32:55,592][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/last_happy/0.1/4/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.18it/s v_num: 0.000      
                                                              val/auc: 0.412    
                                                              val/f1: 0.167     
                                                              val/precision:    
                                                              0.250 val/recall: 
                                                              0.125 val/mre:    
                                                              0.139 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.145             
[2024-06-21 03:34:49,052][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/last_happy/0.1/4>
[2024-06-21 03:34:49,053][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 03:34:49,056][HYDRA] 	#305 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=last_happy data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=5
[rank: 0] Seed set to 5
[2024-06-21 03:34:49,241][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 03:34:49,243][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-21 03:34:49,244][train.py][INFO] - Instantiating callbacks...
[2024-06-21 03:34:49,244][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 03:34:49,246][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 03:34:49,247][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 03:34:49,247][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 03:34:49,250][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 03:34:49,250][train.py][INFO] - Instantiating loggers...
[2024-06-21 03:34:49,251][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 03:34:49,252][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 03:34:49,318][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/last_happy/0.1/5/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.18it/s v_num: 0.000      
                                                              val/auc: 0.300    
                                                              val/f1: 0.000     
                                                              val/precision:    
                                                              0.000 val/recall: 
                                                              0.000 val/mre:    
                                                              0.145 train/auc:  
                                                              0.983 train/f1:   
                                                              0.983             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              0.967 train/mre:  
                                                              0.152             
[2024-06-21 03:36:41,530][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/last_happy/0.1/5>
[2024-06-21 03:36:41,530][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 03:36:41,536][HYDRA] 	#306 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=last_happy data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=6
[rank: 0] Seed set to 6
[2024-06-21 03:36:41,720][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 03:36:41,722][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-21 03:36:41,723][train.py][INFO] - Instantiating callbacks...
[2024-06-21 03:36:41,723][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 03:36:41,725][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 03:36:41,726][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 03:36:41,726][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 03:36:41,729][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 03:36:41,729][train.py][INFO] - Instantiating loggers...
[2024-06-21 03:36:41,729][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 03:36:41,731][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 03:36:41,820][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/last_happy/0.1/6/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.20it/s v_num: 0.000      
                                                              val/auc: 0.312    
                                                              val/f1: 0.143     
                                                              val/precision:    
                                                              0.167 val/recall: 
                                                              0.125 val/mre:    
                                                              0.135 train/auc:  
                                                              0.983 train/f1:   
                                                              0.983             
                                                              train/precision:  
                                                              0.983             
                                                              train/recall:     
                                                              0.983 train/mre:  
                                                              0.142             
[2024-06-21 03:38:33,190][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/last_happy/0.1/6>
[2024-06-21 03:38:33,190][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 03:38:33,193][HYDRA] 	#307 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=last_happy data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=7
[rank: 0] Seed set to 7
[2024-06-21 03:38:33,376][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 03:38:33,377][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-21 03:38:33,378][train.py][INFO] - Instantiating callbacks...
[2024-06-21 03:38:33,379][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 03:38:33,381][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 03:38:33,381][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 03:38:33,382][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 03:38:33,385][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 03:38:33,385][train.py][INFO] - Instantiating loggers...
[2024-06-21 03:38:33,385][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 03:38:33,387][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 03:38:33,454][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/last_happy/0.1/7/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.18it/s v_num: 0.000      
                                                              val/auc: 0.400    
                                                              val/f1: 0.421     
                                                              val/precision:    
                                                              0.364 val/recall: 
                                                              0.500 val/mre:    
                                                              0.143 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.148             
[2024-06-21 03:40:25,958][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/last_happy/0.1/7>
[2024-06-21 03:40:25,959][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 03:40:25,961][HYDRA] 	#308 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=last_happy data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=8
[rank: 0] Seed set to 8
[2024-06-21 03:40:26,144][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 03:40:26,146][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-21 03:40:26,147][train.py][INFO] - Instantiating callbacks...
[2024-06-21 03:40:26,147][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 03:40:26,149][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 03:40:26,150][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 03:40:26,150][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 03:40:26,153][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 03:40:26,153][train.py][INFO] - Instantiating loggers...
[2024-06-21 03:40:26,153][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 03:40:26,154][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 03:40:26,227][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/last_happy/0.1/8/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.18it/s v_num: 0.000      
                                                              val/auc: 0.325    
                                                              val/f1: 0.250     
                                                              val/precision:    
                                                              0.250 val/recall: 
                                                              0.250 val/mre:    
                                                              0.142 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.148             
[2024-06-21 03:42:18,541][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/last_happy/0.1/8>
[2024-06-21 03:42:18,542][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 03:42:18,544][HYDRA] 	#309 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=last_happy data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=9
[rank: 0] Seed set to 9
[2024-06-21 03:42:18,728][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 03:42:18,729][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-21 03:42:18,731][train.py][INFO] - Instantiating callbacks...
[2024-06-21 03:42:18,731][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 03:42:18,733][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 03:42:18,733][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 03:42:18,734][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 03:42:18,736][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 03:42:18,736][train.py][INFO] - Instantiating loggers...
[2024-06-21 03:42:18,737][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 03:42:18,738][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 03:42:18,801][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/last_happy/0.1/9/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.20it/s v_num: 0.000      
                                                              val/auc: 0.488    
                                                              val/f1: 0.400     
                                                              val/precision:    
                                                              0.429 val/recall: 
                                                              0.375 val/mre:    
                                                              0.140 train/auc:  
                                                              0.992 train/f1:   
                                                              0.992             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              0.983 train/mre:  
                                                              0.143             
[2024-06-21 03:44:10,249][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/last_happy/0.1/9>
[2024-06-21 03:44:10,250][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 03:44:10,252][HYDRA] 	#310 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=last_happy data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=0
[2024-06-21 03:44:10,434][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 03:44:10,435][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-21 03:44:10,437][train.py][INFO] - Instantiating callbacks...
[2024-06-21 03:44:10,437][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 03:44:10,439][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 03:44:10,439][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 03:44:10,440][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 03:44:10,442][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 03:44:10,443][train.py][INFO] - Instantiating loggers...
[2024-06-21 03:44:10,443][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 03:44:10,444][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 03:44:10,509][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/last_happy/0.15/0/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.18it/s v_num: 0.000      
                                                              val/auc: 0.400    
                                                              val/f1: 0.000     
                                                              val/precision:    
                                                              0.000 val/recall: 
                                                              0.000 val/mre:    
                                                              0.148 train/auc:  
                                                              0.967 train/f1:   
                                                              0.966             
                                                              train/precision:  
                                                              0.983             
                                                              train/recall:     
                                                              0.950 train/mre:  
                                                              0.151             
[2024-06-21 03:46:03,982][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/last_happy/0.15/0>
[2024-06-21 03:46:03,983][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 03:46:03,987][HYDRA] 	#311 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=last_happy data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=1
[rank: 0] Seed set to 1
[2024-06-21 03:46:04,204][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 03:46:04,205][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-21 03:46:04,207][train.py][INFO] - Instantiating callbacks...
[2024-06-21 03:46:04,207][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 03:46:04,209][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 03:46:04,209][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 03:46:04,210][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 03:46:04,212][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 03:46:04,213][train.py][INFO] - Instantiating loggers...
[2024-06-21 03:46:04,213][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 03:46:04,214][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 03:46:04,283][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/last_happy/0.15/1/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.19it/s v_num: 0.000      
                                                              val/auc: 0.363    
                                                              val/f1: 0.154     
                                                              val/precision:    
                                                              0.200 val/recall: 
                                                              0.125 val/mre:    
                                                              0.151 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.158             
[2024-06-21 03:47:56,516][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/last_happy/0.15/1>
[2024-06-21 03:47:56,517][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 03:47:56,519][HYDRA] 	#312 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=last_happy data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=2
[rank: 0] Seed set to 2
[2024-06-21 03:47:56,704][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 03:47:56,706][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-21 03:47:56,707][train.py][INFO] - Instantiating callbacks...
[2024-06-21 03:47:56,707][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 03:47:56,709][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 03:47:56,710][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 03:47:56,710][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 03:47:56,713][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 03:47:56,713][train.py][INFO] - Instantiating loggers...
[2024-06-21 03:47:56,713][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 03:47:56,714][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 03:47:56,779][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/last_happy/0.15/2/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.17it/s v_num: 0.000      
                                                              val/auc: 0.363    
                                                              val/f1: 0.154     
                                                              val/precision:    
                                                              0.200 val/recall: 
                                                              0.125 val/mre:    
                                                              0.151 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.155             
[2024-06-21 03:49:48,531][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/last_happy/0.15/2>
[2024-06-21 03:49:48,532][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 03:49:48,534][HYDRA] 	#313 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=last_happy data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=3
[rank: 0] Seed set to 3
[2024-06-21 03:49:48,716][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 03:49:48,718][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-21 03:49:48,719][train.py][INFO] - Instantiating callbacks...
[2024-06-21 03:49:48,719][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 03:49:48,721][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 03:49:48,721][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 03:49:48,722][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 03:49:48,725][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 03:49:48,726][train.py][INFO] - Instantiating loggers...
[2024-06-21 03:49:48,726][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 03:49:48,727][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 03:49:48,793][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/last_happy/0.15/3/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.18it/s v_num: 0.000      
                                                              val/auc: 0.500    
                                                              val/f1: 0.471     
                                                              val/precision:    
                                                              0.444 val/recall: 
                                                              0.500 val/mre:    
                                                              0.142 train/auc:  
                                                              0.967 train/f1:   
                                                              0.967             
                                                              train/precision:  
                                                              0.952             
                                                              train/recall:     
                                                              0.983 train/mre:  
                                                              0.148             
[2024-06-21 03:51:42,933][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/last_happy/0.15/3>
[2024-06-21 03:51:42,934][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 03:51:42,936][HYDRA] 	#314 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=last_happy data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=4
[rank: 0] Seed set to 4
[2024-06-21 03:51:43,136][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 03:51:43,137][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-21 03:51:43,138][train.py][INFO] - Instantiating callbacks...
[2024-06-21 03:51:43,139][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 03:51:43,141][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 03:51:43,141][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 03:51:43,142][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 03:51:43,144][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 03:51:43,145][train.py][INFO] - Instantiating loggers...
[2024-06-21 03:51:43,145][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 03:51:43,146][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 03:51:43,270][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/last_happy/0.15/4/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.19it/s v_num: 0.000      
                                                              val/auc: 0.350    
                                                              val/f1: 0.400     
                                                              val/precision:    
                                                              0.333 val/recall: 
                                                              0.500 val/mre:    
                                                              0.144 train/auc:  
                                                              0.967 train/f1:   
                                                              0.967             
                                                              train/precision:  
                                                              0.952             
                                                              train/recall:     
                                                              0.983 train/mre:  
                                                              0.151             
[2024-06-21 03:53:35,614][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/last_happy/0.15/4>
[2024-06-21 03:53:35,615][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 03:53:35,617][HYDRA] 	#315 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=last_happy data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=5
[rank: 0] Seed set to 5
[2024-06-21 03:53:35,802][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 03:53:35,803][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-21 03:53:35,804][train.py][INFO] - Instantiating callbacks...
[2024-06-21 03:53:35,805][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 03:53:35,807][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 03:53:35,807][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 03:53:35,808][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 03:53:35,810][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 03:53:35,811][train.py][INFO] - Instantiating loggers...
[2024-06-21 03:53:35,811][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 03:53:35,812][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 03:53:35,880][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/last_happy/0.15/5/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.18it/s v_num: 0.000      
                                                              val/auc: 0.438    
                                                              val/f1: 0.375     
                                                              val/precision:    
                                                              0.375 val/recall: 
                                                              0.375 val/mre:    
                                                              0.153 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.159             
[2024-06-21 03:55:28,074][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/last_happy/0.15/5>
[2024-06-21 03:55:28,075][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 03:55:28,077][HYDRA] 	#316 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=last_happy data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=6
[rank: 0] Seed set to 6
[2024-06-21 03:55:28,260][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 03:55:28,262][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-21 03:55:28,263][train.py][INFO] - Instantiating callbacks...
[2024-06-21 03:55:28,263][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 03:55:28,265][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 03:55:28,266][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 03:55:28,266][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 03:55:28,269][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 03:55:28,269][train.py][INFO] - Instantiating loggers...
[2024-06-21 03:55:28,269][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 03:55:28,271][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 03:55:28,338][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/last_happy/0.15/6/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.18it/s v_num: 0.000      
                                                              val/auc: 0.488    
                                                              val/f1: 0.400     
                                                              val/precision:    
                                                              0.429 val/recall: 
                                                              0.375 val/mre:    
                                                              0.141 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.148             
[2024-06-21 03:57:21,063][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/last_happy/0.15/6>
[2024-06-21 03:57:21,064][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 03:57:21,070][HYDRA] 	#317 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=last_happy data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=7
[rank: 0] Seed set to 7
[2024-06-21 03:57:21,257][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 03:57:21,259][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-21 03:57:21,260][train.py][INFO] - Instantiating callbacks...
[2024-06-21 03:57:21,260][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 03:57:21,262][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 03:57:21,262][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 03:57:21,263][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 03:57:21,266][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 03:57:21,267][train.py][INFO] - Instantiating loggers...
[2024-06-21 03:57:21,267][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 03:57:21,268][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 03:57:21,336][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/last_happy/0.15/7/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.19it/s v_num: 0.000      
                                                              val/auc: 0.262    
                                                              val/f1: 0.133     
                                                              val/precision:    
                                                              0.143 val/recall: 
                                                              0.125 val/mre:    
                                                              0.149 train/auc:  
                                                              0.992 train/f1:   
                                                              0.992             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              0.983 train/mre:  
                                                              0.154             
[2024-06-21 03:59:13,417][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/last_happy/0.15/7>
[2024-06-21 03:59:13,417][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 03:59:13,420][HYDRA] 	#318 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=last_happy data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=8
[rank: 0] Seed set to 8
[2024-06-21 03:59:13,612][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 03:59:13,614][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-21 03:59:13,615][train.py][INFO] - Instantiating callbacks...
[2024-06-21 03:59:13,615][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 03:59:13,617][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 03:59:13,618][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 03:59:13,618][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 03:59:13,621][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 03:59:13,621][train.py][INFO] - Instantiating loggers...
[2024-06-21 03:59:13,621][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 03:59:13,623][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 03:59:13,689][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/last_happy/0.15/8/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.19it/s v_num: 0.000      
                                                              val/auc: 0.387    
                                                              val/f1: 0.353     
                                                              val/precision:    
                                                              0.333 val/recall: 
                                                              0.375 val/mre:    
                                                              0.147 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.154             
[2024-06-21 04:01:05,575][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/last_happy/0.15/8>
[2024-06-21 04:01:05,576][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 04:01:05,579][HYDRA] 	#319 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=last_happy data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=9
[rank: 0] Seed set to 9
[2024-06-21 04:01:05,767][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 04:01:05,768][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-21 04:01:05,769][train.py][INFO] - Instantiating callbacks...
[2024-06-21 04:01:05,770][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 04:01:05,772][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 04:01:05,772][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 04:01:05,773][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 04:01:05,801][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 04:01:05,801][train.py][INFO] - Instantiating loggers...
[2024-06-21 04:01:05,801][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 04:01:05,803][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 04:01:05,893][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/last_happy/0.15/9/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.18it/s v_num: 0.000      
                                                              val/auc: 0.425    
                                                              val/f1: 0.286     
                                                              val/precision:    
                                                              0.333 val/recall: 
                                                              0.250 val/mre:    
                                                              0.144 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.150             
[2024-06-21 04:02:58,053][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/last_happy/0.15/9>
[2024-06-21 04:02:58,053][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 04:02:58,056][HYDRA] 	#320 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=last_happy data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=0
[2024-06-21 04:02:58,280][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 04:02:58,282][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-21 04:02:58,283][train.py][INFO] - Instantiating callbacks...
[2024-06-21 04:02:58,283][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 04:02:58,286][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 04:02:58,286][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 04:02:58,287][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 04:02:58,289][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 04:02:58,289][train.py][INFO] - Instantiating loggers...
[2024-06-21 04:02:58,290][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 04:02:58,291][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 04:02:58,457][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/last_happy/0.2/0/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.18it/s v_num: 0.000      
                                                              val/auc: 0.325    
                                                              val/f1: 0.250     
                                                              val/precision:    
                                                              0.250 val/recall: 
                                                              0.250 val/mre:    
                                                              0.155 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.158             
[2024-06-21 04:04:50,529][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/last_happy/0.2/0>
[2024-06-21 04:04:50,530][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 04:04:50,533][HYDRA] 	#321 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=last_happy data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=1
[rank: 0] Seed set to 1
[2024-06-21 04:04:50,723][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 04:04:50,724][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-21 04:04:50,725][train.py][INFO] - Instantiating callbacks...
[2024-06-21 04:04:50,726][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 04:04:50,728][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 04:04:50,728][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 04:04:50,729][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 04:04:50,731][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 04:04:50,731][train.py][INFO] - Instantiating loggers...
[2024-06-21 04:04:50,732][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 04:04:50,733][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 04:04:50,800][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/last_happy/0.2/1/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.19it/s v_num: 0.000      
                                                              val/auc: 0.550    
                                                              val/f1: 0.500     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.500 val/mre:    
                                                              0.153 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.161             
[2024-06-21 04:06:42,156][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/last_happy/0.2/1>
[2024-06-21 04:06:42,156][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 04:06:42,159][HYDRA] 	#322 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=last_happy data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=2
[rank: 0] Seed set to 2
[2024-06-21 04:06:42,342][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 04:06:42,343][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-21 04:06:42,345][train.py][INFO] - Instantiating callbacks...
[2024-06-21 04:06:42,345][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 04:06:42,347][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 04:06:42,347][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 04:06:42,348][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 04:06:42,350][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 04:06:42,351][train.py][INFO] - Instantiating loggers...
[2024-06-21 04:06:42,351][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 04:06:42,352][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 04:06:42,416][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/last_happy/0.2/2/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.19it/s v_num: 0.000      
                                                              val/auc: 0.363    
                                                              val/f1: 0.154     
                                                              val/precision:    
                                                              0.200 val/recall: 
                                                              0.125 val/mre:    
                                                              0.155 train/auc:  
                                                              0.992 train/f1:   
                                                              0.992             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              0.983 train/mre:  
                                                              0.161             
[2024-06-21 04:08:34,188][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/last_happy/0.2/2>
[2024-06-21 04:08:34,189][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 04:08:34,191][HYDRA] 	#323 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=last_happy data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=3
[rank: 0] Seed set to 3
[2024-06-21 04:08:34,389][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 04:08:34,391][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-21 04:08:34,392][train.py][INFO] - Instantiating callbacks...
[2024-06-21 04:08:34,392][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 04:08:34,395][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 04:08:34,395][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 04:08:34,395][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 04:08:34,414][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 04:08:34,414][train.py][INFO] - Instantiating loggers...
[2024-06-21 04:08:34,414][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 04:08:34,416][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 04:08:35,181][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/last_happy/0.2/3/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.19it/s v_num: 0.000      
                                                              val/auc: 0.537    
                                                              val/f1: 0.429     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.375 val/mre:    
                                                              0.151 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.156             
[2024-06-21 04:10:26,617][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/last_happy/0.2/3>
[2024-06-21 04:10:26,617][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 04:10:26,619][HYDRA] 	#324 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=last_happy data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=4
[rank: 0] Seed set to 4
[2024-06-21 04:10:26,802][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 04:10:26,804][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-21 04:10:26,805][train.py][INFO] - Instantiating callbacks...
[2024-06-21 04:10:26,805][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 04:10:26,807][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 04:10:26,808][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 04:10:26,808][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 04:10:26,811][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 04:10:26,811][train.py][INFO] - Instantiating loggers...
[2024-06-21 04:10:26,811][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 04:10:26,813][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 04:10:26,882][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/last_happy/0.2/4/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.20it/s v_num: 0.000      
                                                              val/auc: 0.525    
                                                              val/f1: 0.333     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.250 val/mre:    
                                                              0.149 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.156             
[2024-06-21 04:12:19,142][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/last_happy/0.2/4>
[2024-06-21 04:12:19,142][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 04:12:19,145][HYDRA] 	#325 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=last_happy data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=5
[rank: 0] Seed set to 5
[2024-06-21 04:12:19,329][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 04:12:19,330][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-21 04:12:19,331][train.py][INFO] - Instantiating callbacks...
[2024-06-21 04:12:19,332][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 04:12:19,334][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 04:12:19,334][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 04:12:19,335][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 04:12:19,337][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 04:12:19,338][train.py][INFO] - Instantiating loggers...
[2024-06-21 04:12:19,338][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 04:12:19,339][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 04:12:19,419][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/last_happy/0.2/5/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.21it/s v_num: 0.000      
                                                              val/auc: 0.275    
                                                              val/f1: 0.235     
                                                              val/precision:    
                                                              0.222 val/recall: 
                                                              0.250 val/mre:    
                                                              0.159 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.162             
[2024-06-21 04:14:10,584][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/last_happy/0.2/5>
[2024-06-21 04:14:10,584][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 04:14:10,587][HYDRA] 	#326 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=last_happy data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=6
[rank: 0] Seed set to 6
[2024-06-21 04:14:10,772][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 04:14:10,773][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-21 04:14:10,774][train.py][INFO] - Instantiating callbacks...
[2024-06-21 04:14:10,774][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 04:14:10,776][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 04:14:10,777][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 04:14:10,777][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 04:14:10,780][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 04:14:10,780][train.py][INFO] - Instantiating loggers...
[2024-06-21 04:14:10,780][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 04:14:10,782][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 04:14:10,847][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/last_happy/0.2/6/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.18it/s v_num: 0.000      
                                                              val/auc: 0.488    
                                                              val/f1: 0.400     
                                                              val/precision:    
                                                              0.429 val/recall: 
                                                              0.375 val/mre:    
                                                              0.147 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.154             
[2024-06-21 04:16:04,359][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/last_happy/0.2/6>
[2024-06-21 04:16:04,359][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 04:16:04,362][HYDRA] 	#327 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=last_happy data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=7
[rank: 0] Seed set to 7
[2024-06-21 04:16:04,564][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 04:16:04,565][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-21 04:16:04,567][train.py][INFO] - Instantiating callbacks...
[2024-06-21 04:16:04,567][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 04:16:04,569][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 04:16:04,569][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 04:16:04,570][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 04:16:04,573][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 04:16:04,573][train.py][INFO] - Instantiating loggers...
[2024-06-21 04:16:04,573][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 04:16:04,574][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 04:16:04,647][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/last_happy/0.2/7/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.18it/s v_num: 0.000      
                                                              val/auc: 0.213    
                                                              val/f1: 0.125     
                                                              val/precision:    
                                                              0.125 val/recall: 
                                                              0.125 val/mre:    
                                                              0.155 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.158             
[2024-06-21 04:17:58,177][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/last_happy/0.2/7>
[2024-06-21 04:17:58,177][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 04:17:58,180][HYDRA] 	#328 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=last_happy data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=8
[rank: 0] Seed set to 8
[2024-06-21 04:17:58,364][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 04:17:58,365][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-21 04:17:58,366][train.py][INFO] - Instantiating callbacks...
[2024-06-21 04:17:58,367][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 04:17:58,369][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 04:17:58,369][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 04:17:58,370][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 04:17:58,372][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 04:17:58,372][train.py][INFO] - Instantiating loggers...
[2024-06-21 04:17:58,373][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 04:17:58,374][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 04:17:58,441][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/last_happy/0.2/8/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.20it/s v_num: 0.000      
                                                              val/auc: 0.387    
                                                              val/f1: 0.353     
                                                              val/precision:    
                                                              0.333 val/recall: 
                                                              0.375 val/mre:    
                                                              0.154 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.159             
[2024-06-21 04:19:50,891][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/last_happy/0.2/8>
[2024-06-21 04:19:50,892][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 04:19:50,894][HYDRA] 	#329 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=last_happy data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=9
[rank: 0] Seed set to 9
[2024-06-21 04:19:51,076][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 04:19:51,077][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-21 04:19:51,079][train.py][INFO] - Instantiating callbacks...
[2024-06-21 04:19:51,079][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 04:19:51,081][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 04:19:51,082][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 04:19:51,082][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 04:19:51,085][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 04:19:51,085][train.py][INFO] - Instantiating loggers...
[2024-06-21 04:19:51,085][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 04:19:51,086][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 04:19:51,153][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/last_happy/0.2/9/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.18it/s v_num: 0.000      
                                                              val/auc: 0.475    
                                                              val/f1: 0.308     
                                                              val/precision:    
                                                              0.400 val/recall: 
                                                              0.250 val/mre:    
                                                              0.151 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.155             
[2024-06-21 04:21:44,070][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/last_happy/0.2/9>
[2024-06-21 04:21:44,070][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 04:21:44,073][HYDRA] 	#330 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=last_happy data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=0
[2024-06-21 04:21:44,475][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 04:21:44,477][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-21 04:21:44,478][train.py][INFO] - Instantiating callbacks...
[2024-06-21 04:21:44,478][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 04:21:44,480][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 04:21:44,481][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 04:21:44,481][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 04:21:44,491][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 04:21:44,492][train.py][INFO] - Instantiating loggers...
[2024-06-21 04:21:44,492][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 04:21:44,493][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 04:21:44,750][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/last_happy/0.25/0/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.19it/s v_num: 0.000      
                                                              val/auc: 0.375    
                                                              val/f1: 0.267     
                                                              val/precision:    
                                                              0.286 val/recall: 
                                                              0.250 val/mre:    
                                                              0.152 train/auc:  
                                                              0.975 train/f1:   
                                                              0.974             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              0.950 train/mre:  
                                                              0.157             
[2024-06-21 04:23:37,193][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/last_happy/0.25/0>
[2024-06-21 04:23:37,193][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 04:23:37,196][HYDRA] 	#331 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=last_happy data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=1
[rank: 0] Seed set to 1
[2024-06-21 04:23:37,379][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 04:23:37,380][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-21 04:23:37,382][train.py][INFO] - Instantiating callbacks...
[2024-06-21 04:23:37,382][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 04:23:37,384][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 04:23:37,384][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 04:23:37,385][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 04:23:37,411][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 04:23:37,411][train.py][INFO] - Instantiating loggers...
[2024-06-21 04:23:37,412][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 04:23:37,413][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 04:23:37,502][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/last_happy/0.25/1/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.19it/s v_num: 0.000      
                                                              val/auc: 0.600    
                                                              val/f1: 0.533     
                                                              val/precision:    
                                                              0.571 val/recall: 
                                                              0.500 val/mre:    
                                                              0.155 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.162             
[2024-06-21 04:25:29,464][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/last_happy/0.25/1>
[2024-06-21 04:25:29,464][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 04:25:29,466][HYDRA] 	#332 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=last_happy data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=2
[rank: 0] Seed set to 2
[2024-06-21 04:25:29,649][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 04:25:29,650][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-21 04:25:29,652][train.py][INFO] - Instantiating callbacks...
[2024-06-21 04:25:29,652][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 04:25:29,654][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 04:25:29,654][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 04:25:29,655][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 04:25:29,658][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 04:25:29,658][train.py][INFO] - Instantiating loggers...
[2024-06-21 04:25:29,658][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 04:25:29,659][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 04:25:29,724][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/last_happy/0.25/2/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.18it/s v_num: 0.000      
                                                              val/auc: 0.250    
                                                              val/f1: 0.000     
                                                              val/precision:    
                                                              0.000 val/recall: 
                                                              0.000 val/mre:    
                                                              0.158 train/auc:  
                                                              0.992 train/f1:   
                                                              0.992             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              0.983 train/mre:  
                                                              0.165             
[2024-06-21 04:27:23,066][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/last_happy/0.25/2>
[2024-06-21 04:27:23,067][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 04:27:23,069][HYDRA] 	#333 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=last_happy data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=3
[rank: 0] Seed set to 3
[2024-06-21 04:27:23,255][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 04:27:23,256][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-21 04:27:23,258][train.py][INFO] - Instantiating callbacks...
[2024-06-21 04:27:23,258][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 04:27:23,260][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 04:27:23,260][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 04:27:23,261][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 04:27:23,263][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 04:27:23,264][train.py][INFO] - Instantiating loggers...
[2024-06-21 04:27:23,264][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 04:27:23,265][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 04:27:23,334][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/last_happy/0.25/3/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.18it/s v_num: 0.000      
                                                              val/auc: 0.537    
                                                              val/f1: 0.429     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.375 val/mre:    
                                                              0.151 train/auc:  
                                                              0.975 train/f1:   
                                                              0.976             
                                                              train/precision:  
                                                              0.952             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.158             
[2024-06-21 04:29:15,540][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/last_happy/0.25/3>
[2024-06-21 04:29:15,540][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 04:29:15,543][HYDRA] 	#334 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=last_happy data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=4
[rank: 0] Seed set to 4
[2024-06-21 04:29:15,730][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 04:29:15,732][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-21 04:29:15,733][train.py][INFO] - Instantiating callbacks...
[2024-06-21 04:29:15,733][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 04:29:15,735][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 04:29:15,736][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 04:29:15,736][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 04:29:15,739][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 04:29:15,739][train.py][INFO] - Instantiating loggers...
[2024-06-21 04:29:15,739][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 04:29:15,741][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 04:29:15,879][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/last_happy/0.25/4/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.19it/s v_num: 0.000      
                                                              val/auc: 0.425    
                                                              val/f1: 0.286     
                                                              val/precision:    
                                                              0.333 val/recall: 
                                                              0.250 val/mre:    
                                                              0.151 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.157             
[2024-06-21 04:31:08,033][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/last_happy/0.25/4>
[2024-06-21 04:31:08,034][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 04:31:08,036][HYDRA] 	#335 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=last_happy data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=5
[rank: 0] Seed set to 5
[2024-06-21 04:31:08,217][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 04:31:08,218][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-21 04:31:08,220][train.py][INFO] - Instantiating callbacks...
[2024-06-21 04:31:08,220][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 04:31:08,222][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 04:31:08,222][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 04:31:08,223][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 04:31:08,225][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 04:31:08,226][train.py][INFO] - Instantiating loggers...
[2024-06-21 04:31:08,226][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 04:31:08,227][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 04:31:08,293][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/last_happy/0.25/5/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.18it/s v_num: 0.000      
                                                              val/auc: 0.375    
                                                              val/f1: 0.267     
                                                              val/precision:    
                                                              0.286 val/recall: 
                                                              0.250 val/mre:    
                                                              0.162 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.165             
[2024-06-21 04:33:03,736][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/last_happy/0.25/5>
[2024-06-21 04:33:03,736][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 04:33:03,738][HYDRA] 	#336 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=last_happy data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=6
[rank: 0] Seed set to 6
[2024-06-21 04:33:03,923][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 04:33:03,925][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-21 04:33:03,926][train.py][INFO] - Instantiating callbacks...
[2024-06-21 04:33:03,926][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 04:33:03,928][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 04:33:03,929][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 04:33:03,929][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 04:33:03,934][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 04:33:03,934][train.py][INFO] - Instantiating loggers...
[2024-06-21 04:33:03,934][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 04:33:03,936][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 04:33:04,002][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/last_happy/0.25/6/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.18it/s v_num: 0.000      
                                                              val/auc: 0.375    
                                                              val/f1: 0.267     
                                                              val/precision:    
                                                              0.286 val/recall: 
                                                              0.250 val/mre:    
                                                              0.150 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.157             
[2024-06-21 04:34:56,788][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/last_happy/0.25/6>
[2024-06-21 04:34:56,789][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 04:34:56,791][HYDRA] 	#337 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=last_happy data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=7
[rank: 0] Seed set to 7
[2024-06-21 04:34:56,973][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 04:34:56,975][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-21 04:34:56,976][train.py][INFO] - Instantiating callbacks...
[2024-06-21 04:34:56,976][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 04:34:56,978][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 04:34:56,979][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 04:34:56,979][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 04:34:56,982][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 04:34:56,982][train.py][INFO] - Instantiating loggers...
[2024-06-21 04:34:56,983][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 04:34:56,984][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 04:34:57,056][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/last_happy/0.25/7/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.20it/s v_num: 0.000      
                                                              val/auc: 0.213    
                                                              val/f1: 0.125     
                                                              val/precision:    
                                                              0.125 val/recall: 
                                                              0.125 val/mre:    
                                                              0.157 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.161             
[2024-06-21 04:36:49,421][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/last_happy/0.25/7>
[2024-06-21 04:36:49,421][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 04:36:49,424][HYDRA] 	#338 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=last_happy data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=8
[rank: 0] Seed set to 8
[2024-06-21 04:36:49,606][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 04:36:49,608][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-21 04:36:49,609][train.py][INFO] - Instantiating callbacks...
[2024-06-21 04:36:49,609][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 04:36:49,611][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 04:36:49,612][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 04:36:49,612][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 04:36:49,615][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 04:36:49,615][train.py][INFO] - Instantiating loggers...
[2024-06-21 04:36:49,615][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 04:36:49,616][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 04:36:49,682][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/last_happy/0.25/8/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.18it/s v_num: 0.000      
                                                              val/auc: 0.463    
                                                              val/f1: 0.500     
                                                              val/precision:    
                                                              0.417 val/recall: 
                                                              0.625 val/mre:    
                                                              0.159 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.163             
[2024-06-21 04:38:41,809][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/last_happy/0.25/8>
[2024-06-21 04:38:41,810][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 04:38:41,812][HYDRA] 	#339 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=last_happy data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=9
[rank: 0] Seed set to 9
[2024-06-21 04:38:41,994][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 04:38:41,996][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-21 04:38:41,997][train.py][INFO] - Instantiating callbacks...
[2024-06-21 04:38:41,997][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 04:38:41,999][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 04:38:42,000][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 04:38:42,000][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 04:38:42,003][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 04:38:42,003][train.py][INFO] - Instantiating loggers...
[2024-06-21 04:38:42,003][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 04:38:42,004][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 04:38:42,069][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/last_happy/0.25/9/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.19it/s v_num: 0.000      
                                                              val/auc: 0.425    
                                                              val/f1: 0.286     
                                                              val/precision:    
                                                              0.333 val/recall: 
                                                              0.250 val/mre:    
                                                              0.157 train/auc:  
                                                              0.992 train/f1:   
                                                              0.992             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              0.983 train/mre:  
                                                              0.160             
[2024-06-21 04:40:33,958][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/last_happy/0.25/9>
[2024-06-21 04:40:33,958][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 04:40:33,960][HYDRA] 	#340 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=last_happy data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=0
[2024-06-21 04:40:34,141][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 04:40:34,143][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-21 04:40:34,144][train.py][INFO] - Instantiating callbacks...
[2024-06-21 04:40:34,144][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 04:40:34,146][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 04:40:34,147][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 04:40:34,147][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 04:40:34,149][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 04:40:34,150][train.py][INFO] - Instantiating loggers...
[2024-06-21 04:40:34,150][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 04:40:34,151][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 04:40:34,215][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/last_happy/0.3/0/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.19it/s v_num: 0.000      
                                                              val/auc: 0.587    
                                                              val/f1: 0.636     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.875 val/mre:    
                                                              0.160 train/auc:  
                                                              0.967 train/f1:   
                                                              0.967             
                                                              train/precision:  
                                                              0.967             
                                                              train/recall:     
                                                              0.967 train/mre:  
                                                              0.161             
[2024-06-21 04:42:26,514][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/last_happy/0.3/0>
[2024-06-21 04:42:26,514][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 04:42:26,517][HYDRA] 	#341 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=last_happy data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=1
[rank: 0] Seed set to 1
[2024-06-21 04:42:26,708][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 04:42:26,710][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-21 04:42:26,711][train.py][INFO] - Instantiating callbacks...
[2024-06-21 04:42:26,711][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 04:42:26,713][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 04:42:26,714][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 04:42:26,714][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 04:42:26,717][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 04:42:26,717][train.py][INFO] - Instantiating loggers...
[2024-06-21 04:42:26,717][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 04:42:26,718][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 04:42:26,803][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/last_happy/0.3/1/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.20it/s v_num: 0.000      
                                                              val/auc: 0.662    
                                                              val/f1: 0.625     
                                                              val/precision:    
                                                              0.625 val/recall: 
                                                              0.625 val/mre:    
                                                              0.155 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.161             
[2024-06-21 04:44:18,810][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/last_happy/0.3/1>
[2024-06-21 04:44:18,811][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 04:44:18,813][HYDRA] 	#342 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=last_happy data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=2
[rank: 0] Seed set to 2
[2024-06-21 04:44:19,048][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 04:44:19,050][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-21 04:44:19,051][train.py][INFO] - Instantiating callbacks...
[2024-06-21 04:44:19,051][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 04:44:19,053][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 04:44:19,054][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 04:44:19,054][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 04:44:19,056][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 04:44:19,057][train.py][INFO] - Instantiating loggers...
[2024-06-21 04:44:19,057][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 04:44:19,058][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 04:44:19,128][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/last_happy/0.3/2/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.18it/s v_num: 0.000      
                                                              val/auc: 0.312    
                                                              val/f1: 0.143     
                                                              val/precision:    
                                                              0.167 val/recall: 
                                                              0.125 val/mre:    
                                                              0.162 train/auc:  
                                                              0.983 train/f1:   
                                                              0.983             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              0.967 train/mre:  
                                                              0.167             
[2024-06-21 04:46:12,158][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/last_happy/0.3/2>
[2024-06-21 04:46:12,158][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 04:46:12,161][HYDRA] 	#343 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=last_happy data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=3
[rank: 0] Seed set to 3
[2024-06-21 04:46:12,347][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 04:46:12,349][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-21 04:46:12,350][train.py][INFO] - Instantiating callbacks...
[2024-06-21 04:46:12,350][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 04:46:12,352][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 04:46:12,353][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 04:46:12,353][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 04:46:12,358][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 04:46:12,359][train.py][INFO] - Instantiating loggers...
[2024-06-21 04:46:12,359][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 04:46:12,360][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 04:46:12,431][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/last_happy/0.3/3/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.18it/s v_num: 0.000      
                                                              val/auc: 0.537    
                                                              val/f1: 0.429     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.375 val/mre:    
                                                              0.155 train/auc:  
                                                              0.992 train/f1:   
                                                              0.992             
                                                              train/precision:  
                                                              0.984             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.162             
[2024-06-21 04:48:05,630][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/last_happy/0.3/3>
[2024-06-21 04:48:05,633][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 04:48:05,649][HYDRA] 	#344 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=last_happy data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=4
[rank: 0] Seed set to 4
[2024-06-21 04:48:06,055][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 04:48:06,057][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-21 04:48:06,058][train.py][INFO] - Instantiating callbacks...
[2024-06-21 04:48:06,058][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 04:48:06,060][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 04:48:06,061][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 04:48:06,061][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 04:48:06,064][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 04:48:06,065][train.py][INFO] - Instantiating loggers...
[2024-06-21 04:48:06,065][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 04:48:06,066][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 04:48:06,130][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/last_happy/0.3/4/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.19it/s v_num: 0.000      
                                                              val/auc: 0.312    
                                                              val/f1: 0.143     
                                                              val/precision:    
                                                              0.167 val/recall: 
                                                              0.125 val/mre:    
                                                              0.153 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.159             
[2024-06-21 04:49:57,883][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/last_happy/0.3/4>
[2024-06-21 04:49:57,883][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 04:49:57,886][HYDRA] 	#345 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=last_happy data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=5
[rank: 0] Seed set to 5
[2024-06-21 04:49:58,067][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 04:49:58,068][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-21 04:49:58,069][train.py][INFO] - Instantiating callbacks...
[2024-06-21 04:49:58,070][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 04:49:58,072][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 04:49:58,072][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 04:49:58,073][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 04:49:58,075][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 04:49:58,076][train.py][INFO] - Instantiating loggers...
[2024-06-21 04:49:58,076][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 04:49:58,077][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 04:49:58,142][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/last_happy/0.3/5/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.18it/s v_num: 0.000      
                                                              val/auc: 0.400    
                                                              val/f1: 0.421     
                                                              val/precision:    
                                                              0.364 val/recall: 
                                                              0.500 val/mre:    
                                                              0.165 train/auc:  
                                                              0.992 train/f1:   
                                                              0.992             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              0.983 train/mre:  
                                                              0.169             
[2024-06-21 04:51:51,310][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/last_happy/0.3/5>
[2024-06-21 04:51:51,310][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 04:51:51,313][HYDRA] 	#346 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=last_happy data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=6
[rank: 0] Seed set to 6
[2024-06-21 04:51:51,496][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 04:51:51,497][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-21 04:51:51,499][train.py][INFO] - Instantiating callbacks...
[2024-06-21 04:51:51,499][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 04:51:51,501][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 04:51:51,501][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 04:51:51,502][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 04:51:51,505][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 04:51:51,505][train.py][INFO] - Instantiating loggers...
[2024-06-21 04:51:51,505][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 04:51:51,506][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 04:51:51,572][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/last_happy/0.3/6/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.18it/s v_num: 0.000      
                                                              val/auc: 0.225    
                                                              val/f1: 0.222     
                                                              val/precision:    
                                                              0.200 val/recall: 
                                                              0.250 val/mre:    
                                                              0.151 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.158             
[2024-06-21 04:53:44,460][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/last_happy/0.3/6>
[2024-06-21 04:53:44,460][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 04:53:44,463][HYDRA] 	#347 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=last_happy data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=7
[rank: 0] Seed set to 7
[2024-06-21 04:53:44,647][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 04:53:44,649][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-21 04:53:44,650][train.py][INFO] - Instantiating callbacks...
[2024-06-21 04:53:44,650][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 04:53:44,652][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 04:53:44,653][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 04:53:44,653][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 04:53:44,656][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 04:53:44,656][train.py][INFO] - Instantiating loggers...
[2024-06-21 04:53:44,656][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 04:53:44,657][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 04:53:44,723][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/last_happy/0.3/7/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.19it/s v_num: 0.000      
                                                              val/auc: 0.463    
                                                              val/f1: 0.500     
                                                              val/precision:    
                                                              0.417 val/recall: 
                                                              0.625 val/mre:    
                                                              0.157 train/auc:  
                                                              0.942 train/f1:   
                                                              0.940             
                                                              train/precision:  
                                                              0.965             
                                                              train/recall:     
                                                              0.917 train/mre:  
                                                              0.161             
[2024-06-21 04:55:37,564][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/last_happy/0.3/7>
[2024-06-21 04:55:37,564][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 04:55:37,567][HYDRA] 	#348 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=last_happy data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=8
[rank: 0] Seed set to 8
[2024-06-21 04:55:37,749][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 04:55:37,750][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-21 04:55:37,752][train.py][INFO] - Instantiating callbacks...
[2024-06-21 04:55:37,752][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 04:55:37,754][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 04:55:37,754][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 04:55:37,755][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 04:55:37,757][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 04:55:37,758][train.py][INFO] - Instantiating loggers...
[2024-06-21 04:55:37,758][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 04:55:37,759][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 04:55:37,825][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/last_happy/0.3/8/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.17it/s v_num: 0.000      
                                                              val/auc: 0.363    
                                                              val/f1: 0.154     
                                                              val/precision:    
                                                              0.200 val/recall: 
                                                              0.125 val/mre:    
                                                              0.163 train/auc:  
                                                              0.992 train/f1:   
                                                              0.992             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              0.983 train/mre:  
                                                              0.167             
[2024-06-21 04:57:30,471][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/last_happy/0.3/8>
[2024-06-21 04:57:30,472][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 04:57:30,478][HYDRA] 	#349 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=last_happy data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=9
[rank: 0] Seed set to 9
[2024-06-21 04:57:30,661][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 04:57:30,663][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-21 04:57:30,664][train.py][INFO] - Instantiating callbacks...
[2024-06-21 04:57:30,664][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 04:57:30,666][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 04:57:30,667][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 04:57:30,667][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 04:57:30,669][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 04:57:30,670][train.py][INFO] - Instantiating loggers...
[2024-06-21 04:57:30,670][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 04:57:30,671][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 04:57:30,738][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/last_happy/0.3/9/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.19it/s v_num: 0.000      
                                                              val/auc: 0.450    
                                                              val/f1: 0.000     
                                                              val/precision:    
                                                              0.000 val/recall: 
                                                              0.000 val/mre:    
                                                              0.159 train/auc:  
                                                              0.992 train/f1:   
                                                              0.992             
                                                              train/precision:  
                                                              0.984             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.163             
[2024-06-21 04:59:23,273][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/last_happy/0.3/9>
[2024-06-21 04:59:23,274][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 04:59:23,276][HYDRA] 	#350 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=proud_life data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=0
[2024-06-21 04:59:23,463][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 04:59:23,464][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-21 04:59:23,465][train.py][INFO] - Instantiating callbacks...
[2024-06-21 04:59:23,466][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 04:59:23,468][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 04:59:23,468][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 04:59:23,469][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 04:59:23,471][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 04:59:23,472][train.py][INFO] - Instantiating loggers...
[2024-06-21 04:59:23,472][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 04:59:23,473][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 04:59:23,540][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/proud_life/0.0/0/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.15it/s v_num: 0.000      
                                                              val/auc: 0.517    
                                                              val/f1: 0.400     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.333 val/mre: nan
                                                              train/auc: 0.984  
                                                              train/f1: 0.984   
                                                              train/precision:  
                                                              0.984             
                                                              train/recall:     
                                                              0.984 train/mre:  
                                                              nan               
[2024-06-21 05:01:17,156][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/proud_life/0.0/0>
[2024-06-21 05:01:17,156][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 05:01:17,159][HYDRA] 	#351 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=proud_life data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=1
[rank: 0] Seed set to 1
[2024-06-21 05:01:17,341][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 05:01:17,343][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-21 05:01:17,344][train.py][INFO] - Instantiating callbacks...
[2024-06-21 05:01:17,344][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 05:01:17,347][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 05:01:17,347][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 05:01:17,347][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 05:01:17,351][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 05:01:17,351][train.py][INFO] - Instantiating loggers...
[2024-06-21 05:01:17,351][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 05:01:17,352][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 05:01:17,427][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/proud_life/0.0/1/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.14it/s v_num: 0.000      
                                                              val/auc: 0.511    
                                                              val/f1: 0.308     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.222 val/mre: nan
                                                              train/auc: 0.918  
                                                              train/f1: 0.911   
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              0.836 train/mre:  
                                                              nan               
[2024-06-21 05:03:11,583][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/proud_life/0.0/1>
[2024-06-21 05:03:11,584][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 05:03:11,586][HYDRA] 	#352 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=proud_life data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=2
[rank: 0] Seed set to 2
[2024-06-21 05:03:11,768][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 05:03:11,770][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-21 05:03:11,771][train.py][INFO] - Instantiating callbacks...
[2024-06-21 05:03:11,771][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 05:03:11,773][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 05:03:11,774][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 05:03:11,775][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 05:03:11,778][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 05:03:11,778][train.py][INFO] - Instantiating loggers...
[2024-06-21 05:03:11,778][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 05:03:11,780][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 05:03:11,844][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/proud_life/0.0/2/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.15it/s v_num: 0.000      
                                                              val/auc: 0.517    
                                                              val/f1: 0.400     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.333 val/mre: nan
                                                              train/auc: 0.975  
                                                              train/f1: 0.976   
                                                              train/precision:  
                                                              0.968             
                                                              train/recall:     
                                                              0.984 train/mre:  
                                                              nan               
[2024-06-21 05:05:05,925][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/proud_life/0.0/2>
[2024-06-21 05:05:05,925][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 05:05:05,928][HYDRA] 	#353 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=proud_life data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=3
[rank: 0] Seed set to 3
[2024-06-21 05:05:06,110][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 05:05:06,112][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-21 05:05:06,113][train.py][INFO] - Instantiating callbacks...
[2024-06-21 05:05:06,113][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 05:05:06,115][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 05:05:06,116][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 05:05:06,116][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 05:05:06,119][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 05:05:06,119][train.py][INFO] - Instantiating loggers...
[2024-06-21 05:05:06,119][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 05:05:06,120][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 05:05:06,189][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/proud_life/0.0/3/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.16it/s v_num: 0.000      
                                                              val/auc: 0.578    
                                                              val/f1: 0.556     
                                                              val/precision:    
                                                              0.556 val/recall: 
                                                              0.556 val/mre: nan
                                                              train/auc: 0.943  
                                                              train/f1: 0.942   
                                                              train/precision:  
                                                              0.950             
                                                              train/recall:     
                                                              0.934 train/mre:  
                                                              nan               
[2024-06-21 05:06:59,641][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/proud_life/0.0/3>
[2024-06-21 05:06:59,643][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 05:06:59,661][HYDRA] 	#354 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=proud_life data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=4
[rank: 0] Seed set to 4
[2024-06-21 05:06:59,963][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 05:06:59,965][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-21 05:06:59,966][train.py][INFO] - Instantiating callbacks...
[2024-06-21 05:06:59,966][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 05:06:59,968][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 05:06:59,969][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 05:06:59,969][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 05:06:59,972][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 05:06:59,972][train.py][INFO] - Instantiating loggers...
[2024-06-21 05:06:59,973][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 05:06:59,974][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 05:07:00,039][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/proud_life/0.0/4/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.16it/s v_num: 0.000      
                                                              val/auc: 0.456    
                                                              val/f1: 0.167     
                                                              val/precision:    
                                                              0.333 val/recall: 
                                                              0.111 val/mre: nan
                                                              train/auc: 0.967  
                                                              train/f1: 0.967   
                                                              train/precision:  
                                                              0.967             
                                                              train/recall:     
                                                              0.967 train/mre:  
                                                              nan               
[2024-06-21 05:08:53,201][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/proud_life/0.0/4>
[2024-06-21 05:08:53,202][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 05:08:53,204][HYDRA] 	#355 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=proud_life data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=5
[rank: 0] Seed set to 5
[2024-06-21 05:08:53,900][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 05:08:53,901][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-21 05:08:53,903][train.py][INFO] - Instantiating callbacks...
[2024-06-21 05:08:53,903][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 05:08:53,908][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 05:08:53,908][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 05:08:53,909][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 05:08:53,911][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 05:08:53,911][train.py][INFO] - Instantiating loggers...
[2024-06-21 05:08:53,912][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 05:08:53,913][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 05:08:54,158][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/proud_life/0.0/5/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.14it/s v_num: 0.000      
                                                              val/auc: 0.411    
                                                              val/f1: 0.267     
                                                              val/precision:    
                                                              0.333 val/recall: 
                                                              0.222 val/mre: nan
                                                              train/auc: 0.959  
                                                              train/f1: 0.958   
                                                              train/precision:  
                                                              0.983             
                                                              train/recall:     
                                                              0.934 train/mre:  
                                                              nan               
[2024-06-21 05:10:48,691][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/proud_life/0.0/5>
[2024-06-21 05:10:48,692][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 05:10:48,694][HYDRA] 	#356 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=proud_life data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=6
[rank: 0] Seed set to 6
[2024-06-21 05:10:48,879][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 05:10:48,881][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-21 05:10:48,882][train.py][INFO] - Instantiating callbacks...
[2024-06-21 05:10:48,882][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 05:10:48,884][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 05:10:48,885][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 05:10:48,885][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 05:10:48,888][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 05:10:48,888][train.py][INFO] - Instantiating loggers...
[2024-06-21 05:10:48,889][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 05:10:48,890][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 05:10:48,956][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/proud_life/0.0/6/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.16it/s v_num: 0.000      
                                                              val/auc: 0.567    
                                                              val/f1: 0.429     
                                                              val/precision:    
                                                              0.600 val/recall: 
                                                              0.333 val/mre: nan
                                                              train/auc: 0.975  
                                                              train/f1: 0.976   
                                                              train/precision:  
                                                              0.968             
                                                              train/recall:     
                                                              0.984 train/mre:  
                                                              nan               
[2024-06-21 05:12:42,657][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/proud_life/0.0/6>
[2024-06-21 05:12:42,657][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 05:12:42,660][HYDRA] 	#357 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=proud_life data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=7
[rank: 0] Seed set to 7
[2024-06-21 05:12:42,843][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 05:12:42,845][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-21 05:12:42,846][train.py][INFO] - Instantiating callbacks...
[2024-06-21 05:12:42,846][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 05:12:42,848][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 05:12:42,849][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 05:12:42,849][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 05:12:42,852][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 05:12:42,852][train.py][INFO] - Instantiating loggers...
[2024-06-21 05:12:42,852][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 05:12:42,854][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 05:12:42,920][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/proud_life/0.0/7/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.17it/s v_num: 0.000      
                                                              val/auc: 0.517    
                                                              val/f1: 0.400     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.333 val/mre: nan
                                                              train/auc: 0.984  
                                                              train/f1: 0.984   
                                                              train/precision:  
                                                              0.984             
                                                              train/recall:     
                                                              0.984 train/mre:  
                                                              nan               
[2024-06-21 05:14:36,829][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/proud_life/0.0/7>
[2024-06-21 05:14:36,830][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 05:14:36,833][HYDRA] 	#358 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=proud_life data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=8
[rank: 0] Seed set to 8
[2024-06-21 05:14:37,014][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 05:14:37,015][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-21 05:14:37,017][train.py][INFO] - Instantiating callbacks...
[2024-06-21 05:14:37,017][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 05:14:37,019][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 05:14:37,019][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 05:14:37,020][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 05:14:37,022][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 05:14:37,023][train.py][INFO] - Instantiating loggers...
[2024-06-21 05:14:37,023][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 05:14:37,024][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 05:14:37,089][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/proud_life/0.0/8/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.14it/s v_num: 0.000      
                                                              val/auc: 0.567    
                                                              val/f1: 0.429     
                                                              val/precision:    
                                                              0.600 val/recall: 
                                                              0.333 val/mre: nan
                                                              train/auc: 0.967  
                                                              train/f1: 0.967   
                                                              train/precision:  
                                                              0.967             
                                                              train/recall:     
                                                              0.967 train/mre:  
                                                              nan               
[2024-06-21 05:16:31,736][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/proud_life/0.0/8>
[2024-06-21 05:16:31,738][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 05:16:31,740][HYDRA] 	#359 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=proud_life data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=9
[rank: 0] Seed set to 9
[2024-06-21 05:16:31,926][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 05:16:31,927][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-21 05:16:31,928][train.py][INFO] - Instantiating callbacks...
[2024-06-21 05:16:31,929][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 05:16:31,931][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 05:16:31,931][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 05:16:31,932][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 05:16:31,934][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 05:16:31,935][train.py][INFO] - Instantiating loggers...
[2024-06-21 05:16:31,935][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 05:16:31,936][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 05:16:32,022][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/proud_life/0.0/9/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.14it/s v_num: 0.000      
                                                              val/auc: 0.517    
                                                              val/f1: 0.400     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.333 val/mre: nan
                                                              train/auc: 0.984  
                                                              train/f1: 0.984   
                                                              train/precision:  
                                                              0.984             
                                                              train/recall:     
                                                              0.984 train/mre:  
                                                              nan               
[2024-06-21 05:18:26,370][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/proud_life/0.0/9>
[2024-06-21 05:18:26,370][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 05:18:26,372][HYDRA] 	#360 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=proud_life data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=0
[2024-06-21 05:18:26,554][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 05:18:26,555][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-21 05:18:26,557][train.py][INFO] - Instantiating callbacks...
[2024-06-21 05:18:26,557][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 05:18:26,559][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 05:18:26,559][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 05:18:26,560][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 05:18:26,563][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 05:18:26,563][train.py][INFO] - Instantiating loggers...
[2024-06-21 05:18:26,563][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 05:18:26,564][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 05:18:26,628][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/proud_life/0.05/0/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.14it/s v_num: 0.000      
                                                              val/auc: 0.517    
                                                              val/f1: 0.400     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.333 val/mre:    
                                                              0.116 train/auc:  
                                                              0.967 train/f1:   
                                                              0.966             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              0.934 train/mre:  
                                                              0.130             
[2024-06-21 05:20:19,779][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/proud_life/0.05/0>
[2024-06-21 05:20:19,780][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 05:20:19,782][HYDRA] 	#361 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=proud_life data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=1
[rank: 0] Seed set to 1
[2024-06-21 05:20:19,963][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 05:20:19,965][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-21 05:20:19,966][train.py][INFO] - Instantiating callbacks...
[2024-06-21 05:20:19,966][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 05:20:19,968][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 05:20:19,969][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 05:20:19,969][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 05:20:19,972][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 05:20:19,972][train.py][INFO] - Instantiating loggers...
[2024-06-21 05:20:19,972][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 05:20:19,974][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 05:20:20,042][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/proud_life/0.05/1/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.15it/s v_num: 0.000      
                                                              val/auc: 0.522    
                                                              val/f1: 0.471     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.444 val/mre:    
                                                              0.127 train/auc:  
                                                              0.959 train/f1:   
                                                              0.961             
                                                              train/precision:  
                                                              0.924             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.139             
[2024-06-21 05:22:15,622][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/proud_life/0.05/1>
[2024-06-21 05:22:15,622][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 05:22:15,625][HYDRA] 	#362 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=proud_life data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=2
[rank: 0] Seed set to 2
[2024-06-21 05:22:15,821][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 05:22:15,822][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-21 05:22:15,823][train.py][INFO] - Instantiating callbacks...
[2024-06-21 05:22:15,824][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 05:22:15,826][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 05:22:15,826][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 05:22:15,826][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 05:22:15,829][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 05:22:15,829][train.py][INFO] - Instantiating loggers...
[2024-06-21 05:22:15,830][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 05:22:15,831][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 05:22:15,899][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/proud_life/0.05/2/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.15it/s v_num: 0.000      
                                                              val/auc: 0.561    
                                                              val/f1: 0.333     
                                                              val/precision:    
                                                              0.667 val/recall: 
                                                              0.222 val/mre:    
                                                              0.125 train/auc:  
                                                              0.967 train/f1:   
                                                              0.968             
                                                              train/precision:  
                                                              0.938             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.135             
[2024-06-21 05:24:10,080][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/proud_life/0.05/2>
[2024-06-21 05:24:10,082][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 05:24:10,103][HYDRA] 	#363 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=proud_life data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=3
[rank: 0] Seed set to 3
[2024-06-21 05:24:10,609][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 05:24:10,611][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-21 05:24:10,612][train.py][INFO] - Instantiating callbacks...
[2024-06-21 05:24:10,612][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 05:24:10,614][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 05:24:10,615][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 05:24:10,615][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 05:24:10,618][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 05:24:10,618][train.py][INFO] - Instantiating loggers...
[2024-06-21 05:24:10,618][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 05:24:10,620][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 05:24:10,687][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/proud_life/0.05/3/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.15it/s v_num: 0.000      
                                                              val/auc: 0.467    
                                                              val/f1: 0.375     
                                                              val/precision:    
                                                              0.429 val/recall: 
                                                              0.333 val/mre:    
                                                              0.123 train/auc:  
                                                              0.934 train/f1:   
                                                              0.938             
                                                              train/precision:  
                                                              0.884             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.137             
[2024-06-21 05:26:03,981][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/proud_life/0.05/3>
[2024-06-21 05:26:03,982][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 05:26:03,984][HYDRA] 	#364 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=proud_life data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=4
[rank: 0] Seed set to 4
[2024-06-21 05:26:04,167][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 05:26:04,168][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-21 05:26:04,169][train.py][INFO] - Instantiating callbacks...
[2024-06-21 05:26:04,170][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 05:26:04,172][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 05:26:04,172][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 05:26:04,173][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 05:26:04,175][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 05:26:04,176][train.py][INFO] - Instantiating loggers...
[2024-06-21 05:26:04,176][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 05:26:04,177][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 05:26:04,248][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/proud_life/0.05/4/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.14it/s v_num: 0.000      
                                                              val/auc: 0.511    
                                                              val/f1: 0.308     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.222 val/mre:    
                                                              0.122 train/auc:  
                                                              0.984 train/f1:   
                                                              0.983             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              0.967 train/mre:  
                                                              0.136             
[2024-06-21 05:27:58,631][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/proud_life/0.05/4>
[2024-06-21 05:27:58,634][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 05:27:58,650][HYDRA] 	#365 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=proud_life data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=5
[rank: 0] Seed set to 5
[2024-06-21 05:27:58,902][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 05:27:58,903][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-21 05:27:58,904][train.py][INFO] - Instantiating callbacks...
[2024-06-21 05:27:58,905][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 05:27:58,907][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 05:27:58,907][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 05:27:58,908][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 05:27:58,910][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 05:27:58,911][train.py][INFO] - Instantiating loggers...
[2024-06-21 05:27:58,911][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 05:27:58,912][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 05:27:58,982][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/proud_life/0.05/5/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.13it/s v_num: 0.000      
                                                              val/auc: 0.572    
                                                              val/f1: 0.500     
                                                              val/precision:    
                                                              0.571 val/recall: 
                                                              0.444 val/mre:    
                                                              0.132 train/auc:  
                                                              0.975 train/f1:   
                                                              0.975             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              0.951 train/mre:  
                                                              0.143             
[2024-06-21 05:29:52,977][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/proud_life/0.05/5>
[2024-06-21 05:29:52,978][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 05:29:52,981][HYDRA] 	#366 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=proud_life data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=6
[rank: 0] Seed set to 6
[2024-06-21 05:29:53,165][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 05:29:53,167][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-21 05:29:53,168][train.py][INFO] - Instantiating callbacks...
[2024-06-21 05:29:53,168][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 05:29:53,170][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 05:29:53,171][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 05:29:53,171][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 05:29:53,174][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 05:29:53,174][train.py][INFO] - Instantiating loggers...
[2024-06-21 05:29:53,174][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 05:29:53,175][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 05:29:53,243][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/proud_life/0.05/6/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.16it/s v_num: 0.000      
                                                              val/auc: 0.406    
                                                              val/f1: 0.154     
                                                              val/precision:    
                                                              0.250 val/recall: 
                                                              0.111 val/mre:    
                                                              0.119 train/auc:  
                                                              0.992 train/f1:   
                                                              0.992             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              0.984 train/mre:  
                                                              0.130             
[2024-06-21 05:31:46,650][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/proud_life/0.05/6>
[2024-06-21 05:31:46,651][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 05:31:46,654][HYDRA] 	#367 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=proud_life data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=7
[rank: 0] Seed set to 7
[2024-06-21 05:31:46,968][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 05:31:46,970][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-21 05:31:46,971][train.py][INFO] - Instantiating callbacks...
[2024-06-21 05:31:46,971][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 05:31:46,973][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 05:31:46,974][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 05:31:46,974][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 05:31:46,978][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 05:31:46,979][train.py][INFO] - Instantiating loggers...
[2024-06-21 05:31:46,979][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 05:31:46,980][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 05:31:47,162][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/proud_life/0.05/7/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.16it/s v_num: 0.000      
                                                              val/auc: 0.467    
                                                              val/f1: 0.375     
                                                              val/precision:    
                                                              0.429 val/recall: 
                                                              0.333 val/mre:    
                                                              0.126 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.139             
[2024-06-21 05:33:41,439][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/proud_life/0.05/7>
[2024-06-21 05:33:41,440][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 05:33:41,442][HYDRA] 	#368 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=proud_life data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=8
[rank: 0] Seed set to 8
[2024-06-21 05:33:41,624][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 05:33:41,626][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-21 05:33:41,627][train.py][INFO] - Instantiating callbacks...
[2024-06-21 05:33:41,627][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 05:33:41,629][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 05:33:41,630][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 05:33:41,630][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 05:33:41,650][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 05:33:41,650][train.py][INFO] - Instantiating loggers...
[2024-06-21 05:33:41,651][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 05:33:41,652][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 05:33:41,736][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/proud_life/0.05/8/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.11it/s v_num: 0.000      
                                                              val/auc: 0.461    
                                                              val/f1: 0.286     
                                                              val/precision:    
                                                              0.400 val/recall: 
                                                              0.222 val/mre:    
                                                              0.126 train/auc:  
                                                              0.951 train/f1:   
                                                              0.950             
                                                              train/precision:  
                                                              0.966             
                                                              train/recall:     
                                                              0.934 train/mre:  
                                                              0.138             
[2024-06-21 05:35:38,630][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/proud_life/0.05/8>
[2024-06-21 05:35:38,631][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 05:35:38,633][HYDRA] 	#369 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=proud_life data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=9
[rank: 0] Seed set to 9
[2024-06-21 05:35:38,817][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 05:35:38,819][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-21 05:35:38,820][train.py][INFO] - Instantiating callbacks...
[2024-06-21 05:35:38,820][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 05:35:38,822][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 05:35:38,823][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 05:35:38,823][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 05:35:38,826][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 05:35:38,827][train.py][INFO] - Instantiating loggers...
[2024-06-21 05:35:38,827][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 05:35:38,828][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 05:35:38,895][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/proud_life/0.05/9/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.13it/s v_num: 0.000      
                                                              val/auc: 0.361    
                                                              val/f1: 0.250     
                                                              val/precision:    
                                                              0.286 val/recall: 
                                                              0.222 val/mre:    
                                                              0.117 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.133             
[2024-06-21 05:37:33,593][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/proud_life/0.05/9>
[2024-06-21 05:37:33,594][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 05:37:33,596][HYDRA] 	#370 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=proud_life data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=0
[2024-06-21 05:37:33,777][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 05:37:33,778][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-21 05:37:33,779][train.py][INFO] - Instantiating callbacks...
[2024-06-21 05:37:33,780][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 05:37:33,782][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 05:37:33,782][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 05:37:33,783][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 05:37:33,785][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 05:37:33,786][train.py][INFO] - Instantiating loggers...
[2024-06-21 05:37:33,786][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 05:37:33,787][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 05:37:33,855][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/proud_life/0.1/0/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.14it/s v_num: 0.000      
                                                              val/auc: 0.572    
                                                              val/f1: 0.500     
                                                              val/precision:    
                                                              0.571 val/recall: 
                                                              0.444 val/mre:    
                                                              0.128 train/auc:  
                                                              0.852 train/f1:   
                                                              0.833             
                                                              train/precision:  
                                                              0.957             
                                                              train/recall:     
                                                              0.738 train/mre:  
                                                              0.141             
[2024-06-21 05:39:28,319][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/proud_life/0.1/0>
[2024-06-21 05:39:28,320][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 05:39:28,322][HYDRA] 	#371 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=proud_life data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=1
[rank: 0] Seed set to 1
[2024-06-21 05:39:28,504][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 05:39:28,506][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-21 05:39:28,507][train.py][INFO] - Instantiating callbacks...
[2024-06-21 05:39:28,507][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 05:39:28,509][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 05:39:28,510][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 05:39:28,510][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 05:39:28,514][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 05:39:28,514][train.py][INFO] - Instantiating loggers...
[2024-06-21 05:39:28,515][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 05:39:28,516][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 05:39:28,580][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/proud_life/0.1/1/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.13it/s v_num: 0.000      
                                                              val/auc: 0.411    
                                                              val/f1: 0.267     
                                                              val/precision:    
                                                              0.333 val/recall: 
                                                              0.222 val/mre:    
                                                              0.136 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.148             
[2024-06-21 05:41:24,486][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/proud_life/0.1/1>
[2024-06-21 05:41:24,487][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 05:41:24,489][HYDRA] 	#372 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=proud_life data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=2
[rank: 0] Seed set to 2
[2024-06-21 05:41:24,673][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 05:41:24,675][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-21 05:41:24,676][train.py][INFO] - Instantiating callbacks...
[2024-06-21 05:41:24,676][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 05:41:24,678][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 05:41:24,679][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 05:41:24,679][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 05:41:24,685][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 05:41:24,685][train.py][INFO] - Instantiating loggers...
[2024-06-21 05:41:24,686][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 05:41:24,687][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 05:41:24,753][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/proud_life/0.1/2/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.13it/s v_num: 0.000      
                                                              val/auc: 0.672    
                                                              val/f1: 0.571     
                                                              val/precision:    
                                                              0.800 val/recall: 
                                                              0.444 val/mre:    
                                                              0.130 train/auc:  
                                                              0.959 train/f1:   
                                                              0.959             
                                                              train/precision:  
                                                              0.967             
                                                              train/recall:     
                                                              0.951 train/mre:  
                                                              0.142             
[2024-06-21 05:43:22,195][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/proud_life/0.1/2>
[2024-06-21 05:43:22,195][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 05:43:22,198][HYDRA] 	#373 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=proud_life data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=3
[rank: 0] Seed set to 3
[2024-06-21 05:43:22,382][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 05:43:22,383][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-21 05:43:22,384][train.py][INFO] - Instantiating callbacks...
[2024-06-21 05:43:22,385][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 05:43:22,387][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 05:43:22,387][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 05:43:22,388][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 05:43:22,390][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 05:43:22,391][train.py][INFO] - Instantiating loggers...
[2024-06-21 05:43:22,391][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 05:43:22,392][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 05:43:22,457][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/proud_life/0.1/3/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.13it/s v_num: 0.000      
                                                              val/auc: 0.572    
                                                              val/f1: 0.500     
                                                              val/precision:    
                                                              0.571 val/recall: 
                                                              0.444 val/mre:    
                                                              0.131 train/auc:  
                                                              0.992 train/f1:   
                                                              0.992             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              0.984 train/mre:  
                                                              0.145             
[2024-06-21 05:45:19,037][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/proud_life/0.1/3>
[2024-06-21 05:45:19,038][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 05:45:19,040][HYDRA] 	#374 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=proud_life data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=4
[rank: 0] Seed set to 4
[2024-06-21 05:45:19,228][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 05:45:19,229][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-21 05:45:19,231][train.py][INFO] - Instantiating callbacks...
[2024-06-21 05:45:19,231][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 05:45:19,233][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 05:45:19,233][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 05:45:19,234][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 05:45:19,236][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 05:45:19,237][train.py][INFO] - Instantiating loggers...
[2024-06-21 05:45:19,237][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 05:45:19,238][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 05:45:19,307][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/proud_life/0.1/4/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.14it/s v_num: 0.000      
                                                              val/auc: 0.461    
                                                              val/f1: 0.286     
                                                              val/precision:    
                                                              0.400 val/recall: 
                                                              0.222 val/mre:    
                                                              0.130 train/auc:  
                                                              0.984 train/f1:   
                                                              0.983             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              0.967 train/mre:  
                                                              0.144             
[2024-06-21 05:47:15,376][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/proud_life/0.1/4>
[2024-06-21 05:47:15,377][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 05:47:15,379][HYDRA] 	#375 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=proud_life data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=5
[rank: 0] Seed set to 5
[2024-06-21 05:47:15,561][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 05:47:15,562][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-21 05:47:15,563][train.py][INFO] - Instantiating callbacks...
[2024-06-21 05:47:15,564][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 05:47:15,566][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 05:47:15,566][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 05:47:15,566][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 05:47:15,593][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 05:47:15,593][train.py][INFO] - Instantiating loggers...
[2024-06-21 05:47:15,594][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 05:47:15,595][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 05:47:15,675][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/proud_life/0.1/5/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.15it/s v_num: 0.000      
                                                              val/auc: 0.517    
                                                              val/f1: 0.400     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.333 val/mre:    
                                                              0.139 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.151             
[2024-06-21 05:49:10,282][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/proud_life/0.1/5>
[2024-06-21 05:49:10,282][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 05:49:10,285][HYDRA] 	#376 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=proud_life data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=6
[rank: 0] Seed set to 6
[2024-06-21 05:49:10,469][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 05:49:10,470][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-21 05:49:10,472][train.py][INFO] - Instantiating callbacks...
[2024-06-21 05:49:10,472][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 05:49:10,474][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 05:49:10,474][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 05:49:10,475][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 05:49:10,478][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 05:49:10,478][train.py][INFO] - Instantiating loggers...
[2024-06-21 05:49:10,478][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 05:49:10,479][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 05:49:10,545][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/proud_life/0.1/6/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.14it/s v_num: 0.000      
                                                              val/auc: 0.728    
                                                              val/f1: 0.667     
                                                              val/precision:    
                                                              0.833 val/recall: 
                                                              0.556 val/mre:    
                                                              0.126 train/auc:  
                                                              0.934 train/f1:   
                                                              0.938             
                                                              train/precision:  
                                                              0.884             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.137             
[2024-06-21 05:51:05,533][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/proud_life/0.1/6>
[2024-06-21 05:51:05,534][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 05:51:05,536][HYDRA] 	#377 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=proud_life data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=7
[rank: 0] Seed set to 7
[2024-06-21 05:51:05,720][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 05:51:05,721][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-21 05:51:05,723][train.py][INFO] - Instantiating callbacks...
[2024-06-21 05:51:05,723][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 05:51:05,725][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 05:51:05,725][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 05:51:05,726][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 05:51:05,728][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 05:51:05,729][train.py][INFO] - Instantiating loggers...
[2024-06-21 05:51:05,729][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 05:51:05,730][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 05:51:05,796][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/proud_life/0.1/7/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.11it/s v_num: 0.000      
                                                              val/auc: 0.461    
                                                              val/f1: 0.286     
                                                              val/precision:    
                                                              0.400 val/recall: 
                                                              0.222 val/mre:    
                                                              0.132 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.147             
[2024-06-21 05:53:01,641][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/proud_life/0.1/7>
[2024-06-21 05:53:01,642][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 05:53:01,646][HYDRA] 	#378 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=proud_life data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=8
[rank: 0] Seed set to 8
[2024-06-21 05:53:02,045][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 05:53:02,047][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-21 05:53:02,048][train.py][INFO] - Instantiating callbacks...
[2024-06-21 05:53:02,048][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 05:53:02,052][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 05:53:02,053][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 05:53:02,053][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 05:53:02,056][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 05:53:02,056][train.py][INFO] - Instantiating loggers...
[2024-06-21 05:53:02,056][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 05:53:02,058][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 05:53:02,338][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/proud_life/0.1/8/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.10it/s v_num: 0.000      
                                                              val/auc: 0.361    
                                                              val/f1: 0.250     
                                                              val/precision:    
                                                              0.286 val/recall: 
                                                              0.222 val/mre:    
                                                              0.131 train/auc:  
                                                              0.984 train/f1:   
                                                              0.984             
                                                              train/precision:  
                                                              0.984             
                                                              train/recall:     
                                                              0.984 train/mre:  
                                                              0.146             
[2024-06-21 05:55:00,319][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/proud_life/0.1/8>
[2024-06-21 05:55:00,320][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 05:55:00,322][HYDRA] 	#379 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=proud_life data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=9
[rank: 0] Seed set to 9
[2024-06-21 05:55:00,611][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 05:55:00,613][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-21 05:55:00,614][train.py][INFO] - Instantiating callbacks...
[2024-06-21 05:55:00,614][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 05:55:00,617][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 05:55:00,617][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 05:55:00,617][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 05:55:00,623][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 05:55:00,623][train.py][INFO] - Instantiating loggers...
[2024-06-21 05:55:00,623][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 05:55:00,625][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 05:55:00,761][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/proud_life/0.1/9/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.12it/s v_num: 0.000      
                                                              val/auc: 0.406    
                                                              val/f1: 0.154     
                                                              val/precision:    
                                                              0.250 val/recall: 
                                                              0.111 val/mre:    
                                                              0.129 train/auc:  
                                                              0.992 train/f1:   
                                                              0.992             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              0.984 train/mre:  
                                                              0.142             
[2024-06-21 05:56:56,864][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/proud_life/0.1/9>
[2024-06-21 05:56:56,864][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 05:56:56,866][HYDRA] 	#380 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=proud_life data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=0
[2024-06-21 05:56:57,051][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 05:56:57,053][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-21 05:56:57,054][train.py][INFO] - Instantiating callbacks...
[2024-06-21 05:56:57,054][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 05:56:57,056][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 05:56:57,057][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 05:56:57,057][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 05:56:57,060][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 05:56:57,060][train.py][INFO] - Instantiating loggers...
[2024-06-21 05:56:57,060][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 05:56:57,062][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 05:56:57,129][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/proud_life/0.15/0/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.13it/s v_num: 0.000      
                                                              val/auc: 0.517    
                                                              val/f1: 0.400     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.333 val/mre:    
                                                              0.135 train/auc:  
                                                              0.967 train/f1:   
                                                              0.967             
                                                              train/precision:  
                                                              0.983             
                                                              train/recall:     
                                                              0.951 train/mre:  
                                                              0.146             
[2024-06-21 05:58:52,376][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/proud_life/0.15/0>
[2024-06-21 05:58:52,377][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 05:58:52,380][HYDRA] 	#381 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=proud_life data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=1
[rank: 0] Seed set to 1
[2024-06-21 05:58:52,561][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 05:58:52,562][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-21 05:58:52,563][train.py][INFO] - Instantiating callbacks...
[2024-06-21 05:58:52,564][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 05:58:52,566][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 05:58:52,566][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 05:58:52,567][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 05:58:52,569][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 05:58:52,569][train.py][INFO] - Instantiating loggers...
[2024-06-21 05:58:52,569][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 05:58:52,571][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 05:58:52,636][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/proud_life/0.15/1/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.12it/s v_num: 0.000      
                                                              val/auc: 0.517    
                                                              val/f1: 0.400     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.333 val/mre:    
                                                              0.143 train/auc:  
                                                              0.992 train/f1:   
                                                              0.992             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              0.984 train/mre:  
                                                              0.155             
[2024-06-21 06:00:48,619][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/proud_life/0.15/1>
[2024-06-21 06:00:48,620][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 06:00:48,622][HYDRA] 	#382 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=proud_life data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=2
[rank: 0] Seed set to 2
[2024-06-21 06:00:48,806][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 06:00:48,807][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-21 06:00:48,809][train.py][INFO] - Instantiating callbacks...
[2024-06-21 06:00:48,809][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 06:00:48,811][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 06:00:48,811][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 06:00:48,812][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 06:00:48,814][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 06:00:48,815][train.py][INFO] - Instantiating loggers...
[2024-06-21 06:00:48,815][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 06:00:48,816][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 06:00:48,882][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/proud_life/0.15/2/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.12it/s v_num: 0.000      
                                                              val/auc: 0.511    
                                                              val/f1: 0.308     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.222 val/mre:    
                                                              0.136 train/auc:  
                                                              0.992 train/f1:   
                                                              0.992             
                                                              train/precision:  
                                                              0.984             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.149             
[2024-06-21 06:02:44,701][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/proud_life/0.15/2>
[2024-06-21 06:02:44,702][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 06:02:44,704][HYDRA] 	#383 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=proud_life data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=3
[rank: 0] Seed set to 3
[2024-06-21 06:02:44,887][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 06:02:44,888][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-21 06:02:44,890][train.py][INFO] - Instantiating callbacks...
[2024-06-21 06:02:44,890][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 06:02:44,892][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 06:02:44,892][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 06:02:44,893][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 06:02:44,895][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 06:02:44,896][train.py][INFO] - Instantiating loggers...
[2024-06-21 06:02:44,896][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 06:02:44,897][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 06:02:44,962][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/proud_life/0.15/3/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.14it/s v_num: 0.000      
                                                              val/auc: 0.672    
                                                              val/f1: 0.571     
                                                              val/precision:    
                                                              0.800 val/recall: 
                                                              0.444 val/mre:    
                                                              0.140 train/auc:  
                                                              0.984 train/f1:   
                                                              0.983             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              0.967 train/mre:  
                                                              0.153             
[2024-06-21 06:04:39,159][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/proud_life/0.15/3>
[2024-06-21 06:04:39,160][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 06:04:39,163][HYDRA] 	#384 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=proud_life data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=4
[rank: 0] Seed set to 4
[2024-06-21 06:04:39,354][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 06:04:39,356][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-21 06:04:39,357][train.py][INFO] - Instantiating callbacks...
[2024-06-21 06:04:39,357][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 06:04:39,359][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 06:04:39,360][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 06:04:39,360][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 06:04:39,363][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 06:04:39,363][train.py][INFO] - Instantiating loggers...
[2024-06-21 06:04:39,363][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 06:04:39,365][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 06:04:39,437][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/proud_life/0.15/4/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.15it/s v_num: 0.000      
                                                              val/auc: 0.467    
                                                              val/f1: 0.375     
                                                              val/precision:    
                                                              0.429 val/recall: 
                                                              0.333 val/mre:    
                                                              0.139 train/auc:  
                                                              0.992 train/f1:   
                                                              0.992             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              0.984 train/mre:  
                                                              0.152             
[2024-06-21 06:06:33,364][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/proud_life/0.15/4>
[2024-06-21 06:06:33,364][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 06:06:33,367][HYDRA] 	#385 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=proud_life data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=5
[rank: 0] Seed set to 5
[2024-06-21 06:06:33,549][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 06:06:33,551][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-21 06:06:33,552][train.py][INFO] - Instantiating callbacks...
[2024-06-21 06:06:33,552][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 06:06:33,554][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 06:06:33,554][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 06:06:33,555][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 06:06:33,557][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 06:06:33,558][train.py][INFO] - Instantiating loggers...
[2024-06-21 06:06:33,558][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 06:06:33,559][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 06:06:33,624][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/proud_life/0.15/5/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.10it/s v_num: 0.000      
                                                              val/auc: 0.567    
                                                              val/f1: 0.429     
                                                              val/precision:    
                                                              0.600 val/recall: 
                                                              0.333 val/mre:    
                                                              0.149 train/auc:  
                                                              0.951 train/f1:   
                                                              0.952             
                                                              train/precision:  
                                                              0.923             
                                                              train/recall:     
                                                              0.984 train/mre:  
                                                              0.161             
[2024-06-21 06:08:28,460][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/proud_life/0.15/5>
[2024-06-21 06:08:28,461][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 06:08:28,463][HYDRA] 	#386 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=proud_life data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=6
[rank: 0] Seed set to 6
[2024-06-21 06:08:28,653][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 06:08:28,654][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-21 06:08:28,655][train.py][INFO] - Instantiating callbacks...
[2024-06-21 06:08:28,655][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 06:08:28,657][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 06:08:28,658][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 06:08:28,658][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 06:08:28,661][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 06:08:28,661][train.py][INFO] - Instantiating loggers...
[2024-06-21 06:08:28,662][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 06:08:28,663][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 06:08:28,732][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/proud_life/0.15/6/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.14it/s v_num: 0.000      
                                                              val/auc: 0.739    
                                                              val/f1: 0.737     
                                                              val/precision:    
                                                              0.700 val/recall: 
                                                              0.778 val/mre:    
                                                              0.132 train/auc:  
                                                              0.984 train/f1:   
                                                              0.983             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              0.967 train/mre:  
                                                              0.144             
[2024-06-21 06:10:23,204][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/proud_life/0.15/6>
[2024-06-21 06:10:23,204][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 06:10:23,207][HYDRA] 	#387 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=proud_life data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=7
[rank: 0] Seed set to 7
[2024-06-21 06:10:23,391][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 06:10:23,393][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-21 06:10:23,394][train.py][INFO] - Instantiating callbacks...
[2024-06-21 06:10:23,394][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 06:10:23,396][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 06:10:23,397][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 06:10:23,397][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 06:10:23,400][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 06:10:23,400][train.py][INFO] - Instantiating loggers...
[2024-06-21 06:10:23,400][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 06:10:23,401][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 06:10:23,466][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/proud_life/0.15/7/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.14it/s v_num: 0.000      
                                                              val/auc: 0.572    
                                                              val/f1: 0.500     
                                                              val/precision:    
                                                              0.571 val/recall: 
                                                              0.444 val/mre:    
                                                              0.139 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.153             
[2024-06-21 06:12:17,657][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/proud_life/0.15/7>
[2024-06-21 06:12:17,657][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 06:12:17,660][HYDRA] 	#388 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=proud_life data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=8
[rank: 0] Seed set to 8
[2024-06-21 06:12:17,850][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 06:12:17,852][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-21 06:12:17,853][train.py][INFO] - Instantiating callbacks...
[2024-06-21 06:12:17,853][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 06:12:17,855][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 06:12:17,856][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 06:12:17,856][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 06:12:17,859][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 06:12:17,859][train.py][INFO] - Instantiating loggers...
[2024-06-21 06:12:17,859][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 06:12:17,860][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 06:12:17,926][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/proud_life/0.15/8/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.13it/s v_num: 0.000      
                                                              val/auc: 0.572    
                                                              val/f1: 0.500     
                                                              val/precision:    
                                                              0.571 val/recall: 
                                                              0.444 val/mre:    
                                                              0.138 train/auc:  
                                                              0.975 train/f1:   
                                                              0.975             
                                                              train/precision:  
                                                              0.983             
                                                              train/recall:     
                                                              0.967 train/mre:  
                                                              0.152             
[2024-06-21 06:14:13,423][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/proud_life/0.15/8>
[2024-06-21 06:14:13,423][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 06:14:13,426][HYDRA] 	#389 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=proud_life data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=9
[rank: 0] Seed set to 9
[2024-06-21 06:14:13,611][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 06:14:13,612][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-21 06:14:13,614][train.py][INFO] - Instantiating callbacks...
[2024-06-21 06:14:13,614][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 06:14:13,616][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 06:14:13,616][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 06:14:13,617][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 06:14:13,619][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 06:14:13,620][train.py][INFO] - Instantiating loggers...
[2024-06-21 06:14:13,620][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 06:14:13,621][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 06:14:13,688][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/proud_life/0.15/9/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.13it/s v_num: 0.000      
                                                              val/auc: 0.361    
                                                              val/f1: 0.250     
                                                              val/precision:    
                                                              0.286 val/recall: 
                                                              0.222 val/mre:    
                                                              0.135 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.147             
[2024-06-21 06:16:08,716][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/proud_life/0.15/9>
[2024-06-21 06:16:08,717][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 06:16:08,719][HYDRA] 	#390 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=proud_life data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=0
[2024-06-21 06:16:08,900][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 06:16:08,901][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-21 06:16:08,902][train.py][INFO] - Instantiating callbacks...
[2024-06-21 06:16:08,903][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 06:16:08,905][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 06:16:08,905][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 06:16:08,906][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 06:16:08,909][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 06:16:08,909][train.py][INFO] - Instantiating loggers...
[2024-06-21 06:16:08,909][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 06:16:08,910][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 06:16:08,979][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/proud_life/0.2/0/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.14it/s v_num: 0.000      
                                                              val/auc: 0.467    
                                                              val/f1: 0.375     
                                                              val/precision:    
                                                              0.429 val/recall: 
                                                              0.333 val/mre:    
                                                              0.142 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.154             
[2024-06-21 06:18:03,762][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/proud_life/0.2/0>
[2024-06-21 06:18:03,762][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 06:18:03,765][HYDRA] 	#391 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=proud_life data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=1
[rank: 0] Seed set to 1
[2024-06-21 06:18:03,955][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 06:18:03,957][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-21 06:18:03,958][train.py][INFO] - Instantiating callbacks...
[2024-06-21 06:18:03,958][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 06:18:03,960][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 06:18:03,961][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 06:18:03,961][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 06:18:03,964][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 06:18:03,964][train.py][INFO] - Instantiating loggers...
[2024-06-21 06:18:03,964][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 06:18:03,965][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 06:18:04,045][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/proud_life/0.2/1/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.11it/s v_num: 0.000      
                                                              val/auc: 0.517    
                                                              val/f1: 0.400     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.333 val/mre:    
                                                              0.145 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.159             
[2024-06-21 06:19:59,979][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/proud_life/0.2/1>
[2024-06-21 06:19:59,980][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 06:19:59,982][HYDRA] 	#392 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=proud_life data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=2
[rank: 0] Seed set to 2
[2024-06-21 06:20:00,172][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 06:20:00,173][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-21 06:20:00,174][train.py][INFO] - Instantiating callbacks...
[2024-06-21 06:20:00,175][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 06:20:00,177][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 06:20:00,177][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 06:20:00,178][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 06:20:00,180][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 06:20:00,180][train.py][INFO] - Instantiating loggers...
[2024-06-21 06:20:00,181][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 06:20:00,182][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 06:20:00,250][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/proud_life/0.2/2/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.12it/s v_num: 0.000      
                                                              val/auc: 0.422    
                                                              val/f1: 0.421     
                                                              val/precision:    
                                                              0.400 val/recall: 
                                                              0.444 val/mre:    
                                                              0.144 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.157             
[2024-06-21 06:21:56,259][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/proud_life/0.2/2>
[2024-06-21 06:21:56,260][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 06:21:56,264][HYDRA] 	#393 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=proud_life data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=3
[rank: 0] Seed set to 3
[2024-06-21 06:21:56,450][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 06:21:56,452][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-21 06:21:56,453][train.py][INFO] - Instantiating callbacks...
[2024-06-21 06:21:56,453][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 06:21:56,456][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 06:21:56,456][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 06:21:56,456][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 06:21:56,459][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 06:21:56,460][train.py][INFO] - Instantiating loggers...
[2024-06-21 06:21:56,460][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 06:21:56,461][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 06:21:56,530][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/proud_life/0.2/3/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.13it/s v_num: 0.000      
                                                              val/auc: 0.561    
                                                              val/f1: 0.333     
                                                              val/precision:    
                                                              0.667 val/recall: 
                                                              0.222 val/mre:    
                                                              0.143 train/auc:  
                                                              0.975 train/f1:   
                                                              0.976             
                                                              train/precision:  
                                                              0.968             
                                                              train/recall:     
                                                              0.984 train/mre:  
                                                              0.157             
[2024-06-21 06:23:51,447][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/proud_life/0.2/3>
[2024-06-21 06:23:51,447][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 06:23:51,450][HYDRA] 	#394 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=proud_life data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=4
[rank: 0] Seed set to 4
[2024-06-21 06:23:51,633][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 06:23:51,635][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-21 06:23:51,636][train.py][INFO] - Instantiating callbacks...
[2024-06-21 06:23:51,636][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 06:23:51,638][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 06:23:51,639][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 06:23:51,639][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 06:23:51,642][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 06:23:51,642][train.py][INFO] - Instantiating loggers...
[2024-06-21 06:23:51,642][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 06:23:51,644][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 06:23:51,719][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/proud_life/0.2/4/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.14it/s v_num: 0.000      
                                                              val/auc: 0.511    
                                                              val/f1: 0.308     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.222 val/mre:    
                                                              0.138 train/auc:  
                                                              0.984 train/f1:   
                                                              0.984             
                                                              train/precision:  
                                                              0.968             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.152             
[2024-06-21 06:25:46,304][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/proud_life/0.2/4>
[2024-06-21 06:25:46,305][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 06:25:46,307][HYDRA] 	#395 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=proud_life data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=5
[rank: 0] Seed set to 5
[2024-06-21 06:25:46,488][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 06:25:46,490][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-21 06:25:46,491][train.py][INFO] - Instantiating callbacks...
[2024-06-21 06:25:46,491][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 06:25:46,493][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 06:25:46,494][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 06:25:46,494][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 06:25:46,497][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 06:25:46,497][train.py][INFO] - Instantiating loggers...
[2024-06-21 06:25:46,497][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 06:25:46,499][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 06:25:46,570][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/proud_life/0.2/5/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.11it/s v_num: 0.000      
                                                              val/auc: 0.511    
                                                              val/f1: 0.308     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.222 val/mre:    
                                                              0.151 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.163             
[2024-06-21 06:27:41,846][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/proud_life/0.2/5>
[2024-06-21 06:27:41,846][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 06:27:41,849][HYDRA] 	#396 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=proud_life data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=6
[rank: 0] Seed set to 6
[2024-06-21 06:27:42,034][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 06:27:42,036][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-21 06:27:42,037][train.py][INFO] - Instantiating callbacks...
[2024-06-21 06:27:42,037][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 06:27:42,039][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 06:27:42,040][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 06:27:42,040][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 06:27:42,043][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 06:27:42,043][train.py][INFO] - Instantiating loggers...
[2024-06-21 06:27:42,043][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 06:27:42,044][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 06:27:42,113][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/proud_life/0.2/6/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.12it/s v_num: 0.000      
                                                              val/auc: 0.567    
                                                              val/f1: 0.429     
                                                              val/precision:    
                                                              0.600 val/recall: 
                                                              0.333 val/mre:    
                                                              0.137 train/auc:  
                                                              0.975 train/f1:   
                                                              0.976             
                                                              train/precision:  
                                                              0.968             
                                                              train/recall:     
                                                              0.984 train/mre:  
                                                              0.149             
[2024-06-21 06:29:37,079][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/proud_life/0.2/6>
[2024-06-21 06:29:37,080][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 06:29:37,082][HYDRA] 	#397 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=proud_life data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=7
[rank: 0] Seed set to 7
[2024-06-21 06:29:37,264][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 06:29:37,266][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-21 06:29:37,267][train.py][INFO] - Instantiating callbacks...
[2024-06-21 06:29:37,267][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 06:29:37,269][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 06:29:37,270][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 06:29:37,270][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 06:29:37,273][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 06:29:37,273][train.py][INFO] - Instantiating loggers...
[2024-06-21 06:29:37,273][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 06:29:37,274][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 06:29:37,342][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/proud_life/0.2/7/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.13it/s v_num: 0.000      
                                                              val/auc: 0.356    
                                                              val/f1: 0.143     
                                                              val/precision:    
                                                              0.200 val/recall: 
                                                              0.111 val/mre:    
                                                              0.140 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.156             
[2024-06-21 06:31:33,047][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/proud_life/0.2/7>
[2024-06-21 06:31:33,047][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 06:31:33,050][HYDRA] 	#398 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=proud_life data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=8
[rank: 0] Seed set to 8
[2024-06-21 06:31:33,230][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 06:31:33,232][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-21 06:31:33,233][train.py][INFO] - Instantiating callbacks...
[2024-06-21 06:31:33,233][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 06:31:33,235][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 06:31:33,236][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 06:31:33,236][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 06:31:33,239][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 06:31:33,239][train.py][INFO] - Instantiating loggers...
[2024-06-21 06:31:33,240][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 06:31:33,241][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 06:31:33,305][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/proud_life/0.2/8/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.12it/s v_num: 0.000      
                                                              val/auc: 0.694    
                                                              val/f1: 0.727     
                                                              val/precision:    
                                                              0.615 val/recall: 
                                                              0.889 val/mre:    
                                                              0.143 train/auc:  
                                                              0.943 train/f1:   
                                                              0.941             
                                                              train/precision:  
                                                              0.966             
                                                              train/recall:     
                                                              0.918 train/mre:  
                                                              0.158             
[2024-06-21 06:33:29,991][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/proud_life/0.2/8>
[2024-06-21 06:33:29,991][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 06:33:29,994][HYDRA] 	#399 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=proud_life data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=9
[rank: 0] Seed set to 9
[2024-06-21 06:33:30,175][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 06:33:30,177][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-21 06:33:30,178][train.py][INFO] - Instantiating callbacks...
[2024-06-21 06:33:30,178][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 06:33:30,180][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 06:33:30,181][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 06:33:30,181][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 06:33:30,184][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 06:33:30,184][train.py][INFO] - Instantiating loggers...
[2024-06-21 06:33:30,184][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 06:33:30,186][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 06:33:30,250][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/proud_life/0.2/9/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.13it/s v_num: 0.000      
                                                              val/auc: 0.522    
                                                              val/f1: 0.471     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.444 val/mre:    
                                                              0.138 train/auc:  
                                                              0.975 train/f1:   
                                                              0.975             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              0.951 train/mre:  
                                                              0.150             
[2024-06-21 06:35:25,537][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/proud_life/0.2/9>
[2024-06-21 06:35:25,538][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 06:35:25,540][HYDRA] 	#400 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=proud_life data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=0
[2024-06-21 06:35:25,726][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 06:35:25,727][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-21 06:35:25,728][train.py][INFO] - Instantiating callbacks...
[2024-06-21 06:35:25,729][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 06:35:25,731][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 06:35:25,731][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 06:35:25,731][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 06:35:25,734][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 06:35:25,735][train.py][INFO] - Instantiating loggers...
[2024-06-21 06:35:25,735][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 06:35:25,736][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 06:35:25,803][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/proud_life/0.25/0/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.13it/s v_num: 0.000      
                                                              val/auc: 0.417    
                                                              val/f1: 0.353     
                                                              val/precision:    
                                                              0.375 val/recall: 
                                                              0.333 val/mre:    
                                                              0.145 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.158             
[2024-06-21 06:37:20,266][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/proud_life/0.25/0>
[2024-06-21 06:37:20,267][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 06:37:20,269][HYDRA] 	#401 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=proud_life data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=1
[rank: 0] Seed set to 1
[2024-06-21 06:37:20,455][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 06:37:20,456][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-21 06:37:20,457][train.py][INFO] - Instantiating callbacks...
[2024-06-21 06:37:20,457][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 06:37:20,459][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 06:37:20,460][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 06:37:20,460][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 06:37:20,463][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 06:37:20,463][train.py][INFO] - Instantiating loggers...
[2024-06-21 06:37:20,464][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 06:37:20,465][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 06:37:20,539][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/proud_life/0.25/1/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.13it/s v_num: 0.000      
                                                              val/auc: 0.628    
                                                              val/f1: 0.588     
                                                              val/precision:    
                                                              0.625 val/recall: 
                                                              0.556 val/mre:    
                                                              0.146 train/auc:  
                                                              0.992 train/f1:   
                                                              0.992             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              0.984 train/mre:  
                                                              0.160             
[2024-06-21 06:39:15,505][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/proud_life/0.25/1>
[2024-06-21 06:39:15,506][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 06:39:15,508][HYDRA] 	#402 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=proud_life data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=2
[rank: 0] Seed set to 2
[2024-06-21 06:39:15,692][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 06:39:15,694][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-21 06:39:15,695][train.py][INFO] - Instantiating callbacks...
[2024-06-21 06:39:15,695][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 06:39:15,697][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 06:39:15,698][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 06:39:15,698][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 06:39:15,701][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 06:39:15,701][train.py][INFO] - Instantiating loggers...
[2024-06-21 06:39:15,701][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 06:39:15,702][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 06:39:15,770][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/proud_life/0.25/2/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.12it/s v_num: 0.000      
                                                              val/auc: 0.467    
                                                              val/f1: 0.375     
                                                              val/precision:    
                                                              0.429 val/recall: 
                                                              0.333 val/mre:    
                                                              0.148 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.160             
[2024-06-21 06:41:11,950][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/proud_life/0.25/2>
[2024-06-21 06:41:11,953][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 06:41:11,973][HYDRA] 	#403 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=proud_life data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=3
[rank: 0] Seed set to 3
[2024-06-21 06:41:12,240][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 06:41:12,241][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-21 06:41:12,242][train.py][INFO] - Instantiating callbacks...
[2024-06-21 06:41:12,242][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 06:41:12,245][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 06:41:12,245][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 06:41:12,245][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 06:41:12,248][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 06:41:12,249][train.py][INFO] - Instantiating loggers...
[2024-06-21 06:41:12,249][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 06:41:12,250][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 06:41:12,318][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/proud_life/0.25/3/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.13it/s v_num: 0.000      
                                                              val/auc: 0.622    
                                                              val/f1: 0.533     
                                                              val/precision:    
                                                              0.667 val/recall: 
                                                              0.444 val/mre:    
                                                              0.147 train/auc:  
                                                              0.992 train/f1:   
                                                              0.992             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              0.984 train/mre:  
                                                              0.161             
[2024-06-21 06:43:07,259][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/proud_life/0.25/3>
[2024-06-21 06:43:07,260][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 06:43:07,263][HYDRA] 	#404 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=proud_life data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=4
[rank: 0] Seed set to 4
[2024-06-21 06:43:07,446][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 06:43:07,447][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-21 06:43:07,448][train.py][INFO] - Instantiating callbacks...
[2024-06-21 06:43:07,449][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 06:43:07,451][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 06:43:07,451][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 06:43:07,452][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 06:43:07,454][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 06:43:07,455][train.py][INFO] - Instantiating loggers...
[2024-06-21 06:43:07,455][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 06:43:07,456][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 06:43:07,523][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/proud_life/0.25/4/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.14it/s v_num: 0.000      
                                                              val/auc: 0.678    
                                                              val/f1: 0.625     
                                                              val/precision:    
                                                              0.714 val/recall: 
                                                              0.556 val/mre:    
                                                              0.144 train/auc:  
                                                              0.992 train/f1:   
                                                              0.992             
                                                              train/precision:  
                                                              0.984             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.158             
[2024-06-21 06:45:01,638][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/proud_life/0.25/4>
[2024-06-21 06:45:01,638][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 06:45:01,640][HYDRA] 	#405 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=proud_life data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=5
[rank: 0] Seed set to 5
[2024-06-21 06:45:01,824][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 06:45:01,826][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-21 06:45:01,827][train.py][INFO] - Instantiating callbacks...
[2024-06-21 06:45:01,827][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 06:45:01,829][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 06:45:01,830][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 06:45:01,830][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 06:45:01,833][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 06:45:01,833][train.py][INFO] - Instantiating loggers...
[2024-06-21 06:45:01,833][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 06:45:01,835][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 06:45:01,900][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/proud_life/0.25/5/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.11it/s v_num: 0.000      
                                                              val/auc: 0.461    
                                                              val/f1: 0.286     
                                                              val/precision:    
                                                              0.400 val/recall: 
                                                              0.222 val/mre:    
                                                              0.156 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.166             
[2024-06-21 06:46:57,022][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/proud_life/0.25/5>
[2024-06-21 06:46:57,022][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 06:46:57,025][HYDRA] 	#406 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=proud_life data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=6
[rank: 0] Seed set to 6
[2024-06-21 06:46:57,208][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 06:46:57,210][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-21 06:46:57,211][train.py][INFO] - Instantiating callbacks...
[2024-06-21 06:46:57,211][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 06:46:57,213][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 06:46:57,214][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 06:46:57,214][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 06:46:57,217][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 06:46:57,217][train.py][INFO] - Instantiating loggers...
[2024-06-21 06:46:57,218][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 06:46:57,219][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 06:46:57,284][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/proud_life/0.25/6/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.13it/s v_num: 0.000      
                                                              val/auc: 0.739    
                                                              val/f1: 0.737     
                                                              val/precision:    
                                                              0.700 val/recall: 
                                                              0.778 val/mre:    
                                                              0.139 train/auc:  
                                                              0.975 train/f1:   
                                                              0.975             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              0.951 train/mre:  
                                                              0.151             
[2024-06-21 06:48:52,313][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/proud_life/0.25/6>
[2024-06-21 06:48:52,313][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 06:48:52,316][HYDRA] 	#407 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=proud_life data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=7
[rank: 0] Seed set to 7
[2024-06-21 06:48:52,499][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 06:48:52,500][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-21 06:48:52,501][train.py][INFO] - Instantiating callbacks...
[2024-06-21 06:48:52,501][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 06:48:52,504][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 06:48:52,504][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 06:48:52,504][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 06:48:52,507][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 06:48:52,508][train.py][INFO] - Instantiating loggers...
[2024-06-21 06:48:52,508][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 06:48:52,509][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 06:48:52,572][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/proud_life/0.25/7/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.14it/s v_num: 0.000      
                                                              val/auc: 0.517    
                                                              val/f1: 0.400     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.333 val/mre:    
                                                              0.147 train/auc:  
                                                              0.967 train/f1:   
                                                              0.966             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              0.934 train/mre:  
                                                              0.161             
[2024-06-21 06:50:47,248][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/proud_life/0.25/7>
[2024-06-21 06:50:47,249][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 06:50:47,251][HYDRA] 	#408 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=proud_life data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=8
[rank: 0] Seed set to 8
[2024-06-21 06:50:47,445][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 06:50:47,446][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-21 06:50:47,448][train.py][INFO] - Instantiating callbacks...
[2024-06-21 06:50:47,448][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 06:50:47,450][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 06:50:47,450][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 06:50:47,451][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 06:50:47,454][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 06:50:47,454][train.py][INFO] - Instantiating loggers...
[2024-06-21 06:50:47,454][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 06:50:47,456][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 06:50:47,520][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/proud_life/0.25/8/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.15it/s v_num: 0.000      
                                                              val/auc: 0.578    
                                                              val/f1: 0.556     
                                                              val/precision:    
                                                              0.556 val/recall: 
                                                              0.556 val/mre:    
                                                              0.148 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.161             
[2024-06-21 06:52:42,379][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/proud_life/0.25/8>
[2024-06-21 06:52:42,380][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 06:52:42,382][HYDRA] 	#409 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=proud_life data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=9
[rank: 0] Seed set to 9
[2024-06-21 06:52:42,573][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 06:52:42,575][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-21 06:52:42,576][train.py][INFO] - Instantiating callbacks...
[2024-06-21 06:52:42,576][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 06:52:42,578][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 06:52:42,579][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 06:52:42,579][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 06:52:42,582][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 06:52:42,582][train.py][INFO] - Instantiating loggers...
[2024-06-21 06:52:42,582][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 06:52:42,584][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 06:52:42,653][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/proud_life/0.25/9/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.13it/s v_num: 0.000      
                                                              val/auc: 0.483    
                                                              val/f1: 0.545     
                                                              val/precision:    
                                                              0.462 val/recall: 
                                                              0.667 val/mre:    
                                                              0.149 train/auc:  
                                                              0.902 train/f1:   
                                                              0.900             
                                                              train/precision:  
                                                              0.915             
                                                              train/recall:     
                                                              0.885 train/mre:  
                                                              0.159             
[2024-06-21 06:54:38,859][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/proud_life/0.25/9>
[2024-06-21 06:54:38,860][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 06:54:38,862][HYDRA] 	#410 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=proud_life data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=0
[2024-06-21 06:54:39,046][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 06:54:39,047][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-21 06:54:39,048][train.py][INFO] - Instantiating callbacks...
[2024-06-21 06:54:39,049][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 06:54:39,051][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 06:54:39,051][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 06:54:39,052][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 06:54:39,076][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 06:54:39,076][train.py][INFO] - Instantiating loggers...
[2024-06-21 06:54:39,076][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 06:54:39,077][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 06:54:39,154][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/proud_life/0.3/0/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.12it/s v_num: 0.000      
                                                              val/auc: 0.550    
                                                              val/f1: 0.667     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              1.000 val/mre:    
                                                              0.154 train/auc:  
                                                              0.902 train/f1:   
                                                              0.891             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              0.803 train/mre:  
                                                              0.162             
[2024-06-21 06:56:34,571][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/proud_life/0.3/0>
[2024-06-21 06:56:34,571][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 06:56:34,574][HYDRA] 	#411 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=proud_life data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=1
[rank: 0] Seed set to 1
[2024-06-21 06:56:34,759][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 06:56:34,760][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-21 06:56:34,761][train.py][INFO] - Instantiating callbacks...
[2024-06-21 06:56:34,762][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 06:56:34,764][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 06:56:34,764][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 06:56:34,764][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 06:56:34,767][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 06:56:34,767][train.py][INFO] - Instantiating loggers...
[2024-06-21 06:56:34,768][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 06:56:34,769][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 06:56:34,834][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/proud_life/0.3/1/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.13it/s v_num: 0.000      
                                                              val/auc: 0.639    
                                                              val/f1: 0.667     
                                                              val/precision:    
                                                              0.583 val/recall: 
                                                              0.778 val/mre:    
                                                              0.154 train/auc:  
                                                              0.992 train/f1:   
                                                              0.992             
                                                              train/precision:  
                                                              0.984             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.165             
[2024-06-21 06:58:30,464][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/proud_life/0.3/1>
[2024-06-21 06:58:30,465][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 06:58:30,467][HYDRA] 	#412 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=proud_life data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=2
[rank: 0] Seed set to 2
[2024-06-21 06:58:30,651][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 06:58:30,652][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-21 06:58:30,653][train.py][INFO] - Instantiating callbacks...
[2024-06-21 06:58:30,654][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 06:58:30,656][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 06:58:30,656][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 06:58:30,657][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 06:58:30,659][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 06:58:30,660][train.py][INFO] - Instantiating loggers...
[2024-06-21 06:58:30,660][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 06:58:30,661][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 06:58:30,727][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/proud_life/0.3/2/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.14it/s v_num: 0.000      
                                                              val/auc: 0.467    
                                                              val/f1: 0.375     
                                                              val/precision:    
                                                              0.429 val/recall: 
                                                              0.333 val/mre:    
                                                              0.146 train/auc:  
                                                              0.984 train/f1:   
                                                              0.983             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              0.967 train/mre:  
                                                              0.160             
[2024-06-21 07:00:25,714][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/proud_life/0.3/2>
[2024-06-21 07:00:25,715][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 07:00:25,717][HYDRA] 	#413 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=proud_life data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=3
[rank: 0] Seed set to 3
[2024-06-21 07:00:25,900][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 07:00:25,902][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-21 07:00:25,903][train.py][INFO] - Instantiating callbacks...
[2024-06-21 07:00:25,903][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 07:00:25,905][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 07:00:25,906][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 07:00:25,906][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 07:00:25,908][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 07:00:25,909][train.py][INFO] - Instantiating loggers...
[2024-06-21 07:00:25,909][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 07:00:25,910][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 07:00:25,975][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/proud_life/0.3/3/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.13it/s v_num: 0.000      
                                                              val/auc: 0.467    
                                                              val/f1: 0.375     
                                                              val/precision:    
                                                              0.429 val/recall: 
                                                              0.333 val/mre:    
                                                              0.147 train/auc:  
                                                              0.984 train/f1:   
                                                              0.983             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              0.967 train/mre:  
                                                              0.161             
[2024-06-21 07:02:21,094][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/proud_life/0.3/3>
[2024-06-21 07:02:21,095][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 07:02:21,097][HYDRA] 	#414 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=proud_life data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=4
[rank: 0] Seed set to 4
[2024-06-21 07:02:21,282][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 07:02:21,283][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-21 07:02:21,285][train.py][INFO] - Instantiating callbacks...
[2024-06-21 07:02:21,285][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 07:02:21,288][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 07:02:21,288][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 07:02:21,289][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 07:02:21,292][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 07:02:21,293][train.py][INFO] - Instantiating loggers...
[2024-06-21 07:02:21,293][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 07:02:21,294][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 07:02:21,359][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/proud_life/0.3/4/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.14it/s v_num: 0.000      
                                                              val/auc: 0.467    
                                                              val/f1: 0.375     
                                                              val/precision:    
                                                              0.429 val/recall: 
                                                              0.333 val/mre:    
                                                              0.147 train/auc:  
                                                              0.992 train/f1:   
                                                              0.992             
                                                              train/precision:  
                                                              0.984             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.161             
[2024-06-21 07:04:16,160][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/proud_life/0.3/4>
[2024-06-21 07:04:16,161][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 07:04:16,164][HYDRA] 	#415 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=proud_life data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=5
[rank: 0] Seed set to 5
[2024-06-21 07:04:16,348][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 07:04:16,349][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-21 07:04:16,351][train.py][INFO] - Instantiating callbacks...
[2024-06-21 07:04:16,351][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 07:04:16,353][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 07:04:16,353][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 07:04:16,354][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 07:04:16,356][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 07:04:16,357][train.py][INFO] - Instantiating loggers...
[2024-06-21 07:04:16,357][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 07:04:16,358][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 07:04:16,438][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/proud_life/0.3/5/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.13it/s v_num: 0.000      
                                                              val/auc: 0.472    
                                                              val/f1: 0.444     
                                                              val/precision:    
                                                              0.444 val/recall: 
                                                              0.444 val/mre:    
                                                              0.158 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.169             
[2024-06-21 07:06:10,446][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/proud_life/0.3/5>
[2024-06-21 07:06:10,447][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 07:06:10,449][HYDRA] 	#416 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=proud_life data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=6
[rank: 0] Seed set to 6
[2024-06-21 07:06:10,631][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 07:06:10,633][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-21 07:06:10,634][train.py][INFO] - Instantiating callbacks...
[2024-06-21 07:06:10,634][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 07:06:10,636][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 07:06:10,637][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 07:06:10,637][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 07:06:10,640][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 07:06:10,640][train.py][INFO] - Instantiating loggers...
[2024-06-21 07:06:10,640][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 07:06:10,641][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 07:06:10,710][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/proud_life/0.3/6/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.11it/s v_num: 0.000      
                                                              val/auc: 0.511    
                                                              val/f1: 0.308     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.222 val/mre:    
                                                              0.145 train/auc:  
                                                              0.951 train/f1:   
                                                              0.950             
                                                              train/precision:  
                                                              0.966             
                                                              train/recall:     
                                                              0.934 train/mre:  
                                                              0.159             
[2024-06-21 07:08:05,742][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/proud_life/0.3/6>
[2024-06-21 07:08:05,742][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 07:08:05,745][HYDRA] 	#417 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=proud_life data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=7
[rank: 0] Seed set to 7
[2024-06-21 07:08:05,932][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 07:08:05,934][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-21 07:08:05,935][train.py][INFO] - Instantiating callbacks...
[2024-06-21 07:08:05,935][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 07:08:05,937][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 07:08:05,938][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 07:08:05,938][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 07:08:05,941][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 07:08:05,941][train.py][INFO] - Instantiating loggers...
[2024-06-21 07:08:05,941][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 07:08:05,943][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 07:08:06,012][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/proud_life/0.3/7/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.13it/s v_num: 0.000      
                                                              val/auc: 0.461    
                                                              val/f1: 0.286     
                                                              val/precision:    
                                                              0.400 val/recall: 
                                                              0.222 val/mre:    
                                                              0.148 train/auc:  
                                                              0.975 train/f1:   
                                                              0.975             
                                                              train/precision:  
                                                              0.983             
                                                              train/recall:     
                                                              0.967 train/mre:  
                                                              0.163             
[2024-06-21 07:10:00,491][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/proud_life/0.3/7>
[2024-06-21 07:10:00,492][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 07:10:00,494][HYDRA] 	#418 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=proud_life data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=8
[rank: 0] Seed set to 8
[2024-06-21 07:10:00,696][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 07:10:00,698][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-21 07:10:00,699][train.py][INFO] - Instantiating callbacks...
[2024-06-21 07:10:00,699][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 07:10:00,701][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 07:10:00,702][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 07:10:00,702][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 07:10:00,705][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 07:10:00,705][train.py][INFO] - Instantiating loggers...
[2024-06-21 07:10:00,705][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 07:10:00,706][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 07:10:00,771][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/proud_life/0.3/8/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.14it/s v_num: 0.000      
                                                              val/auc: 0.733    
                                                              val/f1: 0.706     
                                                              val/precision:    
                                                              0.750 val/recall: 
                                                              0.667 val/mre:    
                                                              0.147 train/auc:  
                                                              0.984 train/f1:   
                                                              0.983             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              0.967 train/mre:  
                                                              0.161             
[2024-06-21 07:11:55,459][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/proud_life/0.3/8>
[2024-06-21 07:11:55,459][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 07:11:55,462][HYDRA] 	#419 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=proud_life data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=9
[rank: 0] Seed set to 9
[2024-06-21 07:11:55,646][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 07:11:55,647][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-21 07:11:55,648][train.py][INFO] - Instantiating callbacks...
[2024-06-21 07:11:55,648][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 07:11:55,650][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 07:11:55,651][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 07:11:55,651][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 07:11:55,654][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 07:11:55,655][train.py][INFO] - Instantiating loggers...
[2024-06-21 07:11:55,655][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 07:11:55,656][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 07:11:55,723][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/proud_life/0.3/9/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:02 • 0:00:00 1.67it/s v_num: 0.000      
                                                              val/auc: 0.522    
                                                              val/f1: 0.471     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.444 val/mre:    
                                                              0.145 train/auc:  
                                                              0.984 train/f1:   
                                                              0.983             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              0.967 train/mre:  
                                                              0.158             
[2024-06-21 07:13:50,594][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/proud_life/0.3/9>
[2024-06-21 07:13:50,595][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 07:13:50,598][HYDRA] 	#420 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=study_school data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=0
[2024-06-21 07:13:50,791][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 07:13:50,793][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-21 07:13:50,794][train.py][INFO] - Instantiating callbacks...
[2024-06-21 07:13:50,794][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 07:13:50,796][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 07:13:50,797][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 07:13:50,797][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 07:13:50,800][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 07:13:50,800][train.py][INFO] - Instantiating loggers...
[2024-06-21 07:13:50,801][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 07:13:50,802][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 07:13:50,868][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/study_school/0.0/0/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.30it/s v_num: 0.000      
                                                              val/auc: 0.550    
                                                              val/f1: 0.182     
                                                              val/precision:    
                                                              1.000 val/recall: 
                                                              0.100 val/mre: nan
                                                              train/auc: 0.973  
                                                              train/f1: 0.972   
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              0.946 train/mre:  
                                                              nan               
[2024-06-21 07:15:40,111][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/study_school/0.0/0>
[2024-06-21 07:15:40,111][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 07:15:40,114][HYDRA] 	#421 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=study_school data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=1
[rank: 0] Seed set to 1
[2024-06-21 07:15:40,296][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 07:15:40,298][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-21 07:15:40,299][train.py][INFO] - Instantiating callbacks...
[2024-06-21 07:15:40,299][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 07:15:40,301][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 07:15:40,302][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 07:15:40,302][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 07:15:40,305][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 07:15:40,305][train.py][INFO] - Instantiating loggers...
[2024-06-21 07:15:40,306][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 07:15:40,307][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 07:15:40,374][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/study_school/0.0/1/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.31it/s v_num: 0.000      
                                                              val/auc: 0.539    
                                                              val/f1: 0.400     
                                                              val/precision:    
                                                              0.600 val/recall: 
                                                              0.300 val/mre: nan
                                                              train/auc: 1.000  
                                                              train/f1: 1.000   
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              nan               
[2024-06-21 07:17:29,270][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/study_school/0.0/1>
[2024-06-21 07:17:29,270][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 07:17:29,273][HYDRA] 	#422 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=study_school data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=2
[rank: 0] Seed set to 2
[2024-06-21 07:17:29,459][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 07:17:29,460][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-21 07:17:29,462][train.py][INFO] - Instantiating callbacks...
[2024-06-21 07:17:29,462][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 07:17:29,464][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 07:17:29,464][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 07:17:29,465][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 07:17:29,467][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 07:17:29,468][train.py][INFO] - Instantiating loggers...
[2024-06-21 07:17:29,468][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 07:17:29,469][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 07:17:29,535][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/study_school/0.0/2/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.32it/s v_num: 0.000      
                                                              val/auc: 0.533    
                                                              val/f1: 0.471     
                                                              val/precision:    
                                                              0.571 val/recall: 
                                                              0.400 val/mre: nan
                                                              train/auc: 0.991  
                                                              train/f1: 0.991   
                                                              train/precision:  
                                                              0.982             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              nan               
[2024-06-21 07:19:18,660][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/study_school/0.0/2>
[2024-06-21 07:19:18,661][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 07:19:18,663][HYDRA] 	#423 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=study_school data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=3
[rank: 0] Seed set to 3
[2024-06-21 07:19:18,844][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 07:19:18,846][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-21 07:19:18,847][train.py][INFO] - Instantiating callbacks...
[2024-06-21 07:19:18,847][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 07:19:18,849][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 07:19:18,850][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 07:19:18,850][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 07:19:18,853][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 07:19:18,853][train.py][INFO] - Instantiating loggers...
[2024-06-21 07:19:18,853][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 07:19:18,854][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 07:19:18,919][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/study_school/0.0/3/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.30it/s v_num: 0.000      
                                                              val/auc: 0.550    
                                                              val/f1: 0.182     
                                                              val/precision:    
                                                              1.000 val/recall: 
                                                              0.100 val/mre: nan
                                                              train/auc: 0.982  
                                                              train/f1: 0.982   
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              0.964 train/mre:  
                                                              nan               
[2024-06-21 07:21:09,031][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/study_school/0.0/3>
[2024-06-21 07:21:09,031][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 07:21:09,033][HYDRA] 	#424 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=study_school data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=4
[rank: 0] Seed set to 4
[2024-06-21 07:21:09,226][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 07:21:09,227][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-21 07:21:09,229][train.py][INFO] - Instantiating callbacks...
[2024-06-21 07:21:09,229][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 07:21:09,231][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 07:21:09,231][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 07:21:09,232][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 07:21:09,234][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 07:21:09,235][train.py][INFO] - Instantiating loggers...
[2024-06-21 07:21:09,235][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 07:21:09,236][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 07:21:09,357][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/study_school/0.0/4/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.31it/s v_num: 0.000      
                                                              val/auc: 0.494    
                                                              val/f1: 0.167     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.100 val/mre: nan
                                                              train/auc: 0.973  
                                                              train/f1: 0.972   
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              0.946 train/mre:  
                                                              nan               
[2024-06-21 07:22:58,436][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/study_school/0.0/4>
[2024-06-21 07:22:58,436][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 07:22:58,439][HYDRA] 	#425 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=study_school data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=5
[rank: 0] Seed set to 5
[2024-06-21 07:22:58,622][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 07:22:58,624][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-21 07:22:58,625][train.py][INFO] - Instantiating callbacks...
[2024-06-21 07:22:58,625][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 07:22:58,627][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 07:22:58,628][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 07:22:58,628][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 07:22:58,631][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 07:22:58,631][train.py][INFO] - Instantiating loggers...
[2024-06-21 07:22:58,631][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 07:22:58,633][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 07:22:58,701][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/study_school/0.0/5/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.31it/s v_num: 0.000      
                                                              val/auc: 0.494    
                                                              val/f1: 0.167     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.100 val/mre: nan
                                                              train/auc: 0.982  
                                                              train/f1: 0.982   
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              0.964 train/mre:  
                                                              nan               
[2024-06-21 07:24:47,642][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/study_school/0.0/5>
[2024-06-21 07:24:47,643][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 07:24:47,645][HYDRA] 	#426 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=study_school data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=6
[rank: 0] Seed set to 6
[2024-06-21 07:24:47,827][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 07:24:47,828][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-21 07:24:47,830][train.py][INFO] - Instantiating callbacks...
[2024-06-21 07:24:47,830][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 07:24:47,832][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 07:24:47,832][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 07:24:47,833][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 07:24:47,835][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 07:24:47,835][train.py][INFO] - Instantiating loggers...
[2024-06-21 07:24:47,836][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 07:24:47,837][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 07:24:47,902][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/study_school/0.0/6/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.31it/s v_num: 0.000      
                                                              val/auc: 0.594    
                                                              val/f1: 0.429     
                                                              val/precision:    
                                                              0.750 val/recall: 
                                                              0.300 val/mre: nan
                                                              train/auc: 1.000  
                                                              train/f1: 1.000   
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              nan               
[2024-06-21 07:26:36,747][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/study_school/0.0/6>
[2024-06-21 07:26:36,747][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 07:26:36,750][HYDRA] 	#427 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=study_school data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=7
[rank: 0] Seed set to 7
[2024-06-21 07:26:36,932][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 07:26:36,933][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-21 07:26:36,935][train.py][INFO] - Instantiating callbacks...
[2024-06-21 07:26:36,935][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 07:26:36,937][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 07:26:36,937][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 07:26:36,938][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 07:26:36,940][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 07:26:36,941][train.py][INFO] - Instantiating loggers...
[2024-06-21 07:26:36,941][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 07:26:36,942][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 07:26:37,007][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/study_school/0.0/7/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.29it/s v_num: 0.000      
                                                              val/auc: 0.544    
                                                              val/f1: 0.308     
                                                              val/precision:    
                                                              0.667 val/recall: 
                                                              0.200 val/mre: nan
                                                              train/auc: 0.982  
                                                              train/f1: 0.982   
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              0.964 train/mre:  
                                                              nan               
[2024-06-21 07:28:26,661][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/study_school/0.0/7>
[2024-06-21 07:28:26,662][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 07:28:26,664][HYDRA] 	#428 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=study_school data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=8
[rank: 0] Seed set to 8
[2024-06-21 07:28:26,851][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 07:28:26,852][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-21 07:28:26,853][train.py][INFO] - Instantiating callbacks...
[2024-06-21 07:28:26,854][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 07:28:26,856][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 07:28:26,856][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 07:28:26,857][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 07:28:26,859][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 07:28:26,859][train.py][INFO] - Instantiating loggers...
[2024-06-21 07:28:26,860][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 07:28:26,861][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 07:28:26,926][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/study_school/0.0/8/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.30it/s v_num: 0.000      
                                                              val/auc: 0.494    
                                                              val/f1: 0.167     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.100 val/mre: nan
                                                              train/auc: 0.973  
                                                              train/f1: 0.973   
                                                              train/precision:  
                                                              0.982             
                                                              train/recall:     
                                                              0.964 train/mre:  
                                                              nan               
[2024-06-21 07:30:16,170][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/study_school/0.0/8>
[2024-06-21 07:30:16,171][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 07:30:16,173][HYDRA] 	#429 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=study_school data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=9
[rank: 0] Seed set to 9
[2024-06-21 07:30:16,360][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 07:30:16,362][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-21 07:30:16,363][train.py][INFO] - Instantiating callbacks...
[2024-06-21 07:30:16,363][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 07:30:16,365][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 07:30:16,366][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 07:30:16,366][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 07:30:16,369][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 07:30:16,370][train.py][INFO] - Instantiating loggers...
[2024-06-21 07:30:16,370][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 07:30:16,371][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 07:30:16,454][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/study_school/0.0/9/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.31it/s v_num: 0.000      
                                                              val/auc: 0.539    
                                                              val/f1: 0.400     
                                                              val/precision:    
                                                              0.600 val/recall: 
                                                              0.300 val/mre: nan
                                                              train/auc: 0.955  
                                                              train/f1: 0.954   
                                                              train/precision:  
                                                              0.981             
                                                              train/recall:     
                                                              0.929 train/mre:  
                                                              nan               
[2024-06-21 07:32:05,717][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/study_school/0.0/9>
[2024-06-21 07:32:05,718][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 07:32:05,720][HYDRA] 	#430 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=study_school data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=0
[2024-06-21 07:32:05,901][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 07:32:05,902][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-21 07:32:05,903][train.py][INFO] - Instantiating callbacks...
[2024-06-21 07:32:05,903][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 07:32:05,906][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 07:32:05,906][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 07:32:05,906][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 07:32:05,909][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 07:32:05,909][train.py][INFO] - Instantiating loggers...
[2024-06-21 07:32:05,909][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 07:32:05,911][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 07:32:05,977][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/study_school/0.05/0/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.29it/s v_num: 0.000      
                                                              val/auc: 0.550    
                                                              val/f1: 0.182     
                                                              val/precision:    
                                                              1.000 val/recall: 
                                                              0.100 val/mre:    
                                                              0.131 train/auc:  
                                                              0.982 train/f1:   
                                                              0.982             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              0.964 train/mre:  
                                                              0.130             
[2024-06-21 07:33:55,236][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/study_school/0.05/0>
[2024-06-21 07:33:55,237][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 07:33:55,239][HYDRA] 	#431 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=study_school data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=1
[rank: 0] Seed set to 1
[2024-06-21 07:33:55,422][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 07:33:55,423][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-21 07:33:55,424][train.py][INFO] - Instantiating callbacks...
[2024-06-21 07:33:55,425][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 07:33:55,427][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 07:33:55,427][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 07:33:55,428][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 07:33:55,433][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 07:33:55,433][train.py][INFO] - Instantiating loggers...
[2024-06-21 07:33:55,433][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 07:33:55,435][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 07:33:55,504][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/study_school/0.05/1/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.30it/s v_num: 0.000      
                                                              val/auc: 0.483    
                                                              val/f1: 0.375     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.300 val/mre:    
                                                              0.131 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.132             
[2024-06-21 07:35:44,879][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/study_school/0.05/1>
[2024-06-21 07:35:44,879][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 07:35:44,882][HYDRA] 	#432 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=study_school data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=2
[rank: 0] Seed set to 2
[2024-06-21 07:35:45,065][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 07:35:45,066][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-21 07:35:45,068][train.py][INFO] - Instantiating callbacks...
[2024-06-21 07:35:45,068][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 07:35:45,070][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 07:35:45,070][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 07:35:45,071][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 07:35:45,073][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 07:35:45,074][train.py][INFO] - Instantiating loggers...
[2024-06-21 07:35:45,074][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 07:35:45,075][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 07:35:45,140][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/study_school/0.05/2/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.29it/s v_num: 0.000      
                                                              val/auc: 0.483    
                                                              val/f1: 0.375     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.300 val/mre:    
                                                              0.128 train/auc:  
                                                              0.991 train/f1:   
                                                              0.991             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              0.982 train/mre:  
                                                              0.126             
[2024-06-21 07:37:33,337][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/study_school/0.05/2>
[2024-06-21 07:37:33,338][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 07:37:33,340][HYDRA] 	#433 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=study_school data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=3
[rank: 0] Seed set to 3
[2024-06-21 07:37:33,522][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 07:37:33,524][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-21 07:37:33,525][train.py][INFO] - Instantiating callbacks...
[2024-06-21 07:37:33,525][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 07:37:33,527][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 07:37:33,528][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 07:37:33,528][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 07:37:33,531][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 07:37:33,531][train.py][INFO] - Instantiating loggers...
[2024-06-21 07:37:33,532][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 07:37:33,533][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 07:37:33,599][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/study_school/0.05/3/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.32it/s v_num: 0.000      
                                                              val/auc: 0.594    
                                                              val/f1: 0.429     
                                                              val/precision:    
                                                              0.750 val/recall: 
                                                              0.300 val/mre:    
                                                              0.124 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.124             
[2024-06-21 07:39:22,580][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/study_school/0.05/3>
[2024-06-21 07:39:22,581][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 07:39:22,583][HYDRA] 	#434 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=study_school data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=4
[rank: 0] Seed set to 4
[2024-06-21 07:39:22,775][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 07:39:22,777][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-21 07:39:22,778][train.py][INFO] - Instantiating callbacks...
[2024-06-21 07:39:22,778][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 07:39:22,780][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 07:39:22,781][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 07:39:22,781][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 07:39:22,784][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 07:39:22,784][train.py][INFO] - Instantiating loggers...
[2024-06-21 07:39:22,784][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 07:39:22,786][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 07:39:22,851][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/study_school/0.05/4/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.29it/s v_num: 0.000      
                                                              val/auc: 0.494    
                                                              val/f1: 0.167     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.100 val/mre:    
                                                              0.129 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.127             
[2024-06-21 07:41:12,631][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/study_school/0.05/4>
[2024-06-21 07:41:12,631][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 07:41:12,634][HYDRA] 	#435 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=study_school data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=5
[rank: 0] Seed set to 5
[2024-06-21 07:41:12,830][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 07:41:12,832][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-21 07:41:12,833][train.py][INFO] - Instantiating callbacks...
[2024-06-21 07:41:12,834][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 07:41:12,836][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 07:41:12,836][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 07:41:12,836][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 07:41:12,839][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 07:41:12,840][train.py][INFO] - Instantiating loggers...
[2024-06-21 07:41:12,840][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 07:41:12,841][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 07:41:12,906][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/study_school/0.05/5/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.30it/s v_num: 0.000      
                                                              val/auc: 0.644    
                                                              val/f1: 0.533     
                                                              val/precision:    
                                                              0.800 val/recall: 
                                                              0.400 val/mre:    
                                                              0.131 train/auc:  
                                                              0.991 train/f1:   
                                                              0.991             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              0.982 train/mre:  
                                                              0.128             
[2024-06-21 07:43:01,711][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/study_school/0.05/5>
[2024-06-21 07:43:01,712][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 07:43:01,714][HYDRA] 	#436 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=study_school data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=6
[rank: 0] Seed set to 6
[2024-06-21 07:43:01,904][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 07:43:01,906][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-21 07:43:01,907][train.py][INFO] - Instantiating callbacks...
[2024-06-21 07:43:01,907][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 07:43:01,909][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 07:43:01,910][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 07:43:01,910][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 07:43:01,931][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 07:43:01,932][train.py][INFO] - Instantiating loggers...
[2024-06-21 07:43:01,932][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 07:43:01,933][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 07:43:02,020][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/study_school/0.05/6/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.30it/s v_num: 0.000      
                                                              val/auc: 0.594    
                                                              val/f1: 0.429     
                                                              val/precision:    
                                                              0.750 val/recall: 
                                                              0.300 val/mre:    
                                                              0.125 train/auc:  
                                                              0.991 train/f1:   
                                                              0.991             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              0.982 train/mre:  
                                                              0.126             
[2024-06-21 07:44:50,507][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/study_school/0.05/6>
[2024-06-21 07:44:50,508][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 07:44:50,512][HYDRA] 	#437 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=study_school data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=7
[rank: 0] Seed set to 7
[2024-06-21 07:44:50,699][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 07:44:50,700][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-21 07:44:50,702][train.py][INFO] - Instantiating callbacks...
[2024-06-21 07:44:50,702][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 07:44:50,704][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 07:44:50,704][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 07:44:50,705][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 07:44:50,707][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 07:44:50,708][train.py][INFO] - Instantiating loggers...
[2024-06-21 07:44:50,708][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 07:44:50,709][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 07:44:50,776][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/study_school/0.05/7/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.31it/s v_num: 0.000      
                                                              val/auc: 0.794    
                                                              val/f1: 0.778     
                                                              val/precision:    
                                                              0.875 val/recall: 
                                                              0.700 val/mre:    
                                                              0.130 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.129             
[2024-06-21 07:46:39,408][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/study_school/0.05/7>
[2024-06-21 07:46:39,408][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 07:46:39,410][HYDRA] 	#438 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=study_school data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=8
[rank: 0] Seed set to 8
[2024-06-21 07:46:40,207][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 07:46:40,208][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-21 07:46:40,210][train.py][INFO] - Instantiating callbacks...
[2024-06-21 07:46:40,210][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 07:46:40,214][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 07:46:40,215][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 07:46:40,215][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 07:46:40,218][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 07:46:40,218][train.py][INFO] - Instantiating loggers...
[2024-06-21 07:46:40,218][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 07:46:40,219][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 07:46:40,454][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/study_school/0.05/8/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.32it/s v_num: 0.000      
                                                              val/auc: 0.483    
                                                              val/f1: 0.375     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.300 val/mre:    
                                                              0.130 train/auc:  
                                                              0.991 train/f1:   
                                                              0.991             
                                                              train/precision:  
                                                              0.982             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.130             
[2024-06-21 07:48:29,394][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/study_school/0.05/8>
[2024-06-21 07:48:29,395][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 07:48:29,397][HYDRA] 	#439 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=study_school data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=9
[rank: 0] Seed set to 9
[2024-06-21 07:48:29,580][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 07:48:29,582][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-21 07:48:29,583][train.py][INFO] - Instantiating callbacks...
[2024-06-21 07:48:29,583][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 07:48:29,585][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 07:48:29,586][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 07:48:29,586][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 07:48:29,589][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 07:48:29,589][train.py][INFO] - Instantiating loggers...
[2024-06-21 07:48:29,589][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 07:48:29,591][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 07:48:29,656][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/study_school/0.05/9/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.32it/s v_num: 0.000      
                                                              val/auc: 0.594    
                                                              val/f1: 0.429     
                                                              val/precision:    
                                                              0.750 val/recall: 
                                                              0.300 val/mre:    
                                                              0.126 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.126             
[2024-06-21 07:50:17,961][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/study_school/0.05/9>
[2024-06-21 07:50:17,962][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 07:50:17,965][HYDRA] 	#440 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=study_school data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=0
[2024-06-21 07:50:18,146][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 07:50:18,147][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-21 07:50:18,149][train.py][INFO] - Instantiating callbacks...
[2024-06-21 07:50:18,149][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 07:50:18,151][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 07:50:18,151][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 07:50:18,152][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 07:50:18,154][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 07:50:18,154][train.py][INFO] - Instantiating loggers...
[2024-06-21 07:50:18,155][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 07:50:18,156][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 07:50:18,221][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/study_school/0.1/0/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.34it/s v_num: 0.000      
                                                              val/auc: 0.594    
                                                              val/f1: 0.429     
                                                              val/precision:    
                                                              0.750 val/recall: 
                                                              0.300 val/mre:    
                                                              0.136 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.136             
[2024-06-21 07:52:07,284][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/study_school/0.1/0>
[2024-06-21 07:52:07,285][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 07:52:07,287][HYDRA] 	#441 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=study_school data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=1
[rank: 0] Seed set to 1
[2024-06-21 07:52:07,470][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 07:52:07,471][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-21 07:52:07,472][train.py][INFO] - Instantiating callbacks...
[2024-06-21 07:52:07,473][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 07:52:07,475][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 07:52:07,475][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 07:52:07,475][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 07:52:07,494][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 07:52:07,494][train.py][INFO] - Instantiating loggers...
[2024-06-21 07:52:07,495][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 07:52:07,496][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 07:52:07,587][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/study_school/0.1/1/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.30it/s v_num: 0.000      
                                                              val/auc: 0.489    
                                                              val/f1: 0.286     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.200 val/mre:    
                                                              0.138 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.139             
[2024-06-21 07:53:57,438][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/study_school/0.1/1>
[2024-06-21 07:53:57,438][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 07:53:57,441][HYDRA] 	#442 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=study_school data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=2
[rank: 0] Seed set to 2
[2024-06-21 07:53:57,627][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 07:53:57,628][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-21 07:53:57,629][train.py][INFO] - Instantiating callbacks...
[2024-06-21 07:53:57,629][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 07:53:57,631][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 07:53:57,632][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 07:53:57,632][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 07:53:57,635][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 07:53:57,635][train.py][INFO] - Instantiating loggers...
[2024-06-21 07:53:57,635][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 07:53:57,637][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 07:53:57,701][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/study_school/0.1/2/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.30it/s v_num: 0.000      
                                                              val/auc: 0.444    
                                                              val/f1: 0.000     
                                                              val/precision:    
                                                              0.000 val/recall: 
                                                              0.000 val/mre:    
                                                              0.137 train/auc:  
                                                              0.982 train/f1:   
                                                              0.982             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              0.964 train/mre:  
                                                              0.134             
[2024-06-21 07:55:47,288][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/study_school/0.1/2>
[2024-06-21 07:55:47,289][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 07:55:47,294][HYDRA] 	#443 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=study_school data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=3
[rank: 0] Seed set to 3
[2024-06-21 07:55:47,536][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 07:55:47,538][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-21 07:55:47,539][train.py][INFO] - Instantiating callbacks...
[2024-06-21 07:55:47,540][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 07:55:47,542][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 07:55:47,542][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 07:55:47,543][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 07:55:47,545][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 07:55:47,546][train.py][INFO] - Instantiating loggers...
[2024-06-21 07:55:47,546][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 07:55:47,547][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 07:55:47,668][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/study_school/0.1/3/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.31it/s v_num: 0.000      
                                                              val/auc: 0.550    
                                                              val/f1: 0.182     
                                                              val/precision:    
                                                              1.000 val/recall: 
                                                              0.100 val/mre:    
                                                              0.132 train/auc:  
                                                              0.955 train/f1:   
                                                              0.957             
                                                              train/precision:  
                                                              0.918             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.133             
[2024-06-21 07:57:36,876][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/study_school/0.1/3>
[2024-06-21 07:57:36,877][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 07:57:36,879][HYDRA] 	#444 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=study_school data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=4
[rank: 0] Seed set to 4
[2024-06-21 07:57:37,062][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 07:57:37,063][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-21 07:57:37,065][train.py][INFO] - Instantiating callbacks...
[2024-06-21 07:57:37,065][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 07:57:37,067][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 07:57:37,067][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 07:57:37,068][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 07:57:37,070][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 07:57:37,071][train.py][INFO] - Instantiating loggers...
[2024-06-21 07:57:37,071][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 07:57:37,072][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 07:57:37,139][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/study_school/0.1/4/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.30it/s v_num: 0.000      
                                                              val/auc: 0.489    
                                                              val/f1: 0.286     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.200 val/mre:    
                                                              0.136 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.134             
[2024-06-21 07:59:26,083][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/study_school/0.1/4>
[2024-06-21 07:59:26,084][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 07:59:26,086][HYDRA] 	#445 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=study_school data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=5
[rank: 0] Seed set to 5
[2024-06-21 07:59:26,268][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 07:59:26,269][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-21 07:59:26,270][train.py][INFO] - Instantiating callbacks...
[2024-06-21 07:59:26,271][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 07:59:26,273][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 07:59:26,273][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 07:59:26,274][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 07:59:26,276][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 07:59:26,277][train.py][INFO] - Instantiating loggers...
[2024-06-21 07:59:26,277][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 07:59:26,278][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 07:59:26,344][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/study_school/0.1/5/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.30it/s v_num: 0.000      
                                                              val/auc: 0.650    
                                                              val/f1: 0.462     
                                                              val/precision:    
                                                              1.000 val/recall: 
                                                              0.300 val/mre:    
                                                              0.136 train/auc:  
                                                              0.991 train/f1:   
                                                              0.991             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              0.982 train/mre:  
                                                              0.136             
[2024-06-21 08:01:16,130][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/study_school/0.1/5>
[2024-06-21 08:01:16,130][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 08:01:16,133][HYDRA] 	#446 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=study_school data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=6
[rank: 0] Seed set to 6
[2024-06-21 08:01:16,317][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 08:01:16,319][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-21 08:01:16,320][train.py][INFO] - Instantiating callbacks...
[2024-06-21 08:01:16,320][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 08:01:16,322][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 08:01:16,323][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 08:01:16,323][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 08:01:16,346][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 08:01:16,347][train.py][INFO] - Instantiating loggers...
[2024-06-21 08:01:16,347][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 08:01:16,348][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 08:01:16,430][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/study_school/0.1/6/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.31it/s v_num: 0.000      
                                                              val/auc: 0.583    
                                                              val/f1: 0.556     
                                                              val/precision:    
                                                              0.625 val/recall: 
                                                              0.500 val/mre:    
                                                              0.133 train/auc:  
                                                              0.991 train/f1:   
                                                              0.991             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              0.982 train/mre:  
                                                              0.133             
[2024-06-21 08:03:05,353][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/study_school/0.1/6>
[2024-06-21 08:03:05,354][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 08:03:05,357][HYDRA] 	#447 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=study_school data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=7
[rank: 0] Seed set to 7
[2024-06-21 08:03:05,548][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 08:03:05,550][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-21 08:03:05,551][train.py][INFO] - Instantiating callbacks...
[2024-06-21 08:03:05,551][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 08:03:05,553][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 08:03:05,554][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 08:03:05,554][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 08:03:05,557][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 08:03:05,558][train.py][INFO] - Instantiating loggers...
[2024-06-21 08:03:05,558][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 08:03:05,559][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 08:03:05,629][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/study_school/0.1/7/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.32it/s v_num: 0.000      
                                                              val/auc: 0.650    
                                                              val/f1: 0.462     
                                                              val/precision:    
                                                              1.000 val/recall: 
                                                              0.300 val/mre:    
                                                              0.139 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.138             
[2024-06-21 08:04:53,705][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/study_school/0.1/7>
[2024-06-21 08:04:53,705][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 08:04:53,707][HYDRA] 	#448 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=study_school data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=8
[rank: 0] Seed set to 8
[2024-06-21 08:04:53,891][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 08:04:53,893][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-21 08:04:53,894][train.py][INFO] - Instantiating callbacks...
[2024-06-21 08:04:53,894][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 08:04:53,896][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 08:04:53,897][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 08:04:53,897][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 08:04:53,900][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 08:04:53,900][train.py][INFO] - Instantiating loggers...
[2024-06-21 08:04:53,901][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 08:04:53,902][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 08:04:53,966][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/study_school/0.1/8/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.33it/s v_num: 0.000      
                                                              val/auc: 0.378    
                                                              val/f1: 0.250     
                                                              val/precision:    
                                                              0.333 val/recall: 
                                                              0.200 val/mre:    
                                                              0.141 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.138             
[2024-06-21 08:06:41,617][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/study_school/0.1/8>
[2024-06-21 08:06:41,617][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 08:06:41,620][HYDRA] 	#449 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=study_school data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=9
[rank: 0] Seed set to 9
[2024-06-21 08:06:41,805][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 08:06:41,806][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-21 08:06:41,807][train.py][INFO] - Instantiating callbacks...
[2024-06-21 08:06:41,808][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 08:06:41,810][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 08:06:41,810][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 08:06:41,811][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 08:06:41,813][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 08:06:41,814][train.py][INFO] - Instantiating loggers...
[2024-06-21 08:06:41,814][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 08:06:41,815][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 08:06:41,879][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/study_school/0.1/9/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.30it/s v_num: 0.000      
                                                              val/auc: 0.583    
                                                              val/f1: 0.556     
                                                              val/precision:    
                                                              0.625 val/recall: 
                                                              0.500 val/mre:    
                                                              0.135 train/auc:  
                                                              0.982 train/f1:   
                                                              0.982             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              0.964 train/mre:  
                                                              0.133             
[2024-06-21 08:08:31,162][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/study_school/0.1/9>
[2024-06-21 08:08:31,163][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 08:08:31,170][HYDRA] 	#450 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=study_school data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=0
[2024-06-21 08:08:31,402][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 08:08:31,403][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-21 08:08:31,404][train.py][INFO] - Instantiating callbacks...
[2024-06-21 08:08:31,405][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 08:08:31,407][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 08:08:31,407][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 08:08:31,408][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 08:08:31,410][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 08:08:31,411][train.py][INFO] - Instantiating loggers...
[2024-06-21 08:08:31,411][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 08:08:31,412][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 08:08:31,510][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/study_school/0.15/0/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.31it/s v_num: 0.000      
                                                              val/auc: 0.594    
                                                              val/f1: 0.429     
                                                              val/precision:    
                                                              0.750 val/recall: 
                                                              0.300 val/mre:    
                                                              0.141 train/auc:  
                                                              0.991 train/f1:   
                                                              0.991             
                                                              train/precision:  
                                                              0.982             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.141             
[2024-06-21 08:10:20,830][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/study_school/0.15/0>
[2024-06-21 08:10:20,831][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 08:10:20,833][HYDRA] 	#451 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=study_school data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=1
[rank: 0] Seed set to 1
[2024-06-21 08:10:21,022][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 08:10:21,023][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-21 08:10:21,025][train.py][INFO] - Instantiating callbacks...
[2024-06-21 08:10:21,025][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 08:10:21,027][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 08:10:21,027][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 08:10:21,028][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 08:10:21,031][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 08:10:21,031][train.py][INFO] - Instantiating loggers...
[2024-06-21 08:10:21,031][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 08:10:21,033][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 08:10:21,133][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/study_school/0.15/1/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.31it/s v_num: 0.000      
                                                              val/auc: 0.489    
                                                              val/f1: 0.286     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.200 val/mre:    
                                                              0.146 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.143             
[2024-06-21 08:12:09,692][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/study_school/0.15/1>
[2024-06-21 08:12:09,693][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 08:12:09,695][HYDRA] 	#452 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=study_school data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=2
[rank: 0] Seed set to 2
[2024-06-21 08:12:09,881][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 08:12:09,883][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-21 08:12:09,884][train.py][INFO] - Instantiating callbacks...
[2024-06-21 08:12:09,884][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 08:12:09,886][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 08:12:09,887][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 08:12:09,887][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 08:12:09,890][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 08:12:09,891][train.py][INFO] - Instantiating loggers...
[2024-06-21 08:12:09,891][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 08:12:09,892][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 08:12:09,957][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/study_school/0.15/2/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.30it/s v_num: 0.000      
                                                              val/auc: 0.383    
                                                              val/f1: 0.143     
                                                              val/precision:    
                                                              0.250 val/recall: 
                                                              0.100 val/mre:    
                                                              0.143 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.142             
[2024-06-21 08:13:59,427][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/study_school/0.15/2>
[2024-06-21 08:13:59,428][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 08:13:59,430][HYDRA] 	#453 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=study_school data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=3
[rank: 0] Seed set to 3
[2024-06-21 08:13:59,611][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 08:13:59,613][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-21 08:13:59,614][train.py][INFO] - Instantiating callbacks...
[2024-06-21 08:13:59,614][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 08:13:59,616][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 08:13:59,617][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 08:13:59,617][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 08:13:59,620][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 08:13:59,620][train.py][INFO] - Instantiating loggers...
[2024-06-21 08:13:59,620][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 08:13:59,622][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 08:13:59,688][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/study_school/0.15/3/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.31it/s v_num: 0.000      
                                                              val/auc: 0.589    
                                                              val/f1: 0.500     
                                                              val/precision:    
                                                              0.667 val/recall: 
                                                              0.400 val/mre:    
                                                              0.138 train/auc:  
                                                              0.991 train/f1:   
                                                              0.991             
                                                              train/precision:  
                                                              0.982             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.139             
[2024-06-21 08:15:49,051][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/study_school/0.15/3>
[2024-06-21 08:15:49,052][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 08:15:49,054][HYDRA] 	#454 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=study_school data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=4
[rank: 0] Seed set to 4
[2024-06-21 08:15:49,236][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 08:15:49,238][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-21 08:15:49,239][train.py][INFO] - Instantiating callbacks...
[2024-06-21 08:15:49,239][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 08:15:49,241][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 08:15:49,242][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 08:15:49,242][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 08:15:49,245][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 08:15:49,245][train.py][INFO] - Instantiating loggers...
[2024-06-21 08:15:49,246][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 08:15:49,247][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 08:15:49,326][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/study_school/0.15/4/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.32it/s v_num: 0.000      
                                                              val/auc: 0.539    
                                                              val/f1: 0.400     
                                                              val/precision:    
                                                              0.600 val/recall: 
                                                              0.300 val/mre:    
                                                              0.145 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.142             
[2024-06-21 08:17:38,057][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/study_school/0.15/4>
[2024-06-21 08:17:38,058][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 08:17:38,061][HYDRA] 	#455 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=study_school data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=5
[rank: 0] Seed set to 5
[2024-06-21 08:17:38,256][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 08:17:38,258][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-21 08:17:38,259][train.py][INFO] - Instantiating callbacks...
[2024-06-21 08:17:38,259][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 08:17:38,261][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 08:17:38,262][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 08:17:38,262][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 08:17:38,265][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 08:17:38,265][train.py][INFO] - Instantiating loggers...
[2024-06-21 08:17:38,266][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 08:17:38,267][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 08:17:38,334][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/study_school/0.15/5/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.29it/s v_num: 0.000      
                                                              val/auc: 0.544    
                                                              val/f1: 0.308     
                                                              val/precision:    
                                                              0.667 val/recall: 
                                                              0.200 val/mre:    
                                                              0.143 train/auc:  
                                                              0.991 train/f1:   
                                                              0.991             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              0.982 train/mre:  
                                                              0.142             
[2024-06-21 08:19:26,831][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/study_school/0.15/5>
[2024-06-21 08:19:26,832][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 08:19:26,834][HYDRA] 	#456 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=study_school data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=6
[rank: 0] Seed set to 6
[2024-06-21 08:19:27,016][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 08:19:27,018][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-21 08:19:27,019][train.py][INFO] - Instantiating callbacks...
[2024-06-21 08:19:27,019][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 08:19:27,021][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 08:19:27,022][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 08:19:27,022][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 08:19:27,025][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 08:19:27,025][train.py][INFO] - Instantiating loggers...
[2024-06-21 08:19:27,025][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 08:19:27,026][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 08:19:27,092][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/study_school/0.15/6/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.30it/s v_num: 0.000      
                                                              val/auc: 0.739    
                                                              val/f1: 0.737     
                                                              val/precision:    
                                                              0.778 val/recall: 
                                                              0.700 val/mre:    
                                                              0.139 train/auc:  
                                                              0.991 train/f1:   
                                                              0.991             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              0.982 train/mre:  
                                                              0.139             
[2024-06-21 08:21:17,156][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/study_school/0.15/6>
[2024-06-21 08:21:17,157][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 08:21:17,159][HYDRA] 	#457 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=study_school data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=7
[rank: 0] Seed set to 7
[2024-06-21 08:21:17,341][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 08:21:17,342][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-21 08:21:17,343][train.py][INFO] - Instantiating callbacks...
[2024-06-21 08:21:17,344][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 08:21:17,346][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 08:21:17,346][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 08:21:17,347][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 08:21:17,349][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 08:21:17,350][train.py][INFO] - Instantiating loggers...
[2024-06-21 08:21:17,350][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 08:21:17,351][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 08:21:17,418][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/study_school/0.15/7/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.31it/s v_num: 0.000      
                                                              val/auc: 0.644    
                                                              val/f1: 0.533     
                                                              val/precision:    
                                                              0.800 val/recall: 
                                                              0.400 val/mre:    
                                                              0.147 train/auc:  
                                                              0.973 train/f1:   
                                                              0.972             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              0.946 train/mre:  
                                                              0.143             
[2024-06-21 08:23:06,656][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/study_school/0.15/7>
[2024-06-21 08:23:06,656][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 08:23:06,659][HYDRA] 	#458 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=study_school data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=8
[rank: 0] Seed set to 8
[2024-06-21 08:23:06,844][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 08:23:06,845][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-21 08:23:06,847][train.py][INFO] - Instantiating callbacks...
[2024-06-21 08:23:06,847][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 08:23:06,849][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 08:23:06,849][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 08:23:06,850][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 08:23:06,852][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 08:23:06,853][train.py][INFO] - Instantiating loggers...
[2024-06-21 08:23:06,853][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 08:23:06,854][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 08:23:06,920][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/study_school/0.15/8/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.29it/s v_num: 0.000      
                                                              val/auc: 0.322    
                                                              val/f1: 0.235     
                                                              val/precision:    
                                                              0.286 val/recall: 
                                                              0.200 val/mre:    
                                                              0.143 train/auc:  
                                                              0.973 train/f1:   
                                                              0.972             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              0.946 train/mre:  
                                                              0.142             
[2024-06-21 08:24:56,269][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/study_school/0.15/8>
[2024-06-21 08:24:56,270][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 08:24:56,275][HYDRA] 	#459 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=study_school data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=9
[rank: 0] Seed set to 9
[2024-06-21 08:24:56,458][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 08:24:56,459][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-21 08:24:56,461][train.py][INFO] - Instantiating callbacks...
[2024-06-21 08:24:56,461][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 08:24:56,463][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 08:24:56,463][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 08:24:56,464][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 08:24:56,467][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 08:24:56,467][train.py][INFO] - Instantiating loggers...
[2024-06-21 08:24:56,467][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 08:24:56,468][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 08:24:56,536][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/study_school/0.15/9/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.32it/s v_num: 0.000      
                                                              val/auc: 0.639    
                                                              val/f1: 0.588     
                                                              val/precision:    
                                                              0.714 val/recall: 
                                                              0.500 val/mre:    
                                                              0.138 train/auc:  
                                                              0.982 train/f1:   
                                                              0.982             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              0.964 train/mre:  
                                                              0.139             
[2024-06-21 08:26:45,301][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/study_school/0.15/9>
[2024-06-21 08:26:45,302][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 08:26:45,305][HYDRA] 	#460 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=study_school data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=0
[2024-06-21 08:26:45,484][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 08:26:45,485][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-21 08:26:45,486][train.py][INFO] - Instantiating callbacks...
[2024-06-21 08:26:45,487][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 08:26:45,489][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 08:26:45,489][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 08:26:45,489][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 08:26:45,492][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 08:26:45,492][train.py][INFO] - Instantiating loggers...
[2024-06-21 08:26:45,493][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 08:26:45,494][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 08:26:45,557][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/study_school/0.2/0/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.28it/s v_num: 0.000      
                                                              val/auc: 0.528    
                                                              val/f1: 0.526     
                                                              val/precision:    
                                                              0.556 val/recall: 
                                                              0.500 val/mre:    
                                                              0.148 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.148             
[2024-06-21 08:28:35,952][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/study_school/0.2/0>
[2024-06-21 08:28:35,953][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 08:28:35,955][HYDRA] 	#461 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=study_school data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=1
[rank: 0] Seed set to 1
[2024-06-21 08:28:36,140][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 08:28:36,142][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-21 08:28:36,143][train.py][INFO] - Instantiating callbacks...
[2024-06-21 08:28:36,143][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 08:28:36,145][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 08:28:36,146][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 08:28:36,146][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 08:28:36,149][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 08:28:36,149][train.py][INFO] - Instantiating loggers...
[2024-06-21 08:28:36,149][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 08:28:36,151][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 08:28:36,221][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/study_school/0.2/1/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.29it/s v_num: 0.000      
                                                              val/auc: 0.439    
                                                              val/f1: 0.154     
                                                              val/precision:    
                                                              0.333 val/recall: 
                                                              0.100 val/mre:    
                                                              0.152 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.150             
[2024-06-21 08:30:25,233][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/study_school/0.2/1>
[2024-06-21 08:30:25,233][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 08:30:25,236][HYDRA] 	#462 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=study_school data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=2
[rank: 0] Seed set to 2
[2024-06-21 08:30:25,417][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 08:30:25,418][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-21 08:30:25,420][train.py][INFO] - Instantiating callbacks...
[2024-06-21 08:30:25,420][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 08:30:25,422][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 08:30:25,422][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 08:30:25,423][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 08:30:25,425][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 08:30:25,426][train.py][INFO] - Instantiating loggers...
[2024-06-21 08:30:25,426][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 08:30:25,427][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 08:30:25,492][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/study_school/0.2/2/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.32it/s v_num: 0.000      
                                                              val/auc: 0.444    
                                                              val/f1: 0.000     
                                                              val/precision:    
                                                              0.000 val/recall: 
                                                              0.000 val/mre:    
                                                              0.149 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.146             
[2024-06-21 08:32:14,350][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/study_school/0.2/2>
[2024-06-21 08:32:14,351][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 08:32:14,353][HYDRA] 	#463 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=study_school data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=3
[rank: 0] Seed set to 3
[2024-06-21 08:32:14,534][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 08:32:14,535][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-21 08:32:14,537][train.py][INFO] - Instantiating callbacks...
[2024-06-21 08:32:14,537][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 08:32:14,539][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 08:32:14,539][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 08:32:14,540][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 08:32:14,542][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 08:32:14,542][train.py][INFO] - Instantiating loggers...
[2024-06-21 08:32:14,543][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 08:32:14,544][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 08:32:14,618][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/study_school/0.2/3/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.27it/s v_num: 0.000      
                                                              val/auc: 0.533    
                                                              val/f1: 0.471     
                                                              val/precision:    
                                                              0.571 val/recall: 
                                                              0.400 val/mre:    
                                                              0.140 train/auc:  
                                                              0.982 train/f1:   
                                                              0.982             
                                                              train/precision:  
                                                              0.966             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.139             
[2024-06-21 08:34:03,718][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/study_school/0.2/3>
[2024-06-21 08:34:03,718][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 08:34:03,721][HYDRA] 	#464 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=study_school data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=4
[rank: 0] Seed set to 4
[2024-06-21 08:34:03,907][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 08:34:03,908][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-21 08:34:03,909][train.py][INFO] - Instantiating callbacks...
[2024-06-21 08:34:03,910][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 08:34:03,912][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 08:34:03,912][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 08:34:03,913][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 08:34:03,915][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 08:34:03,916][train.py][INFO] - Instantiating loggers...
[2024-06-21 08:34:03,916][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 08:34:03,917][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 08:34:03,985][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/study_school/0.2/4/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.31it/s v_num: 0.000      
                                                              val/auc: 0.544    
                                                              val/f1: 0.308     
                                                              val/precision:    
                                                              0.667 val/recall: 
                                                              0.200 val/mre:    
                                                              0.151 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.146             
[2024-06-21 08:35:53,076][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/study_school/0.2/4>
[2024-06-21 08:35:53,077][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 08:35:53,080][HYDRA] 	#465 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=study_school data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=5
[rank: 0] Seed set to 5
[2024-06-21 08:35:53,265][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 08:35:53,267][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-21 08:35:53,268][train.py][INFO] - Instantiating callbacks...
[2024-06-21 08:35:53,268][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 08:35:53,270][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 08:35:53,271][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 08:35:53,271][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 08:35:53,274][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 08:35:53,274][train.py][INFO] - Instantiating loggers...
[2024-06-21 08:35:53,274][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 08:35:53,276][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 08:35:53,340][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/study_school/0.2/5/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.31it/s v_num: 0.000      
                                                              val/auc: 0.550    
                                                              val/f1: 0.182     
                                                              val/precision:    
                                                              1.000 val/recall: 
                                                              0.100 val/mre:    
                                                              0.145 train/auc:  
                                                              0.991 train/f1:   
                                                              0.991             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              0.982 train/mre:  
                                                              0.144             
[2024-06-21 08:37:41,788][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/study_school/0.2/5>
[2024-06-21 08:37:41,789][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 08:37:41,791][HYDRA] 	#466 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=study_school data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=6
[rank: 0] Seed set to 6
[2024-06-21 08:37:41,991][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 08:37:41,993][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-21 08:37:41,994][train.py][INFO] - Instantiating callbacks...
[2024-06-21 08:37:41,994][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 08:37:41,996][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 08:37:41,997][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 08:37:41,997][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 08:37:42,001][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 08:37:42,001][train.py][INFO] - Instantiating loggers...
[2024-06-21 08:37:42,001][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 08:37:42,003][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 08:37:42,084][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/study_school/0.2/6/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.32it/s v_num: 0.000      
                                                              val/auc: 0.689    
                                                              val/f1: 0.667     
                                                              val/precision:    
                                                              0.750 val/recall: 
                                                              0.600 val/mre:    
                                                              0.143 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.143             
[2024-06-21 08:39:30,712][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/study_school/0.2/6>
[2024-06-21 08:39:30,712][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 08:39:30,715][HYDRA] 	#467 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=study_school data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=7
[rank: 0] Seed set to 7
[2024-06-21 08:39:30,897][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 08:39:30,898][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-21 08:39:30,899][train.py][INFO] - Instantiating callbacks...
[2024-06-21 08:39:30,900][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 08:39:30,902][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 08:39:30,902][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 08:39:30,902][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 08:39:30,906][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 08:39:30,907][train.py][INFO] - Instantiating loggers...
[2024-06-21 08:39:30,907][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 08:39:30,908][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 08:39:30,973][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/study_school/0.2/7/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.31it/s v_num: 0.000      
                                                              val/auc: 0.550    
                                                              val/f1: 0.182     
                                                              val/precision:    
                                                              1.000 val/recall: 
                                                              0.100 val/mre:    
                                                              0.150 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.146             
[2024-06-21 08:41:20,614][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/study_school/0.2/7>
[2024-06-21 08:41:20,614][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 08:41:20,617][HYDRA] 	#468 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=study_school data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=8
[rank: 0] Seed set to 8
[2024-06-21 08:41:20,802][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 08:41:20,803][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-21 08:41:20,804][train.py][INFO] - Instantiating callbacks...
[2024-06-21 08:41:20,805][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 08:41:20,807][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 08:41:20,807][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 08:41:20,807][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 08:41:20,810][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 08:41:20,811][train.py][INFO] - Instantiating loggers...
[2024-06-21 08:41:20,811][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 08:41:20,812][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 08:41:20,880][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/study_school/0.2/8/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.32it/s v_num: 0.000      
                                                              val/auc: 0.472    
                                                              val/f1: 0.500     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.500 val/mre:    
                                                              0.147 train/auc:  
                                                              0.982 train/f1:   
                                                              0.982             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              0.964 train/mre:  
                                                              0.146             
[2024-06-21 08:43:09,005][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/study_school/0.2/8>
[2024-06-21 08:43:09,006][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 08:43:09,008][HYDRA] 	#469 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=study_school data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=9
[rank: 0] Seed set to 9
[2024-06-21 08:43:09,191][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 08:43:09,192][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-21 08:43:09,194][train.py][INFO] - Instantiating callbacks...
[2024-06-21 08:43:09,194][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 08:43:09,196][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 08:43:09,196][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 08:43:09,197][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 08:43:09,199][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 08:43:09,200][train.py][INFO] - Instantiating loggers...
[2024-06-21 08:43:09,200][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 08:43:09,201][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 08:43:09,267][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/study_school/0.2/9/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.32it/s v_num: 0.000      
                                                              val/auc: 0.589    
                                                              val/f1: 0.500     
                                                              val/precision:    
                                                              0.667 val/recall: 
                                                              0.400 val/mre:    
                                                              0.143 train/auc:  
                                                              0.982 train/f1:   
                                                              0.982             
                                                              train/precision:  
                                                              0.982             
                                                              train/recall:     
                                                              0.982 train/mre:  
                                                              0.142             
[2024-06-21 08:44:57,715][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/study_school/0.2/9>
[2024-06-21 08:44:57,716][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 08:44:57,718][HYDRA] 	#470 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=study_school data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=0
[2024-06-21 08:44:57,901][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 08:44:57,902][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-21 08:44:57,903][train.py][INFO] - Instantiating callbacks...
[2024-06-21 08:44:57,904][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 08:44:57,906][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 08:44:57,906][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 08:44:57,907][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 08:44:57,909][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 08:44:57,910][train.py][INFO] - Instantiating loggers...
[2024-06-21 08:44:57,910][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 08:44:57,911][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 08:44:57,975][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/study_school/0.25/0/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.31it/s v_num: 0.000      
                                                              val/auc: 0.694    
                                                              val/f1: 0.625     
                                                              val/precision:    
                                                              0.833 val/recall: 
                                                              0.500 val/mre:    
                                                              0.153 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.152             
[2024-06-21 08:46:46,676][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/study_school/0.25/0>
[2024-06-21 08:46:46,677][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 08:46:46,680][HYDRA] 	#471 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=study_school data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=1
[rank: 0] Seed set to 1
[2024-06-21 08:46:46,873][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 08:46:46,874][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-21 08:46:46,876][train.py][INFO] - Instantiating callbacks...
[2024-06-21 08:46:46,876][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 08:46:46,878][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 08:46:46,878][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 08:46:46,879][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 08:46:46,881][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 08:46:46,882][train.py][INFO] - Instantiating loggers...
[2024-06-21 08:46:46,882][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 08:46:46,883][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 08:46:46,947][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/study_school/0.25/1/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.30it/s v_num: 0.000      
                                                              val/auc: 0.489    
                                                              val/f1: 0.286     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.200 val/mre:    
                                                              0.151 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.150             
[2024-06-21 08:48:37,086][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/study_school/0.25/1>
[2024-06-21 08:48:37,086][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 08:48:37,088][HYDRA] 	#472 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=study_school data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=2
[rank: 0] Seed set to 2
[2024-06-21 08:48:37,269][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 08:48:37,271][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-21 08:48:37,272][train.py][INFO] - Instantiating callbacks...
[2024-06-21 08:48:37,272][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 08:48:37,274][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 08:48:37,275][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 08:48:37,275][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 08:48:37,278][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 08:48:37,278][train.py][INFO] - Instantiating loggers...
[2024-06-21 08:48:37,278][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 08:48:37,279][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 08:48:37,343][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/study_school/0.25/2/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.29it/s v_num: 0.000      
                                                              val/auc: 0.533    
                                                              val/f1: 0.471     
                                                              val/precision:    
                                                              0.571 val/recall: 
                                                              0.400 val/mre:    
                                                              0.153 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.152             
[2024-06-21 08:50:26,648][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/study_school/0.25/2>
[2024-06-21 08:50:26,648][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 08:50:26,651][HYDRA] 	#473 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=study_school data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=3
[rank: 0] Seed set to 3
[2024-06-21 08:50:26,833][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 08:50:26,834][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-21 08:50:26,836][train.py][INFO] - Instantiating callbacks...
[2024-06-21 08:50:26,836][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 08:50:26,838][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 08:50:26,838][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 08:50:26,839][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 08:50:26,841][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 08:50:26,842][train.py][INFO] - Instantiating loggers...
[2024-06-21 08:50:26,842][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 08:50:26,843][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 08:50:26,914][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/study_school/0.25/3/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.30it/s v_num: 0.000      
                                                              val/auc: 0.483    
                                                              val/f1: 0.375     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.300 val/mre:    
                                                              0.146 train/auc:  
                                                              0.991 train/f1:   
                                                              0.991             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              0.982 train/mre:  
                                                              0.147             
[2024-06-21 08:52:15,887][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/study_school/0.25/3>
[2024-06-21 08:52:15,888][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 08:52:15,890][HYDRA] 	#474 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=study_school data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=4
[rank: 0] Seed set to 4
[2024-06-21 08:52:16,075][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 08:52:16,076][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-21 08:52:16,077][train.py][INFO] - Instantiating callbacks...
[2024-06-21 08:52:16,078][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 08:52:16,080][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 08:52:16,080][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 08:52:16,081][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 08:52:16,083][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 08:52:16,084][train.py][INFO] - Instantiating loggers...
[2024-06-21 08:52:16,084][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 08:52:16,085][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 08:52:16,151][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/study_school/0.25/4/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.31it/s v_num: 0.000      
                                                              val/auc: 0.539    
                                                              val/f1: 0.400     
                                                              val/precision:    
                                                              0.600 val/recall: 
                                                              0.300 val/mre:    
                                                              0.152 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.149             
[2024-06-21 08:54:05,001][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/study_school/0.25/4>
[2024-06-21 08:54:05,002][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 08:54:05,004][HYDRA] 	#475 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=study_school data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=5
[rank: 0] Seed set to 5
[2024-06-21 08:54:05,187][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 08:54:05,188][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-21 08:54:05,190][train.py][INFO] - Instantiating callbacks...
[2024-06-21 08:54:05,190][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 08:54:05,192][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 08:54:05,192][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 08:54:05,193][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 08:54:05,195][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 08:54:05,196][train.py][INFO] - Instantiating loggers...
[2024-06-21 08:54:05,196][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 08:54:05,197][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 08:54:05,263][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/study_school/0.25/5/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.29it/s v_num: 0.000      
                                                              val/auc: 0.544    
                                                              val/f1: 0.308     
                                                              val/precision:    
                                                              0.667 val/recall: 
                                                              0.200 val/mre:    
                                                              0.152 train/auc:  
                                                              0.991 train/f1:   
                                                              0.991             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              0.982 train/mre:  
                                                              0.151             
[2024-06-21 08:55:55,093][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/study_school/0.25/5>
[2024-06-21 08:55:55,094][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 08:55:55,097][HYDRA] 	#476 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=study_school data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=6
[rank: 0] Seed set to 6
[2024-06-21 08:55:55,284][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 08:55:55,285][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-21 08:55:55,286][train.py][INFO] - Instantiating callbacks...
[2024-06-21 08:55:55,287][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 08:55:55,289][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 08:55:55,289][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 08:55:55,290][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 08:55:55,293][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 08:55:55,293][train.py][INFO] - Instantiating loggers...
[2024-06-21 08:55:55,293][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 08:55:55,294][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 08:55:55,361][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/study_school/0.25/6/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.30it/s v_num: 0.000      
                                                              val/auc: 0.522    
                                                              val/f1: 0.571     
                                                              val/precision:    
                                                              0.545 val/recall: 
                                                              0.600 val/mre:    
                                                              0.145 train/auc:  
                                                              0.991 train/f1:   
                                                              0.991             
                                                              train/precision:  
                                                              0.982             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.144             
[2024-06-21 08:57:44,811][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/study_school/0.25/6>
[2024-06-21 08:57:44,811][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 08:57:44,814][HYDRA] 	#477 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=study_school data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=7
[rank: 0] Seed set to 7
[2024-06-21 08:57:44,996][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 08:57:44,998][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-21 08:57:44,999][train.py][INFO] - Instantiating callbacks...
[2024-06-21 08:57:44,999][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 08:57:45,001][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 08:57:45,002][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 08:57:45,002][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 08:57:45,005][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 08:57:45,005][train.py][INFO] - Instantiating loggers...
[2024-06-21 08:57:45,006][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 08:57:45,007][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 08:57:45,075][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/study_school/0.25/7/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.29it/s v_num: 0.000      
                                                              val/auc: 0.550    
                                                              val/f1: 0.182     
                                                              val/precision:    
                                                              1.000 val/recall: 
                                                              0.100 val/mre:    
                                                              0.151 train/auc:  
                                                              0.982 train/f1:   
                                                              0.982             
                                                              train/precision:  
                                                              0.966             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.147             
[2024-06-21 08:59:33,775][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/study_school/0.25/7>
[2024-06-21 08:59:33,775][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 08:59:33,778][HYDRA] 	#478 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=study_school data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=8
[rank: 0] Seed set to 8
[2024-06-21 08:59:33,963][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 08:59:33,965][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-21 08:59:33,966][train.py][INFO] - Instantiating callbacks...
[2024-06-21 08:59:33,966][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 08:59:33,968][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 08:59:33,969][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 08:59:33,969][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 08:59:33,972][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 08:59:33,972][train.py][INFO] - Instantiating loggers...
[2024-06-21 08:59:33,973][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 08:59:33,974][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 08:59:34,046][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/study_school/0.25/8/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.31it/s v_num: 0.000      
                                                              val/auc: 0.539    
                                                              val/f1: 0.400     
                                                              val/precision:    
                                                              0.600 val/recall: 
                                                              0.300 val/mre:    
                                                              0.147 train/auc:  
                                                              0.991 train/f1:   
                                                              0.991             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              0.982 train/mre:  
                                                              0.146             
[2024-06-21 09:01:22,258][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/study_school/0.25/8>
[2024-06-21 09:01:22,261][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 09:01:22,278][HYDRA] 	#479 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=study_school data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=9
[rank: 0] Seed set to 9
[2024-06-21 09:01:22,544][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 09:01:22,546][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-21 09:01:22,547][train.py][INFO] - Instantiating callbacks...
[2024-06-21 09:01:22,547][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 09:01:22,549][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 09:01:22,550][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 09:01:22,550][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 09:01:22,553][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 09:01:22,554][train.py][INFO] - Instantiating loggers...
[2024-06-21 09:01:22,554][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 09:01:22,555][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 09:01:22,622][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/study_school/0.25/9/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.28it/s v_num: 0.000      
                                                              val/auc: 0.528    
                                                              val/f1: 0.526     
                                                              val/precision:    
                                                              0.556 val/recall: 
                                                              0.500 val/mre:    
                                                              0.143 train/auc:  
                                                              0.982 train/f1:   
                                                              0.982             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              0.964 train/mre:  
                                                              0.142             
[2024-06-21 09:03:12,999][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/study_school/0.25/9>
[2024-06-21 09:03:12,999][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 09:03:13,002][HYDRA] 	#480 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=study_school data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=0
[2024-06-21 09:03:13,182][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 09:03:13,183][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-21 09:03:13,185][train.py][INFO] - Instantiating callbacks...
[2024-06-21 09:03:13,185][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 09:03:13,187][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 09:03:13,187][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 09:03:13,188][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 09:03:13,191][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 09:03:13,191][train.py][INFO] - Instantiating loggers...
[2024-06-21 09:03:13,191][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 09:03:13,192][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 09:03:13,262][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/study_school/0.3/0/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.29it/s v_num: 0.000      
                                                              val/auc: 0.750    
                                                              val/f1: 0.667     
                                                              val/precision:    
                                                              1.000 val/recall: 
                                                              0.500 val/mre:    
                                                              0.155 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.154             
[2024-06-21 09:05:03,012][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/study_school/0.3/0>
[2024-06-21 09:05:03,014][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 09:05:03,031][HYDRA] 	#481 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=study_school data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=1
[rank: 0] Seed set to 1
[2024-06-21 09:05:03,298][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 09:05:03,300][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-21 09:05:03,301][train.py][INFO] - Instantiating callbacks...
[2024-06-21 09:05:03,301][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 09:05:03,303][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 09:05:03,304][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 09:05:03,304][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 09:05:03,307][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 09:05:03,307][train.py][INFO] - Instantiating loggers...
[2024-06-21 09:05:03,308][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 09:05:03,309][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 09:05:03,374][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/study_school/0.3/1/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.31it/s v_num: 0.000      
                                                              val/auc: 0.589    
                                                              val/f1: 0.500     
                                                              val/precision:    
                                                              0.667 val/recall: 
                                                              0.400 val/mre:    
                                                              0.151 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.150             
[2024-06-21 09:06:51,968][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/study_school/0.3/1>
[2024-06-21 09:06:51,969][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 09:06:51,971][HYDRA] 	#482 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=study_school data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=2
[rank: 0] Seed set to 2
[2024-06-21 09:06:52,153][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 09:06:52,155][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-21 09:06:52,156][train.py][INFO] - Instantiating callbacks...
[2024-06-21 09:06:52,156][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 09:06:52,158][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 09:06:52,159][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 09:06:52,159][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 09:06:52,162][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 09:06:52,162][train.py][INFO] - Instantiating loggers...
[2024-06-21 09:06:52,162][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 09:06:52,163][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 09:06:52,229][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/study_school/0.3/2/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.30it/s v_num: 0.000      
                                                              val/auc: 0.478    
                                                              val/f1: 0.444     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.400 val/mre:    
                                                              0.158 train/auc:  
                                                              0.991 train/f1:   
                                                              0.991             
                                                              train/precision:  
                                                              0.982             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.156             
[2024-06-21 09:08:40,970][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/study_school/0.3/2>
[2024-06-21 09:08:40,970][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 09:08:40,973][HYDRA] 	#483 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=study_school data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=3
[rank: 0] Seed set to 3
[2024-06-21 09:08:41,154][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 09:08:41,156][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-21 09:08:41,157][train.py][INFO] - Instantiating callbacks...
[2024-06-21 09:08:41,157][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 09:08:41,159][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 09:08:41,160][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 09:08:41,160][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 09:08:41,163][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 09:08:41,163][train.py][INFO] - Instantiating loggers...
[2024-06-21 09:08:41,164][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 09:08:41,165][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 09:08:41,229][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/study_school/0.3/3/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.29it/s v_num: 0.000      
                                                              val/auc: 0.383    
                                                              val/f1: 0.143     
                                                              val/precision:    
                                                              0.250 val/recall: 
                                                              0.100 val/mre:    
                                                              0.154 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.152             
[2024-06-21 09:10:31,188][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/study_school/0.3/3>
[2024-06-21 09:10:31,189][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 09:10:31,191][HYDRA] 	#484 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=study_school data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=4
[rank: 0] Seed set to 4
[2024-06-21 09:10:31,375][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 09:10:31,376][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-21 09:10:31,378][train.py][INFO] - Instantiating callbacks...
[2024-06-21 09:10:31,378][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 09:10:31,380][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 09:10:31,380][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 09:10:31,381][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 09:10:31,383][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 09:10:31,384][train.py][INFO] - Instantiating loggers...
[2024-06-21 09:10:31,384][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 09:10:31,385][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 09:10:31,451][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/study_school/0.3/4/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.30it/s v_num: 0.000      
                                                              val/auc: 0.594    
                                                              val/f1: 0.429     
                                                              val/precision:    
                                                              0.750 val/recall: 
                                                              0.300 val/mre:    
                                                              0.157 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.153             
[2024-06-21 09:12:21,542][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/study_school/0.3/4>
[2024-06-21 09:12:21,542][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 09:12:21,545][HYDRA] 	#485 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=study_school data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=5
[rank: 0] Seed set to 5
[2024-06-21 09:12:21,727][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 09:12:21,729][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-21 09:12:21,730][train.py][INFO] - Instantiating callbacks...
[2024-06-21 09:12:21,730][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 09:12:21,732][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 09:12:21,732][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 09:12:21,733][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 09:12:21,735][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 09:12:21,736][train.py][INFO] - Instantiating loggers...
[2024-06-21 09:12:21,736][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 09:12:21,737][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 09:12:21,802][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/study_school/0.3/5/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.30it/s v_num: 0.000      
                                                              val/auc: 0.650    
                                                              val/f1: 0.462     
                                                              val/precision:    
                                                              1.000 val/recall: 
                                                              0.300 val/mre:    
                                                              0.154 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.155             
[2024-06-21 09:14:11,210][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/study_school/0.3/5>
[2024-06-21 09:14:11,210][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 09:14:11,213][HYDRA] 	#486 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=study_school data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=6
[rank: 0] Seed set to 6
[2024-06-21 09:14:11,394][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 09:14:11,396][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-21 09:14:11,397][train.py][INFO] - Instantiating callbacks...
[2024-06-21 09:14:11,397][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 09:14:11,399][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 09:14:11,400][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 09:14:11,400][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 09:14:11,403][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 09:14:11,403][train.py][INFO] - Instantiating loggers...
[2024-06-21 09:14:11,403][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 09:14:11,404][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 09:14:11,491][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/study_school/0.3/6/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.29it/s v_num: 0.000      
                                                              val/auc: 0.533    
                                                              val/f1: 0.471     
                                                              val/precision:    
                                                              0.571 val/recall: 
                                                              0.400 val/mre:    
                                                              0.151 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.150             
[2024-06-21 09:16:00,479][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/study_school/0.3/6>
[2024-06-21 09:16:00,479][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 09:16:00,482][HYDRA] 	#487 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=study_school data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=7
[rank: 0] Seed set to 7
[2024-06-21 09:16:00,664][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 09:16:00,666][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-21 09:16:00,667][train.py][INFO] - Instantiating callbacks...
[2024-06-21 09:16:00,667][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 09:16:00,669][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 09:16:00,670][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 09:16:00,670][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 09:16:00,673][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 09:16:00,673][train.py][INFO] - Instantiating loggers...
[2024-06-21 09:16:00,673][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 09:16:00,674][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 09:16:00,740][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/study_school/0.3/7/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.29it/s v_num: 0.000      
                                                              val/auc: 0.639    
                                                              val/f1: 0.588     
                                                              val/precision:    
                                                              0.714 val/recall: 
                                                              0.500 val/mre:    
                                                              0.154 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.152             
[2024-06-21 09:17:50,144][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/study_school/0.3/7>
[2024-06-21 09:17:50,145][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 09:17:50,147][HYDRA] 	#488 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=study_school data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=8
[rank: 0] Seed set to 8
[2024-06-21 09:17:50,334][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 09:17:50,336][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-21 09:17:50,337][train.py][INFO] - Instantiating callbacks...
[2024-06-21 09:17:50,337][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 09:17:50,339][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 09:17:50,339][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 09:17:50,340][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 09:17:50,343][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 09:17:50,343][train.py][INFO] - Instantiating loggers...
[2024-06-21 09:17:50,343][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 09:17:50,345][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 09:17:50,412][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/study_school/0.3/8/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.29it/s v_num: 0.000      
                                                              val/auc: 0.383    
                                                              val/f1: 0.143     
                                                              val/precision:    
                                                              0.250 val/recall: 
                                                              0.100 val/mre:    
                                                              0.154 train/auc:  
                                                              0.982 train/f1:   
                                                              0.982             
                                                              train/precision:  
                                                              0.982             
                                                              train/recall:     
                                                              0.982 train/mre:  
                                                              0.150             
[2024-06-21 09:19:38,832][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/study_school/0.3/8>
[2024-06-21 09:19:38,832][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 09:19:38,835][HYDRA] 	#489 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=study_school data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=9
[rank: 0] Seed set to 9
[2024-06-21 09:19:39,024][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 09:19:39,025][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-21 09:19:39,027][train.py][INFO] - Instantiating callbacks...
[2024-06-21 09:19:39,027][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 09:19:39,029][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 09:19:39,030][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 09:19:39,030][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 09:19:39,033][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 09:19:39,033][train.py][INFO] - Instantiating loggers...
[2024-06-21 09:19:39,033][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 09:19:39,035][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 09:19:39,135][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/study_school/0.3/9/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.31it/s v_num: 0.000      
                                                              val/auc: 0.639    
                                                              val/f1: 0.588     
                                                              val/precision:    
                                                              0.714 val/recall: 
                                                              0.500 val/mre:    
                                                              0.147 train/auc:  
                                                              0.991 train/f1:   
                                                              0.991             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              0.982 train/mre:  
                                                              0.145             
[2024-06-21 09:21:27,923][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/study_school/0.3/9>
[2024-06-21 09:21:27,923][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 09:21:27,926][HYDRA] 	#490 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=travel_lot data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=0
[2024-06-21 09:21:28,108][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 09:21:28,110][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-21 09:21:28,111][train.py][INFO] - Instantiating callbacks...
[2024-06-21 09:21:28,111][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 09:21:28,113][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 09:21:28,113][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 09:21:28,114][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 09:21:28,117][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 09:21:28,117][train.py][INFO] - Instantiating loggers...
[2024-06-21 09:21:28,117][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 09:21:28,119][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 09:21:28,185][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/travel_lot/0.0/0/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.24it/s v_num: 0.000      
                                                              val/auc: 0.461    
                                                              val/f1: 0.286     
                                                              val/precision:    
                                                              0.400 val/recall: 
                                                              0.222 val/mre: nan
                                                              train/auc: 0.948  
                                                              train/f1: 0.950   
                                                              train/precision:  
                                                              0.919             
                                                              train/recall:     
                                                              0.983 train/mre:  
                                                              nan               
[2024-06-21 09:23:19,702][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/travel_lot/0.0/0>
[2024-06-21 09:23:19,704][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 09:23:19,723][HYDRA] 	#491 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=travel_lot data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=1
[rank: 0] Seed set to 1
[2024-06-21 09:23:20,191][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 09:23:20,193][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-21 09:23:20,194][train.py][INFO] - Instantiating callbacks...
[2024-06-21 09:23:20,194][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 09:23:20,198][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 09:23:20,199][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 09:23:20,199][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 09:23:20,202][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 09:23:20,202][train.py][INFO] - Instantiating loggers...
[2024-06-21 09:23:20,202][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 09:23:20,203][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 09:23:20,275][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/travel_lot/0.0/1/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.22it/s v_num: 0.000      
                                                              val/auc: 0.450    
                                                              val/f1: 0.000     
                                                              val/precision:    
                                                              0.000 val/recall: 
                                                              0.000 val/mre: nan
                                                              train/auc: 0.948  
                                                              train/f1: 0.949   
                                                              train/precision:  
                                                              0.933             
                                                              train/recall:     
                                                              0.966 train/mre:  
                                                              nan               
[2024-06-21 09:25:12,029][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/travel_lot/0.0/1>
[2024-06-21 09:25:12,029][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 09:25:12,032][HYDRA] 	#492 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=travel_lot data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=2
[rank: 0] Seed set to 2
[2024-06-21 09:25:12,221][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 09:25:12,223][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-21 09:25:12,224][train.py][INFO] - Instantiating callbacks...
[2024-06-21 09:25:12,224][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 09:25:12,226][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 09:25:12,227][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 09:25:12,227][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 09:25:12,230][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 09:25:12,230][train.py][INFO] - Instantiating loggers...
[2024-06-21 09:25:12,230][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 09:25:12,232][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 09:25:12,297][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/travel_lot/0.0/2/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.23it/s v_num: 0.000      
                                                              val/auc: 0.461    
                                                              val/f1: 0.286     
                                                              val/precision:    
                                                              0.400 val/recall: 
                                                              0.222 val/mre: nan
                                                              train/auc: 0.966  
                                                              train/f1: 0.964   
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              0.931 train/mre:  
                                                              nan               
[2024-06-21 09:27:03,734][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/travel_lot/0.0/2>
[2024-06-21 09:27:03,735][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 09:27:03,737][HYDRA] 	#493 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=travel_lot data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=3
[rank: 0] Seed set to 3
[2024-06-21 09:27:03,922][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 09:27:03,924][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-21 09:27:03,925][train.py][INFO] - Instantiating callbacks...
[2024-06-21 09:27:03,925][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 09:27:03,927][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 09:27:03,928][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 09:27:03,928][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 09:27:03,931][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 09:27:03,931][train.py][INFO] - Instantiating loggers...
[2024-06-21 09:27:03,931][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 09:27:03,933][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 09:27:03,997][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/travel_lot/0.0/3/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.23it/s v_num: 0.000      
                                                              val/auc: 0.456    
                                                              val/f1: 0.167     
                                                              val/precision:    
                                                              0.333 val/recall: 
                                                              0.111 val/mre: nan
                                                              train/auc: 0.974  
                                                              train/f1: 0.974   
                                                              train/precision:  
                                                              0.966             
                                                              train/recall:     
                                                              0.983 train/mre:  
                                                              nan               
[2024-06-21 09:28:55,021][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/travel_lot/0.0/3>
[2024-06-21 09:28:55,022][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 09:28:55,024][HYDRA] 	#494 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=travel_lot data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=4
[rank: 0] Seed set to 4
[2024-06-21 09:28:55,210][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 09:28:55,211][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-21 09:28:55,213][train.py][INFO] - Instantiating callbacks...
[2024-06-21 09:28:55,213][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 09:28:55,215][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 09:28:55,215][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 09:28:55,216][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 09:28:55,219][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 09:28:55,219][train.py][INFO] - Instantiating loggers...
[2024-06-21 09:28:55,219][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 09:28:55,220][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 09:28:55,288][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/travel_lot/0.0/4/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:02 • 0:00:00 1.70it/s v_num: 0.000      
                                                              val/auc: 0.350    
                                                              val/f1: 0.000     
                                                              val/precision:    
                                                              0.000 val/recall: 
                                                              0.000 val/mre: nan
                                                              train/auc: 0.948  
                                                              train/f1: 0.951   
                                                              train/precision:  
                                                              0.906             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              nan               
[2024-06-21 09:30:46,037][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/travel_lot/0.0/4>
[2024-06-21 09:30:46,037][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 09:30:46,040][HYDRA] 	#495 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=travel_lot data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=5
[rank: 0] Seed set to 5
[2024-06-21 09:30:46,248][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 09:30:46,249][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-21 09:30:46,250][train.py][INFO] - Instantiating callbacks...
[2024-06-21 09:30:46,250][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 09:30:46,252][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 09:30:46,253][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 09:30:46,253][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 09:30:46,256][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 09:30:46,256][train.py][INFO] - Instantiating loggers...
[2024-06-21 09:30:46,257][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 09:30:46,258][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 09:30:46,323][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/travel_lot/0.0/5/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.22it/s v_num: 0.000      
                                                              val/auc: 0.461    
                                                              val/f1: 0.286     
                                                              val/precision:    
                                                              0.400 val/recall: 
                                                              0.222 val/mre: nan
                                                              train/auc: 0.940  
                                                              train/f1: 0.941   
                                                              train/precision:  
                                                              0.918             
                                                              train/recall:     
                                                              0.966 train/mre:  
                                                              nan               
[2024-06-21 09:32:37,567][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/travel_lot/0.0/5>
[2024-06-21 09:32:37,568][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 09:32:37,570][HYDRA] 	#496 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=travel_lot data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=6
[rank: 0] Seed set to 6
[2024-06-21 09:32:37,756][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 09:32:37,757][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-21 09:32:37,759][train.py][INFO] - Instantiating callbacks...
[2024-06-21 09:32:37,759][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 09:32:37,761][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 09:32:37,761][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 09:32:37,762][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 09:32:37,764][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 09:32:37,765][train.py][INFO] - Instantiating loggers...
[2024-06-21 09:32:37,765][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 09:32:37,766][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 09:32:37,832][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/travel_lot/0.0/6/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.24it/s v_num: 0.000      
                                                              val/auc: 0.356    
                                                              val/f1: 0.143     
                                                              val/precision:    
                                                              0.200 val/recall: 
                                                              0.111 val/mre: nan
                                                              train/auc: 0.940  
                                                              train/f1: 0.938   
                                                              train/precision:  
                                                              0.964             
                                                              train/recall:     
                                                              0.914 train/mre:  
                                                              nan               
[2024-06-21 09:34:28,652][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/travel_lot/0.0/6>
[2024-06-21 09:34:28,653][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 09:34:28,655][HYDRA] 	#497 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=travel_lot data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=7
[rank: 0] Seed set to 7
[2024-06-21 09:34:28,836][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 09:34:28,838][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-21 09:34:28,839][train.py][INFO] - Instantiating callbacks...
[2024-06-21 09:34:28,839][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 09:34:28,841][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 09:34:28,842][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 09:34:28,842][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 09:34:28,852][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 09:34:28,853][train.py][INFO] - Instantiating loggers...
[2024-06-21 09:34:28,853][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 09:34:28,854][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 09:34:28,920][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/travel_lot/0.0/7/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.24it/s v_num: 0.000      
                                                              val/auc: 0.572    
                                                              val/f1: 0.500     
                                                              val/precision:    
                                                              0.571 val/recall: 
                                                              0.444 val/mre: nan
                                                              train/auc: 0.922  
                                                              train/f1: 0.922   
                                                              train/precision:  
                                                              0.930             
                                                              train/recall:     
                                                              0.914 train/mre:  
                                                              nan               
[2024-06-21 09:36:19,026][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/travel_lot/0.0/7>
[2024-06-21 09:36:19,027][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 09:36:19,029][HYDRA] 	#498 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=travel_lot data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=8
[rank: 0] Seed set to 8
[2024-06-21 09:36:19,217][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 09:36:19,219][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-21 09:36:19,220][train.py][INFO] - Instantiating callbacks...
[2024-06-21 09:36:19,220][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 09:36:19,222][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 09:36:19,223][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 09:36:19,223][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 09:36:19,226][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 09:36:19,226][train.py][INFO] - Instantiating loggers...
[2024-06-21 09:36:19,226][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 09:36:19,228][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 09:36:19,299][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/travel_lot/0.0/8/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.21it/s v_num: 0.000      
                                                              val/auc: 0.456    
                                                              val/f1: 0.167     
                                                              val/precision:    
                                                              0.333 val/recall: 
                                                              0.111 val/mre: nan
                                                              train/auc: 0.948  
                                                              train/f1: 0.951   
                                                              train/precision:  
                                                              0.906             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              nan               
[2024-06-21 09:38:09,512][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/travel_lot/0.0/8>
[2024-06-21 09:38:09,512][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 09:38:09,515][HYDRA] 	#499 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=travel_lot data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=9
[rank: 0] Seed set to 9
[2024-06-21 09:38:09,697][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 09:38:09,699][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-21 09:38:09,700][train.py][INFO] - Instantiating callbacks...
[2024-06-21 09:38:09,700][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 09:38:09,702][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 09:38:09,702][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 09:38:09,703][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 09:38:09,705][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 09:38:09,706][train.py][INFO] - Instantiating loggers...
[2024-06-21 09:38:09,706][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 09:38:09,707][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 09:38:09,771][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/travel_lot/0.0/9/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.23it/s v_num: 0.000      
                                                              val/auc: 0.517    
                                                              val/f1: 0.400     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.333 val/mre: nan
                                                              train/auc: 0.948  
                                                              train/f1: 0.948   
                                                              train/precision:  
                                                              0.948             
                                                              train/recall:     
                                                              0.948 train/mre:  
                                                              nan               
[2024-06-21 09:40:00,835][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/travel_lot/0.0/9>
[2024-06-21 09:40:00,835][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 09:40:00,838][HYDRA] 	#500 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=travel_lot data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=0
[2024-06-21 09:40:01,018][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 09:40:01,019][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-21 09:40:01,021][train.py][INFO] - Instantiating callbacks...
[2024-06-21 09:40:01,021][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 09:40:01,023][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 09:40:01,023][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 09:40:01,024][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 09:40:01,027][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 09:40:01,028][train.py][INFO] - Instantiating loggers...
[2024-06-21 09:40:01,028][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 09:40:01,029][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 09:40:01,110][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/travel_lot/0.05/0/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.23it/s v_num: 0.000      
                                                              val/auc: 0.517    
                                                              val/f1: 0.400     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.333 val/mre:    
                                                              0.084 train/auc:  
                                                              0.922 train/f1:   
                                                              0.927             
                                                              train/precision:  
                                                              0.877             
                                                              train/recall:     
                                                              0.983 train/mre:  
                                                              0.088             
[2024-06-21 09:41:52,078][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/travel_lot/0.05/0>
[2024-06-21 09:41:52,079][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 09:41:52,085][HYDRA] 	#501 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=travel_lot data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=1
[rank: 0] Seed set to 1
[2024-06-21 09:41:52,324][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 09:41:52,325][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-21 09:41:52,327][train.py][INFO] - Instantiating callbacks...
[2024-06-21 09:41:52,327][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 09:41:52,329][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 09:41:52,330][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 09:41:52,330][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 09:41:52,334][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 09:41:52,334][train.py][INFO] - Instantiating loggers...
[2024-06-21 09:41:52,334][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 09:41:52,336][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 09:41:52,471][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/travel_lot/0.05/1/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.24it/s v_num: 0.000      
                                                              val/auc: 0.450    
                                                              val/f1: 0.000     
                                                              val/precision:    
                                                              0.000 val/recall: 
                                                              0.000 val/mre:    
                                                              0.093 train/auc:  
                                                              0.974 train/f1:   
                                                              0.975             
                                                              train/precision:  
                                                              0.951             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.094             
[2024-06-21 09:43:42,745][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/travel_lot/0.05/1>
[2024-06-21 09:43:42,746][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 09:43:42,748][HYDRA] 	#502 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=travel_lot data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=2
[rank: 0] Seed set to 2
[2024-06-21 09:43:42,931][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 09:43:42,932][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-21 09:43:42,933][train.py][INFO] - Instantiating callbacks...
[2024-06-21 09:43:42,934][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 09:43:42,936][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 09:43:42,936][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 09:43:42,937][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 09:43:42,939][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 09:43:42,940][train.py][INFO] - Instantiating loggers...
[2024-06-21 09:43:42,940][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 09:43:42,941][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 09:43:43,008][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/travel_lot/0.05/2/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.23it/s v_num: 0.000      
                                                              val/auc: 0.406    
                                                              val/f1: 0.154     
                                                              val/precision:    
                                                              0.250 val/recall: 
                                                              0.111 val/mre:    
                                                              0.086 train/auc:  
                                                              0.940 train/f1:   
                                                              0.943             
                                                              train/precision:  
                                                              0.892             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.086             
[2024-06-21 09:45:34,312][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/travel_lot/0.05/2>
[2024-06-21 09:45:34,313][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 09:45:34,315][HYDRA] 	#503 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=travel_lot data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=3
[rank: 0] Seed set to 3
[2024-06-21 09:45:34,497][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 09:45:34,498][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-21 09:45:34,500][train.py][INFO] - Instantiating callbacks...
[2024-06-21 09:45:34,500][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 09:45:34,502][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 09:45:34,502][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 09:45:34,503][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 09:45:34,505][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 09:45:34,506][train.py][INFO] - Instantiating loggers...
[2024-06-21 09:45:34,506][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 09:45:34,507][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 09:45:34,573][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/travel_lot/0.05/3/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.23it/s v_num: 0.000      
                                                              val/auc: 0.406    
                                                              val/f1: 0.154     
                                                              val/precision:    
                                                              0.250 val/recall: 
                                                              0.111 val/mre:    
                                                              0.084 train/auc:  
                                                              0.966 train/f1:   
                                                              0.966             
                                                              train/precision:  
                                                              0.966             
                                                              train/recall:     
                                                              0.966 train/mre:  
                                                              0.089             
[2024-06-21 09:47:26,947][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/travel_lot/0.05/3>
[2024-06-21 09:47:26,947][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 09:47:26,950][HYDRA] 	#504 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=travel_lot data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=4
[rank: 0] Seed set to 4
[2024-06-21 09:47:27,134][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 09:47:27,135][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-21 09:47:27,136][train.py][INFO] - Instantiating callbacks...
[2024-06-21 09:47:27,136][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 09:47:27,138][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 09:47:27,139][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 09:47:27,139][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 09:47:27,142][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 09:47:27,142][train.py][INFO] - Instantiating loggers...
[2024-06-21 09:47:27,143][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 09:47:27,144][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 09:47:27,216][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/travel_lot/0.05/4/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.23it/s v_num: 0.000      
                                                              val/auc: 0.406    
                                                              val/f1: 0.154     
                                                              val/precision:    
                                                              0.250 val/recall: 
                                                              0.111 val/mre:    
                                                              0.085 train/auc:  
                                                              0.974 train/f1:   
                                                              0.975             
                                                              train/precision:  
                                                              0.951             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.087             
[2024-06-21 09:49:18,472][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/travel_lot/0.05/4>
[2024-06-21 09:49:18,472][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 09:49:18,475][HYDRA] 	#505 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=travel_lot data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=5
[rank: 0] Seed set to 5
[2024-06-21 09:49:18,659][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 09:49:18,661][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-21 09:49:18,662][train.py][INFO] - Instantiating callbacks...
[2024-06-21 09:49:18,662][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 09:49:18,664][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 09:49:18,665][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 09:49:18,665][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 09:49:18,668][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 09:49:18,668][train.py][INFO] - Instantiating loggers...
[2024-06-21 09:49:18,668][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 09:49:18,670][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 09:49:18,734][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/travel_lot/0.05/5/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.24it/s v_num: 0.000      
                                                              val/auc: 0.517    
                                                              val/f1: 0.400     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.333 val/mre:    
                                                              0.089 train/auc:  
                                                              0.983 train/f1:   
                                                              0.983             
                                                              train/precision:  
                                                              0.983             
                                                              train/recall:     
                                                              0.983 train/mre:  
                                                              0.091             
[2024-06-21 09:51:09,155][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/travel_lot/0.05/5>
[2024-06-21 09:51:09,155][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 09:51:09,158][HYDRA] 	#506 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=travel_lot data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=6
[rank: 0] Seed set to 6
[2024-06-21 09:51:09,340][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 09:51:09,341][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-21 09:51:09,343][train.py][INFO] - Instantiating callbacks...
[2024-06-21 09:51:09,343][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 09:51:09,345][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 09:51:09,345][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 09:51:09,346][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 09:51:09,348][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 09:51:09,349][train.py][INFO] - Instantiating loggers...
[2024-06-21 09:51:09,349][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 09:51:09,350][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 09:51:09,418][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/travel_lot/0.05/6/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.22it/s v_num: 0.000      
                                                              val/auc: 0.517    
                                                              val/f1: 0.400     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.333 val/mre:    
                                                              0.088 train/auc:  
                                                              0.948 train/f1:   
                                                              0.949             
                                                              train/precision:  
                                                              0.933             
                                                              train/recall:     
                                                              0.966 train/mre:  
                                                              0.088             
[2024-06-21 09:53:01,517][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/travel_lot/0.05/6>
[2024-06-21 09:53:01,517][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 09:53:01,520][HYDRA] 	#507 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=travel_lot data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=7
[rank: 0] Seed set to 7
[2024-06-21 09:53:01,704][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 09:53:01,705][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-21 09:53:01,706][train.py][INFO] - Instantiating callbacks...
[2024-06-21 09:53:01,707][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 09:53:01,709][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 09:53:01,709][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 09:53:01,710][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 09:53:01,713][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 09:53:01,713][train.py][INFO] - Instantiating loggers...
[2024-06-21 09:53:01,713][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 09:53:01,715][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 09:53:01,782][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/travel_lot/0.05/7/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.23it/s v_num: 0.000      
                                                              val/auc: 0.506    
                                                              val/f1: 0.182     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.111 val/mre:    
                                                              0.090 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.093             
[2024-06-21 09:54:53,370][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/travel_lot/0.05/7>
[2024-06-21 09:54:53,371][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 09:54:53,373][HYDRA] 	#508 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=travel_lot data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=8
[rank: 0] Seed set to 8
[2024-06-21 09:54:53,557][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 09:54:53,559][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-21 09:54:53,560][train.py][INFO] - Instantiating callbacks...
[2024-06-21 09:54:53,560][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 09:54:53,562][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 09:54:53,563][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 09:54:53,563][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 09:54:53,566][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 09:54:53,566][train.py][INFO] - Instantiating loggers...
[2024-06-21 09:54:53,566][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 09:54:53,568][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 09:54:53,633][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/travel_lot/0.05/8/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.24it/s v_num: 0.000      
                                                              val/auc: 0.517    
                                                              val/f1: 0.400     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.333 val/mre:    
                                                              0.090 train/auc:  
                                                              0.905 train/f1:   
                                                              0.906             
                                                              train/precision:  
                                                              0.898             
                                                              train/recall:     
                                                              0.914 train/mre:  
                                                              0.092             
[2024-06-21 09:56:43,952][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/travel_lot/0.05/8>
[2024-06-21 09:56:43,952][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 09:56:43,955][HYDRA] 	#509 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=travel_lot data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=9
[rank: 0] Seed set to 9
[2024-06-21 09:56:44,140][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 09:56:44,141][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-21 09:56:44,142][train.py][INFO] - Instantiating callbacks...
[2024-06-21 09:56:44,143][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 09:56:44,145][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 09:56:44,145][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 09:56:44,146][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 09:56:44,148][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 09:56:44,149][train.py][INFO] - Instantiating loggers...
[2024-06-21 09:56:44,149][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 09:56:44,150][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 09:56:44,217][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/travel_lot/0.05/9/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.25it/s v_num: 0.000      
                                                              val/auc: 0.517    
                                                              val/f1: 0.400     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.333 val/mre:    
                                                              0.089 train/auc:  
                                                              0.966 train/f1:   
                                                              0.967             
                                                              train/precision:  
                                                              0.935             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.091             
[2024-06-21 09:58:34,200][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/travel_lot/0.05/9>
[2024-06-21 09:58:34,201][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 09:58:34,203][HYDRA] 	#510 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=travel_lot data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=0
[2024-06-21 09:58:34,387][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 09:58:34,388][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-21 09:58:34,389][train.py][INFO] - Instantiating callbacks...
[2024-06-21 09:58:34,390][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 09:58:34,392][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 09:58:34,392][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 09:58:34,392][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 09:58:34,395][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 09:58:34,395][train.py][INFO] - Instantiating loggers...
[2024-06-21 09:58:34,395][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 09:58:34,397][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 09:58:34,461][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/travel_lot/0.1/0/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.23it/s v_num: 0.000      
                                                              val/auc: 0.461    
                                                              val/f1: 0.286     
                                                              val/precision:    
                                                              0.400 val/recall: 
                                                              0.222 val/mre:    
                                                              0.095 train/auc:  
                                                              0.948 train/f1:   
                                                              0.951             
                                                              train/precision:  
                                                              0.906             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.095             
[2024-06-21 10:00:25,616][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/travel_lot/0.1/0>
[2024-06-21 10:00:25,616][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 10:00:25,619][HYDRA] 	#511 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=travel_lot data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=1
[rank: 0] Seed set to 1
[2024-06-21 10:00:25,803][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 10:00:25,805][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-21 10:00:25,806][train.py][INFO] - Instantiating callbacks...
[2024-06-21 10:00:25,806][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 10:00:25,808][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 10:00:25,809][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 10:00:25,809][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 10:00:25,812][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 10:00:25,812][train.py][INFO] - Instantiating loggers...
[2024-06-21 10:00:25,812][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 10:00:25,814][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 10:00:25,881][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/travel_lot/0.1/1/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.23it/s v_num: 0.000      
                                                              val/auc: 0.567    
                                                              val/f1: 0.429     
                                                              val/precision:    
                                                              0.600 val/recall: 
                                                              0.333 val/mre:    
                                                              0.099 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.102             
[2024-06-21 10:02:17,050][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/travel_lot/0.1/1>
[2024-06-21 10:02:17,050][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 10:02:17,053][HYDRA] 	#512 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=travel_lot data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=2
[rank: 0] Seed set to 2
[2024-06-21 10:02:17,250][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 10:02:17,251][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-21 10:02:17,253][train.py][INFO] - Instantiating callbacks...
[2024-06-21 10:02:17,253][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 10:02:17,255][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 10:02:17,255][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 10:02:17,256][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 10:02:17,259][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 10:02:17,259][train.py][INFO] - Instantiating loggers...
[2024-06-21 10:02:17,259][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 10:02:17,260][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 10:02:17,326][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/travel_lot/0.1/2/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.24it/s v_num: 0.000      
                                                              val/auc: 0.456    
                                                              val/f1: 0.167     
                                                              val/precision:    
                                                              0.333 val/recall: 
                                                              0.111 val/mre:    
                                                              0.094 train/auc:  
                                                              0.931 train/f1:   
                                                              0.935             
                                                              train/precision:  
                                                              0.879             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.095             
[2024-06-21 10:04:07,722][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/travel_lot/0.1/2>
[2024-06-21 10:04:07,723][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 10:04:07,725][HYDRA] 	#513 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=travel_lot data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=3
[rank: 0] Seed set to 3
[2024-06-21 10:04:07,910][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 10:04:07,912][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-21 10:04:07,913][train.py][INFO] - Instantiating callbacks...
[2024-06-21 10:04:07,913][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 10:04:07,915][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 10:04:07,916][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 10:04:07,916][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 10:04:07,919][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 10:04:07,920][train.py][INFO] - Instantiating loggers...
[2024-06-21 10:04:07,920][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 10:04:07,921][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 10:04:07,985][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/travel_lot/0.1/3/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.21it/s v_num: 0.000      
                                                              val/auc: 0.567    
                                                              val/f1: 0.429     
                                                              val/precision:    
                                                              0.600 val/recall: 
                                                              0.333 val/mre:    
                                                              0.090 train/auc:  
                                                              0.974 train/f1:   
                                                              0.975             
                                                              train/precision:  
                                                              0.951             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.093             
[2024-06-21 10:05:58,792][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/travel_lot/0.1/3>
[2024-06-21 10:05:58,793][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 10:05:58,795][HYDRA] 	#514 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=travel_lot data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=4
[rank: 0] Seed set to 4
[2024-06-21 10:05:58,978][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 10:05:58,979][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-21 10:05:58,980][train.py][INFO] - Instantiating callbacks...
[2024-06-21 10:05:58,981][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 10:05:58,983][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 10:05:58,983][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 10:05:58,984][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 10:05:58,986][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 10:05:58,987][train.py][INFO] - Instantiating loggers...
[2024-06-21 10:05:58,987][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 10:05:58,988][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 10:05:59,056][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/travel_lot/0.1/4/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.23it/s v_num: 0.000      
                                                              val/auc: 0.406    
                                                              val/f1: 0.154     
                                                              val/precision:    
                                                              0.250 val/recall: 
                                                              0.111 val/mre:    
                                                              0.091 train/auc:  
                                                              0.966 train/f1:   
                                                              0.964             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              0.931 train/mre:  
                                                              0.093             
[2024-06-21 10:07:50,696][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/travel_lot/0.1/4>
[2024-06-21 10:07:50,696][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 10:07:50,699][HYDRA] 	#515 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=travel_lot data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=5
[rank: 0] Seed set to 5
[2024-06-21 10:07:50,882][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 10:07:50,884][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-21 10:07:50,885][train.py][INFO] - Instantiating callbacks...
[2024-06-21 10:07:50,885][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 10:07:50,887][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 10:07:50,888][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 10:07:50,888][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 10:07:50,891][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 10:07:50,891][train.py][INFO] - Instantiating loggers...
[2024-06-21 10:07:50,891][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 10:07:50,893][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 10:07:50,960][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/travel_lot/0.1/5/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.23it/s v_num: 0.000      
                                                              val/auc: 0.461    
                                                              val/f1: 0.286     
                                                              val/precision:    
                                                              0.400 val/recall: 
                                                              0.222 val/mre:    
                                                              0.096 train/auc:  
                                                              0.966 train/f1:   
                                                              0.967             
                                                              train/precision:  
                                                              0.935             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.098             
[2024-06-21 10:09:42,203][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/travel_lot/0.1/5>
[2024-06-21 10:09:42,203][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 10:09:42,206][HYDRA] 	#516 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=travel_lot data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=6
[rank: 0] Seed set to 6
[2024-06-21 10:09:42,388][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 10:09:42,390][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-21 10:09:42,391][train.py][INFO] - Instantiating callbacks...
[2024-06-21 10:09:42,391][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 10:09:42,393][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 10:09:42,394][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 10:09:42,394][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 10:09:42,397][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 10:09:42,397][train.py][INFO] - Instantiating loggers...
[2024-06-21 10:09:42,398][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 10:09:42,399][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 10:09:42,467][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/travel_lot/0.1/6/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.24it/s v_num: 0.000      
                                                              val/auc: 0.572    
                                                              val/f1: 0.500     
                                                              val/precision:    
                                                              0.571 val/recall: 
                                                              0.444 val/mre:    
                                                              0.094 train/auc:  
                                                              0.931 train/f1:   
                                                              0.935             
                                                              train/precision:  
                                                              0.879             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.095             
[2024-06-21 10:11:33,146][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/travel_lot/0.1/6>
[2024-06-21 10:11:33,147][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 10:11:33,149][HYDRA] 	#517 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=travel_lot data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=7
[rank: 0] Seed set to 7
[2024-06-21 10:11:33,331][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 10:11:33,332][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-21 10:11:33,333][train.py][INFO] - Instantiating callbacks...
[2024-06-21 10:11:33,334][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 10:11:33,336][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 10:11:33,336][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 10:11:33,337][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 10:11:33,340][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 10:11:33,340][train.py][INFO] - Instantiating loggers...
[2024-06-21 10:11:33,340][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 10:11:33,342][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 10:11:33,408][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/travel_lot/0.1/7/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.24it/s v_num: 0.000      
                                                              val/auc: 0.400    
                                                              val/f1: 0.000     
                                                              val/precision:    
                                                              0.000 val/recall: 
                                                              0.000 val/mre:    
                                                              0.098 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.100             
[2024-06-21 10:13:24,267][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/travel_lot/0.1/7>
[2024-06-21 10:13:24,267][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 10:13:24,270][HYDRA] 	#518 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=travel_lot data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=8
[rank: 0] Seed set to 8
[2024-06-21 10:13:24,452][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 10:13:24,453][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-21 10:13:24,454][train.py][INFO] - Instantiating callbacks...
[2024-06-21 10:13:24,454][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 10:13:24,456][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 10:13:24,457][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 10:13:24,457][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 10:13:24,460][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 10:13:24,461][train.py][INFO] - Instantiating loggers...
[2024-06-21 10:13:24,461][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 10:13:24,462][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 10:13:24,526][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/travel_lot/0.1/8/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.23it/s v_num: 0.000      
                                                              val/auc: 0.456    
                                                              val/f1: 0.167     
                                                              val/precision:    
                                                              0.333 val/recall: 
                                                              0.111 val/mre:    
                                                              0.092 train/auc:  
                                                              0.991 train/f1:   
                                                              0.991             
                                                              train/precision:  
                                                              0.983             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.093             
[2024-06-21 10:15:15,656][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/travel_lot/0.1/8>
[2024-06-21 10:15:15,656][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 10:15:15,660][HYDRA] 	#519 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=travel_lot data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=9
[rank: 0] Seed set to 9
[2024-06-21 10:15:15,845][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 10:15:15,847][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-21 10:15:15,848][train.py][INFO] - Instantiating callbacks...
[2024-06-21 10:15:15,848][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 10:15:15,850][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 10:15:15,851][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 10:15:15,851][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 10:15:15,854][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 10:15:15,854][train.py][INFO] - Instantiating loggers...
[2024-06-21 10:15:15,855][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 10:15:15,856][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 10:15:15,921][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/travel_lot/0.1/9/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.24it/s v_num: 0.000      
                                                              val/auc: 0.461    
                                                              val/f1: 0.286     
                                                              val/precision:    
                                                              0.400 val/recall: 
                                                              0.222 val/mre:    
                                                              0.094 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.094             
[2024-06-21 10:17:06,726][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/travel_lot/0.1/9>
[2024-06-21 10:17:06,727][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 10:17:06,729][HYDRA] 	#520 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=travel_lot data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=0
[2024-06-21 10:17:06,910][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 10:17:06,912][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-21 10:17:06,913][train.py][INFO] - Instantiating callbacks...
[2024-06-21 10:17:06,913][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 10:17:06,915][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 10:17:06,916][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 10:17:06,916][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 10:17:06,919][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 10:17:06,919][train.py][INFO] - Instantiating loggers...
[2024-06-21 10:17:06,920][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 10:17:06,921][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 10:17:06,985][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/travel_lot/0.15/0/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.24it/s v_num: 0.000      
                                                              val/auc: 0.411    
                                                              val/f1: 0.267     
                                                              val/precision:    
                                                              0.333 val/recall: 
                                                              0.222 val/mre:    
                                                              0.099 train/auc:  
                                                              0.966 train/f1:   
                                                              0.967             
                                                              train/precision:  
                                                              0.935             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.099             
[2024-06-21 10:18:57,394][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/travel_lot/0.15/0>
[2024-06-21 10:18:57,394][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 10:18:57,397][HYDRA] 	#521 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=travel_lot data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=1
[rank: 0] Seed set to 1
[2024-06-21 10:18:57,580][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 10:18:57,581][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-21 10:18:57,583][train.py][INFO] - Instantiating callbacks...
[2024-06-21 10:18:57,583][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 10:18:57,585][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 10:18:57,585][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 10:18:57,586][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 10:18:57,589][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 10:18:57,589][train.py][INFO] - Instantiating loggers...
[2024-06-21 10:18:57,589][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 10:18:57,591][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 10:18:57,656][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/travel_lot/0.15/1/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.24it/s v_num: 0.000      
                                                              val/auc: 0.450    
                                                              val/f1: 0.000     
                                                              val/precision:    
                                                              0.000 val/recall: 
                                                              0.000 val/mre:    
                                                              0.104 train/auc:  
                                                              0.991 train/f1:   
                                                              0.991             
                                                              train/precision:  
                                                              0.983             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.105             
[2024-06-21 10:20:48,463][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/travel_lot/0.15/1>
[2024-06-21 10:20:48,463][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 10:20:48,466][HYDRA] 	#522 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=travel_lot data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=2
[rank: 0] Seed set to 2
[2024-06-21 10:20:48,648][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 10:20:48,650][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-21 10:20:48,651][train.py][INFO] - Instantiating callbacks...
[2024-06-21 10:20:48,651][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 10:20:48,653][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 10:20:48,654][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 10:20:48,654][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 10:20:48,657][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 10:20:48,657][train.py][INFO] - Instantiating loggers...
[2024-06-21 10:20:48,657][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 10:20:48,659][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 10:20:48,731][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/travel_lot/0.15/2/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.21it/s v_num: 0.000      
                                                              val/auc: 0.467    
                                                              val/f1: 0.375     
                                                              val/precision:    
                                                              0.429 val/recall: 
                                                              0.333 val/mre:    
                                                              0.100 train/auc:  
                                                              0.974 train/f1:   
                                                              0.975             
                                                              train/precision:  
                                                              0.951             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.100             
[2024-06-21 10:22:40,319][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/travel_lot/0.15/2>
[2024-06-21 10:22:40,319][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 10:22:40,322][HYDRA] 	#523 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=travel_lot data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=3
[rank: 0] Seed set to 3
[2024-06-21 10:22:40,505][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 10:22:40,506][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-21 10:22:40,508][train.py][INFO] - Instantiating callbacks...
[2024-06-21 10:22:40,508][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 10:22:40,510][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 10:22:40,510][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 10:22:40,511][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 10:22:40,514][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 10:22:40,514][train.py][INFO] - Instantiating loggers...
[2024-06-21 10:22:40,514][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 10:22:40,515][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 10:22:40,580][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/travel_lot/0.15/3/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.23it/s v_num: 0.000      
                                                              val/auc: 0.567    
                                                              val/f1: 0.429     
                                                              val/precision:    
                                                              0.600 val/recall: 
                                                              0.333 val/mre:    
                                                              0.098 train/auc:  
                                                              0.931 train/f1:   
                                                              0.935             
                                                              train/precision:  
                                                              0.879             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.100             
[2024-06-21 10:24:31,900][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/travel_lot/0.15/3>
[2024-06-21 10:24:31,900][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 10:24:31,903][HYDRA] 	#524 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=travel_lot data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=4
[rank: 0] Seed set to 4
[2024-06-21 10:24:32,086][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 10:24:32,088][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-21 10:24:32,089][train.py][INFO] - Instantiating callbacks...
[2024-06-21 10:24:32,089][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 10:24:32,091][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 10:24:32,092][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 10:24:32,092][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 10:24:32,095][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 10:24:32,095][train.py][INFO] - Instantiating loggers...
[2024-06-21 10:24:32,095][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 10:24:32,096][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 10:24:32,161][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/travel_lot/0.15/4/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.25it/s v_num: 0.000      
                                                              val/auc: 0.400    
                                                              val/f1: 0.000     
                                                              val/precision:    
                                                              0.000 val/recall: 
                                                              0.000 val/mre:    
                                                              0.098 train/auc:  
                                                              0.957 train/f1:   
                                                              0.959             
                                                              train/precision:  
                                                              0.921             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.098             
[2024-06-21 10:26:22,867][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/travel_lot/0.15/4>
[2024-06-21 10:26:22,867][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 10:26:22,870][HYDRA] 	#525 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=travel_lot data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=5
[rank: 0] Seed set to 5
[2024-06-21 10:26:23,059][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 10:26:23,061][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-21 10:26:23,062][train.py][INFO] - Instantiating callbacks...
[2024-06-21 10:26:23,062][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 10:26:23,064][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 10:26:23,065][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 10:26:23,065][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 10:26:23,068][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 10:26:23,068][train.py][INFO] - Instantiating loggers...
[2024-06-21 10:26:23,068][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 10:26:23,069][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 10:26:23,325][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/travel_lot/0.15/5/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.25it/s v_num: 0.000      
                                                              val/auc: 0.517    
                                                              val/f1: 0.400     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.333 val/mre:    
                                                              0.100 train/auc:  
                                                              0.957 train/f1:   
                                                              0.957             
                                                              train/precision:  
                                                              0.949             
                                                              train/recall:     
                                                              0.966 train/mre:  
                                                              0.103             
[2024-06-21 10:28:13,575][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/travel_lot/0.15/5>
[2024-06-21 10:28:13,576][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 10:28:13,578][HYDRA] 	#526 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=travel_lot data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=6
[rank: 0] Seed set to 6
[2024-06-21 10:28:13,761][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 10:28:13,763][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-21 10:28:13,764][train.py][INFO] - Instantiating callbacks...
[2024-06-21 10:28:13,764][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 10:28:13,766][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 10:28:13,767][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 10:28:13,767][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 10:28:13,770][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 10:28:13,770][train.py][INFO] - Instantiating loggers...
[2024-06-21 10:28:13,770][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 10:28:13,771][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 10:28:13,838][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/travel_lot/0.15/6/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.22it/s v_num: 0.000      
                                                              val/auc: 0.411    
                                                              val/f1: 0.267     
                                                              val/precision:    
                                                              0.333 val/recall: 
                                                              0.222 val/mre:    
                                                              0.102 train/auc:  
                                                              0.966 train/f1:   
                                                              0.966             
                                                              train/precision:  
                                                              0.966             
                                                              train/recall:     
                                                              0.966 train/mre:  
                                                              0.102             
[2024-06-21 10:30:05,898][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/travel_lot/0.15/6>
[2024-06-21 10:30:05,900][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 10:30:05,911][HYDRA] 	#527 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=travel_lot data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=7
[rank: 0] Seed set to 7
[2024-06-21 10:30:06,292][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 10:30:06,294][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-21 10:30:06,295][train.py][INFO] - Instantiating callbacks...
[2024-06-21 10:30:06,295][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 10:30:06,298][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 10:30:06,299][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 10:30:06,299][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 10:30:06,302][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 10:30:06,302][train.py][INFO] - Instantiating loggers...
[2024-06-21 10:30:06,302][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 10:30:06,304][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 10:30:06,531][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/travel_lot/0.15/7/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.23it/s v_num: 0.000      
                                                              val/auc: 0.456    
                                                              val/f1: 0.167     
                                                              val/precision:    
                                                              0.333 val/recall: 
                                                              0.111 val/mre:    
                                                              0.105 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.107             
[2024-06-21 10:31:57,440][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/travel_lot/0.15/7>
[2024-06-21 10:31:57,441][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 10:31:57,443][HYDRA] 	#528 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=travel_lot data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=8
[rank: 0] Seed set to 8
[2024-06-21 10:31:57,654][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 10:31:57,656][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-21 10:31:57,657][train.py][INFO] - Instantiating callbacks...
[2024-06-21 10:31:57,657][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 10:31:57,660][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 10:31:57,661][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 10:31:57,661][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 10:31:57,664][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 10:31:57,664][train.py][INFO] - Instantiating loggers...
[2024-06-21 10:31:57,665][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 10:31:57,666][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 10:31:57,765][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/travel_lot/0.15/8/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.24it/s v_num: 0.000      
                                                              val/auc: 0.461    
                                                              val/f1: 0.286     
                                                              val/precision:    
                                                              0.400 val/recall: 
                                                              0.222 val/mre:    
                                                              0.096 train/auc:  
                                                              0.948 train/f1:   
                                                              0.951             
                                                              train/precision:  
                                                              0.906             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.099             
[2024-06-21 10:33:48,451][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/travel_lot/0.15/8>
[2024-06-21 10:33:48,451][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 10:33:48,454][HYDRA] 	#529 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=travel_lot data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=9
[rank: 0] Seed set to 9
[2024-06-21 10:33:48,635][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 10:33:48,637][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-21 10:33:48,638][train.py][INFO] - Instantiating callbacks...
[2024-06-21 10:33:48,638][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 10:33:48,640][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 10:33:48,641][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 10:33:48,641][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 10:33:48,644][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 10:33:48,644][train.py][INFO] - Instantiating loggers...
[2024-06-21 10:33:48,644][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 10:33:48,645][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 10:33:48,713][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/travel_lot/0.15/9/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.24it/s v_num: 0.000      
                                                              val/auc: 0.461    
                                                              val/f1: 0.286     
                                                              val/precision:    
                                                              0.400 val/recall: 
                                                              0.222 val/mre:    
                                                              0.099 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.100             
[2024-06-21 10:35:39,583][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/travel_lot/0.15/9>
[2024-06-21 10:35:39,584][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 10:35:39,586][HYDRA] 	#530 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=travel_lot data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=0
[2024-06-21 10:35:39,769][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 10:35:39,771][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-21 10:35:39,772][train.py][INFO] - Instantiating callbacks...
[2024-06-21 10:35:39,772][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 10:35:39,774][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 10:35:39,775][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 10:35:39,775][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 10:35:39,778][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 10:35:39,778][train.py][INFO] - Instantiating loggers...
[2024-06-21 10:35:39,779][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 10:35:39,780][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 10:35:39,850][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/travel_lot/0.2/0/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.23it/s v_num: 0.000      
                                                              val/auc: 0.400    
                                                              val/f1: 0.000     
                                                              val/precision:    
                                                              0.000 val/recall: 
                                                              0.000 val/mre:    
                                                              0.106 train/auc:  
                                                              0.983 train/f1:   
                                                              0.983             
                                                              train/precision:  
                                                              0.967             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.105             
[2024-06-21 10:37:30,979][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/travel_lot/0.2/0>
[2024-06-21 10:37:30,979][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 10:37:30,981][HYDRA] 	#531 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=travel_lot data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=1
[rank: 0] Seed set to 1
[2024-06-21 10:37:31,163][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 10:37:31,164][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-21 10:37:31,165][train.py][INFO] - Instantiating callbacks...
[2024-06-21 10:37:31,166][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 10:37:31,168][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 10:37:31,168][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 10:37:31,169][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 10:37:31,171][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 10:37:31,172][train.py][INFO] - Instantiating loggers...
[2024-06-21 10:37:31,172][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 10:37:31,173][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 10:37:31,238][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/travel_lot/0.2/1/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.23it/s v_num: 0.000      
                                                              val/auc: 0.572    
                                                              val/f1: 0.500     
                                                              val/precision:    
                                                              0.571 val/recall: 
                                                              0.444 val/mre:    
                                                              0.106 train/auc:  
                                                              0.991 train/f1:   
                                                              0.991             
                                                              train/precision:  
                                                              0.983             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.107             
[2024-06-21 10:39:21,873][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/travel_lot/0.2/1>
[2024-06-21 10:39:21,874][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 10:39:21,876][HYDRA] 	#532 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=travel_lot data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=2
[rank: 0] Seed set to 2
[2024-06-21 10:39:22,061][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 10:39:22,062][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-21 10:39:22,063][train.py][INFO] - Instantiating callbacks...
[2024-06-21 10:39:22,064][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 10:39:22,066][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 10:39:22,066][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 10:39:22,067][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 10:39:22,069][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 10:39:22,070][train.py][INFO] - Instantiating loggers...
[2024-06-21 10:39:22,070][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 10:39:22,071][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 10:39:22,136][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/travel_lot/0.2/2/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.23it/s v_num: 0.000      
                                                              val/auc: 0.517    
                                                              val/f1: 0.400     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.333 val/mre:    
                                                              0.106 train/auc:  
                                                              0.991 train/f1:   
                                                              0.991             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              0.983 train/mre:  
                                                              0.109             
[2024-06-21 10:41:13,686][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/travel_lot/0.2/2>
[2024-06-21 10:41:13,687][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 10:41:13,689][HYDRA] 	#533 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=travel_lot data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=3
[rank: 0] Seed set to 3
[2024-06-21 10:41:13,876][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 10:41:13,878][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-21 10:41:13,879][train.py][INFO] - Instantiating callbacks...
[2024-06-21 10:41:13,879][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 10:41:13,881][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 10:41:13,882][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 10:41:13,882][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 10:41:13,909][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 10:41:13,909][train.py][INFO] - Instantiating loggers...
[2024-06-21 10:41:13,909][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 10:41:13,911][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 10:41:13,999][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/travel_lot/0.2/3/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.24it/s v_num: 0.000      
                                                              val/auc: 0.456    
                                                              val/f1: 0.167     
                                                              val/precision:    
                                                              0.333 val/recall: 
                                                              0.111 val/mre:    
                                                              0.106 train/auc:  
                                                              0.991 train/f1:   
                                                              0.991             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              0.983 train/mre:  
                                                              0.104             
[2024-06-21 10:43:04,800][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/travel_lot/0.2/3>
[2024-06-21 10:43:04,801][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 10:43:04,803][HYDRA] 	#534 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=travel_lot data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=4
[rank: 0] Seed set to 4
[2024-06-21 10:43:04,988][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 10:43:04,990][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-21 10:43:04,991][train.py][INFO] - Instantiating callbacks...
[2024-06-21 10:43:04,991][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 10:43:04,993][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 10:43:04,994][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 10:43:04,994][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 10:43:04,997][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 10:43:04,997][train.py][INFO] - Instantiating loggers...
[2024-06-21 10:43:04,997][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 10:43:04,998][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 10:43:05,068][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/travel_lot/0.2/4/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.22it/s v_num: 0.000      
                                                              val/auc: 0.456    
                                                              val/f1: 0.167     
                                                              val/precision:    
                                                              0.333 val/recall: 
                                                              0.111 val/mre:    
                                                              0.101 train/auc:  
                                                              0.983 train/f1:   
                                                              0.983             
                                                              train/precision:  
                                                              0.967             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.101             
[2024-06-21 10:44:58,171][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/travel_lot/0.2/4>
[2024-06-21 10:44:58,171][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 10:44:58,173][HYDRA] 	#535 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=travel_lot data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=5
[rank: 0] Seed set to 5
[2024-06-21 10:44:58,353][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 10:44:58,354][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-21 10:44:58,355][train.py][INFO] - Instantiating callbacks...
[2024-06-21 10:44:58,356][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 10:44:58,358][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 10:44:58,358][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 10:44:58,359][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 10:44:58,361][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 10:44:58,362][train.py][INFO] - Instantiating loggers...
[2024-06-21 10:44:58,362][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 10:44:58,363][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 10:44:58,428][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/travel_lot/0.2/5/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.22it/s v_num: 0.000      
                                                              val/auc: 0.517    
                                                              val/f1: 0.400     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.333 val/mre:    
                                                              0.102 train/auc:  
                                                              0.991 train/f1:   
                                                              0.991             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              0.983 train/mre:  
                                                              0.105             
[2024-06-21 10:46:50,822][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/travel_lot/0.2/5>
[2024-06-21 10:46:50,822][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 10:46:50,825][HYDRA] 	#536 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=travel_lot data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=6
[rank: 0] Seed set to 6
[2024-06-21 10:46:51,012][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 10:46:51,013][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-21 10:46:51,014][train.py][INFO] - Instantiating callbacks...
[2024-06-21 10:46:51,015][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 10:46:51,017][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 10:46:51,017][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 10:46:51,018][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 10:46:51,020][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 10:46:51,020][train.py][INFO] - Instantiating loggers...
[2024-06-21 10:46:51,021][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 10:46:51,022][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 10:46:51,091][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/travel_lot/0.2/6/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.23it/s v_num: 0.000      
                                                              val/auc: 0.461    
                                                              val/f1: 0.286     
                                                              val/precision:    
                                                              0.400 val/recall: 
                                                              0.222 val/mre:    
                                                              0.104 train/auc:  
                                                              0.983 train/f1:   
                                                              0.983             
                                                              train/precision:  
                                                              0.967             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.105             
[2024-06-21 10:48:42,560][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/travel_lot/0.2/6>
[2024-06-21 10:48:42,560][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 10:48:42,563][HYDRA] 	#537 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=travel_lot data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=7
[rank: 0] Seed set to 7
[2024-06-21 10:48:42,746][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 10:48:42,748][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-21 10:48:42,749][train.py][INFO] - Instantiating callbacks...
[2024-06-21 10:48:42,749][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 10:48:42,751][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 10:48:42,752][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 10:48:42,752][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 10:48:42,755][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 10:48:42,755][train.py][INFO] - Instantiating loggers...
[2024-06-21 10:48:42,755][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 10:48:42,756][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 10:48:42,826][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/travel_lot/0.2/7/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.24it/s v_num: 0.000      
                                                              val/auc: 0.400    
                                                              val/f1: 0.000     
                                                              val/precision:    
                                                              0.000 val/recall: 
                                                              0.000 val/mre:    
                                                              0.101 train/auc:  
                                                              0.974 train/f1:   
                                                              0.973             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              0.948 train/mre:  
                                                              0.103             
[2024-06-21 10:50:32,981][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/travel_lot/0.2/7>
[2024-06-21 10:50:32,982][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 10:50:32,984][HYDRA] 	#538 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=travel_lot data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=8
[rank: 0] Seed set to 8
[2024-06-21 10:50:33,165][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 10:50:33,166][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-21 10:50:33,168][train.py][INFO] - Instantiating callbacks...
[2024-06-21 10:50:33,168][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 10:50:33,170][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 10:50:33,170][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 10:50:33,171][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 10:50:33,173][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 10:50:33,174][train.py][INFO] - Instantiating loggers...
[2024-06-21 10:50:33,174][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 10:50:33,175][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 10:50:33,239][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/travel_lot/0.2/8/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.26it/s v_num: 0.000      
                                                              val/auc: 0.406    
                                                              val/f1: 0.154     
                                                              val/precision:    
                                                              0.250 val/recall: 
                                                              0.111 val/mre:    
                                                              0.106 train/auc:  
                                                              0.974 train/f1:   
                                                              0.975             
                                                              train/precision:  
                                                              0.951             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.105             
[2024-06-21 10:52:23,890][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/travel_lot/0.2/8>
[2024-06-21 10:52:23,890][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 10:52:23,893][HYDRA] 	#539 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=travel_lot data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=9
[rank: 0] Seed set to 9
[2024-06-21 10:52:24,073][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 10:52:24,075][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-21 10:52:24,076][train.py][INFO] - Instantiating callbacks...
[2024-06-21 10:52:24,076][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 10:52:24,078][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 10:52:24,079][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 10:52:24,079][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 10:52:24,082][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 10:52:24,082][train.py][INFO] - Instantiating loggers...
[2024-06-21 10:52:24,082][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 10:52:24,083][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 10:52:24,150][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/travel_lot/0.2/9/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.24it/s v_num: 0.000      
                                                              val/auc: 0.572    
                                                              val/f1: 0.500     
                                                              val/precision:    
                                                              0.571 val/recall: 
                                                              0.444 val/mre:    
                                                              0.101 train/auc:  
                                                              0.991 train/f1:   
                                                              0.991             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              0.983 train/mre:  
                                                              0.103             
[2024-06-21 10:54:14,765][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/travel_lot/0.2/9>
[2024-06-21 10:54:14,765][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 10:54:14,769][HYDRA] 	#540 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=travel_lot data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=0
[2024-06-21 10:54:14,953][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 10:54:14,954][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-21 10:54:14,956][train.py][INFO] - Instantiating callbacks...
[2024-06-21 10:54:14,956][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 10:54:14,958][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 10:54:14,958][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 10:54:14,959][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 10:54:14,961][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 10:54:14,962][train.py][INFO] - Instantiating loggers...
[2024-06-21 10:54:14,962][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 10:54:14,963][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 10:54:15,030][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/travel_lot/0.25/0/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.25it/s v_num: 0.000      
                                                              val/auc: 0.511    
                                                              val/f1: 0.308     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.222 val/mre:    
                                                              0.111 train/auc:  
                                                              0.983 train/f1:   
                                                              0.983             
                                                              train/precision:  
                                                              0.967             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.108             
[2024-06-21 10:56:05,564][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/travel_lot/0.25/0>
[2024-06-21 10:56:05,564][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 10:56:05,567][HYDRA] 	#541 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=travel_lot data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=1
[rank: 0] Seed set to 1
[2024-06-21 10:56:05,754][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 10:56:05,755][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-21 10:56:05,756][train.py][INFO] - Instantiating callbacks...
[2024-06-21 10:56:05,757][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 10:56:05,759][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 10:56:05,759][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 10:56:05,760][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 10:56:05,762][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 10:56:05,762][train.py][INFO] - Instantiating loggers...
[2024-06-21 10:56:05,763][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 10:56:05,764][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 10:56:05,831][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/travel_lot/0.25/1/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.26it/s v_num: 0.000      
                                                              val/auc: 0.567    
                                                              val/f1: 0.429     
                                                              val/precision:    
                                                              0.600 val/recall: 
                                                              0.333 val/mre:    
                                                              0.105 train/auc:  
                                                              0.905 train/f1:   
                                                              0.899             
                                                              train/precision:  
                                                              0.961             
                                                              train/recall:     
                                                              0.845 train/mre:  
                                                              0.107             
[2024-06-21 10:57:55,396][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/travel_lot/0.25/1>
[2024-06-21 10:57:55,396][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 10:57:55,399][HYDRA] 	#542 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=travel_lot data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=2
[rank: 0] Seed set to 2
[2024-06-21 10:57:55,582][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 10:57:55,583][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-21 10:57:55,584][train.py][INFO] - Instantiating callbacks...
[2024-06-21 10:57:55,585][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 10:57:55,587][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 10:57:55,587][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 10:57:55,588][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 10:57:55,591][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 10:57:55,591][train.py][INFO] - Instantiating loggers...
[2024-06-21 10:57:55,591][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 10:57:55,593][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 10:57:55,658][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/travel_lot/0.25/2/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.24it/s v_num: 0.000      
                                                              val/auc: 0.511    
                                                              val/f1: 0.308     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.222 val/mre:    
                                                              0.109 train/auc:  
                                                              0.940 train/f1:   
                                                              0.943             
                                                              train/precision:  
                                                              0.892             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.112             
[2024-06-21 10:59:45,922][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/travel_lot/0.25/2>
[2024-06-21 10:59:45,922][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 10:59:45,925][HYDRA] 	#543 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=travel_lot data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=3
[rank: 0] Seed set to 3
[2024-06-21 10:59:46,106][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 10:59:46,108][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-21 10:59:46,109][train.py][INFO] - Instantiating callbacks...
[2024-06-21 10:59:46,109][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 10:59:46,111][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 10:59:46,112][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 10:59:46,112][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 10:59:46,115][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 10:59:46,115][train.py][INFO] - Instantiating loggers...
[2024-06-21 10:59:46,116][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 10:59:46,117][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 10:59:46,180][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/travel_lot/0.25/3/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.23it/s v_num: 0.000      
                                                              val/auc: 0.506    
                                                              val/f1: 0.182     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.111 val/mre:    
                                                              0.106 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.104             
[2024-06-21 11:01:37,618][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/travel_lot/0.25/3>
[2024-06-21 11:01:37,618][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 11:01:37,620][HYDRA] 	#544 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=travel_lot data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=4
[rank: 0] Seed set to 4
[2024-06-21 11:01:37,803][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 11:01:37,805][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-21 11:01:37,806][train.py][INFO] - Instantiating callbacks...
[2024-06-21 11:01:37,806][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 11:01:37,808][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 11:01:37,809][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 11:01:37,809][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 11:01:37,812][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 11:01:37,813][train.py][INFO] - Instantiating loggers...
[2024-06-21 11:01:37,813][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 11:01:37,814][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 11:01:37,879][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/travel_lot/0.25/4/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.24it/s v_num: 0.000      
                                                              val/auc: 0.456    
                                                              val/f1: 0.167     
                                                              val/precision:    
                                                              0.333 val/recall: 
                                                              0.111 val/mre:    
                                                              0.107 train/auc:  
                                                              0.983 train/f1:   
                                                              0.983             
                                                              train/precision:  
                                                              0.983             
                                                              train/recall:     
                                                              0.983 train/mre:  
                                                              0.110             
[2024-06-21 11:03:29,275][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/travel_lot/0.25/4>
[2024-06-21 11:03:29,276][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 11:03:29,279][HYDRA] 	#545 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=travel_lot data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=5
[rank: 0] Seed set to 5
[2024-06-21 11:03:29,461][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 11:03:29,462][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-21 11:03:29,464][train.py][INFO] - Instantiating callbacks...
[2024-06-21 11:03:29,464][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 11:03:29,466][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 11:03:29,466][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 11:03:29,467][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 11:03:29,469][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 11:03:29,470][train.py][INFO] - Instantiating loggers...
[2024-06-21 11:03:29,470][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 11:03:29,471][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 11:03:29,538][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/travel_lot/0.25/5/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.24it/s v_num: 0.000      
                                                              val/auc: 0.572    
                                                              val/f1: 0.500     
                                                              val/precision:    
                                                              0.571 val/recall: 
                                                              0.444 val/mre:    
                                                              0.102 train/auc:  
                                                              0.974 train/f1:   
                                                              0.973             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              0.948 train/mre:  
                                                              0.107             
[2024-06-21 11:05:20,311][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/travel_lot/0.25/5>
[2024-06-21 11:05:20,311][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 11:05:20,314][HYDRA] 	#546 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=travel_lot data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=6
[rank: 0] Seed set to 6
[2024-06-21 11:05:20,497][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 11:05:20,498][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-21 11:05:20,499][train.py][INFO] - Instantiating callbacks...
[2024-06-21 11:05:20,500][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 11:05:20,502][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 11:05:20,502][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 11:05:20,503][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 11:05:20,505][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 11:05:20,506][train.py][INFO] - Instantiating loggers...
[2024-06-21 11:05:20,506][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 11:05:20,507][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 11:05:20,586][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/travel_lot/0.25/6/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.25it/s v_num: 0.000      
                                                              val/auc: 0.422    
                                                              val/f1: 0.421     
                                                              val/precision:    
                                                              0.400 val/recall: 
                                                              0.444 val/mre:    
                                                              0.105 train/auc:  
                                                              0.879 train/f1:   
                                                              0.883             
                                                              train/precision:  
                                                              0.855             
                                                              train/recall:     
                                                              0.914 train/mre:  
                                                              0.109             
[2024-06-21 11:07:11,223][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/travel_lot/0.25/6>
[2024-06-21 11:07:11,223][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 11:07:11,226][HYDRA] 	#547 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=travel_lot data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=7
[rank: 0] Seed set to 7
[2024-06-21 11:07:11,630][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 11:07:11,632][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-21 11:07:11,633][train.py][INFO] - Instantiating callbacks...
[2024-06-21 11:07:11,633][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 11:07:11,638][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 11:07:11,639][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 11:07:11,639][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 11:07:11,642][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 11:07:11,643][train.py][INFO] - Instantiating loggers...
[2024-06-21 11:07:11,643][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 11:07:11,644][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 11:07:12,069][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/travel_lot/0.25/7/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.22it/s v_num: 0.000      
                                                              val/auc: 0.406    
                                                              val/f1: 0.154     
                                                              val/precision:    
                                                              0.250 val/recall: 
                                                              0.111 val/mre:    
                                                              0.110 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.111             
[2024-06-21 11:09:04,304][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/travel_lot/0.25/7>
[2024-06-21 11:09:04,304][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 11:09:04,307][HYDRA] 	#548 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=travel_lot data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=8
[rank: 0] Seed set to 8
[2024-06-21 11:09:04,494][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 11:09:04,495][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-21 11:09:04,497][train.py][INFO] - Instantiating callbacks...
[2024-06-21 11:09:04,497][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 11:09:04,499][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 11:09:04,499][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 11:09:04,500][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 11:09:04,503][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 11:09:04,503][train.py][INFO] - Instantiating loggers...
[2024-06-21 11:09:04,503][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 11:09:04,504][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 11:09:04,574][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/travel_lot/0.25/8/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.23it/s v_num: 0.000      
                                                              val/auc: 0.511    
                                                              val/f1: 0.308     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.222 val/mre:    
                                                              0.111 train/auc:  
                                                              0.974 train/f1:   
                                                              0.975             
                                                              train/precision:  
                                                              0.951             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.112             
[2024-06-21 11:10:55,795][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/travel_lot/0.25/8>
[2024-06-21 11:10:55,795][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 11:10:55,799][HYDRA] 	#549 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=travel_lot data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=9
[rank: 0] Seed set to 9
[2024-06-21 11:10:55,984][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 11:10:55,985][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-21 11:10:55,986][train.py][INFO] - Instantiating callbacks...
[2024-06-21 11:10:55,987][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 11:10:55,989][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 11:10:55,989][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 11:10:55,990][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 11:10:55,992][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 11:10:55,993][train.py][INFO] - Instantiating loggers...
[2024-06-21 11:10:55,993][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 11:10:55,994][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 11:10:56,062][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/travel_lot/0.25/9/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.24it/s v_num: 0.000      
                                                              val/auc: 0.517    
                                                              val/f1: 0.400     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.333 val/mre:    
                                                              0.107 train/auc:  
                                                              0.983 train/f1:   
                                                              0.983             
                                                              train/precision:  
                                                              0.983             
                                                              train/recall:     
                                                              0.983 train/mre:  
                                                              0.110             
[2024-06-21 11:12:46,471][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/travel_lot/0.25/9>
[2024-06-21 11:12:46,472][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 11:12:46,475][HYDRA] 	#550 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=travel_lot data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=0
[2024-06-21 11:12:46,689][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 11:12:46,690][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-21 11:12:46,692][train.py][INFO] - Instantiating callbacks...
[2024-06-21 11:12:46,692][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 11:12:46,694][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 11:12:46,694][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 11:12:46,695][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 11:12:46,697][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 11:12:46,698][train.py][INFO] - Instantiating loggers...
[2024-06-21 11:12:46,698][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 11:12:46,699][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 11:12:46,764][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/travel_lot/0.3/0/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.22it/s v_num: 0.000      
                                                              val/auc: 0.250    
                                                              val/f1: 0.000     
                                                              val/precision:    
                                                              0.000 val/recall: 
                                                              0.000 val/mre:    
                                                              0.112 train/auc:  
                                                              0.983 train/f1:   
                                                              0.983             
                                                              train/precision:  
                                                              0.967             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.112             
[2024-06-21 11:14:36,768][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/travel_lot/0.3/0>
[2024-06-21 11:14:36,768][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 11:14:36,771][HYDRA] 	#551 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=travel_lot data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=1
[rank: 0] Seed set to 1
[2024-06-21 11:14:36,970][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 11:14:36,971][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-21 11:14:36,972][train.py][INFO] - Instantiating callbacks...
[2024-06-21 11:14:36,973][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 11:14:36,975][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 11:14:36,975][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 11:14:36,976][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 11:14:36,979][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 11:14:36,979][train.py][INFO] - Instantiating loggers...
[2024-06-21 11:14:36,979][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 11:14:36,980][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 11:14:37,075][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/travel_lot/0.3/1/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.22it/s v_num: 0.000      
                                                              val/auc: 0.622    
                                                              val/f1: 0.533     
                                                              val/precision:    
                                                              0.667 val/recall: 
                                                              0.444 val/mre:    
                                                              0.111 train/auc:  
                                                              0.940 train/f1:   
                                                              0.943             
                                                              train/precision:  
                                                              0.892             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.115             
[2024-06-21 11:16:28,976][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/travel_lot/0.3/1>
[2024-06-21 11:16:28,976][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 11:16:28,979][HYDRA] 	#552 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=travel_lot data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=2
[rank: 0] Seed set to 2
[2024-06-21 11:16:29,166][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 11:16:29,168][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-21 11:16:29,169][train.py][INFO] - Instantiating callbacks...
[2024-06-21 11:16:29,169][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 11:16:29,171][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 11:16:29,172][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 11:16:29,172][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 11:16:29,174][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 11:16:29,175][train.py][INFO] - Instantiating loggers...
[2024-06-21 11:16:29,175][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 11:16:29,176][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 11:16:29,243][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/travel_lot/0.3/2/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.24it/s v_num: 0.000      
                                                              val/auc: 0.367    
                                                              val/f1: 0.333     
                                                              val/precision:    
                                                              0.333 val/recall: 
                                                              0.333 val/mre:    
                                                              0.113 train/auc:  
                                                              0.974 train/f1:   
                                                              0.975             
                                                              train/precision:  
                                                              0.951             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.115             
[2024-06-21 11:18:20,377][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/travel_lot/0.3/2>
[2024-06-21 11:18:20,377][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 11:18:20,380][HYDRA] 	#553 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=travel_lot data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=3
[rank: 0] Seed set to 3
[2024-06-21 11:18:20,568][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 11:18:20,570][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-21 11:18:20,571][train.py][INFO] - Instantiating callbacks...
[2024-06-21 11:18:20,571][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 11:18:20,573][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 11:18:20,574][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 11:18:20,574][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 11:18:20,577][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 11:18:20,577][train.py][INFO] - Instantiating loggers...
[2024-06-21 11:18:20,577][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 11:18:20,578][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 11:18:20,645][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/travel_lot/0.3/3/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.25it/s v_num: 0.000      
                                                              val/auc: 0.506    
                                                              val/f1: 0.182     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.111 val/mre:    
                                                              0.120 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.113             
[2024-06-21 11:20:10,066][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/travel_lot/0.3/3>
[2024-06-21 11:20:10,066][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 11:20:10,069][HYDRA] 	#554 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=travel_lot data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=4
[rank: 0] Seed set to 4
[2024-06-21 11:20:10,254][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 11:20:10,255][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-21 11:20:10,257][train.py][INFO] - Instantiating callbacks...
[2024-06-21 11:20:10,257][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 11:20:10,259][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 11:20:10,259][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 11:20:10,260][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 11:20:10,262][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 11:20:10,263][train.py][INFO] - Instantiating loggers...
[2024-06-21 11:20:10,263][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 11:20:10,264][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 11:20:10,331][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/travel_lot/0.3/4/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.24it/s v_num: 0.000      
                                                              val/auc: 0.561    
                                                              val/f1: 0.333     
                                                              val/precision:    
                                                              0.667 val/recall: 
                                                              0.222 val/mre:    
                                                              0.106 train/auc:  
                                                              0.966 train/f1:   
                                                              0.966             
                                                              train/precision:  
                                                              0.950             
                                                              train/recall:     
                                                              0.983 train/mre:  
                                                              0.105             
[2024-06-21 11:22:01,404][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/travel_lot/0.3/4>
[2024-06-21 11:22:01,404][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 11:22:01,407][HYDRA] 	#555 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=travel_lot data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=5
[rank: 0] Seed set to 5
[2024-06-21 11:22:01,599][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 11:22:01,600][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-21 11:22:01,602][train.py][INFO] - Instantiating callbacks...
[2024-06-21 11:22:01,602][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 11:22:01,604][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 11:22:01,604][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 11:22:01,605][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 11:22:01,608][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 11:22:01,608][train.py][INFO] - Instantiating loggers...
[2024-06-21 11:22:01,608][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 11:22:01,610][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 11:22:01,697][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/travel_lot/0.3/5/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.24it/s v_num: 0.000      
                                                              val/auc: 0.517    
                                                              val/f1: 0.400     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.333 val/mre:    
                                                              0.117 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.117             
[2024-06-21 11:23:53,616][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/travel_lot/0.3/5>
[2024-06-21 11:23:53,616][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 11:23:53,619][HYDRA] 	#556 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=travel_lot data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=6
[rank: 0] Seed set to 6
[2024-06-21 11:23:53,810][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 11:23:53,812][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-21 11:23:53,813][train.py][INFO] - Instantiating callbacks...
[2024-06-21 11:23:53,813][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 11:23:53,815][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 11:23:53,816][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 11:23:53,816][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 11:23:53,819][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 11:23:53,819][train.py][INFO] - Instantiating loggers...
[2024-06-21 11:23:53,819][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 11:23:53,821][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 11:23:53,889][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/travel_lot/0.3/6/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.24it/s v_num: 0.000      
                                                              val/auc: 0.361    
                                                              val/f1: 0.250     
                                                              val/precision:    
                                                              0.286 val/recall: 
                                                              0.222 val/mre:    
                                                              0.105 train/auc:  
                                                              0.940 train/f1:   
                                                              0.943             
                                                              train/precision:  
                                                              0.892             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.109             
[2024-06-21 11:25:44,810][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/travel_lot/0.3/6>
[2024-06-21 11:25:44,811][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 11:25:44,813][HYDRA] 	#557 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=travel_lot data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=7
[rank: 0] Seed set to 7
[2024-06-21 11:25:45,001][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 11:25:45,003][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-21 11:25:45,004][train.py][INFO] - Instantiating callbacks...
[2024-06-21 11:25:45,004][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 11:25:45,006][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 11:25:45,007][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 11:25:45,007][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 11:25:45,010][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 11:25:45,010][train.py][INFO] - Instantiating loggers...
[2024-06-21 11:25:45,011][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 11:25:45,012][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 11:25:45,080][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/travel_lot/0.3/7/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.25it/s v_num: 0.000      
                                                              val/auc: 0.506    
                                                              val/f1: 0.182     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.111 val/mre:    
                                                              0.112 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.111             
[2024-06-21 11:27:35,263][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/travel_lot/0.3/7>
[2024-06-21 11:27:35,264][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 11:27:35,266][HYDRA] 	#558 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=travel_lot data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=8
[rank: 0] Seed set to 8
[2024-06-21 11:27:35,455][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 11:27:35,456][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-21 11:27:35,458][train.py][INFO] - Instantiating callbacks...
[2024-06-21 11:27:35,458][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 11:27:35,460][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 11:27:35,460][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 11:27:35,461][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 11:27:35,463][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 11:27:35,464][train.py][INFO] - Instantiating loggers...
[2024-06-21 11:27:35,464][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 11:27:35,465][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 11:27:35,540][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/travel_lot/0.3/8/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.24it/s v_num: 0.000      
                                                              val/auc: 0.467    
                                                              val/f1: 0.375     
                                                              val/precision:    
                                                              0.429 val/recall: 
                                                              0.333 val/mre:    
                                                              0.116 train/auc:  
                                                              0.991 train/f1:   
                                                              0.991             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              0.983 train/mre:  
                                                              0.115             
[2024-06-21 11:29:24,611][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/travel_lot/0.3/8>
[2024-06-21 11:29:24,611][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-21 11:29:24,613][HYDRA] 	#559 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=travel_lot data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=usgan callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=9
[rank: 0] Seed set to 9
[2024-06-21 11:29:24,795][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-21 11:29:24,797][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-06-21 11:29:24,798][train.py][INFO] - Instantiating callbacks...
[2024-06-21 11:29:24,798][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-21 11:29:24,800][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-21 11:29:24,801][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-21 11:29:24,801][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-21 11:29:24,803][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-21 11:29:24,804][train.py][INFO] - Instantiating loggers...
[2024-06-21 11:29:24,804][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-21 11:29:24,805][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-21 11:29:24,873][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/travel_lot/0.3/9/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  1.6 M │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  1.6 M │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  784 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │  595 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │ 41.6 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  784 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │  595 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │ 41.7 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │ 41.6 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │ 15.4 K │
│ 17 │ generator.W_s1                          │ Linear             │  7.7 K │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │  977 K │
│ 20 │ discriminator.biRNN                     │ GRU                │  893 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 83.1 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 2.6 M                                                         
Non-trainable params: 0                                                         
Total params: 2.6 M                                                             
Total estimated model params size (MB): 10                                      
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.23it/s v_num: 0.000      
                                                              val/auc: 0.467    
                                                              val/f1: 0.375     
                                                              val/precision:    
                                                              0.429 val/recall: 
                                                              0.333 val/mre:    
                                                              0.103 train/auc:  
                                                              0.931 train/f1:   
                                                              0.935             
                                                              train/precision:  
                                                              0.879             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.110             
[2024-06-21 11:31:16,844][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-20-17-06-51/travel_lot/0.3/9>
[2024-06-21 11:31:16,845][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
Error executing job with overrides: ['task=debug', 'trainer=cpu', 'trainer.max_epochs=50', 'data=daicwoz', 'data.type_missing=Random', 'data.ricardo=True', 'data.question=dream_job', 'data.open_face=all', 'data.ratio_missing=0.15', 'data.num_workers=1', 'data.batch_size=32', 'model=usgan', 'callbacks=[default,imputation_metrics]', 'callbacks.model_checkpoint.monitor=val/f1', 'callbacks.clf_metrics.boot_val=False', 'callbacks.imputation_metrics.boot_val=False', 'logger=csv', 'test=False', 'seed=4']
Traceback (most recent call last):
  File "/home/khickey/test_impute/train.py", line 55, in <module>
    main()
  File "/home/khickey/test_impute/env/lib/python3.12/site-packages/hydra/main.py", line 94, in decorated_main
    _run_hydra(
  File "/home/khickey/test_impute/env/lib/python3.12/site-packages/hydra/_internal/utils.py", line 394, in _run_hydra
    _run_app(
  File "/home/khickey/test_impute/env/lib/python3.12/site-packages/hydra/_internal/utils.py", line 465, in _run_app
    run_and_report(
  File "/home/khickey/test_impute/env/lib/python3.12/site-packages/hydra/_internal/utils.py", line 223, in run_and_report
    raise ex
  File "/home/khickey/test_impute/env/lib/python3.12/site-packages/hydra/_internal/utils.py", line 220, in run_and_report
    return func()
           ^^^^^^
  File "/home/khickey/test_impute/env/lib/python3.12/site-packages/hydra/_internal/utils.py", line 466, in <lambda>
    lambda: hydra.multirun(
            ^^^^^^^^^^^^^^^
  File "/home/khickey/test_impute/env/lib/python3.12/site-packages/hydra/_internal/hydra.py", line 162, in multirun
    ret = sweeper.sweep(arguments=task_overrides)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/khickey/test_impute/env/lib/python3.12/site-packages/hydra/_internal/core_plugins/basic_sweeper.py", line 181, in sweep
    _ = r.return_value
        ^^^^^^^^^^^^^^
  File "/home/khickey/test_impute/env/lib/python3.12/site-packages/hydra/core/utils.py", line 260, in return_value
    raise self._return_value
  File "/home/khickey/test_impute/env/lib/python3.12/site-packages/hydra/core/utils.py", line 186, in run_job
    ret.return_value = task_function(task_cfg)
                       ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/khickey/test_impute/train.py", line 16, in main
    metric_dict = run(cfg=cfg)
                  ^^^^^^^^^^^^
  File "/home/khickey/test_impute/train.py", line 43, in run
    trainer.fit(model=model, datamodule=dm)
  File "/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py", line 544, in fit
    call._call_and_handle_interrupt(
  File "/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/trainer/call.py", line 44, in _call_and_handle_interrupt
    return trainer_fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py", line 580, in _fit_impl
    self._run(model, ckpt_path=ckpt_path)
  File "/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py", line 949, in _run
    call._call_setup_hook(self)  # allow user to set up LightningModule in accelerator environment
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/trainer/call.py", line 92, in _call_setup_hook
    _call_lightning_datamodule_hook(trainer, "setup", stage=fn)
  File "/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/trainer/call.py", line 179, in _call_lightning_datamodule_hook
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/khickey/test_impute/src/datamodules/daicwoz.py", line 297, in setup
    self.dataset = DAICWOZDataset(
                   ^^^^^^^^^^^^^^^
  File "/home/khickey/test_impute/src/datamodules/daicwoz.py", line 341, in __init__
    self.all_data = torch.load(self.data_file_path)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/khickey/test_impute/env/lib/python3.12/site-packages/torch/serialization.py", line 1040, in load
    return _legacy_load(opened_file, map_location, pickle_module, **pickle_load_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/khickey/test_impute/env/lib/python3.12/site-packages/torch/serialization.py", line 1258, in _legacy_load
    magic_number = pickle_module.load(f, **pickle_load_args)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
EOFError: Ran out of input
srun: error: compute-2-01: task 0: Exited with exit code 1
