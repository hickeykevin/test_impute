[2024-06-24 10:02:38,433][HYDRA] Launching 490 jobs locally
[2024-06-24 10:02:38,433][HYDRA] 	#0 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=False data.question=advice_yourself data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=saits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=True seed=0
[2024-06-24 10:02:38,622][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-24 10:02:40,128][train.py][INFO] - Instantiating model <src.methods.saits.lightningmodule.SAITSLightningModule>
2024-06-24 10:02:41 [ERROR]: ❌ No module named 'torch_geometric'
Note torch_geometric is missing, please install it with 'pip install torch_geometric torch_scatter torch_sparse' or 'conda install -c pyg pyg pytorch-scatter pytorch-sparse'
2024-06-24 10:02:41 [ERROR]: ❌ name 'MessagePassing' is not defined
Note torch_geometric is missing, please install it with 'pip install torch_geometric torch_scatter torch_sparse' or 'conda install -c pyg pyg pytorch-scatter pytorch-sparse'
[2024-06-24 10:02:41,336][train.py][INFO] - Instantiating callbacks...
[2024-06-24 10:02:41,337][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-24 10:02:41,343][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-24 10:02:41,344][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-24 10:02:41,346][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-24 10:02:41,347][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-24 10:02:41,347][train.py][INFO] - Instantiating loggers...
[2024-06-24 10:02:41,347][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-24 10:02:41,349][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
/home/khickey/test_impute/env/lib/python3.12/site-packages/torch/cuda/__init__.py:628: UserWarning: Can't initialize NVML
  warnings.warn("Can't initialize NVML")
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-24 10:02:42,032][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-24-10-06-32/advice_yourself/0.0/0/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳┳━┓
┃┃ Name                                                                     ┃┃ ┃
┡╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇╇━┩
││ model                                                                    ││ │
││ model.layer_stack_for_first_block                                        ││ │
││ model.layer_stack_for_first_block.0                                      ││ │
││ model.layer_stack_for_first_block.0.slf_attn                             ││ │
││ model.layer_stack_for_first_block.0.slf_attn.w_qs                        ││ │
││ model.layer_stack_for_first_block.0.slf_attn.w_ks                        ││ │
││ model.layer_stack_for_first_block.0.slf_attn.w_vs                        ││ │
││ model.layer_stack_for_first_block.0.slf_attn.attention_operator          ││ │
││ model.layer_stack_for_first_block.0.slf_attn.attention_operator.dropout  ││ │
││ model.layer_stack_for_first_block.0.slf_attn.fc                          ││ │
││ model.layer_stack_for_first_block.0.dropout                              ││ │
││ model.layer_stack_for_first_block.0.layer_norm                           ││ │
││ model.layer_stack_for_first_block.0.pos_ffn                              ││ │
││ model.layer_stack_for_first_block.0.pos_ffn.linear_1                     ││ │
││ model.layer_stack_for_first_block.0.pos_ffn.linear_2                     ││ │
││ model.layer_stack_for_first_block.0.pos_ffn.layer_norm                   ││ │
││ model.layer_stack_for_first_block.0.pos_ffn.dropout                      ││ │
││ model.layer_stack_for_second_block                                       ││ │
││ model.layer_stack_for_second_block.0                                     ││ │
││ model.layer_stack_for_second_block.0.slf_attn                            ││ │
││ model.layer_stack_for_second_block.0.slf_attn.w_qs                       ││ │
││ model.layer_stack_for_second_block.0.slf_attn.w_ks                       ││ │
││ model.layer_stack_for_second_block.0.slf_attn.w_vs                       ││ │
││ model.layer_stack_for_second_block.0.slf_attn.attention_operator         ││ │
││ model.layer_stack_for_second_block.0.slf_attn.attention_operator.dropout ││ │
││ model.layer_stack_for_second_block.0.slf_attn.fc                         ││ │
││ model.layer_stack_for_second_block.0.dropout                             ││ │
││ model.layer_stack_for_second_block.0.layer_norm                          ││ │
││ model.layer_stack_for_second_block.0.pos_ffn                             ││ │
││ model.layer_stack_for_second_block.0.pos_ffn.linear_1                    ││ │
││ model.layer_stack_for_second_block.0.pos_ffn.linear_2                    ││ │
││ model.layer_stack_for_second_block.0.pos_ffn.layer_norm                  ││ │
││ model.layer_stack_for_second_block.0.pos_ffn.dropout                     ││ │
││ model.dropout                                                            ││ │
││ model.position_enc                                                       ││ │
││ model.embedding_1                                                        ││ │
││ model.reduce_dim_z                                                       ││ │
││ model.embedding_2                                                        ││ │
││ model.reduce_dim_beta                                                    ││ │
││ model.reduce_dim_gamma                                                   ││ │
││ model.weight_combine                                                     ││ │
││ model.output_projection                                                  ││ │
││ model.self_attention                                                     ││ │
││ model.output_net                                                         ││ │
││ model.output_net.0                                                       ││ │
││ model.output_net.1                                                       ││ │
││ model.output_net.2                                                       ││ │
││ model.output_net.3                                                       ││ │
││ model.output_net.4                                                       ││ │
││ clf_criterion                                                            ││ │
││ saits_loss_func                                                          ││ │
└┴──────────────────────────────────────────────────────────────────────────┴┴─┘
Trainable params: 107 K                                                         
Non-trainable params: 0                                                         
Total params: 107 K                                                             
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 26.44it/s v_num: 0.000      
                                                              val/auc: 0.833    
                                                              val/f1: 0.778     
                                                              val/precision:    
                                                              0.636 val/recall: 
                                                              1.000 val/mre: nan
                                                              train/auc: 0.703  
                                                              train/f1: 0.688   
                                                              train/precision:  
                                                              0.767             
                                                              train/recall:     
                                                              0.623 train/mre:  
                                                              nan               
[2024-06-24 10:03:04,630][train.py][INFO] - Starting testing...
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
Restoring states from the checkpoint path at /home/khickey/test_impute/logs/debug/multiruns/06-24-10-06-32/advice_yourself/0.0/0/checkpoints/49-0.00.ckpt
Loaded model weights from the checkpoint at /home/khickey/test_impute/logs/debug/multiruns/06-24-10-06-32/advice_yourself/0.0/0/checkpoints/49-0.00.ckpt
SLURM auto-requeueing enabled. Setting signal handlers.
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃        Test metric        ┃       DataLoader 0        ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
│       test/auc_mean       │    0.5775389671325684     │
│       test/auc_std        │     0.114631287753582     │
│       test/f1_mean        │    0.5906173586845398     │
│        test/f1_std        │    0.1340114027261734     │
│         test/mre          │            nan            │
│        test/mre_ci        │            nan            │
│    test/precision_mean    │    0.5475606918334961     │
│    test/precision_std     │     0.156452015042305     │
│     test/recall_mean      │    0.6609560251235962     │
│      test/recall_std      │    0.16381937265396118    │
└───────────────────────────┴───────────────────────────┘
Testing ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1/1 0:00:00 • 0:00:00 0.00it/s 
[2024-06-24 10:03:18,864][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-24-10-06-32/advice_yourself/0.0/0>
[2024-06-24 10:03:18,865][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-24 10:03:18,867][HYDRA] 	#1 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=False data.question=advice_yourself data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=saits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=True seed=1
[rank: 0] Seed set to 1
[2024-06-24 10:03:19,068][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-24 10:03:19,070][train.py][INFO] - Instantiating model <src.methods.saits.lightningmodule.SAITSLightningModule>
[2024-06-24 10:03:19,072][train.py][INFO] - Instantiating callbacks...
[2024-06-24 10:03:19,072][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-24 10:03:19,075][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-24 10:03:19,076][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-24 10:03:19,076][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-24 10:03:19,077][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-24 10:03:19,078][train.py][INFO] - Instantiating loggers...
[2024-06-24 10:03:19,078][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-24 10:03:19,079][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-24 10:03:19,112][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-24-10-06-32/advice_yourself/0.0/1/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳┳━┓
┃┃ Name                                                                     ┃┃ ┃
┡╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇╇━┩
││ model                                                                    ││ │
││ model.layer_stack_for_first_block                                        ││ │
││ model.layer_stack_for_first_block.0                                      ││ │
││ model.layer_stack_for_first_block.0.slf_attn                             ││ │
││ model.layer_stack_for_first_block.0.slf_attn.w_qs                        ││ │
││ model.layer_stack_for_first_block.0.slf_attn.w_ks                        ││ │
││ model.layer_stack_for_first_block.0.slf_attn.w_vs                        ││ │
││ model.layer_stack_for_first_block.0.slf_attn.attention_operator          ││ │
││ model.layer_stack_for_first_block.0.slf_attn.attention_operator.dropout  ││ │
││ model.layer_stack_for_first_block.0.slf_attn.fc                          ││ │
││ model.layer_stack_for_first_block.0.dropout                              ││ │
││ model.layer_stack_for_first_block.0.layer_norm                           ││ │
││ model.layer_stack_for_first_block.0.pos_ffn                              ││ │
││ model.layer_stack_for_first_block.0.pos_ffn.linear_1                     ││ │
││ model.layer_stack_for_first_block.0.pos_ffn.linear_2                     ││ │
││ model.layer_stack_for_first_block.0.pos_ffn.layer_norm                   ││ │
││ model.layer_stack_for_first_block.0.pos_ffn.dropout                      ││ │
││ model.layer_stack_for_second_block                                       ││ │
││ model.layer_stack_for_second_block.0                                     ││ │
││ model.layer_stack_for_second_block.0.slf_attn                            ││ │
││ model.layer_stack_for_second_block.0.slf_attn.w_qs                       ││ │
││ model.layer_stack_for_second_block.0.slf_attn.w_ks                       ││ │
││ model.layer_stack_for_second_block.0.slf_attn.w_vs                       ││ │
││ model.layer_stack_for_second_block.0.slf_attn.attention_operator         ││ │
││ model.layer_stack_for_second_block.0.slf_attn.attention_operator.dropout ││ │
││ model.layer_stack_for_second_block.0.slf_attn.fc                         ││ │
││ model.layer_stack_for_second_block.0.dropout                             ││ │
││ model.layer_stack_for_second_block.0.layer_norm                          ││ │
││ model.layer_stack_for_second_block.0.pos_ffn                             ││ │
││ model.layer_stack_for_second_block.0.pos_ffn.linear_1                    ││ │
││ model.layer_stack_for_second_block.0.pos_ffn.linear_2                    ││ │
││ model.layer_stack_for_second_block.0.pos_ffn.layer_norm                  ││ │
││ model.layer_stack_for_second_block.0.pos_ffn.dropout                     ││ │
││ model.dropout                                                            ││ │
││ model.position_enc                                                       ││ │
││ model.embedding_1                                                        ││ │
││ model.reduce_dim_z                                                       ││ │
││ model.embedding_2                                                        ││ │
││ model.reduce_dim_beta                                                    ││ │
││ model.reduce_dim_gamma                                                   ││ │
││ model.weight_combine                                                     ││ │
││ model.output_projection                                                  ││ │
││ model.self_attention                                                     ││ │
││ model.output_net                                                         ││ │
││ model.output_net.0                                                       ││ │
││ model.output_net.1                                                       ││ │
││ model.output_net.2                                                       ││ │
││ model.output_net.3                                                       ││ │
││ model.output_net.4                                                       ││ │
││ clf_criterion                                                            ││ │
││ saits_loss_func                                                          ││ │
└┴──────────────────────────────────────────────────────────────────────────┴┴─┘
Trainable params: 107 K                                                         
Non-trainable params: 0                                                         
Total params: 107 K                                                             
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 26.54it/s v_num: 0.000      
                                                              val/auc: 0.833    
                                                              val/f1: 0.800     
                                                              val/precision:    
                                                              1.000 val/recall: 
                                                              0.667 val/mre: nan
                                                              train/auc: 0.783  
                                                              train/f1: 0.761   
                                                              train/precision:  
                                                              0.875             
                                                              train/recall:     
                                                              0.673 train/mre:  
                                                              nan               
[2024-06-24 10:03:40,541][train.py][INFO] - Starting testing...
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
Restoring states from the checkpoint path at /home/khickey/test_impute/logs/debug/multiruns/06-24-10-06-32/advice_yourself/0.0/1/checkpoints/33-0.00.ckpt
Loaded model weights from the checkpoint at /home/khickey/test_impute/logs/debug/multiruns/06-24-10-06-32/advice_yourself/0.0/1/checkpoints/33-0.00.ckpt
SLURM auto-requeueing enabled. Setting signal handlers.
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃        Test metric        ┃       DataLoader 0        ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
│       test/auc_mean       │    0.5241078734397888     │
│       test/auc_std        │    0.11641143262386322    │
│       test/f1_mean        │    0.45328590273857117    │
│        test/f1_std        │    0.1594187617301941     │
│         test/mre          │            nan            │
│        test/mre_ci        │            nan            │
│    test/precision_mean    │    0.5017475485801697     │
│    test/precision_std     │    0.1809026598930359     │
│     test/recall_mean      │    0.44581401348114014    │
│      test/recall_std      │    0.17325402796268463    │
└───────────────────────────┴───────────────────────────┘
Testing ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1/1 0:00:00 • 0:00:00 0.00it/s 
[2024-06-24 10:03:55,265][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-24-10-06-32/advice_yourself/0.0/1>
[2024-06-24 10:03:55,265][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-24 10:03:55,268][HYDRA] 	#2 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=False data.question=advice_yourself data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=saits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=True seed=2
[rank: 0] Seed set to 2
[2024-06-24 10:03:55,465][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-24 10:03:55,467][train.py][INFO] - Instantiating model <src.methods.saits.lightningmodule.SAITSLightningModule>
[2024-06-24 10:03:55,469][train.py][INFO] - Instantiating callbacks...
[2024-06-24 10:03:55,469][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-24 10:03:55,472][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-24 10:03:55,473][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-24 10:03:55,473][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-24 10:03:55,476][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-24 10:03:55,477][train.py][INFO] - Instantiating loggers...
[2024-06-24 10:03:55,477][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-24 10:03:55,479][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-24 10:03:55,555][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-24-10-06-32/advice_yourself/0.0/2/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳┳━┓
┃┃ Name                                                                     ┃┃ ┃
┡╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇╇━┩
││ model                                                                    ││ │
││ model.layer_stack_for_first_block                                        ││ │
││ model.layer_stack_for_first_block.0                                      ││ │
││ model.layer_stack_for_first_block.0.slf_attn                             ││ │
││ model.layer_stack_for_first_block.0.slf_attn.w_qs                        ││ │
││ model.layer_stack_for_first_block.0.slf_attn.w_ks                        ││ │
││ model.layer_stack_for_first_block.0.slf_attn.w_vs                        ││ │
││ model.layer_stack_for_first_block.0.slf_attn.attention_operator          ││ │
││ model.layer_stack_for_first_block.0.slf_attn.attention_operator.dropout  ││ │
││ model.layer_stack_for_first_block.0.slf_attn.fc                          ││ │
││ model.layer_stack_for_first_block.0.dropout                              ││ │
││ model.layer_stack_for_first_block.0.layer_norm                           ││ │
││ model.layer_stack_for_first_block.0.pos_ffn                              ││ │
││ model.layer_stack_for_first_block.0.pos_ffn.linear_1                     ││ │
││ model.layer_stack_for_first_block.0.pos_ffn.linear_2                     ││ │
││ model.layer_stack_for_first_block.0.pos_ffn.layer_norm                   ││ │
││ model.layer_stack_for_first_block.0.pos_ffn.dropout                      ││ │
││ model.layer_stack_for_second_block                                       ││ │
││ model.layer_stack_for_second_block.0                                     ││ │
││ model.layer_stack_for_second_block.0.slf_attn                            ││ │
││ model.layer_stack_for_second_block.0.slf_attn.w_qs                       ││ │
││ model.layer_stack_for_second_block.0.slf_attn.w_ks                       ││ │
││ model.layer_stack_for_second_block.0.slf_attn.w_vs                       ││ │
││ model.layer_stack_for_second_block.0.slf_attn.attention_operator         ││ │
││ model.layer_stack_for_second_block.0.slf_attn.attention_operator.dropout ││ │
││ model.layer_stack_for_second_block.0.slf_attn.fc                         ││ │
││ model.layer_stack_for_second_block.0.dropout                             ││ │
││ model.layer_stack_for_second_block.0.layer_norm                          ││ │
││ model.layer_stack_for_second_block.0.pos_ffn                             ││ │
││ model.layer_stack_for_second_block.0.pos_ffn.linear_1                    ││ │
││ model.layer_stack_for_second_block.0.pos_ffn.linear_2                    ││ │
││ model.layer_stack_for_second_block.0.pos_ffn.layer_norm                  ││ │
││ model.layer_stack_for_second_block.0.pos_ffn.dropout                     ││ │
││ model.dropout                                                            ││ │
││ model.position_enc                                                       ││ │
││ model.embedding_1                                                        ││ │
││ model.reduce_dim_z                                                       ││ │
││ model.embedding_2                                                        ││ │
││ model.reduce_dim_beta                                                    ││ │
││ model.reduce_dim_gamma                                                   ││ │
││ model.weight_combine                                                     ││ │
││ model.output_projection                                                  ││ │
││ model.self_attention                                                     ││ │
││ model.output_net                                                         ││ │
││ model.output_net.0                                                       ││ │
││ model.output_net.1                                                       ││ │
││ model.output_net.2                                                       ││ │
││ model.output_net.3                                                       ││ │
││ model.output_net.4                                                       ││ │
││ clf_criterion                                                            ││ │
││ saits_loss_func                                                          ││ │
└┴──────────────────────────────────────────────────────────────────────────┴┴─┘
Trainable params: 107 K                                                         
Non-trainable params: 0                                                         
Total params: 107 K                                                             
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 27.79it/s v_num: 0.000      
                                                              val/auc: 0.556    
                                                              val/f1: 0.714     
                                                              val/precision:    
                                                              0.556 val/recall: 
                                                              1.000 val/mre: nan
                                                              train/auc: 0.750  
                                                              train/f1: 0.810   
                                                              train/precision:  
                                                              0.680             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              nan               
[2024-06-24 10:04:16,540][train.py][INFO] - Starting testing...
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
Restoring states from the checkpoint path at /home/khickey/test_impute/logs/debug/multiruns/06-24-10-06-32/advice_yourself/0.0/2/checkpoints/48-0.00.ckpt
Loaded model weights from the checkpoint at /home/khickey/test_impute/logs/debug/multiruns/06-24-10-06-32/advice_yourself/0.0/2/checkpoints/48-0.00.ckpt
SLURM auto-requeueing enabled. Setting signal handlers.
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃        Test metric        ┃       DataLoader 0        ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
│       test/auc_mean       │    0.5800874829292297     │
│       test/auc_std        │    0.11455496400594711    │
│       test/f1_mean        │    0.4925139546394348     │
│        test/f1_std        │    0.15533240139484406    │
│         test/mre          │            nan            │
│        test/mre_ci        │            nan            │
│    test/precision_mean    │    0.5660771727561951     │
│    test/precision_std     │    0.2022211253643036     │
│     test/recall_mean      │    0.43890783190727234    │
│      test/recall_std      │    0.17431284487247467    │
└───────────────────────────┴───────────────────────────┘
Testing ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1/1 0:00:00 • 0:00:00 0.00it/s 
[2024-06-24 10:04:31,650][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-24-10-06-32/advice_yourself/0.0/2>
[2024-06-24 10:04:31,650][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-24 10:04:31,652][HYDRA] 	#3 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=False data.question=advice_yourself data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=saits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=True seed=3
[rank: 0] Seed set to 3
[2024-06-24 10:04:31,849][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-24 10:04:31,851][train.py][INFO] - Instantiating model <src.methods.saits.lightningmodule.SAITSLightningModule>
[2024-06-24 10:04:31,853][train.py][INFO] - Instantiating callbacks...
[2024-06-24 10:04:31,853][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-24 10:04:31,856][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-24 10:04:31,857][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-24 10:04:31,857][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-24 10:04:31,858][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-24 10:04:31,859][train.py][INFO] - Instantiating loggers...
[2024-06-24 10:04:31,859][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-24 10:04:31,860][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-24 10:04:31,892][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-24-10-06-32/advice_yourself/0.0/3/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳┳━┓
┃┃ Name                                                                     ┃┃ ┃
┡╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇╇━┩
││ model                                                                    ││ │
││ model.layer_stack_for_first_block                                        ││ │
││ model.layer_stack_for_first_block.0                                      ││ │
││ model.layer_stack_for_first_block.0.slf_attn                             ││ │
││ model.layer_stack_for_first_block.0.slf_attn.w_qs                        ││ │
││ model.layer_stack_for_first_block.0.slf_attn.w_ks                        ││ │
││ model.layer_stack_for_first_block.0.slf_attn.w_vs                        ││ │
││ model.layer_stack_for_first_block.0.slf_attn.attention_operator          ││ │
││ model.layer_stack_for_first_block.0.slf_attn.attention_operator.dropout  ││ │
││ model.layer_stack_for_first_block.0.slf_attn.fc                          ││ │
││ model.layer_stack_for_first_block.0.dropout                              ││ │
││ model.layer_stack_for_first_block.0.layer_norm                           ││ │
││ model.layer_stack_for_first_block.0.pos_ffn                              ││ │
││ model.layer_stack_for_first_block.0.pos_ffn.linear_1                     ││ │
││ model.layer_stack_for_first_block.0.pos_ffn.linear_2                     ││ │
││ model.layer_stack_for_first_block.0.pos_ffn.layer_norm                   ││ │
││ model.layer_stack_for_first_block.0.pos_ffn.dropout                      ││ │
││ model.layer_stack_for_second_block                                       ││ │
││ model.layer_stack_for_second_block.0                                     ││ │
││ model.layer_stack_for_second_block.0.slf_attn                            ││ │
││ model.layer_stack_for_second_block.0.slf_attn.w_qs                       ││ │
││ model.layer_stack_for_second_block.0.slf_attn.w_ks                       ││ │
││ model.layer_stack_for_second_block.0.slf_attn.w_vs                       ││ │
││ model.layer_stack_for_second_block.0.slf_attn.attention_operator         ││ │
││ model.layer_stack_for_second_block.0.slf_attn.attention_operator.dropout ││ │
││ model.layer_stack_for_second_block.0.slf_attn.fc                         ││ │
││ model.layer_stack_for_second_block.0.dropout                             ││ │
││ model.layer_stack_for_second_block.0.layer_norm                          ││ │
││ model.layer_stack_for_second_block.0.pos_ffn                             ││ │
││ model.layer_stack_for_second_block.0.pos_ffn.linear_1                    ││ │
││ model.layer_stack_for_second_block.0.pos_ffn.linear_2                    ││ │
││ model.layer_stack_for_second_block.0.pos_ffn.layer_norm                  ││ │
││ model.layer_stack_for_second_block.0.pos_ffn.dropout                     ││ │
││ model.dropout                                                            ││ │
││ model.position_enc                                                       ││ │
││ model.embedding_1                                                        ││ │
││ model.reduce_dim_z                                                       ││ │
││ model.embedding_2                                                        ││ │
││ model.reduce_dim_beta                                                    ││ │
││ model.reduce_dim_gamma                                                   ││ │
││ model.weight_combine                                                     ││ │
││ model.output_projection                                                  ││ │
││ model.self_attention                                                     ││ │
││ model.output_net                                                         ││ │
││ model.output_net.0                                                       ││ │
││ model.output_net.1                                                       ││ │
││ model.output_net.2                                                       ││ │
││ model.output_net.3                                                       ││ │
││ model.output_net.4                                                       ││ │
││ clf_criterion                                                            ││ │
││ saits_loss_func                                                          ││ │
└┴──────────────────────────────────────────────────────────────────────────┴┴─┘
Trainable params: 107 K                                                         
Non-trainable params: 0                                                         
Total params: 107 K                                                             
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 27.56it/s v_num: 0.000      
                                                              val/auc: 0.727    
                                                              val/f1: 0.727     
                                                              val/precision:    
                                                              0.571 val/recall: 
                                                              1.000 val/mre: nan
                                                              train/auc: 0.735  
                                                              train/f1: 0.711   
                                                              train/precision:  
                                                              0.762             
                                                              train/recall:     
                                                              0.667 train/mre:  
                                                              nan               
[2024-06-24 10:04:53,557][train.py][INFO] - Starting testing...
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
Restoring states from the checkpoint path at /home/khickey/test_impute/logs/debug/multiruns/06-24-10-06-32/advice_yourself/0.0/3/checkpoints/34-0.00.ckpt
Loaded model weights from the checkpoint at /home/khickey/test_impute/logs/debug/multiruns/06-24-10-06-32/advice_yourself/0.0/3/checkpoints/34-0.00.ckpt
SLURM auto-requeueing enabled. Setting signal handlers.
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃        Test metric        ┃       DataLoader 0        ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
│       test/auc_mean       │     0.584019660949707     │
│       test/auc_std        │    0.11410675942897797    │
│       test/f1_mean        │    0.5421376824378967     │
│        test/f1_std        │    0.14884714782238007    │
│         test/mre          │            nan            │
│        test/mre_ci        │            nan            │
│    test/precision_mean    │    0.5616276264190674     │
│    test/precision_std     │    0.17252513766288757    │
│     test/recall_mean      │    0.5598800778388977     │
│      test/recall_std      │    0.1720922440290451     │
└───────────────────────────┴───────────────────────────┘
Testing ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1/1 0:00:00 • 0:00:00 0.00it/s 
[2024-06-24 10:05:08,141][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-24-10-06-32/advice_yourself/0.0/3>
[2024-06-24 10:05:08,142][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-24 10:05:08,144][HYDRA] 	#4 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=False data.question=advice_yourself data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=saits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=True seed=4
[rank: 0] Seed set to 4
[2024-06-24 10:05:08,341][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-24 10:05:08,343][train.py][INFO] - Instantiating model <src.methods.saits.lightningmodule.SAITSLightningModule>
[2024-06-24 10:05:08,344][train.py][INFO] - Instantiating callbacks...
[2024-06-24 10:05:08,345][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-24 10:05:08,348][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-24 10:05:08,348][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-24 10:05:08,349][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-24 10:05:08,351][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-24 10:05:08,352][train.py][INFO] - Instantiating loggers...
[2024-06-24 10:05:08,352][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-24 10:05:08,354][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-24 10:05:08,426][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-24-10-06-32/advice_yourself/0.0/4/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳┳━┓
┃┃ Name                                                                     ┃┃ ┃
┡╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇╇━┩
││ model                                                                    ││ │
││ model.layer_stack_for_first_block                                        ││ │
││ model.layer_stack_for_first_block.0                                      ││ │
││ model.layer_stack_for_first_block.0.slf_attn                             ││ │
││ model.layer_stack_for_first_block.0.slf_attn.w_qs                        ││ │
││ model.layer_stack_for_first_block.0.slf_attn.w_ks                        ││ │
││ model.layer_stack_for_first_block.0.slf_attn.w_vs                        ││ │
││ model.layer_stack_for_first_block.0.slf_attn.attention_operator          ││ │
││ model.layer_stack_for_first_block.0.slf_attn.attention_operator.dropout  ││ │
││ model.layer_stack_for_first_block.0.slf_attn.fc                          ││ │
││ model.layer_stack_for_first_block.0.dropout                              ││ │
││ model.layer_stack_for_first_block.0.layer_norm                           ││ │
││ model.layer_stack_for_first_block.0.pos_ffn                              ││ │
││ model.layer_stack_for_first_block.0.pos_ffn.linear_1                     ││ │
││ model.layer_stack_for_first_block.0.pos_ffn.linear_2                     ││ │
││ model.layer_stack_for_first_block.0.pos_ffn.layer_norm                   ││ │
││ model.layer_stack_for_first_block.0.pos_ffn.dropout                      ││ │
││ model.layer_stack_for_second_block                                       ││ │
││ model.layer_stack_for_second_block.0                                     ││ │
││ model.layer_stack_for_second_block.0.slf_attn                            ││ │
││ model.layer_stack_for_second_block.0.slf_attn.w_qs                       ││ │
││ model.layer_stack_for_second_block.0.slf_attn.w_ks                       ││ │
││ model.layer_stack_for_second_block.0.slf_attn.w_vs                       ││ │
││ model.layer_stack_for_second_block.0.slf_attn.attention_operator         ││ │
││ model.layer_stack_for_second_block.0.slf_attn.attention_operator.dropout ││ │
││ model.layer_stack_for_second_block.0.slf_attn.fc                         ││ │
││ model.layer_stack_for_second_block.0.dropout                             ││ │
││ model.layer_stack_for_second_block.0.layer_norm                          ││ │
││ model.layer_stack_for_second_block.0.pos_ffn                             ││ │
││ model.layer_stack_for_second_block.0.pos_ffn.linear_1                    ││ │
││ model.layer_stack_for_second_block.0.pos_ffn.linear_2                    ││ │
││ model.layer_stack_for_second_block.0.pos_ffn.layer_norm                  ││ │
││ model.layer_stack_for_second_block.0.pos_ffn.dropout                     ││ │
││ model.dropout                                                            ││ │
││ model.position_enc                                                       ││ │
││ model.embedding_1                                                        ││ │
││ model.reduce_dim_z                                                       ││ │
││ model.embedding_2                                                        ││ │
││ model.reduce_dim_beta                                                    ││ │
││ model.reduce_dim_gamma                                                   ││ │
││ model.weight_combine                                                     ││ │
││ model.output_projection                                                  ││ │
││ model.self_attention                                                     ││ │
││ model.output_net                                                         ││ │
││ model.output_net.0                                                       ││ │
││ model.output_net.1                                                       ││ │
││ model.output_net.2                                                       ││ │
││ model.output_net.3                                                       ││ │
││ model.output_net.4                                                       ││ │
││ clf_criterion                                                            ││ │
││ saits_loss_func                                                          ││ │
└┴──────────────────────────────────────────────────────────────────────────┴┴─┘
Trainable params: 107 K                                                         
Non-trainable params: 0                                                         
Total params: 107 K                                                             
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 26.41it/s v_num: 0.000      
                                                              val/auc: 0.667    
                                                              val/f1: 0.500     
                                                              val/precision:    
                                                              1.000 val/recall: 
                                                              0.333 val/mre: nan
                                                              train/auc: 0.666  
                                                              train/f1: 0.711   
                                                              train/precision:  
                                                              0.573             
                                                              train/recall:     
                                                              0.935 train/mre:  
                                                              nan               
[2024-06-24 10:05:29,857][train.py][INFO] - Starting testing...
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
Restoring states from the checkpoint path at /home/khickey/test_impute/logs/debug/multiruns/06-24-10-06-32/advice_yourself/0.0/4/checkpoints/40-0.00.ckpt
Loaded model weights from the checkpoint at /home/khickey/test_impute/logs/debug/multiruns/06-24-10-06-32/advice_yourself/0.0/4/checkpoints/40-0.00.ckpt
SLURM auto-requeueing enabled. Setting signal handlers.
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃        Test metric        ┃       DataLoader 0        ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
│       test/auc_mean       │    0.6273555159568787     │
│       test/auc_std        │    0.1195194274187088     │
│       test/f1_mean        │    0.5736895203590393     │
│        test/f1_std        │    0.14915834367275238    │
│         test/mre          │            nan            │
│        test/mre_ci        │            nan            │
│    test/precision_mean    │    0.6210042238235474     │
│    test/precision_std     │    0.1798761934041977     │
│     test/recall_mean      │    0.5501750111579895     │
│      test/recall_std      │    0.16974963247776031    │
└───────────────────────────┴───────────────────────────┘
Testing ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1/1 0:00:00 • 0:00:00 0.00it/s 
[2024-06-24 10:05:44,623][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-24-10-06-32/advice_yourself/0.0/4>
[2024-06-24 10:05:44,623][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-24 10:05:44,626][HYDRA] 	#5 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=False data.question=advice_yourself data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=saits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=True seed=5
[rank: 0] Seed set to 5
[2024-06-24 10:05:45,478][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-24 10:05:45,480][train.py][INFO] - Instantiating model <src.methods.saits.lightningmodule.SAITSLightningModule>
[2024-06-24 10:05:45,482][train.py][INFO] - Instantiating callbacks...
[2024-06-24 10:05:45,482][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-24 10:05:45,485][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-24 10:05:45,486][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-24 10:05:45,486][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-24 10:05:45,488][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-24 10:05:45,488][train.py][INFO] - Instantiating loggers...
[2024-06-24 10:05:45,489][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-24 10:05:45,491][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-24 10:05:45,524][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-24-10-06-32/advice_yourself/0.0/5/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳┳━┓
┃┃ Name                                                                     ┃┃ ┃
┡╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇╇━┩
││ model                                                                    ││ │
││ model.layer_stack_for_first_block                                        ││ │
││ model.layer_stack_for_first_block.0                                      ││ │
││ model.layer_stack_for_first_block.0.slf_attn                             ││ │
││ model.layer_stack_for_first_block.0.slf_attn.w_qs                        ││ │
││ model.layer_stack_for_first_block.0.slf_attn.w_ks                        ││ │
││ model.layer_stack_for_first_block.0.slf_attn.w_vs                        ││ │
││ model.layer_stack_for_first_block.0.slf_attn.attention_operator          ││ │
││ model.layer_stack_for_first_block.0.slf_attn.attention_operator.dropout  ││ │
││ model.layer_stack_for_first_block.0.slf_attn.fc                          ││ │
││ model.layer_stack_for_first_block.0.dropout                              ││ │
││ model.layer_stack_for_first_block.0.layer_norm                           ││ │
││ model.layer_stack_for_first_block.0.pos_ffn                              ││ │
││ model.layer_stack_for_first_block.0.pos_ffn.linear_1                     ││ │
││ model.layer_stack_for_first_block.0.pos_ffn.linear_2                     ││ │
││ model.layer_stack_for_first_block.0.pos_ffn.layer_norm                   ││ │
││ model.layer_stack_for_first_block.0.pos_ffn.dropout                      ││ │
││ model.layer_stack_for_second_block                                       ││ │
││ model.layer_stack_for_second_block.0                                     ││ │
││ model.layer_stack_for_second_block.0.slf_attn                            ││ │
││ model.layer_stack_for_second_block.0.slf_attn.w_qs                       ││ │
││ model.layer_stack_for_second_block.0.slf_attn.w_ks                       ││ │
││ model.layer_stack_for_second_block.0.slf_attn.w_vs                       ││ │
││ model.layer_stack_for_second_block.0.slf_attn.attention_operator         ││ │
││ model.layer_stack_for_second_block.0.slf_attn.attention_operator.dropout ││ │
││ model.layer_stack_for_second_block.0.slf_attn.fc                         ││ │
││ model.layer_stack_for_second_block.0.dropout                             ││ │
││ model.layer_stack_for_second_block.0.layer_norm                          ││ │
││ model.layer_stack_for_second_block.0.pos_ffn                             ││ │
││ model.layer_stack_for_second_block.0.pos_ffn.linear_1                    ││ │
││ model.layer_stack_for_second_block.0.pos_ffn.linear_2                    ││ │
││ model.layer_stack_for_second_block.0.pos_ffn.layer_norm                  ││ │
││ model.layer_stack_for_second_block.0.pos_ffn.dropout                     ││ │
││ model.dropout                                                            ││ │
││ model.position_enc                                                       ││ │
││ model.embedding_1                                                        ││ │
││ model.reduce_dim_z                                                       ││ │
││ model.embedding_2                                                        ││ │
││ model.reduce_dim_beta                                                    ││ │
││ model.reduce_dim_gamma                                                   ││ │
││ model.weight_combine                                                     ││ │
││ model.output_projection                                                  ││ │
││ model.self_attention                                                     ││ │
││ model.output_net                                                         ││ │
││ model.output_net.0                                                       ││ │
││ model.output_net.1                                                       ││ │
││ model.output_net.2                                                       ││ │
││ model.output_net.3                                                       ││ │
││ model.output_net.4                                                       ││ │
││ clf_criterion                                                            ││ │
││ saits_loss_func                                                          ││ │
└┴──────────────────────────────────────────────────────────────────────────┴┴─┘
Trainable params: 107 K                                                         
Non-trainable params: 0                                                         
Total params: 107 K                                                             
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 27.76it/s v_num: 0.000      
                                                              val/auc: 0.635    
                                                              val/f1: 0.769     
                                                              val/precision:    
                                                              0.769 val/recall: 
                                                              0.769 val/mre: nan
                                                              train/auc: 0.752  
                                                              train/f1: 0.800   
                                                              train/precision:  
                                                              0.696             
                                                              train/recall:     
                                                              0.941 train/mre:  
                                                              nan               
[2024-06-24 10:06:06,859][train.py][INFO] - Starting testing...
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
Restoring states from the checkpoint path at /home/khickey/test_impute/logs/debug/multiruns/06-24-10-06-32/advice_yourself/0.0/5/checkpoints/20-0.00.ckpt
Loaded model weights from the checkpoint at /home/khickey/test_impute/logs/debug/multiruns/06-24-10-06-32/advice_yourself/0.0/5/checkpoints/20-0.00.ckpt
SLURM auto-requeueing enabled. Setting signal handlers.
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃        Test metric        ┃       DataLoader 0        ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
│       test/auc_mean       │            0.5            │
│       test/auc_std        │            0.0            │
│       test/f1_mean        │     0.629287838935852     │
│        test/f1_std        │    0.10991019010543823    │
│         test/mre          │            nan            │
│        test/mre_ci        │            nan            │
│    test/precision_mean    │    0.4773157835006714     │
│    test/precision_std     │    0.11232612282037735    │
│     test/recall_mean      │            1.0            │
│      test/recall_std      │            0.0            │
└───────────────────────────┴───────────────────────────┘
Testing ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1/1 0:00:00 • 0:00:00 0.00it/s 
[2024-06-24 10:06:21,428][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-24-10-06-32/advice_yourself/0.0/5>
[2024-06-24 10:06:21,428][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-24 10:06:21,434][HYDRA] 	#6 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=False data.question=advice_yourself data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=saits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=True seed=6
[rank: 0] Seed set to 6
[2024-06-24 10:06:21,629][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-24 10:06:21,631][train.py][INFO] - Instantiating model <src.methods.saits.lightningmodule.SAITSLightningModule>
[2024-06-24 10:06:21,632][train.py][INFO] - Instantiating callbacks...
[2024-06-24 10:06:21,633][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-24 10:06:21,635][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-24 10:06:21,636][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-24 10:06:21,636][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-24 10:06:21,638][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-24 10:06:21,639][train.py][INFO] - Instantiating loggers...
[2024-06-24 10:06:21,639][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-24 10:06:21,641][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-24 10:06:21,715][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-24-10-06-32/advice_yourself/0.0/6/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳┳━┓
┃┃ Name                                                                     ┃┃ ┃
┡╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇╇━┩
││ model                                                                    ││ │
││ model.layer_stack_for_first_block                                        ││ │
││ model.layer_stack_for_first_block.0                                      ││ │
││ model.layer_stack_for_first_block.0.slf_attn                             ││ │
││ model.layer_stack_for_first_block.0.slf_attn.w_qs                        ││ │
││ model.layer_stack_for_first_block.0.slf_attn.w_ks                        ││ │
││ model.layer_stack_for_first_block.0.slf_attn.w_vs                        ││ │
││ model.layer_stack_for_first_block.0.slf_attn.attention_operator          ││ │
││ model.layer_stack_for_first_block.0.slf_attn.attention_operator.dropout  ││ │
││ model.layer_stack_for_first_block.0.slf_attn.fc                          ││ │
││ model.layer_stack_for_first_block.0.dropout                              ││ │
││ model.layer_stack_for_first_block.0.layer_norm                           ││ │
││ model.layer_stack_for_first_block.0.pos_ffn                              ││ │
││ model.layer_stack_for_first_block.0.pos_ffn.linear_1                     ││ │
││ model.layer_stack_for_first_block.0.pos_ffn.linear_2                     ││ │
││ model.layer_stack_for_first_block.0.pos_ffn.layer_norm                   ││ │
││ model.layer_stack_for_first_block.0.pos_ffn.dropout                      ││ │
││ model.layer_stack_for_second_block                                       ││ │
││ model.layer_stack_for_second_block.0                                     ││ │
││ model.layer_stack_for_second_block.0.slf_attn                            ││ │
││ model.layer_stack_for_second_block.0.slf_attn.w_qs                       ││ │
││ model.layer_stack_for_second_block.0.slf_attn.w_ks                       ││ │
││ model.layer_stack_for_second_block.0.slf_attn.w_vs                       ││ │
││ model.layer_stack_for_second_block.0.slf_attn.attention_operator         ││ │
││ model.layer_stack_for_second_block.0.slf_attn.attention_operator.dropout ││ │
││ model.layer_stack_for_second_block.0.slf_attn.fc                         ││ │
││ model.layer_stack_for_second_block.0.dropout                             ││ │
││ model.layer_stack_for_second_block.0.layer_norm                          ││ │
││ model.layer_stack_for_second_block.0.pos_ffn                             ││ │
││ model.layer_stack_for_second_block.0.pos_ffn.linear_1                    ││ │
││ model.layer_stack_for_second_block.0.pos_ffn.linear_2                    ││ │
││ model.layer_stack_for_second_block.0.pos_ffn.layer_norm                  ││ │
││ model.layer_stack_for_second_block.0.pos_ffn.dropout                     ││ │
││ model.dropout                                                            ││ │
││ model.position_enc                                                       ││ │
││ model.embedding_1                                                        ││ │
││ model.reduce_dim_z                                                       ││ │
││ model.embedding_2                                                        ││ │
││ model.reduce_dim_beta                                                    ││ │
││ model.reduce_dim_gamma                                                   ││ │
││ model.weight_combine                                                     ││ │
││ model.output_projection                                                  ││ │
││ model.self_attention                                                     ││ │
││ model.output_net                                                         ││ │
││ model.output_net.0                                                       ││ │
││ model.output_net.1                                                       ││ │
││ model.output_net.2                                                       ││ │
││ model.output_net.3                                                       ││ │
││ model.output_net.4                                                       ││ │
││ clf_criterion                                                            ││ │
││ saits_loss_func                                                          ││ │
└┴──────────────────────────────────────────────────────────────────────────┴┴─┘
Trainable params: 107 K                                                         
Non-trainable params: 0                                                         
Total params: 107 K                                                             
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 25.78it/s v_num: 0.000      
                                                              val/auc: 0.929    
                                                              val/f1: 0.960     
                                                              val/precision:    
                                                              0.923 val/recall: 
                                                              1.000 val/mre: nan
                                                              train/auc: 0.835  
                                                              train/f1: 0.855   
                                                              train/precision:  
                                                              0.825             
                                                              train/recall:     
                                                              0.887 train/mre:  
                                                              nan               
[2024-06-24 10:06:43,080][train.py][INFO] - Starting testing...
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
Restoring states from the checkpoint path at /home/khickey/test_impute/logs/debug/multiruns/06-24-10-06-32/advice_yourself/0.0/6/checkpoints/48-0.00.ckpt
Loaded model weights from the checkpoint at /home/khickey/test_impute/logs/debug/multiruns/06-24-10-06-32/advice_yourself/0.0/6/checkpoints/48-0.00.ckpt
SLURM auto-requeueing enabled. Setting signal handlers.
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃        Test metric        ┃       DataLoader 0        ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
│       test/auc_mean       │    0.6254868507385254     │
│       test/auc_std        │    0.11281658709049225    │
│       test/f1_mean        │    0.5777689814567566     │
│        test/f1_std        │    0.1505921185016632     │
│         test/mre          │            nan            │
│        test/mre_ci        │            nan            │
│    test/precision_mean    │    0.6300273537635803     │
│    test/precision_std     │    0.17638777196407318    │
│     test/recall_mean      │    0.5535420775413513     │
│      test/recall_std      │    0.17197874188423157    │
└───────────────────────────┴───────────────────────────┘
Testing ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1/1 0:00:00 • 0:00:00 0.00it/s 
[2024-06-24 10:06:58,078][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-24-10-06-32/advice_yourself/0.0/6>
[2024-06-24 10:06:58,079][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-24 10:06:58,082][HYDRA] 	#7 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=False data.question=advice_yourself data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=saits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=True seed=7
[rank: 0] Seed set to 7
[2024-06-24 10:06:58,272][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-24 10:06:58,274][train.py][INFO] - Instantiating model <src.methods.saits.lightningmodule.SAITSLightningModule>
[2024-06-24 10:06:58,275][train.py][INFO] - Instantiating callbacks...
[2024-06-24 10:06:58,275][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-24 10:06:58,278][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-24 10:06:58,279][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-24 10:06:58,279][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-24 10:06:58,280][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-24 10:06:58,280][train.py][INFO] - Instantiating loggers...
[2024-06-24 10:06:58,280][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-24 10:06:58,282][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-24 10:06:58,314][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-24-10-06-32/advice_yourself/0.0/7/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳┳━┓
┃┃ Name                                                                     ┃┃ ┃
┡╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇╇━┩
││ model                                                                    ││ │
││ model.layer_stack_for_first_block                                        ││ │
││ model.layer_stack_for_first_block.0                                      ││ │
││ model.layer_stack_for_first_block.0.slf_attn                             ││ │
││ model.layer_stack_for_first_block.0.slf_attn.w_qs                        ││ │
││ model.layer_stack_for_first_block.0.slf_attn.w_ks                        ││ │
││ model.layer_stack_for_first_block.0.slf_attn.w_vs                        ││ │
││ model.layer_stack_for_first_block.0.slf_attn.attention_operator          ││ │
││ model.layer_stack_for_first_block.0.slf_attn.attention_operator.dropout  ││ │
││ model.layer_stack_for_first_block.0.slf_attn.fc                          ││ │
││ model.layer_stack_for_first_block.0.dropout                              ││ │
││ model.layer_stack_for_first_block.0.layer_norm                           ││ │
││ model.layer_stack_for_first_block.0.pos_ffn                              ││ │
││ model.layer_stack_for_first_block.0.pos_ffn.linear_1                     ││ │
││ model.layer_stack_for_first_block.0.pos_ffn.linear_2                     ││ │
││ model.layer_stack_for_first_block.0.pos_ffn.layer_norm                   ││ │
││ model.layer_stack_for_first_block.0.pos_ffn.dropout                      ││ │
││ model.layer_stack_for_second_block                                       ││ │
││ model.layer_stack_for_second_block.0                                     ││ │
││ model.layer_stack_for_second_block.0.slf_attn                            ││ │
││ model.layer_stack_for_second_block.0.slf_attn.w_qs                       ││ │
││ model.layer_stack_for_second_block.0.slf_attn.w_ks                       ││ │
││ model.layer_stack_for_second_block.0.slf_attn.w_vs                       ││ │
││ model.layer_stack_for_second_block.0.slf_attn.attention_operator         ││ │
││ model.layer_stack_for_second_block.0.slf_attn.attention_operator.dropout ││ │
││ model.layer_stack_for_second_block.0.slf_attn.fc                         ││ │
││ model.layer_stack_for_second_block.0.dropout                             ││ │
││ model.layer_stack_for_second_block.0.layer_norm                          ││ │
││ model.layer_stack_for_second_block.0.pos_ffn                             ││ │
││ model.layer_stack_for_second_block.0.pos_ffn.linear_1                    ││ │
││ model.layer_stack_for_second_block.0.pos_ffn.linear_2                    ││ │
││ model.layer_stack_for_second_block.0.pos_ffn.layer_norm                  ││ │
││ model.layer_stack_for_second_block.0.pos_ffn.dropout                     ││ │
││ model.dropout                                                            ││ │
││ model.position_enc                                                       ││ │
││ model.embedding_1                                                        ││ │
││ model.reduce_dim_z                                                       ││ │
││ model.embedding_2                                                        ││ │
││ model.reduce_dim_beta                                                    ││ │
││ model.reduce_dim_gamma                                                   ││ │
││ model.weight_combine                                                     ││ │
││ model.output_projection                                                  ││ │
││ model.self_attention                                                     ││ │
││ model.output_net                                                         ││ │
││ model.output_net.0                                                       ││ │
││ model.output_net.1                                                       ││ │
││ model.output_net.2                                                       ││ │
││ model.output_net.3                                                       ││ │
││ model.output_net.4                                                       ││ │
││ clf_criterion                                                            ││ │
││ saits_loss_func                                                          ││ │
└┴──────────────────────────────────────────────────────────────────────────┴┴─┘
Trainable params: 107 K                                                         
Non-trainable params: 0                                                         
Total params: 107 K                                                             
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 28.87it/s v_num: 0.000      
                                                              val/auc: 0.909    
                                                              val/f1: 0.900     
                                                              val/precision:    
                                                              1.000 val/recall: 
                                                              0.818 val/mre: nan
                                                              train/auc: 0.763  
                                                              train/f1: 0.714   
                                                              train/precision:  
                                                              0.909             
                                                              train/recall:     
                                                              0.588 train/mre:  
                                                              nan               
[2024-06-24 10:07:19,445][train.py][INFO] - Starting testing...
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
Restoring states from the checkpoint path at /home/khickey/test_impute/logs/debug/multiruns/06-24-10-06-32/advice_yourself/0.0/7/checkpoints/38-0.00.ckpt
Loaded model weights from the checkpoint at /home/khickey/test_impute/logs/debug/multiruns/06-24-10-06-32/advice_yourself/0.0/7/checkpoints/38-0.00.ckpt
SLURM auto-requeueing enabled. Setting signal handlers.
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃        Test metric        ┃       DataLoader 0        ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
│       test/auc_mean       │     0.730713427066803     │
│       test/auc_std        │    0.09875748306512833    │
│       test/f1_mean        │    0.6495860815048218     │
│        test/f1_std        │    0.15296515822410583    │
│         test/mre          │            nan            │
│        test/mre_ci        │            nan            │
│    test/precision_mean    │     0.829588770866394     │
│    test/precision_std     │    0.16354848444461823    │
│     test/recall_mean      │     0.555926501750946     │
│      test/recall_std      │    0.16606156527996063    │
└───────────────────────────┴───────────────────────────┘
Testing ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1/1 0:00:00 • 0:00:00 0.00it/s 
[2024-06-24 10:07:34,023][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-24-10-06-32/advice_yourself/0.0/7>
[2024-06-24 10:07:34,024][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-24 10:07:34,029][HYDRA] 	#8 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=False data.question=advice_yourself data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=saits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=True seed=8
[rank: 0] Seed set to 8
[2024-06-24 10:07:34,222][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-24 10:07:34,224][train.py][INFO] - Instantiating model <src.methods.saits.lightningmodule.SAITSLightningModule>
[2024-06-24 10:07:34,226][train.py][INFO] - Instantiating callbacks...
[2024-06-24 10:07:34,226][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-24 10:07:34,229][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-24 10:07:34,230][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-24 10:07:34,230][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-24 10:07:34,232][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-24 10:07:34,233][train.py][INFO] - Instantiating loggers...
[2024-06-24 10:07:34,233][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-24 10:07:34,234][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
[2024-06-24 10:07:34,306][train.py][INFO] - Starting training...
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-24-10-06-32/advice_yourself/0.0/8/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳┳━┓
┃┃ Name                                                                     ┃┃ ┃
┡╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇╇━┩
││ model                                                                    ││ │
││ model.layer_stack_for_first_block                                        ││ │
││ model.layer_stack_for_first_block.0                                      ││ │
││ model.layer_stack_for_first_block.0.slf_attn                             ││ │
││ model.layer_stack_for_first_block.0.slf_attn.w_qs                        ││ │
││ model.layer_stack_for_first_block.0.slf_attn.w_ks                        ││ │
││ model.layer_stack_for_first_block.0.slf_attn.w_vs                        ││ │
││ model.layer_stack_for_first_block.0.slf_attn.attention_operator          ││ │
││ model.layer_stack_for_first_block.0.slf_attn.attention_operator.dropout  ││ │
││ model.layer_stack_for_first_block.0.slf_attn.fc                          ││ │
││ model.layer_stack_for_first_block.0.dropout                              ││ │
││ model.layer_stack_for_first_block.0.layer_norm                           ││ │
││ model.layer_stack_for_first_block.0.pos_ffn                              ││ │
││ model.layer_stack_for_first_block.0.pos_ffn.linear_1                     ││ │
││ model.layer_stack_for_first_block.0.pos_ffn.linear_2                     ││ │
││ model.layer_stack_for_first_block.0.pos_ffn.layer_norm                   ││ │
││ model.layer_stack_for_first_block.0.pos_ffn.dropout                      ││ │
││ model.layer_stack_for_second_block                                       ││ │
││ model.layer_stack_for_second_block.0                                     ││ │
││ model.layer_stack_for_second_block.0.slf_attn                            ││ │
││ model.layer_stack_for_second_block.0.slf_attn.w_qs                       ││ │
││ model.layer_stack_for_second_block.0.slf_attn.w_ks                       ││ │
││ model.layer_stack_for_second_block.0.slf_attn.w_vs                       ││ │
││ model.layer_stack_for_second_block.0.slf_attn.attention_operator         ││ │
││ model.layer_stack_for_second_block.0.slf_attn.attention_operator.dropout ││ │
││ model.layer_stack_for_second_block.0.slf_attn.fc                         ││ │
││ model.layer_stack_for_second_block.0.dropout                             ││ │
││ model.layer_stack_for_second_block.0.layer_norm                          ││ │
││ model.layer_stack_for_second_block.0.pos_ffn                             ││ │
││ model.layer_stack_for_second_block.0.pos_ffn.linear_1                    ││ │
││ model.layer_stack_for_second_block.0.pos_ffn.linear_2                    ││ │
││ model.layer_stack_for_second_block.0.pos_ffn.layer_norm                  ││ │
││ model.layer_stack_for_second_block.0.pos_ffn.dropout                     ││ │
││ model.dropout                                                            ││ │
││ model.position_enc                                                       ││ │
││ model.embedding_1                                                        ││ │
││ model.reduce_dim_z                                                       ││ │
││ model.embedding_2                                                        ││ │
││ model.reduce_dim_beta                                                    ││ │
││ model.reduce_dim_gamma                                                   ││ │
││ model.weight_combine                                                     ││ │
││ model.output_projection                                                  ││ │
││ model.self_attention                                                     ││ │
││ model.output_net                                                         ││ │
││ model.output_net.0                                                       ││ │
││ model.output_net.1                                                       ││ │
││ model.output_net.2                                                       ││ │
││ model.output_net.3                                                       ││ │
││ model.output_net.4                                                       ││ │
││ clf_criterion                                                            ││ │
││ saits_loss_func                                                          ││ │
└┴──────────────────────────────────────────────────────────────────────────┴┴─┘
Trainable params: 107 K                                                         
Non-trainable params: 0                                                         
Total params: 107 K                                                             
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 25.82it/s v_num: 0.000      
                                                              val/auc: 0.556    
                                                              val/f1: 0.200     
                                                              val/precision:    
                                                              1.000 val/recall: 
                                                              0.111 val/mre: nan
                                                              train/auc: 0.644  
                                                              train/f1: 0.448   
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              0.289 train/mre:  
                                                              nan               
[2024-06-24 10:07:57,056][train.py][INFO] - Starting testing...
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
Restoring states from the checkpoint path at /home/khickey/test_impute/logs/debug/multiruns/06-24-10-06-32/advice_yourself/0.0/8/checkpoints/29-0.00.ckpt
Loaded model weights from the checkpoint at /home/khickey/test_impute/logs/debug/multiruns/06-24-10-06-32/advice_yourself/0.0/8/checkpoints/29-0.00.ckpt
SLURM auto-requeueing enabled. Setting signal handlers.
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃        Test metric        ┃       DataLoader 0        ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
│       test/auc_mean       │    0.4367440342903137     │
│       test/auc_std        │    0.08552328497171402    │
│       test/f1_mean        │    0.5580407977104187     │
│        test/f1_std        │     0.119303859770298     │
│         test/mre          │            nan            │
│        test/mre_ci        │            nan            │
│    test/precision_mean    │    0.42966336011886597    │
│    test/precision_std     │    0.12613196671009064    │
│     test/recall_mean      │    0.7723926305770874     │
│      test/recall_std      │    0.14164163172245026    │
└───────────────────────────┴───────────────────────────┘
Testing ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1/1 0:00:00 • 0:00:00 0.00it/s 
[2024-06-24 10:08:11,663][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-24-10-06-32/advice_yourself/0.0/8>
[2024-06-24 10:08:11,665][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-24 10:08:11,670][HYDRA] 	#9 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=False data.question=advice_yourself data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=saits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=True seed=9
[rank: 0] Seed set to 9
[2024-06-24 10:08:11,892][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-24 10:08:11,894][train.py][INFO] - Instantiating model <src.methods.saits.lightningmodule.SAITSLightningModule>
[2024-06-24 10:08:11,896][train.py][INFO] - Instantiating callbacks...
[2024-06-24 10:08:11,896][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-24 10:08:11,899][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-24 10:08:11,899][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-24 10:08:11,900][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-24 10:08:11,900][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-24 10:08:11,901][train.py][INFO] - Instantiating loggers...
[2024-06-24 10:08:11,901][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-24 10:08:11,903][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-24 10:08:11,941][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-24-10-06-32/advice_yourself/0.0/9/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳┳━┓
┃┃ Name                                                                     ┃┃ ┃
┡╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇╇━┩
││ model                                                                    ││ │
││ model.layer_stack_for_first_block                                        ││ │
││ model.layer_stack_for_first_block.0                                      ││ │
││ model.layer_stack_for_first_block.0.slf_attn                             ││ │
││ model.layer_stack_for_first_block.0.slf_attn.w_qs                        ││ │
││ model.layer_stack_for_first_block.0.slf_attn.w_ks                        ││ │
││ model.layer_stack_for_first_block.0.slf_attn.w_vs                        ││ │
││ model.layer_stack_for_first_block.0.slf_attn.attention_operator          ││ │
││ model.layer_stack_for_first_block.0.slf_attn.attention_operator.dropout  ││ │
││ model.layer_stack_for_first_block.0.slf_attn.fc                          ││ │
││ model.layer_stack_for_first_block.0.dropout                              ││ │
││ model.layer_stack_for_first_block.0.layer_norm                           ││ │
││ model.layer_stack_for_first_block.0.pos_ffn                              ││ │
││ model.layer_stack_for_first_block.0.pos_ffn.linear_1                     ││ │
││ model.layer_stack_for_first_block.0.pos_ffn.linear_2                     ││ │
││ model.layer_stack_for_first_block.0.pos_ffn.layer_norm                   ││ │
││ model.layer_stack_for_first_block.0.pos_ffn.dropout                      ││ │
││ model.layer_stack_for_second_block                                       ││ │
││ model.layer_stack_for_second_block.0                                     ││ │
││ model.layer_stack_for_second_block.0.slf_attn                            ││ │
││ model.layer_stack_for_second_block.0.slf_attn.w_qs                       ││ │
││ model.layer_stack_for_second_block.0.slf_attn.w_ks                       ││ │
││ model.layer_stack_for_second_block.0.slf_attn.w_vs                       ││ │
││ model.layer_stack_for_second_block.0.slf_attn.attention_operator         ││ │
││ model.layer_stack_for_second_block.0.slf_attn.attention_operator.dropout ││ │
││ model.layer_stack_for_second_block.0.slf_attn.fc                         ││ │
││ model.layer_stack_for_second_block.0.dropout                             ││ │
││ model.layer_stack_for_second_block.0.layer_norm                          ││ │
││ model.layer_stack_for_second_block.0.pos_ffn                             ││ │
││ model.layer_stack_for_second_block.0.pos_ffn.linear_1                    ││ │
││ model.layer_stack_for_second_block.0.pos_ffn.linear_2                    ││ │
││ model.layer_stack_for_second_block.0.pos_ffn.layer_norm                  ││ │
││ model.layer_stack_for_second_block.0.pos_ffn.dropout                     ││ │
││ model.dropout                                                            ││ │
││ model.position_enc                                                       ││ │
││ model.embedding_1                                                        ││ │
││ model.reduce_dim_z                                                       ││ │
││ model.embedding_2                                                        ││ │
││ model.reduce_dim_beta                                                    ││ │
││ model.reduce_dim_gamma                                                   ││ │
││ model.weight_combine                                                     ││ │
││ model.output_projection                                                  ││ │
││ model.self_attention                                                     ││ │
││ model.output_net                                                         ││ │
││ model.output_net.0                                                       ││ │
││ model.output_net.1                                                       ││ │
││ model.output_net.2                                                       ││ │
││ model.output_net.3                                                       ││ │
││ model.output_net.4                                                       ││ │
││ clf_criterion                                                            ││ │
││ saits_loss_func                                                          ││ │
└┴──────────────────────────────────────────────────────────────────────────┴┴─┘
Trainable params: 107 K                                                         
Non-trainable params: 0                                                         
Total params: 107 K                                                             
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 28.58it/s v_num: 0.000      
                                                              val/auc: 0.801    
                                                              val/f1: 0.714     
                                                              val/precision:    
                                                              0.625 val/recall: 
                                                              0.833 val/mre: nan
                                                              train/auc: 0.802  
                                                              train/f1: 0.804   
                                                              train/precision:  
                                                              0.745             
                                                              train/recall:     
                                                              0.872 train/mre:  
                                                              nan               
[2024-06-24 10:08:33,208][train.py][INFO] - Starting testing...
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
Restoring states from the checkpoint path at /home/khickey/test_impute/logs/debug/multiruns/06-24-10-06-32/advice_yourself/0.0/9/checkpoints/30-0.00.ckpt
Loaded model weights from the checkpoint at /home/khickey/test_impute/logs/debug/multiruns/06-24-10-06-32/advice_yourself/0.0/9/checkpoints/30-0.00.ckpt
SLURM auto-requeueing enabled. Setting signal handlers.
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃        Test metric        ┃       DataLoader 0        ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
│       test/auc_mean       │    0.6748610138893127     │
│       test/auc_std        │    0.10559702664613724    │
│       test/f1_mean        │    0.6094318628311157     │
│        test/f1_std        │    0.14665119349956512    │
│         test/mre          │            nan            │
│        test/mre_ci        │            nan            │
│    test/precision_mean    │    0.7231091260910034     │
│    test/precision_std     │    0.17646583914756775    │
│     test/recall_mean      │    0.5575182437896729     │
│      test/recall_std      │    0.1741323173046112     │
└───────────────────────────┴───────────────────────────┘
Testing ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1/1 0:00:00 • 0:00:00 0.00it/s 
[2024-06-24 10:08:48,269][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-24-10-06-32/advice_yourself/0.0/9>
[2024-06-24 10:08:48,270][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-24 10:08:48,273][HYDRA] 	#10 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=False data.question=advice_yourself data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=saits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=True seed=0
[2024-06-24 10:08:48,492][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-24 10:08:48,495][train.py][INFO] - Instantiating model <src.methods.saits.lightningmodule.SAITSLightningModule>
[2024-06-24 10:08:48,496][train.py][INFO] - Instantiating callbacks...
[2024-06-24 10:08:48,497][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-24 10:08:48,500][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-24 10:08:48,500][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-24 10:08:48,501][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-24 10:08:48,503][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-24 10:08:48,504][train.py][INFO] - Instantiating loggers...
[2024-06-24 10:08:48,504][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-24 10:08:48,505][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-24 10:08:48,579][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-24-10-06-32/advice_yourself/0.05/0/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳┳━┓
┃┃ Name                                                                     ┃┃ ┃
┡╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇╇━┩
││ model                                                                    ││ │
││ model.layer_stack_for_first_block                                        ││ │
││ model.layer_stack_for_first_block.0                                      ││ │
││ model.layer_stack_for_first_block.0.slf_attn                             ││ │
││ model.layer_stack_for_first_block.0.slf_attn.w_qs                        ││ │
││ model.layer_stack_for_first_block.0.slf_attn.w_ks                        ││ │
││ model.layer_stack_for_first_block.0.slf_attn.w_vs                        ││ │
││ model.layer_stack_for_first_block.0.slf_attn.attention_operator          ││ │
││ model.layer_stack_for_first_block.0.slf_attn.attention_operator.dropout  ││ │
││ model.layer_stack_for_first_block.0.slf_attn.fc                          ││ │
││ model.layer_stack_for_first_block.0.dropout                              ││ │
││ model.layer_stack_for_first_block.0.layer_norm                           ││ │
││ model.layer_stack_for_first_block.0.pos_ffn                              ││ │
││ model.layer_stack_for_first_block.0.pos_ffn.linear_1                     ││ │
││ model.layer_stack_for_first_block.0.pos_ffn.linear_2                     ││ │
││ model.layer_stack_for_first_block.0.pos_ffn.layer_norm                   ││ │
││ model.layer_stack_for_first_block.0.pos_ffn.dropout                      ││ │
││ model.layer_stack_for_second_block                                       ││ │
││ model.layer_stack_for_second_block.0                                     ││ │
││ model.layer_stack_for_second_block.0.slf_attn                            ││ │
││ model.layer_stack_for_second_block.0.slf_attn.w_qs                       ││ │
││ model.layer_stack_for_second_block.0.slf_attn.w_ks                       ││ │
││ model.layer_stack_for_second_block.0.slf_attn.w_vs                       ││ │
││ model.layer_stack_for_second_block.0.slf_attn.attention_operator         ││ │
││ model.layer_stack_for_second_block.0.slf_attn.attention_operator.dropout ││ │
││ model.layer_stack_for_second_block.0.slf_attn.fc                         ││ │
││ model.layer_stack_for_second_block.0.dropout                             ││ │
││ model.layer_stack_for_second_block.0.layer_norm                          ││ │
││ model.layer_stack_for_second_block.0.pos_ffn                             ││ │
││ model.layer_stack_for_second_block.0.pos_ffn.linear_1                    ││ │
││ model.layer_stack_for_second_block.0.pos_ffn.linear_2                    ││ │
││ model.layer_stack_for_second_block.0.pos_ffn.layer_norm                  ││ │
││ model.layer_stack_for_second_block.0.pos_ffn.dropout                     ││ │
││ model.dropout                                                            ││ │
││ model.position_enc                                                       ││ │
││ model.embedding_1                                                        ││ │
││ model.reduce_dim_z                                                       ││ │
││ model.embedding_2                                                        ││ │
││ model.reduce_dim_beta                                                    ││ │
││ model.reduce_dim_gamma                                                   ││ │
││ model.weight_combine                                                     ││ │
││ model.output_projection                                                  ││ │
││ model.self_attention                                                     ││ │
││ model.output_net                                                         ││ │
││ model.output_net.0                                                       ││ │
││ model.output_net.1                                                       ││ │
││ model.output_net.2                                                       ││ │
││ model.output_net.3                                                       ││ │
││ model.output_net.4                                                       ││ │
││ clf_criterion                                                            ││ │
││ saits_loss_func                                                          ││ │
└┴──────────────────────────────────────────────────────────────────────────┴┴─┘
Trainable params: 107 K                                                         
Non-trainable params: 0                                                         
Total params: 107 K                                                             
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 28.65it/s v_num: 0.000      
                                                              val/auc: 0.917    
                                                              val/f1: 0.909     
                                                              val/precision:    
                                                              1.000 val/recall: 
                                                              0.833 val/mre:    
                                                              0.069 train/auc:  
                                                              0.879 train/f1:   
                                                              0.874             
                                                              train/precision:  
                                                              0.957             
                                                              train/recall:     
                                                              0.804 train/mre:  
                                                              0.076             
[2024-06-24 10:09:09,543][train.py][INFO] - Starting testing...
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
Restoring states from the checkpoint path at /home/khickey/test_impute/logs/debug/multiruns/06-24-10-06-32/advice_yourself/0.05/0/checkpoints/49-0.00.ckpt
Loaded model weights from the checkpoint at /home/khickey/test_impute/logs/debug/multiruns/06-24-10-06-32/advice_yourself/0.05/0/checkpoints/49-0.00.ckpt
SLURM auto-requeueing enabled. Setting signal handlers.
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃        Test metric        ┃       DataLoader 0        ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
│       test/auc_mean       │     0.580660343170166     │
│       test/auc_std        │    0.10495692491531372    │
│       test/f1_mean        │    0.6217232346534729     │
│        test/f1_std        │    0.12174224108457565    │
│         test/mre          │    0.07832056283950806    │
│        test/mre_ci        │   0.004074759781360626    │
│    test/precision_mean    │    0.5417956113815308     │
│    test/precision_std     │    0.13613879680633545    │
│     test/recall_mean      │    0.7756326198577881     │
│      test/recall_std      │    0.14780786633491516    │
└───────────────────────────┴───────────────────────────┘
Testing ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1/1 0:00:00 • 0:00:00 0.00it/s 
[2024-06-24 10:09:24,177][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-24-10-06-32/advice_yourself/0.05/0>
[2024-06-24 10:09:24,177][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-24 10:09:24,180][HYDRA] 	#11 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=False data.question=advice_yourself data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=saits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=True seed=1
[rank: 0] Seed set to 1
[2024-06-24 10:09:24,374][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-24 10:09:24,376][train.py][INFO] - Instantiating model <src.methods.saits.lightningmodule.SAITSLightningModule>
[2024-06-24 10:09:24,378][train.py][INFO] - Instantiating callbacks...
[2024-06-24 10:09:24,378][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-24 10:09:24,381][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-24 10:09:24,382][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-24 10:09:24,382][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-24 10:09:24,384][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-24 10:09:24,384][train.py][INFO] - Instantiating loggers...
[2024-06-24 10:09:24,384][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-24 10:09:24,386][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-24 10:09:24,417][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-24-10-06-32/advice_yourself/0.05/1/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳┳━┓
┃┃ Name                                                                     ┃┃ ┃
┡╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇╇━┩
││ model                                                                    ││ │
││ model.layer_stack_for_first_block                                        ││ │
││ model.layer_stack_for_first_block.0                                      ││ │
││ model.layer_stack_for_first_block.0.slf_attn                             ││ │
││ model.layer_stack_for_first_block.0.slf_attn.w_qs                        ││ │
││ model.layer_stack_for_first_block.0.slf_attn.w_ks                        ││ │
││ model.layer_stack_for_first_block.0.slf_attn.w_vs                        ││ │
││ model.layer_stack_for_first_block.0.slf_attn.attention_operator          ││ │
││ model.layer_stack_for_first_block.0.slf_attn.attention_operator.dropout  ││ │
││ model.layer_stack_for_first_block.0.slf_attn.fc                          ││ │
││ model.layer_stack_for_first_block.0.dropout                              ││ │
││ model.layer_stack_for_first_block.0.layer_norm                           ││ │
││ model.layer_stack_for_first_block.0.pos_ffn                              ││ │
││ model.layer_stack_for_first_block.0.pos_ffn.linear_1                     ││ │
││ model.layer_stack_for_first_block.0.pos_ffn.linear_2                     ││ │
││ model.layer_stack_for_first_block.0.pos_ffn.layer_norm                   ││ │
││ model.layer_stack_for_first_block.0.pos_ffn.dropout                      ││ │
││ model.layer_stack_for_second_block                                       ││ │
││ model.layer_stack_for_second_block.0                                     ││ │
││ model.layer_stack_for_second_block.0.slf_attn                            ││ │
││ model.layer_stack_for_second_block.0.slf_attn.w_qs                       ││ │
││ model.layer_stack_for_second_block.0.slf_attn.w_ks                       ││ │
││ model.layer_stack_for_second_block.0.slf_attn.w_vs                       ││ │
││ model.layer_stack_for_second_block.0.slf_attn.attention_operator         ││ │
││ model.layer_stack_for_second_block.0.slf_attn.attention_operator.dropout ││ │
││ model.layer_stack_for_second_block.0.slf_attn.fc                         ││ │
││ model.layer_stack_for_second_block.0.dropout                             ││ │
││ model.layer_stack_for_second_block.0.layer_norm                          ││ │
││ model.layer_stack_for_second_block.0.pos_ffn                             ││ │
││ model.layer_stack_for_second_block.0.pos_ffn.linear_1                    ││ │
││ model.layer_stack_for_second_block.0.pos_ffn.linear_2                    ││ │
││ model.layer_stack_for_second_block.0.pos_ffn.layer_norm                  ││ │
││ model.layer_stack_for_second_block.0.pos_ffn.dropout                     ││ │
││ model.dropout                                                            ││ │
││ model.position_enc                                                       ││ │
││ model.embedding_1                                                        ││ │
││ model.reduce_dim_z                                                       ││ │
││ model.embedding_2                                                        ││ │
││ model.reduce_dim_beta                                                    ││ │
││ model.reduce_dim_gamma                                                   ││ │
││ model.weight_combine                                                     ││ │
││ model.output_projection                                                  ││ │
││ model.self_attention                                                     ││ │
││ model.output_net                                                         ││ │
││ model.output_net.0                                                       ││ │
││ model.output_net.1                                                       ││ │
││ model.output_net.2                                                       ││ │
││ model.output_net.3                                                       ││ │
││ model.output_net.4                                                       ││ │
││ clf_criterion                                                            ││ │
││ saits_loss_func                                                          ││ │
└┴──────────────────────────────────────────────────────────────────────────┴┴─┘
Trainable params: 107 K                                                         
Non-trainable params: 0                                                         
Total params: 107 K                                                             
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 28.71it/s v_num: 0.000      
                                                              val/auc: 0.944    
                                                              val/f1: 0.941     
                                                              val/precision:    
                                                              1.000 val/recall: 
                                                              0.889 val/mre:    
                                                              0.062 train/auc:  
                                                              0.970 train/f1:   
                                                              0.971             
                                                              train/precision:  
                                                              0.980             
                                                              train/recall:     
                                                              0.962 train/mre:  
                                                              0.073             
[2024-06-24 10:09:45,111][train.py][INFO] - Starting testing...
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
Restoring states from the checkpoint path at /home/khickey/test_impute/logs/debug/multiruns/06-24-10-06-32/advice_yourself/0.05/1/checkpoints/44-0.00.ckpt
Loaded model weights from the checkpoint at /home/khickey/test_impute/logs/debug/multiruns/06-24-10-06-32/advice_yourself/0.05/1/checkpoints/44-0.00.ckpt
SLURM auto-requeueing enabled. Setting signal handlers.
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃        Test metric        ┃       DataLoader 0        ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
│       test/auc_mean       │    0.42800262570381165    │
│       test/auc_std        │    0.11630278080701828    │
│       test/f1_mean        │    0.46376386284828186    │
│        test/f1_std        │    0.14080612361431122    │
│         test/mre          │    0.07268652319908142    │
│        test/mre_ci        │   0.004016191698610783    │
│    test/precision_mean    │    0.41236770153045654    │
│    test/precision_std     │    0.14360545575618744    │
│     test/recall_mean      │    0.5507502555847168     │
│      test/recall_std      │    0.1731307953596115     │
└───────────────────────────┴───────────────────────────┘
Testing ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1/1 0:00:00 • 0:00:00 0.00it/s 
[2024-06-24 10:09:59,990][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-24-10-06-32/advice_yourself/0.05/1>
[2024-06-24 10:09:59,991][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-24 10:09:59,993][HYDRA] 	#12 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=False data.question=advice_yourself data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=saits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=True seed=2
[rank: 0] Seed set to 2
[2024-06-24 10:10:00,188][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-24 10:10:00,191][train.py][INFO] - Instantiating model <src.methods.saits.lightningmodule.SAITSLightningModule>
[2024-06-24 10:10:00,192][train.py][INFO] - Instantiating callbacks...
[2024-06-24 10:10:00,193][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-24 10:10:00,196][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-24 10:10:00,196][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-24 10:10:00,197][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-24 10:10:00,199][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-24 10:10:00,199][train.py][INFO] - Instantiating loggers...
[2024-06-24 10:10:00,199][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-24 10:10:00,201][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-24 10:10:00,274][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-24-10-06-32/advice_yourself/0.05/2/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳┳━┓
┃┃ Name                                                                     ┃┃ ┃
┡╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇╇━┩
││ model                                                                    ││ │
││ model.layer_stack_for_first_block                                        ││ │
││ model.layer_stack_for_first_block.0                                      ││ │
││ model.layer_stack_for_first_block.0.slf_attn                             ││ │
││ model.layer_stack_for_first_block.0.slf_attn.w_qs                        ││ │
││ model.layer_stack_for_first_block.0.slf_attn.w_ks                        ││ │
││ model.layer_stack_for_first_block.0.slf_attn.w_vs                        ││ │
││ model.layer_stack_for_first_block.0.slf_attn.attention_operator          ││ │
││ model.layer_stack_for_first_block.0.slf_attn.attention_operator.dropout  ││ │
││ model.layer_stack_for_first_block.0.slf_attn.fc                          ││ │
││ model.layer_stack_for_first_block.0.dropout                              ││ │
││ model.layer_stack_for_first_block.0.layer_norm                           ││ │
││ model.layer_stack_for_first_block.0.pos_ffn                              ││ │
││ model.layer_stack_for_first_block.0.pos_ffn.linear_1                     ││ │
││ model.layer_stack_for_first_block.0.pos_ffn.linear_2                     ││ │
││ model.layer_stack_for_first_block.0.pos_ffn.layer_norm                   ││ │
││ model.layer_stack_for_first_block.0.pos_ffn.dropout                      ││ │
││ model.layer_stack_for_second_block                                       ││ │
││ model.layer_stack_for_second_block.0                                     ││ │
││ model.layer_stack_for_second_block.0.slf_attn                            ││ │
││ model.layer_stack_for_second_block.0.slf_attn.w_qs                       ││ │
││ model.layer_stack_for_second_block.0.slf_attn.w_ks                       ││ │
││ model.layer_stack_for_second_block.0.slf_attn.w_vs                       ││ │
││ model.layer_stack_for_second_block.0.slf_attn.attention_operator         ││ │
││ model.layer_stack_for_second_block.0.slf_attn.attention_operator.dropout ││ │
││ model.layer_stack_for_second_block.0.slf_attn.fc                         ││ │
││ model.layer_stack_for_second_block.0.dropout                             ││ │
││ model.layer_stack_for_second_block.0.layer_norm                          ││ │
││ model.layer_stack_for_second_block.0.pos_ffn                             ││ │
││ model.layer_stack_for_second_block.0.pos_ffn.linear_1                    ││ │
││ model.layer_stack_for_second_block.0.pos_ffn.linear_2                    ││ │
││ model.layer_stack_for_second_block.0.pos_ffn.layer_norm                  ││ │
││ model.layer_stack_for_second_block.0.pos_ffn.dropout                     ││ │
││ model.dropout                                                            ││ │
││ model.position_enc                                                       ││ │
││ model.embedding_1                                                        ││ │
││ model.reduce_dim_z                                                       ││ │
││ model.embedding_2                                                        ││ │
││ model.reduce_dim_beta                                                    ││ │
││ model.reduce_dim_gamma                                                   ││ │
││ model.weight_combine                                                     ││ │
││ model.output_projection                                                  ││ │
││ model.self_attention                                                     ││ │
││ model.output_net                                                         ││ │
││ model.output_net.0                                                       ││ │
││ model.output_net.1                                                       ││ │
││ model.output_net.2                                                       ││ │
││ model.output_net.3                                                       ││ │
││ model.output_net.4                                                       ││ │
││ clf_criterion                                                            ││ │
││ saits_loss_func                                                          ││ │
└┴──────────────────────────────────────────────────────────────────────────┴┴─┘
Trainable params: 107 K                                                         
Non-trainable params: 0                                                         
Total params: 107 K                                                             
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 28.35it/s v_num: 0.000      
                                                              val/auc: 0.778    
                                                              val/f1: 0.833     
                                                              val/precision:    
                                                              0.714 val/recall: 
                                                              1.000 val/mre:    
                                                              0.063 train/auc:  
                                                              0.846 train/f1:   
                                                              0.865             
                                                              train/precision:  
                                                              0.800             
                                                              train/recall:     
                                                              0.941 train/mre:  
                                                              0.069             
[2024-06-24 10:10:21,411][train.py][INFO] - Starting testing...
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
Restoring states from the checkpoint path at /home/khickey/test_impute/logs/debug/multiruns/06-24-10-06-32/advice_yourself/0.05/2/checkpoints/45-0.00.ckpt
Loaded model weights from the checkpoint at /home/khickey/test_impute/logs/debug/multiruns/06-24-10-06-32/advice_yourself/0.05/2/checkpoints/45-0.00.ckpt
SLURM auto-requeueing enabled. Setting signal handlers.
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃        Test metric        ┃       DataLoader 0        ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
│       test/auc_mean       │    0.4360395669937134     │
│       test/auc_std        │    0.11629307270050049    │
│       test/f1_mean        │    0.4679129421710968     │
│        test/f1_std        │    0.13463494181632996    │
│         test/mre          │    0.07391022145748138    │
│        test/mre_ci        │   0.004247342236340046    │
│    test/precision_mean    │    0.4135836064815521     │
│    test/precision_std     │    0.14567729830741882    │
│     test/recall_mean      │    0.5474262237548828     │
│      test/recall_std      │    0.17381854355335236    │
└───────────────────────────┴───────────────────────────┘
Testing ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1/1 0:00:00 • 0:00:00 0.00it/s 
[2024-06-24 10:10:36,215][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-24-10-06-32/advice_yourself/0.05/2>
[2024-06-24 10:10:36,217][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-24 10:10:36,223][HYDRA] 	#13 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=False data.question=advice_yourself data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=saits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=True seed=3
[rank: 0] Seed set to 3
[2024-06-24 10:10:36,415][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-24 10:10:36,417][train.py][INFO] - Instantiating model <src.methods.saits.lightningmodule.SAITSLightningModule>
[2024-06-24 10:10:36,418][train.py][INFO] - Instantiating callbacks...
[2024-06-24 10:10:36,419][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-24 10:10:36,421][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-24 10:10:36,422][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-24 10:10:36,422][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-24 10:10:36,423][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-24 10:10:36,424][train.py][INFO] - Instantiating loggers...
[2024-06-24 10:10:36,424][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-24 10:10:36,425][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-24 10:10:36,457][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-24-10-06-32/advice_yourself/0.05/3/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳┳━┓
┃┃ Name                                                                     ┃┃ ┃
┡╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇╇━┩
││ model                                                                    ││ │
││ model.layer_stack_for_first_block                                        ││ │
││ model.layer_stack_for_first_block.0                                      ││ │
││ model.layer_stack_for_first_block.0.slf_attn                             ││ │
││ model.layer_stack_for_first_block.0.slf_attn.w_qs                        ││ │
││ model.layer_stack_for_first_block.0.slf_attn.w_ks                        ││ │
││ model.layer_stack_for_first_block.0.slf_attn.w_vs                        ││ │
││ model.layer_stack_for_first_block.0.slf_attn.attention_operator          ││ │
││ model.layer_stack_for_first_block.0.slf_attn.attention_operator.dropout  ││ │
││ model.layer_stack_for_first_block.0.slf_attn.fc                          ││ │
││ model.layer_stack_for_first_block.0.dropout                              ││ │
││ model.layer_stack_for_first_block.0.layer_norm                           ││ │
││ model.layer_stack_for_first_block.0.pos_ffn                              ││ │
││ model.layer_stack_for_first_block.0.pos_ffn.linear_1                     ││ │
││ model.layer_stack_for_first_block.0.pos_ffn.linear_2                     ││ │
││ model.layer_stack_for_first_block.0.pos_ffn.layer_norm                   ││ │
││ model.layer_stack_for_first_block.0.pos_ffn.dropout                      ││ │
││ model.layer_stack_for_second_block                                       ││ │
││ model.layer_stack_for_second_block.0                                     ││ │
││ model.layer_stack_for_second_block.0.slf_attn                            ││ │
││ model.layer_stack_for_second_block.0.slf_attn.w_qs                       ││ │
││ model.layer_stack_for_second_block.0.slf_attn.w_ks                       ││ │
││ model.layer_stack_for_second_block.0.slf_attn.w_vs                       ││ │
││ model.layer_stack_for_second_block.0.slf_attn.attention_operator         ││ │
││ model.layer_stack_for_second_block.0.slf_attn.attention_operator.dropout ││ │
││ model.layer_stack_for_second_block.0.slf_attn.fc                         ││ │
││ model.layer_stack_for_second_block.0.dropout                             ││ │
││ model.layer_stack_for_second_block.0.layer_norm                          ││ │
││ model.layer_stack_for_second_block.0.pos_ffn                             ││ │
││ model.layer_stack_for_second_block.0.pos_ffn.linear_1                    ││ │
││ model.layer_stack_for_second_block.0.pos_ffn.linear_2                    ││ │
││ model.layer_stack_for_second_block.0.pos_ffn.layer_norm                  ││ │
││ model.layer_stack_for_second_block.0.pos_ffn.dropout                     ││ │
││ model.dropout                                                            ││ │
││ model.position_enc                                                       ││ │
││ model.embedding_1                                                        ││ │
││ model.reduce_dim_z                                                       ││ │
││ model.embedding_2                                                        ││ │
││ model.reduce_dim_beta                                                    ││ │
││ model.reduce_dim_gamma                                                   ││ │
││ model.weight_combine                                                     ││ │
││ model.output_projection                                                  ││ │
││ model.self_attention                                                     ││ │
││ model.output_net                                                         ││ │
││ model.output_net.0                                                       ││ │
││ model.output_net.1                                                       ││ │
││ model.output_net.2                                                       ││ │
││ model.output_net.3                                                       ││ │
││ model.output_net.4                                                       ││ │
││ clf_criterion                                                            ││ │
││ saits_loss_func                                                          ││ │
└┴──────────────────────────────────────────────────────────────────────────┴┴─┘
Trainable params: 107 K                                                         
Non-trainable params: 0                                                         
Total params: 107 K                                                             
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 27.52it/s v_num: 0.000      
                                                              val/auc: 0.892    
                                                              val/f1: 0.875     
                                                              val/precision:    
                                                              0.875 val/recall: 
                                                              0.875 val/mre:    
                                                              0.072 train/auc:  
                                                              0.755 train/f1:   
                                                              0.793             
                                                              train/precision:  
                                                              0.658             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.079             
[2024-06-24 10:10:57,531][train.py][INFO] - Starting testing...
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
Restoring states from the checkpoint path at /home/khickey/test_impute/logs/debug/multiruns/06-24-10-06-32/advice_yourself/0.05/3/checkpoints/44-0.00.ckpt
Loaded model weights from the checkpoint at /home/khickey/test_impute/logs/debug/multiruns/06-24-10-06-32/advice_yourself/0.05/3/checkpoints/44-0.00.ckpt
SLURM auto-requeueing enabled. Setting signal handlers.
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃        Test metric        ┃       DataLoader 0        ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
│       test/auc_mean       │    0.5687745809555054     │
│       test/auc_std        │    0.10324742645025253    │
│       test/f1_mean        │     0.412960410118103     │
│        test/f1_std        │    0.1755119413137436     │
│         test/mre          │    0.08073528856039047    │
│        test/mre_ci        │   0.004193189088255167    │
│    test/precision_mean    │    0.6059539318084717     │
│    test/precision_std     │    0.23514501750469208    │
│     test/recall_mean      │    0.33708587288856506    │
│      test/recall_std      │     0.160674050450325     │
└───────────────────────────┴───────────────────────────┘
Testing ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1/1 0:00:00 • 0:00:00 0.00it/s 
[2024-06-24 10:11:12,392][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-24-10-06-32/advice_yourself/0.05/3>
[2024-06-24 10:11:12,393][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-24 10:11:12,395][HYDRA] 	#14 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=False data.question=advice_yourself data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=saits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=True seed=4
[rank: 0] Seed set to 4
[2024-06-24 10:11:13,321][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-24 10:11:13,323][train.py][INFO] - Instantiating model <src.methods.saits.lightningmodule.SAITSLightningModule>
[2024-06-24 10:11:13,325][train.py][INFO] - Instantiating callbacks...
[2024-06-24 10:11:13,325][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-24 10:11:13,328][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-24 10:11:13,328][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-24 10:11:13,329][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-24 10:11:13,331][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-24 10:11:13,331][train.py][INFO] - Instantiating loggers...
[2024-06-24 10:11:13,331][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-24 10:11:13,333][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-24 10:11:13,403][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-24-10-06-32/advice_yourself/0.05/4/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳┳━┓
┃┃ Name                                                                     ┃┃ ┃
┡╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇╇━┩
││ model                                                                    ││ │
││ model.layer_stack_for_first_block                                        ││ │
││ model.layer_stack_for_first_block.0                                      ││ │
││ model.layer_stack_for_first_block.0.slf_attn                             ││ │
││ model.layer_stack_for_first_block.0.slf_attn.w_qs                        ││ │
││ model.layer_stack_for_first_block.0.slf_attn.w_ks                        ││ │
││ model.layer_stack_for_first_block.0.slf_attn.w_vs                        ││ │
││ model.layer_stack_for_first_block.0.slf_attn.attention_operator          ││ │
││ model.layer_stack_for_first_block.0.slf_attn.attention_operator.dropout  ││ │
││ model.layer_stack_for_first_block.0.slf_attn.fc                          ││ │
││ model.layer_stack_for_first_block.0.dropout                              ││ │
││ model.layer_stack_for_first_block.0.layer_norm                           ││ │
││ model.layer_stack_for_first_block.0.pos_ffn                              ││ │
││ model.layer_stack_for_first_block.0.pos_ffn.linear_1                     ││ │
││ model.layer_stack_for_first_block.0.pos_ffn.linear_2                     ││ │
││ model.layer_stack_for_first_block.0.pos_ffn.layer_norm                   ││ │
││ model.layer_stack_for_first_block.0.pos_ffn.dropout                      ││ │
││ model.layer_stack_for_second_block                                       ││ │
││ model.layer_stack_for_second_block.0                                     ││ │
││ model.layer_stack_for_second_block.0.slf_attn                            ││ │
││ model.layer_stack_for_second_block.0.slf_attn.w_qs                       ││ │
││ model.layer_stack_for_second_block.0.slf_attn.w_ks                       ││ │
││ model.layer_stack_for_second_block.0.slf_attn.w_vs                       ││ │
││ model.layer_stack_for_second_block.0.slf_attn.attention_operator         ││ │
││ model.layer_stack_for_second_block.0.slf_attn.attention_operator.dropout ││ │
││ model.layer_stack_for_second_block.0.slf_attn.fc                         ││ │
││ model.layer_stack_for_second_block.0.dropout                             ││ │
││ model.layer_stack_for_second_block.0.layer_norm                          ││ │
││ model.layer_stack_for_second_block.0.pos_ffn                             ││ │
││ model.layer_stack_for_second_block.0.pos_ffn.linear_1                    ││ │
││ model.layer_stack_for_second_block.0.pos_ffn.linear_2                    ││ │
││ model.layer_stack_for_second_block.0.pos_ffn.layer_norm                  ││ │
││ model.layer_stack_for_second_block.0.pos_ffn.dropout                     ││ │
││ model.dropout                                                            ││ │
││ model.position_enc                                                       ││ │
││ model.embedding_1                                                        ││ │
││ model.reduce_dim_z                                                       ││ │
││ model.embedding_2                                                        ││ │
││ model.reduce_dim_beta                                                    ││ │
││ model.reduce_dim_gamma                                                   ││ │
││ model.weight_combine                                                     ││ │
││ model.output_projection                                                  ││ │
││ model.self_attention                                                     ││ │
││ model.output_net                                                         ││ │
││ model.output_net.0                                                       ││ │
││ model.output_net.1                                                       ││ │
││ model.output_net.2                                                       ││ │
││ model.output_net.3                                                       ││ │
││ model.output_net.4                                                       ││ │
││ clf_criterion                                                            ││ │
││ saits_loss_func                                                          ││ │
└┴──────────────────────────────────────────────────────────────────────────┴┴─┘
Trainable params: 107 K                                                         
Non-trainable params: 0                                                         
Total params: 107 K                                                             
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 29.32it/s v_num: 0.000      
                                                              val/auc: 0.722    
                                                              val/f1: 0.615     
                                                              val/precision:    
                                                              1.000 val/recall: 
                                                              0.444 val/mre:    
                                                              0.060 train/auc:  
                                                              0.645 train/f1:   
                                                              0.694             
                                                              train/precision:  
                                                              0.560             
                                                              train/recall:     
                                                              0.913 train/mre:  
                                                              0.067             
[2024-06-24 10:11:34,238][train.py][INFO] - Starting testing...
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
Restoring states from the checkpoint path at /home/khickey/test_impute/logs/debug/multiruns/06-24-10-06-32/advice_yourself/0.05/4/checkpoints/48-0.00.ckpt
Loaded model weights from the checkpoint at /home/khickey/test_impute/logs/debug/multiruns/06-24-10-06-32/advice_yourself/0.05/4/checkpoints/48-0.00.ckpt
SLURM auto-requeueing enabled. Setting signal handlers.
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃        Test metric        ┃       DataLoader 0        ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
│       test/auc_mean       │    0.5380531549453735     │
│       test/auc_std        │    0.1043805330991745     │
│       test/f1_mean        │    0.5975154042243958     │
│        test/f1_std        │    0.1226290762424469     │
│         test/mre          │    0.0718187540769577     │
│        test/mre_ci        │   0.004021795466542244    │
│    test/precision_mean    │    0.4951664209365845     │
│    test/precision_std     │    0.13524438440799713    │
│     test/recall_mean      │     0.776279091835022     │
│      test/recall_std      │    0.1456669718027115     │
└───────────────────────────┴───────────────────────────┘
Testing ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1/1 0:00:00 • 0:00:00 0.00it/s 
[2024-06-24 10:11:48,792][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-24-10-06-32/advice_yourself/0.05/4>
[2024-06-24 10:11:48,793][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-24 10:11:48,795][HYDRA] 	#15 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=False data.question=advice_yourself data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=saits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=True seed=5
[rank: 0] Seed set to 5
[2024-06-24 10:11:48,986][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-24 10:11:48,988][train.py][INFO] - Instantiating model <src.methods.saits.lightningmodule.SAITSLightningModule>
[2024-06-24 10:11:48,989][train.py][INFO] - Instantiating callbacks...
[2024-06-24 10:11:48,989][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-24 10:11:48,992][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-24 10:11:48,992][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-24 10:11:48,993][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-24 10:11:48,994][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-24 10:11:48,994][train.py][INFO] - Instantiating loggers...
[2024-06-24 10:11:48,994][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-24 10:11:48,996][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-24 10:11:49,025][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-24-10-06-32/advice_yourself/0.05/5/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳┳━┓
┃┃ Name                                                                     ┃┃ ┃
┡╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇╇━┩
││ model                                                                    ││ │
││ model.layer_stack_for_first_block                                        ││ │
││ model.layer_stack_for_first_block.0                                      ││ │
││ model.layer_stack_for_first_block.0.slf_attn                             ││ │
││ model.layer_stack_for_first_block.0.slf_attn.w_qs                        ││ │
││ model.layer_stack_for_first_block.0.slf_attn.w_ks                        ││ │
││ model.layer_stack_for_first_block.0.slf_attn.w_vs                        ││ │
││ model.layer_stack_for_first_block.0.slf_attn.attention_operator          ││ │
││ model.layer_stack_for_first_block.0.slf_attn.attention_operator.dropout  ││ │
││ model.layer_stack_for_first_block.0.slf_attn.fc                          ││ │
││ model.layer_stack_for_first_block.0.dropout                              ││ │
││ model.layer_stack_for_first_block.0.layer_norm                           ││ │
││ model.layer_stack_for_first_block.0.pos_ffn                              ││ │
││ model.layer_stack_for_first_block.0.pos_ffn.linear_1                     ││ │
││ model.layer_stack_for_first_block.0.pos_ffn.linear_2                     ││ │
││ model.layer_stack_for_first_block.0.pos_ffn.layer_norm                   ││ │
││ model.layer_stack_for_first_block.0.pos_ffn.dropout                      ││ │
││ model.layer_stack_for_second_block                                       ││ │
││ model.layer_stack_for_second_block.0                                     ││ │
││ model.layer_stack_for_second_block.0.slf_attn                            ││ │
││ model.layer_stack_for_second_block.0.slf_attn.w_qs                       ││ │
││ model.layer_stack_for_second_block.0.slf_attn.w_ks                       ││ │
││ model.layer_stack_for_second_block.0.slf_attn.w_vs                       ││ │
││ model.layer_stack_for_second_block.0.slf_attn.attention_operator         ││ │
││ model.layer_stack_for_second_block.0.slf_attn.attention_operator.dropout ││ │
││ model.layer_stack_for_second_block.0.slf_attn.fc                         ││ │
││ model.layer_stack_for_second_block.0.dropout                             ││ │
││ model.layer_stack_for_second_block.0.layer_norm                          ││ │
││ model.layer_stack_for_second_block.0.pos_ffn                             ││ │
││ model.layer_stack_for_second_block.0.pos_ffn.linear_1                    ││ │
││ model.layer_stack_for_second_block.0.pos_ffn.linear_2                    ││ │
││ model.layer_stack_for_second_block.0.pos_ffn.layer_norm                  ││ │
││ model.layer_stack_for_second_block.0.pos_ffn.dropout                     ││ │
││ model.dropout                                                            ││ │
││ model.position_enc                                                       ││ │
││ model.embedding_1                                                        ││ │
││ model.reduce_dim_z                                                       ││ │
││ model.embedding_2                                                        ││ │
││ model.reduce_dim_beta                                                    ││ │
││ model.reduce_dim_gamma                                                   ││ │
││ model.weight_combine                                                     ││ │
││ model.output_projection                                                  ││ │
││ model.self_attention                                                     ││ │
││ model.output_net                                                         ││ │
││ model.output_net.0                                                       ││ │
││ model.output_net.1                                                       ││ │
││ model.output_net.2                                                       ││ │
││ model.output_net.3                                                       ││ │
││ model.output_net.4                                                       ││ │
││ clf_criterion                                                            ││ │
││ saits_loss_func                                                          ││ │
└┴──────────────────────────────────────────────────────────────────────────┴┴─┘
Trainable params: 107 K                                                         
Non-trainable params: 0                                                         
Total params: 107 K                                                             
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 26.76it/s v_num: 0.000      
                                                              val/auc: 0.712    
                                                              val/f1: 0.857     
                                                              val/precision:    
                                                              0.800 val/recall: 
                                                              0.923 val/mre:    
                                                              0.070 train/auc:  
                                                              0.744 train/f1:   
                                                              0.675             
                                                              train/precision:  
                                                              0.931             
                                                              train/recall:     
                                                              0.529 train/mre:  
                                                              0.078             
[2024-06-24 10:12:09,977][train.py][INFO] - Starting testing...
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
Restoring states from the checkpoint path at /home/khickey/test_impute/logs/debug/multiruns/06-24-10-06-32/advice_yourself/0.05/5/checkpoints/38-0.00.ckpt
Loaded model weights from the checkpoint at /home/khickey/test_impute/logs/debug/multiruns/06-24-10-06-32/advice_yourself/0.05/5/checkpoints/38-0.00.ckpt
SLURM auto-requeueing enabled. Setting signal handlers.
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃        Test metric        ┃       DataLoader 0        ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
│       test/auc_mean       │    0.5283195972442627     │
│       test/auc_std        │    0.11389584094285965    │
│       test/f1_mean        │    0.5566868185997009     │
│        test/f1_std        │    0.1329057216644287     │
│         test/mre          │    0.0706549882888794     │
│        test/mre_ci        │   0.003986984491348267    │
│    test/precision_mean    │    0.5019773244857788     │
│    test/precision_std     │    0.14803871512413025    │
│     test/recall_mean      │    0.6576473116874695     │
│      test/recall_std      │    0.16914460062980652    │
└───────────────────────────┴───────────────────────────┘
Testing ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1/1 0:00:01 • 0:00:00 0.00it/s 
[2024-06-24 10:12:25,016][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-24-10-06-32/advice_yourself/0.05/5>
[2024-06-24 10:12:25,017][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-24 10:12:25,019][HYDRA] 	#16 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=False data.question=advice_yourself data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=saits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=True seed=6
[rank: 0] Seed set to 6
[2024-06-24 10:12:25,213][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-24 10:12:25,215][train.py][INFO] - Instantiating model <src.methods.saits.lightningmodule.SAITSLightningModule>
[2024-06-24 10:12:25,217][train.py][INFO] - Instantiating callbacks...
[2024-06-24 10:12:25,217][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-24 10:12:25,219][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-24 10:12:25,220][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-24 10:12:25,220][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-24 10:12:25,223][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-24 10:12:25,223][train.py][INFO] - Instantiating loggers...
[2024-06-24 10:12:25,223][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-24 10:12:25,225][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-24 10:12:25,293][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-24-10-06-32/advice_yourself/0.05/6/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳┳━┓
┃┃ Name                                                                     ┃┃ ┃
┡╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇╇━┩
││ model                                                                    ││ │
││ model.layer_stack_for_first_block                                        ││ │
││ model.layer_stack_for_first_block.0                                      ││ │
││ model.layer_stack_for_first_block.0.slf_attn                             ││ │
││ model.layer_stack_for_first_block.0.slf_attn.w_qs                        ││ │
││ model.layer_stack_for_first_block.0.slf_attn.w_ks                        ││ │
││ model.layer_stack_for_first_block.0.slf_attn.w_vs                        ││ │
││ model.layer_stack_for_first_block.0.slf_attn.attention_operator          ││ │
││ model.layer_stack_for_first_block.0.slf_attn.attention_operator.dropout  ││ │
││ model.layer_stack_for_first_block.0.slf_attn.fc                          ││ │
││ model.layer_stack_for_first_block.0.dropout                              ││ │
││ model.layer_stack_for_first_block.0.layer_norm                           ││ │
││ model.layer_stack_for_first_block.0.pos_ffn                              ││ │
││ model.layer_stack_for_first_block.0.pos_ffn.linear_1                     ││ │
││ model.layer_stack_for_first_block.0.pos_ffn.linear_2                     ││ │
││ model.layer_stack_for_first_block.0.pos_ffn.layer_norm                   ││ │
││ model.layer_stack_for_first_block.0.pos_ffn.dropout                      ││ │
││ model.layer_stack_for_second_block                                       ││ │
││ model.layer_stack_for_second_block.0                                     ││ │
││ model.layer_stack_for_second_block.0.slf_attn                            ││ │
││ model.layer_stack_for_second_block.0.slf_attn.w_qs                       ││ │
││ model.layer_stack_for_second_block.0.slf_attn.w_ks                       ││ │
││ model.layer_stack_for_second_block.0.slf_attn.w_vs                       ││ │
││ model.layer_stack_for_second_block.0.slf_attn.attention_operator         ││ │
││ model.layer_stack_for_second_block.0.slf_attn.attention_operator.dropout ││ │
││ model.layer_stack_for_second_block.0.slf_attn.fc                         ││ │
││ model.layer_stack_for_second_block.0.dropout                             ││ │
││ model.layer_stack_for_second_block.0.layer_norm                          ││ │
││ model.layer_stack_for_second_block.0.pos_ffn                             ││ │
││ model.layer_stack_for_second_block.0.pos_ffn.linear_1                    ││ │
││ model.layer_stack_for_second_block.0.pos_ffn.linear_2                    ││ │
││ model.layer_stack_for_second_block.0.pos_ffn.layer_norm                  ││ │
││ model.layer_stack_for_second_block.0.pos_ffn.dropout                     ││ │
││ model.dropout                                                            ││ │
││ model.position_enc                                                       ││ │
││ model.embedding_1                                                        ││ │
││ model.reduce_dim_z                                                       ││ │
││ model.embedding_2                                                        ││ │
││ model.reduce_dim_beta                                                    ││ │
││ model.reduce_dim_gamma                                                   ││ │
││ model.weight_combine                                                     ││ │
││ model.output_projection                                                  ││ │
││ model.self_attention                                                     ││ │
││ model.output_net                                                         ││ │
││ model.output_net.0                                                       ││ │
││ model.output_net.1                                                       ││ │
││ model.output_net.2                                                       ││ │
││ model.output_net.3                                                       ││ │
││ model.output_net.4                                                       ││ │
││ clf_criterion                                                            ││ │
││ saits_loss_func                                                          ││ │
└┴──────────────────────────────────────────────────────────────────────────┴┴─┘
Trainable params: 107 K                                                         
Non-trainable params: 0                                                         
Total params: 107 K                                                             
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 27.13it/s v_num: 0.000      
                                                              val/auc: 0.929    
                                                              val/f1: 0.960     
                                                              val/precision:    
                                                              0.923 val/recall: 
                                                              1.000 val/mre:    
                                                              0.093 train/auc:  
                                                              0.772 train/f1:   
                                                              0.835             
                                                              train/precision:  
                                                              0.716             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.081             
[2024-06-24 10:12:46,282][train.py][INFO] - Starting testing...
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
Restoring states from the checkpoint path at /home/khickey/test_impute/logs/debug/multiruns/06-24-10-06-32/advice_yourself/0.05/6/checkpoints/46-0.00.ckpt
Loaded model weights from the checkpoint at /home/khickey/test_impute/logs/debug/multiruns/06-24-10-06-32/advice_yourself/0.05/6/checkpoints/46-0.00.ckpt
SLURM auto-requeueing enabled. Setting signal handlers.
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃        Test metric        ┃       DataLoader 0        ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
│       test/auc_mean       │     0.683788537979126     │
│       test/auc_std        │    0.10794632881879807    │
│       test/f1_mean        │    0.6565162539482117     │
│        test/f1_std        │    0.13600124418735504    │
│         test/mre          │    0.07822594046592712    │
│        test/mre_ci        │   0.004283925984054804    │
│    test/precision_mean    │    0.6639202833175659     │
│    test/precision_std     │    0.16181686520576477    │
│     test/recall_mean      │    0.6679362058639526     │
│      test/recall_std      │    0.16213266551494598    │
└───────────────────────────┴───────────────────────────┘
Testing ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1/1 0:00:00 • 0:00:00 0.00it/s 
[2024-06-24 10:13:00,896][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-24-10-06-32/advice_yourself/0.05/6>
[2024-06-24 10:13:00,896][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-24 10:13:00,898][HYDRA] 	#17 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=False data.question=advice_yourself data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=saits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=True seed=7
[rank: 0] Seed set to 7
[2024-06-24 10:13:01,093][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-24 10:13:01,095][train.py][INFO] - Instantiating model <src.methods.saits.lightningmodule.SAITSLightningModule>
[2024-06-24 10:13:01,096][train.py][INFO] - Instantiating callbacks...
[2024-06-24 10:13:01,097][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-24 10:13:01,099][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-24 10:13:01,100][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-24 10:13:01,100][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-24 10:13:01,101][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-24 10:13:01,102][train.py][INFO] - Instantiating loggers...
[2024-06-24 10:13:01,102][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-24 10:13:01,103][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-24 10:13:01,134][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-24-10-06-32/advice_yourself/0.05/7/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳┳━┓
┃┃ Name                                                                     ┃┃ ┃
┡╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇╇━┩
││ model                                                                    ││ │
││ model.layer_stack_for_first_block                                        ││ │
││ model.layer_stack_for_first_block.0                                      ││ │
││ model.layer_stack_for_first_block.0.slf_attn                             ││ │
││ model.layer_stack_for_first_block.0.slf_attn.w_qs                        ││ │
││ model.layer_stack_for_first_block.0.slf_attn.w_ks                        ││ │
││ model.layer_stack_for_first_block.0.slf_attn.w_vs                        ││ │
││ model.layer_stack_for_first_block.0.slf_attn.attention_operator          ││ │
││ model.layer_stack_for_first_block.0.slf_attn.attention_operator.dropout  ││ │
││ model.layer_stack_for_first_block.0.slf_attn.fc                          ││ │
││ model.layer_stack_for_first_block.0.dropout                              ││ │
││ model.layer_stack_for_first_block.0.layer_norm                           ││ │
││ model.layer_stack_for_first_block.0.pos_ffn                              ││ │
││ model.layer_stack_for_first_block.0.pos_ffn.linear_1                     ││ │
││ model.layer_stack_for_first_block.0.pos_ffn.linear_2                     ││ │
││ model.layer_stack_for_first_block.0.pos_ffn.layer_norm                   ││ │
││ model.layer_stack_for_first_block.0.pos_ffn.dropout                      ││ │
││ model.layer_stack_for_second_block                                       ││ │
││ model.layer_stack_for_second_block.0                                     ││ │
││ model.layer_stack_for_second_block.0.slf_attn                            ││ │
││ model.layer_stack_for_second_block.0.slf_attn.w_qs                       ││ │
││ model.layer_stack_for_second_block.0.slf_attn.w_ks                       ││ │
││ model.layer_stack_for_second_block.0.slf_attn.w_vs                       ││ │
││ model.layer_stack_for_second_block.0.slf_attn.attention_operator         ││ │
││ model.layer_stack_for_second_block.0.slf_attn.attention_operator.dropout ││ │
││ model.layer_stack_for_second_block.0.slf_attn.fc                         ││ │
││ model.layer_stack_for_second_block.0.dropout                             ││ │
││ model.layer_stack_for_second_block.0.layer_norm                          ││ │
││ model.layer_stack_for_second_block.0.pos_ffn                             ││ │
││ model.layer_stack_for_second_block.0.pos_ffn.linear_1                    ││ │
││ model.layer_stack_for_second_block.0.pos_ffn.linear_2                    ││ │
││ model.layer_stack_for_second_block.0.pos_ffn.layer_norm                  ││ │
││ model.layer_stack_for_second_block.0.pos_ffn.dropout                     ││ │
││ model.dropout                                                            ││ │
││ model.position_enc                                                       ││ │
││ model.embedding_1                                                        ││ │
││ model.reduce_dim_z                                                       ││ │
││ model.embedding_2                                                        ││ │
││ model.reduce_dim_beta                                                    ││ │
││ model.reduce_dim_gamma                                                   ││ │
││ model.weight_combine                                                     ││ │
││ model.output_projection                                                  ││ │
││ model.self_attention                                                     ││ │
││ model.output_net                                                         ││ │
││ model.output_net.0                                                       ││ │
││ model.output_net.1                                                       ││ │
││ model.output_net.2                                                       ││ │
││ model.output_net.3                                                       ││ │
││ model.output_net.4                                                       ││ │
││ clf_criterion                                                            ││ │
││ saits_loss_func                                                          ││ │
└┴──────────────────────────────────────────────────────────────────────────┴┴─┘
Trainable params: 107 K                                                         
Non-trainable params: 0                                                         
Total params: 107 K                                                             
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 26.69it/s v_num: 0.000      
                                                              val/auc: 0.909    
                                                              val/f1: 0.900     
                                                              val/precision:    
                                                              1.000 val/recall: 
                                                              0.818 val/mre:    
                                                              0.060 train/auc:  
                                                              0.855 train/f1:   
                                                              0.877             
                                                              train/precision:  
                                                              0.794             
                                                              train/recall:     
                                                              0.980 train/mre:  
                                                              0.075             
[2024-06-24 10:13:22,231][train.py][INFO] - Starting testing...
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
Restoring states from the checkpoint path at /home/khickey/test_impute/logs/debug/multiruns/06-24-10-06-32/advice_yourself/0.05/7/checkpoints/44-0.00.ckpt
Loaded model weights from the checkpoint at /home/khickey/test_impute/logs/debug/multiruns/06-24-10-06-32/advice_yourself/0.05/7/checkpoints/44-0.00.ckpt
SLURM auto-requeueing enabled. Setting signal handlers.
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃        Test metric        ┃       DataLoader 0        ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
│       test/auc_mean       │    0.7315100431442261     │
│       test/auc_std        │    0.10619238764047623    │
│       test/f1_mean        │    0.6908789873123169     │
│        test/f1_std        │    0.1341596096754074     │
│         test/mre          │    0.08211511373519897    │
│        test/mre_ci        │   0.004597750958055258    │
│    test/precision_mean    │    0.7500608563423157     │
│    test/precision_std     │    0.15481844544410706    │
│     test/recall_mean      │    0.6671318411827087     │
│      test/recall_std      │    0.15705004334449768    │
└───────────────────────────┴───────────────────────────┘
Testing ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1/1 0:00:00 • 0:00:00 0.00it/s 
[2024-06-24 10:13:37,124][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-24-10-06-32/advice_yourself/0.05/7>
[2024-06-24 10:13:37,124][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-24 10:13:37,127][HYDRA] 	#18 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=False data.question=advice_yourself data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=saits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=True seed=8
[rank: 0] Seed set to 8
[2024-06-24 10:13:37,328][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-24 10:13:37,330][train.py][INFO] - Instantiating model <src.methods.saits.lightningmodule.SAITSLightningModule>
[2024-06-24 10:13:37,331][train.py][INFO] - Instantiating callbacks...
[2024-06-24 10:13:37,332][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-24 10:13:37,334][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-24 10:13:37,335][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-24 10:13:37,335][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-24 10:13:37,338][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-24 10:13:37,338][train.py][INFO] - Instantiating loggers...
[2024-06-24 10:13:37,338][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-24 10:13:37,340][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-24 10:13:37,416][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-24-10-06-32/advice_yourself/0.05/8/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳┳━┓
┃┃ Name                                                                     ┃┃ ┃
┡╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇╇━┩
││ model                                                                    ││ │
││ model.layer_stack_for_first_block                                        ││ │
││ model.layer_stack_for_first_block.0                                      ││ │
││ model.layer_stack_for_first_block.0.slf_attn                             ││ │
││ model.layer_stack_for_first_block.0.slf_attn.w_qs                        ││ │
││ model.layer_stack_for_first_block.0.slf_attn.w_ks                        ││ │
││ model.layer_stack_for_first_block.0.slf_attn.w_vs                        ││ │
││ model.layer_stack_for_first_block.0.slf_attn.attention_operator          ││ │
││ model.layer_stack_for_first_block.0.slf_attn.attention_operator.dropout  ││ │
││ model.layer_stack_for_first_block.0.slf_attn.fc                          ││ │
││ model.layer_stack_for_first_block.0.dropout                              ││ │
││ model.layer_stack_for_first_block.0.layer_norm                           ││ │
││ model.layer_stack_for_first_block.0.pos_ffn                              ││ │
││ model.layer_stack_for_first_block.0.pos_ffn.linear_1                     ││ │
││ model.layer_stack_for_first_block.0.pos_ffn.linear_2                     ││ │
││ model.layer_stack_for_first_block.0.pos_ffn.layer_norm                   ││ │
││ model.layer_stack_for_first_block.0.pos_ffn.dropout                      ││ │
││ model.layer_stack_for_second_block                                       ││ │
││ model.layer_stack_for_second_block.0                                     ││ │
││ model.layer_stack_for_second_block.0.slf_attn                            ││ │
││ model.layer_stack_for_second_block.0.slf_attn.w_qs                       ││ │
││ model.layer_stack_for_second_block.0.slf_attn.w_ks                       ││ │
││ model.layer_stack_for_second_block.0.slf_attn.w_vs                       ││ │
││ model.layer_stack_for_second_block.0.slf_attn.attention_operator         ││ │
││ model.layer_stack_for_second_block.0.slf_attn.attention_operator.dropout ││ │
││ model.layer_stack_for_second_block.0.slf_attn.fc                         ││ │
││ model.layer_stack_for_second_block.0.dropout                             ││ │
││ model.layer_stack_for_second_block.0.layer_norm                          ││ │
││ model.layer_stack_for_second_block.0.pos_ffn                             ││ │
││ model.layer_stack_for_second_block.0.pos_ffn.linear_1                    ││ │
││ model.layer_stack_for_second_block.0.pos_ffn.linear_2                    ││ │
││ model.layer_stack_for_second_block.0.pos_ffn.layer_norm                  ││ │
││ model.layer_stack_for_second_block.0.pos_ffn.dropout                     ││ │
││ model.dropout                                                            ││ │
││ model.position_enc                                                       ││ │
││ model.embedding_1                                                        ││ │
││ model.reduce_dim_z                                                       ││ │
││ model.embedding_2                                                        ││ │
││ model.reduce_dim_beta                                                    ││ │
││ model.reduce_dim_gamma                                                   ││ │
││ model.weight_combine                                                     ││ │
││ model.output_projection                                                  ││ │
││ model.self_attention                                                     ││ │
││ model.output_net                                                         ││ │
││ model.output_net.0                                                       ││ │
││ model.output_net.1                                                       ││ │
││ model.output_net.2                                                       ││ │
││ model.output_net.3                                                       ││ │
││ model.output_net.4                                                       ││ │
││ clf_criterion                                                            ││ │
││ saits_loss_func                                                          ││ │
└┴──────────────────────────────────────────────────────────────────────────┴┴─┘
Trainable params: 107 K                                                         
Non-trainable params: 0                                                         
Total params: 107 K                                                             
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 29.60it/s v_num: 0.000      
                                                              val/auc: 0.628    
                                                              val/f1: 0.588     
                                                              val/precision:    
                                                              0.625 val/recall: 
                                                              0.556 val/mre:    
                                                              0.073 train/auc:  
                                                              0.770 train/f1:   
                                                              0.712             
                                                              train/precision:  
                                                              0.929             
                                                              train/recall:     
                                                              0.578 train/mre:  
                                                              0.078             
[2024-06-24 10:13:58,464][train.py][INFO] - Starting testing...
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
Restoring states from the checkpoint path at /home/khickey/test_impute/logs/debug/multiruns/06-24-10-06-32/advice_yourself/0.05/8/checkpoints/43-0.00.ckpt
Loaded model weights from the checkpoint at /home/khickey/test_impute/logs/debug/multiruns/06-24-10-06-32/advice_yourself/0.05/8/checkpoints/43-0.00.ckpt
SLURM auto-requeueing enabled. Setting signal handlers.
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃        Test metric        ┃       DataLoader 0        ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
│       test/auc_mean       │    0.4599677622318268     │
│       test/auc_std        │    0.10581617057323456    │
│       test/f1_mean        │    0.27394983172416687    │
│        test/f1_std        │    0.15624931454658508    │
│         test/mre          │    0.06959830969572067    │
│        test/mre_ci        │   0.003743700450286269    │
│    test/precision_mean    │     0.380190908908844     │
│    test/precision_std     │    0.23893551528453827    │
│     test/recall_mean      │    0.21434248983860016    │
│      test/recall_std      │    0.14066649973392487    │
└───────────────────────────┴───────────────────────────┘
Testing ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1/1 0:00:00 • 0:00:00 0.00it/s 
[2024-06-24 10:14:13,363][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-24-10-06-32/advice_yourself/0.05/8>
[2024-06-24 10:14:13,364][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-24 10:14:13,366][HYDRA] 	#19 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=False data.question=advice_yourself data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=saits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=True seed=9
[rank: 0] Seed set to 9
[2024-06-24 10:14:14,314][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-24 10:14:14,316][train.py][INFO] - Instantiating model <src.methods.saits.lightningmodule.SAITSLightningModule>
[2024-06-24 10:14:14,318][train.py][INFO] - Instantiating callbacks...
[2024-06-24 10:14:14,318][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-24 10:14:14,321][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-24 10:14:14,321][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-24 10:14:14,322][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-24 10:14:14,323][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-24 10:14:14,324][train.py][INFO] - Instantiating loggers...
[2024-06-24 10:14:14,324][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-24 10:14:14,325][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-24 10:14:14,356][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-24-10-06-32/advice_yourself/0.05/9/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳┳━┓
┃┃ Name                                                                     ┃┃ ┃
┡╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇╇━┩
││ model                                                                    ││ │
││ model.layer_stack_for_first_block                                        ││ │
││ model.layer_stack_for_first_block.0                                      ││ │
││ model.layer_stack_for_first_block.0.slf_attn                             ││ │
││ model.layer_stack_for_first_block.0.slf_attn.w_qs                        ││ │
││ model.layer_stack_for_first_block.0.slf_attn.w_ks                        ││ │
││ model.layer_stack_for_first_block.0.slf_attn.w_vs                        ││ │
││ model.layer_stack_for_first_block.0.slf_attn.attention_operator          ││ │
││ model.layer_stack_for_first_block.0.slf_attn.attention_operator.dropout  ││ │
││ model.layer_stack_for_first_block.0.slf_attn.fc                          ││ │
││ model.layer_stack_for_first_block.0.dropout                              ││ │
││ model.layer_stack_for_first_block.0.layer_norm                           ││ │
││ model.layer_stack_for_first_block.0.pos_ffn                              ││ │
││ model.layer_stack_for_first_block.0.pos_ffn.linear_1                     ││ │
││ model.layer_stack_for_first_block.0.pos_ffn.linear_2                     ││ │
││ model.layer_stack_for_first_block.0.pos_ffn.layer_norm                   ││ │
││ model.layer_stack_for_first_block.0.pos_ffn.dropout                      ││ │
││ model.layer_stack_for_second_block                                       ││ │
││ model.layer_stack_for_second_block.0                                     ││ │
││ model.layer_stack_for_second_block.0.slf_attn                            ││ │
││ model.layer_stack_for_second_block.0.slf_attn.w_qs                       ││ │
││ model.layer_stack_for_second_block.0.slf_attn.w_ks                       ││ │
││ model.layer_stack_for_second_block.0.slf_attn.w_vs                       ││ │
││ model.layer_stack_for_second_block.0.slf_attn.attention_operator         ││ │
││ model.layer_stack_for_second_block.0.slf_attn.attention_operator.dropout ││ │
││ model.layer_stack_for_second_block.0.slf_attn.fc                         ││ │
││ model.layer_stack_for_second_block.0.dropout                             ││ │
││ model.layer_stack_for_second_block.0.layer_norm                          ││ │
││ model.layer_stack_for_second_block.0.pos_ffn                             ││ │
││ model.layer_stack_for_second_block.0.pos_ffn.linear_1                    ││ │
││ model.layer_stack_for_second_block.0.pos_ffn.linear_2                    ││ │
││ model.layer_stack_for_second_block.0.pos_ffn.layer_norm                  ││ │
││ model.layer_stack_for_second_block.0.pos_ffn.dropout                     ││ │
││ model.dropout                                                            ││ │
││ model.position_enc                                                       ││ │
││ model.embedding_1                                                        ││ │
││ model.reduce_dim_z                                                       ││ │
││ model.embedding_2                                                        ││ │
││ model.reduce_dim_beta                                                    ││ │
││ model.reduce_dim_gamma                                                   ││ │
││ model.weight_combine                                                     ││ │
││ model.output_projection                                                  ││ │
││ model.self_attention                                                     ││ │
││ model.output_net                                                         ││ │
││ model.output_net.0                                                       ││ │
││ model.output_net.1                                                       ││ │
││ model.output_net.2                                                       ││ │
││ model.output_net.3                                                       ││ │
││ model.output_net.4                                                       ││ │
││ clf_criterion                                                            ││ │
││ saits_loss_func                                                          ││ │
└┴──────────────────────────────────────────────────────────────────────────┴┴─┘
Trainable params: 107 K                                                         
Non-trainable params: 0                                                         
Total params: 107 K                                                             
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 27.45it/s v_num: 0.000      
                                                              val/auc: 0.917    
                                                              val/f1: 0.909     
                                                              val/precision:    
                                                              1.000 val/recall: 
                                                              0.833 val/mre:    
                                                              0.068 train/auc:  
                                                              0.962 train/f1:   
                                                              0.959             
                                                              train/precision:  
                                                              0.922             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.075             
[2024-06-24 10:14:35,307][train.py][INFO] - Starting testing...
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
Restoring states from the checkpoint path at /home/khickey/test_impute/logs/debug/multiruns/06-24-10-06-32/advice_yourself/0.05/9/checkpoints/43-0.00.ckpt
Loaded model weights from the checkpoint at /home/khickey/test_impute/logs/debug/multiruns/06-24-10-06-32/advice_yourself/0.05/9/checkpoints/43-0.00.ckpt
SLURM auto-requeueing enabled. Setting signal handlers.
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃        Test metric        ┃       DataLoader 0        ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
│       test/auc_mean       │    0.6808874607086182     │
│       test/auc_std        │    0.10601845383644104    │
│       test/f1_mean        │    0.6530039310455322     │
│        test/f1_std        │    0.1302659958600998     │
│         test/mre          │    0.07739613205194473    │
│        test/mre_ci        │   0.004227065946906805    │
│    test/precision_mean    │    0.6683956384658813     │
│    test/precision_std     │     0.160166397690773     │
│     test/recall_mean      │    0.6663178205490112     │
│      test/recall_std      │    0.16686539351940155    │
└───────────────────────────┴───────────────────────────┘
Testing ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1/1 0:00:00 • 0:00:00 0.00it/s 
[2024-06-24 10:14:49,744][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-24-10-06-32/advice_yourself/0.05/9>
[2024-06-24 10:14:49,744][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-24 10:14:49,747][HYDRA] 	#20 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=False data.question=advice_yourself data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=saits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=True seed=0
[2024-06-24 10:14:49,946][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-24 10:14:49,948][train.py][INFO] - Instantiating model <src.methods.saits.lightningmodule.SAITSLightningModule>
[2024-06-24 10:14:49,949][train.py][INFO] - Instantiating callbacks...
[2024-06-24 10:14:49,950][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-24 10:14:49,952][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-24 10:14:49,953][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-24 10:14:49,953][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-24 10:14:49,955][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-24 10:14:49,956][train.py][INFO] - Instantiating loggers...
[2024-06-24 10:14:49,956][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-24 10:14:49,958][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-24 10:14:50,028][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-24-10-06-32/advice_yourself/0.1/0/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳┳━┓
┃┃ Name                                                                     ┃┃ ┃
┡╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇╇━┩
││ model                                                                    ││ │
││ model.layer_stack_for_first_block                                        ││ │
││ model.layer_stack_for_first_block.0                                      ││ │
││ model.layer_stack_for_first_block.0.slf_attn                             ││ │
││ model.layer_stack_for_first_block.0.slf_attn.w_qs                        ││ │
││ model.layer_stack_for_first_block.0.slf_attn.w_ks                        ││ │
││ model.layer_stack_for_first_block.0.slf_attn.w_vs                        ││ │
││ model.layer_stack_for_first_block.0.slf_attn.attention_operator          ││ │
││ model.layer_stack_for_first_block.0.slf_attn.attention_operator.dropout  ││ │
││ model.layer_stack_for_first_block.0.slf_attn.fc                          ││ │
││ model.layer_stack_for_first_block.0.dropout                              ││ │
││ model.layer_stack_for_first_block.0.layer_norm                           ││ │
││ model.layer_stack_for_first_block.0.pos_ffn                              ││ │
││ model.layer_stack_for_first_block.0.pos_ffn.linear_1                     ││ │
││ model.layer_stack_for_first_block.0.pos_ffn.linear_2                     ││ │
││ model.layer_stack_for_first_block.0.pos_ffn.layer_norm                   ││ │
││ model.layer_stack_for_first_block.0.pos_ffn.dropout                      ││ │
││ model.layer_stack_for_second_block                                       ││ │
││ model.layer_stack_for_second_block.0                                     ││ │
││ model.layer_stack_for_second_block.0.slf_attn                            ││ │
││ model.layer_stack_for_second_block.0.slf_attn.w_qs                       ││ │
││ model.layer_stack_for_second_block.0.slf_attn.w_ks                       ││ │
││ model.layer_stack_for_second_block.0.slf_attn.w_vs                       ││ │
││ model.layer_stack_for_second_block.0.slf_attn.attention_operator         ││ │
││ model.layer_stack_for_second_block.0.slf_attn.attention_operator.dropout ││ │
││ model.layer_stack_for_second_block.0.slf_attn.fc                         ││ │
││ model.layer_stack_for_second_block.0.dropout                             ││ │
││ model.layer_stack_for_second_block.0.layer_norm                          ││ │
││ model.layer_stack_for_second_block.0.pos_ffn                             ││ │
││ model.layer_stack_for_second_block.0.pos_ffn.linear_1                    ││ │
││ model.layer_stack_for_second_block.0.pos_ffn.linear_2                    ││ │
││ model.layer_stack_for_second_block.0.pos_ffn.layer_norm                  ││ │
││ model.layer_stack_for_second_block.0.pos_ffn.dropout                     ││ │
││ model.dropout                                                            ││ │
││ model.position_enc                                                       ││ │
││ model.embedding_1                                                        ││ │
││ model.reduce_dim_z                                                       ││ │
││ model.embedding_2                                                        ││ │
││ model.reduce_dim_beta                                                    ││ │
││ model.reduce_dim_gamma                                                   ││ │
││ model.weight_combine                                                     ││ │
││ model.output_projection                                                  ││ │
││ model.self_attention                                                     ││ │
││ model.output_net                                                         ││ │
││ model.output_net.0                                                       ││ │
││ model.output_net.1                                                       ││ │
││ model.output_net.2                                                       ││ │
││ model.output_net.3                                                       ││ │
││ model.output_net.4                                                       ││ │
││ clf_criterion                                                            ││ │
││ saits_loss_func                                                          ││ │
└┴──────────────────────────────────────────────────────────────────────────┴┴─┘
Trainable params: 107 K                                                         
Non-trainable params: 0                                                         
Total params: 107 K                                                             
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 29.78it/s v_num: 0.000      
                                                              val/auc: 0.804    
                                                              val/f1: 0.818     
                                                              val/precision:    
                                                              0.900 val/recall: 
                                                              0.750 val/mre:    
                                                              0.063 train/auc:  
                                                              0.702 train/f1:   
                                                              0.712             
                                                              train/precision:  
                                                              0.771             
                                                              train/recall:     
                                                              0.661 train/mre:  
                                                              0.070             
[2024-06-24 10:15:10,893][train.py][INFO] - Starting testing...
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
Restoring states from the checkpoint path at /home/khickey/test_impute/logs/debug/multiruns/06-24-10-06-32/advice_yourself/0.1/0/checkpoints/40-0.00.ckpt
Loaded model weights from the checkpoint at /home/khickey/test_impute/logs/debug/multiruns/06-24-10-06-32/advice_yourself/0.1/0/checkpoints/40-0.00.ckpt
SLURM auto-requeueing enabled. Setting signal handlers.
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃        Test metric        ┃       DataLoader 0        ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
│       test/auc_mean       │    0.5998926758766174     │
│       test/auc_std        │    0.06359938532114029    │
│       test/f1_mean        │     0.685266375541687     │
│        test/f1_std        │    0.10460204631090164    │
│         test/mre          │    0.07562214136123657    │
│        test/mre_ci        │   0.004060822539031506    │
│    test/precision_mean    │    0.5285893678665161     │
│    test/precision_std     │    0.12012097984552383    │
│     test/recall_mean      │            1.0            │
│      test/recall_std      │            0.0            │
└───────────────────────────┴───────────────────────────┘
Testing ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1/1 0:00:00 • 0:00:00 0.00it/s 
[2024-06-24 10:15:24,915][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-24-10-06-32/advice_yourself/0.1/0>
[2024-06-24 10:15:24,916][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-24 10:15:24,919][HYDRA] 	#21 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=False data.question=advice_yourself data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=saits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=True seed=1
[rank: 0] Seed set to 1
[2024-06-24 10:15:25,104][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-24 10:15:25,106][train.py][INFO] - Instantiating model <src.methods.saits.lightningmodule.SAITSLightningModule>
[2024-06-24 10:15:25,107][train.py][INFO] - Instantiating callbacks...
[2024-06-24 10:15:25,108][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-24 10:15:25,110][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-24 10:15:25,111][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-24 10:15:25,111][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-24 10:15:25,112][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-24 10:15:25,113][train.py][INFO] - Instantiating loggers...
[2024-06-24 10:15:25,113][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-24 10:15:25,114][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-24 10:15:25,145][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-24-10-06-32/advice_yourself/0.1/1/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳┳━┓
┃┃ Name                                                                     ┃┃ ┃
┡╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇╇━┩
││ model                                                                    ││ │
││ model.layer_stack_for_first_block                                        ││ │
││ model.layer_stack_for_first_block.0                                      ││ │
││ model.layer_stack_for_first_block.0.slf_attn                             ││ │
││ model.layer_stack_for_first_block.0.slf_attn.w_qs                        ││ │
││ model.layer_stack_for_first_block.0.slf_attn.w_ks                        ││ │
││ model.layer_stack_for_first_block.0.slf_attn.w_vs                        ││ │
││ model.layer_stack_for_first_block.0.slf_attn.attention_operator          ││ │
││ model.layer_stack_for_first_block.0.slf_attn.attention_operator.dropout  ││ │
││ model.layer_stack_for_first_block.0.slf_attn.fc                          ││ │
││ model.layer_stack_for_first_block.0.dropout                              ││ │
││ model.layer_stack_for_first_block.0.layer_norm                           ││ │
││ model.layer_stack_for_first_block.0.pos_ffn                              ││ │
││ model.layer_stack_for_first_block.0.pos_ffn.linear_1                     ││ │
││ model.layer_stack_for_first_block.0.pos_ffn.linear_2                     ││ │
││ model.layer_stack_for_first_block.0.pos_ffn.layer_norm                   ││ │
││ model.layer_stack_for_first_block.0.pos_ffn.dropout                      ││ │
││ model.layer_stack_for_second_block                                       ││ │
││ model.layer_stack_for_second_block.0                                     ││ │
││ model.layer_stack_for_second_block.0.slf_attn                            ││ │
││ model.layer_stack_for_second_block.0.slf_attn.w_qs                       ││ │
││ model.layer_stack_for_second_block.0.slf_attn.w_ks                       ││ │
││ model.layer_stack_for_second_block.0.slf_attn.w_vs                       ││ │
││ model.layer_stack_for_second_block.0.slf_attn.attention_operator         ││ │
││ model.layer_stack_for_second_block.0.slf_attn.attention_operator.dropout ││ │
││ model.layer_stack_for_second_block.0.slf_attn.fc                         ││ │
││ model.layer_stack_for_second_block.0.dropout                             ││ │
││ model.layer_stack_for_second_block.0.layer_norm                          ││ │
││ model.layer_stack_for_second_block.0.pos_ffn                             ││ │
││ model.layer_stack_for_second_block.0.pos_ffn.linear_1                    ││ │
││ model.layer_stack_for_second_block.0.pos_ffn.linear_2                    ││ │
││ model.layer_stack_for_second_block.0.pos_ffn.layer_norm                  ││ │
││ model.layer_stack_for_second_block.0.pos_ffn.dropout                     ││ │
││ model.dropout                                                            ││ │
││ model.position_enc                                                       ││ │
││ model.embedding_1                                                        ││ │
││ model.reduce_dim_z                                                       ││ │
││ model.embedding_2                                                        ││ │
││ model.reduce_dim_beta                                                    ││ │
││ model.reduce_dim_gamma                                                   ││ │
││ model.weight_combine                                                     ││ │
││ model.output_projection                                                  ││ │
││ model.self_attention                                                     ││ │
││ model.output_net                                                         ││ │
││ model.output_net.0                                                       ││ │
││ model.output_net.1                                                       ││ │
││ model.output_net.2                                                       ││ │
││ model.output_net.3                                                       ││ │
││ model.output_net.4                                                       ││ │
││ clf_criterion                                                            ││ │
││ saits_loss_func                                                          ││ │
└┴──────────────────────────────────────────────────────────────────────────┴┴─┘
Trainable params: 107 K                                                         
Non-trainable params: 0                                                         
Total params: 107 K                                                             
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 30.25it/s v_num: 0.000      
                                                              val/auc: 1.000    
                                                              val/f1: 1.000     
                                                              val/precision:    
                                                              1.000 val/recall: 
                                                              1.000 val/mre:    
                                                              0.064 train/auc:  
                                                              0.959 train/f1:   
                                                              0.962             
                                                              train/precision:  
                                                              0.962             
                                                              train/recall:     
                                                              0.962 train/mre:  
                                                              0.074             
[2024-06-24 10:15:45,751][train.py][INFO] - Starting testing...
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
Restoring states from the checkpoint path at /home/khickey/test_impute/logs/debug/multiruns/06-24-10-06-32/advice_yourself/0.1/1/checkpoints/37-0.00.ckpt
Loaded model weights from the checkpoint at /home/khickey/test_impute/logs/debug/multiruns/06-24-10-06-32/advice_yourself/0.1/1/checkpoints/37-0.00.ckpt
SLURM auto-requeueing enabled. Setting signal handlers.
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃        Test metric        ┃       DataLoader 0        ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
│       test/auc_mean       │    0.5216857194900513     │
│       test/auc_std        │    0.11358825862407684    │
│       test/f1_mean        │    0.4538022577762604     │
│        test/f1_std        │    0.14648881554603577    │
│         test/mre          │    0.08965224772691727    │
│        test/mre_ci        │   0.005846414715051651    │
│    test/precision_mean    │    0.5011797547340393     │
│    test/precision_std     │    0.18573544919490814    │
│     test/recall_mean      │    0.43958768248558044    │
│      test/recall_std      │    0.1731434464454651     │
└───────────────────────────┴───────────────────────────┘
Testing ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1/1 0:00:00 • 0:00:00 0.00it/s 
[2024-06-24 10:16:00,602][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-24-10-06-32/advice_yourself/0.1/1>
[2024-06-24 10:16:00,602][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-24 10:16:00,605][HYDRA] 	#22 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=False data.question=advice_yourself data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=saits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=True seed=2
[rank: 0] Seed set to 2
[2024-06-24 10:16:01,565][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-24 10:16:01,567][train.py][INFO] - Instantiating model <src.methods.saits.lightningmodule.SAITSLightningModule>
[2024-06-24 10:16:01,568][train.py][INFO] - Instantiating callbacks...
[2024-06-24 10:16:01,568][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-24 10:16:01,571][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-24 10:16:01,571][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-24 10:16:01,572][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-24 10:16:01,574][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-24 10:16:01,575][train.py][INFO] - Instantiating loggers...
[2024-06-24 10:16:01,575][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-24 10:16:01,577][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
[2024-06-24 10:16:01,649][train.py][INFO] - Starting training...
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-24-10-06-32/advice_yourself/0.1/2/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳┳━┓
┃┃ Name                                                                     ┃┃ ┃
┡╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇╇━┩
││ model                                                                    ││ │
││ model.layer_stack_for_first_block                                        ││ │
││ model.layer_stack_for_first_block.0                                      ││ │
││ model.layer_stack_for_first_block.0.slf_attn                             ││ │
││ model.layer_stack_for_first_block.0.slf_attn.w_qs                        ││ │
││ model.layer_stack_for_first_block.0.slf_attn.w_ks                        ││ │
││ model.layer_stack_for_first_block.0.slf_attn.w_vs                        ││ │
││ model.layer_stack_for_first_block.0.slf_attn.attention_operator          ││ │
││ model.layer_stack_for_first_block.0.slf_attn.attention_operator.dropout  ││ │
││ model.layer_stack_for_first_block.0.slf_attn.fc                          ││ │
││ model.layer_stack_for_first_block.0.dropout                              ││ │
││ model.layer_stack_for_first_block.0.layer_norm                           ││ │
││ model.layer_stack_for_first_block.0.pos_ffn                              ││ │
││ model.layer_stack_for_first_block.0.pos_ffn.linear_1                     ││ │
││ model.layer_stack_for_first_block.0.pos_ffn.linear_2                     ││ │
││ model.layer_stack_for_first_block.0.pos_ffn.layer_norm                   ││ │
││ model.layer_stack_for_first_block.0.pos_ffn.dropout                      ││ │
││ model.layer_stack_for_second_block                                       ││ │
││ model.layer_stack_for_second_block.0                                     ││ │
││ model.layer_stack_for_second_block.0.slf_attn                            ││ │
││ model.layer_stack_for_second_block.0.slf_attn.w_qs                       ││ │
││ model.layer_stack_for_second_block.0.slf_attn.w_ks                       ││ │
││ model.layer_stack_for_second_block.0.slf_attn.w_vs                       ││ │
││ model.layer_stack_for_second_block.0.slf_attn.attention_operator         ││ │
││ model.layer_stack_for_second_block.0.slf_attn.attention_operator.dropout ││ │
││ model.layer_stack_for_second_block.0.slf_attn.fc                         ││ │
││ model.layer_stack_for_second_block.0.dropout                             ││ │
││ model.layer_stack_for_second_block.0.layer_norm                          ││ │
││ model.layer_stack_for_second_block.0.pos_ffn                             ││ │
││ model.layer_stack_for_second_block.0.pos_ffn.linear_1                    ││ │
││ model.layer_stack_for_second_block.0.pos_ffn.linear_2                    ││ │
││ model.layer_stack_for_second_block.0.pos_ffn.layer_norm                  ││ │
││ model.layer_stack_for_second_block.0.pos_ffn.dropout                     ││ │
││ model.dropout                                                            ││ │
││ model.position_enc                                                       ││ │
││ model.embedding_1                                                        ││ │
││ model.reduce_dim_z                                                       ││ │
││ model.embedding_2                                                        ││ │
││ model.reduce_dim_beta                                                    ││ │
││ model.reduce_dim_gamma                                                   ││ │
││ model.weight_combine                                                     ││ │
││ model.output_projection                                                  ││ │
││ model.self_attention                                                     ││ │
││ model.output_net                                                         ││ │
││ model.output_net.0                                                       ││ │
││ model.output_net.1                                                       ││ │
││ model.output_net.2                                                       ││ │
││ model.output_net.3                                                       ││ │
││ model.output_net.4                                                       ││ │
││ clf_criterion                                                            ││ │
││ saits_loss_func                                                          ││ │
└┴──────────────────────────────────────────────────────────────────────────┴┴─┘
Trainable params: 107 K                                                         
Non-trainable params: 0                                                         
Total params: 107 K                                                             
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 28.09it/s v_num: 0.000      
                                                              val/auc: 0.850    
                                                              val/f1: 0.824     
                                                              val/precision:    
                                                              1.000 val/recall: 
                                                              0.700 val/mre:    
                                                              0.071 train/auc:  
                                                              0.667 train/f1:   
                                                              0.500             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              0.333 train/mre:  
                                                              0.079             
[2024-06-24 10:16:22,677][train.py][INFO] - Starting testing...
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
Restoring states from the checkpoint path at /home/khickey/test_impute/logs/debug/multiruns/06-24-10-06-32/advice_yourself/0.1/2/checkpoints/47-0.00.ckpt
Loaded model weights from the checkpoint at /home/khickey/test_impute/logs/debug/multiruns/06-24-10-06-32/advice_yourself/0.1/2/checkpoints/47-0.00.ckpt
SLURM auto-requeueing enabled. Setting signal handlers.
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃        Test metric        ┃       DataLoader 0        ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
│       test/auc_mean       │    0.5780839920043945     │
│       test/auc_std        │    0.11721840500831604    │
│       test/f1_mean        │    0.5517993569374084     │
│        test/f1_std        │    0.14529813826084137    │
│         test/mre          │    0.07574209570884705    │
│        test/mre_ci        │   0.0041990079917013645   │
│    test/precision_mean    │    0.5422911047935486     │
│    test/precision_std     │    0.18139006197452545    │
│     test/recall_mean      │    0.5481822490692139     │
│      test/recall_std      │    0.16361188888549805    │
└───────────────────────────┴───────────────────────────┘
Testing ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1/1 0:00:00 • 0:00:00 0.00it/s 
[2024-06-24 10:16:37,289][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-24-10-06-32/advice_yourself/0.1/2>
[2024-06-24 10:16:37,289][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-24 10:16:37,291][HYDRA] 	#23 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=False data.question=advice_yourself data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=saits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=True seed=3
[rank: 0] Seed set to 3
[2024-06-24 10:16:37,482][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-24 10:16:37,484][train.py][INFO] - Instantiating model <src.methods.saits.lightningmodule.SAITSLightningModule>
[2024-06-24 10:16:37,486][train.py][INFO] - Instantiating callbacks...
[2024-06-24 10:16:37,486][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-24 10:16:37,489][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-24 10:16:37,489][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-24 10:16:37,489][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-24 10:16:37,491][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-24 10:16:37,491][train.py][INFO] - Instantiating loggers...
[2024-06-24 10:16:37,491][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-24 10:16:37,493][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
[2024-06-24 10:16:37,525][train.py][INFO] - Starting training...
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-24-10-06-32/advice_yourself/0.1/3/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳┳━┓
┃┃ Name                                                                     ┃┃ ┃
┡╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇╇━┩
││ model                                                                    ││ │
││ model.layer_stack_for_first_block                                        ││ │
││ model.layer_stack_for_first_block.0                                      ││ │
││ model.layer_stack_for_first_block.0.slf_attn                             ││ │
││ model.layer_stack_for_first_block.0.slf_attn.w_qs                        ││ │
││ model.layer_stack_for_first_block.0.slf_attn.w_ks                        ││ │
││ model.layer_stack_for_first_block.0.slf_attn.w_vs                        ││ │
││ model.layer_stack_for_first_block.0.slf_attn.attention_operator          ││ │
││ model.layer_stack_for_first_block.0.slf_attn.attention_operator.dropout  ││ │
││ model.layer_stack_for_first_block.0.slf_attn.fc                          ││ │
││ model.layer_stack_for_first_block.0.dropout                              ││ │
││ model.layer_stack_for_first_block.0.layer_norm                           ││ │
││ model.layer_stack_for_first_block.0.pos_ffn                              ││ │
││ model.layer_stack_for_first_block.0.pos_ffn.linear_1                     ││ │
││ model.layer_stack_for_first_block.0.pos_ffn.linear_2                     ││ │
││ model.layer_stack_for_first_block.0.pos_ffn.layer_norm                   ││ │
││ model.layer_stack_for_first_block.0.pos_ffn.dropout                      ││ │
││ model.layer_stack_for_second_block                                       ││ │
││ model.layer_stack_for_second_block.0                                     ││ │
││ model.layer_stack_for_second_block.0.slf_attn                            ││ │
││ model.layer_stack_for_second_block.0.slf_attn.w_qs                       ││ │
││ model.layer_stack_for_second_block.0.slf_attn.w_ks                       ││ │
││ model.layer_stack_for_second_block.0.slf_attn.w_vs                       ││ │
││ model.layer_stack_for_second_block.0.slf_attn.attention_operator         ││ │
││ model.layer_stack_for_second_block.0.slf_attn.attention_operator.dropout ││ │
││ model.layer_stack_for_second_block.0.slf_attn.fc                         ││ │
││ model.layer_stack_for_second_block.0.dropout                             ││ │
││ model.layer_stack_for_second_block.0.layer_norm                          ││ │
││ model.layer_stack_for_second_block.0.pos_ffn                             ││ │
││ model.layer_stack_for_second_block.0.pos_ffn.linear_1                    ││ │
││ model.layer_stack_for_second_block.0.pos_ffn.linear_2                    ││ │
││ model.layer_stack_for_second_block.0.pos_ffn.layer_norm                  ││ │
││ model.layer_stack_for_second_block.0.pos_ffn.dropout                     ││ │
││ model.dropout                                                            ││ │
││ model.position_enc                                                       ││ │
││ model.embedding_1                                                        ││ │
││ model.reduce_dim_z                                                       ││ │
││ model.embedding_2                                                        ││ │
││ model.reduce_dim_beta                                                    ││ │
││ model.reduce_dim_gamma                                                   ││ │
││ model.weight_combine                                                     ││ │
││ model.output_projection                                                  ││ │
││ model.self_attention                                                     ││ │
││ model.output_net                                                         ││ │
││ model.output_net.0                                                       ││ │
││ model.output_net.1                                                       ││ │
││ model.output_net.2                                                       ││ │
││ model.output_net.3                                                       ││ │
││ model.output_net.4                                                       ││ │
││ clf_criterion                                                            ││ │
││ saits_loss_func                                                          ││ │
└┴──────────────────────────────────────────────────────────────────────────┴┴─┘
Trainable params: 107 K                                                         
Non-trainable params: 0                                                         
Total params: 107 K                                                             
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 27.98it/s v_num: 0.000      
                                                              val/auc: 1.000    
                                                              val/f1: 1.000     
                                                              val/precision:    
                                                              1.000 val/recall: 
                                                              1.000 val/mre:    
                                                              0.072 train/auc:  
                                                              0.958 train/f1:   
                                                              0.957             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              0.917 train/mre:  
                                                              0.085             
[2024-06-24 10:16:58,426][train.py][INFO] - Starting testing...
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
Restoring states from the checkpoint path at /home/khickey/test_impute/logs/debug/multiruns/06-24-10-06-32/advice_yourself/0.1/3/checkpoints/44-0.00.ckpt
Loaded model weights from the checkpoint at /home/khickey/test_impute/logs/debug/multiruns/06-24-10-06-32/advice_yourself/0.1/3/checkpoints/44-0.00.ckpt
SLURM auto-requeueing enabled. Setting signal handlers.
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃        Test metric        ┃       DataLoader 0        ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
│       test/auc_mean       │    0.5049858689308167     │
│       test/auc_std        │    0.0744934007525444     │
│       test/f1_mean        │    0.16475540399551392    │
│        test/f1_std        │    0.14916138350963593    │
│         test/mre          │    0.07871204614639282    │
│        test/mre_ci        │   0.0034601720981299877   │
│    test/precision_mean    │    0.4265047609806061     │
│    test/precision_std     │     0.391072541475296     │
│     test/recall_mean      │    0.11073128879070282    │
│      test/recall_std      │    0.10911662131547928    │
└───────────────────────────┴───────────────────────────┘
Testing ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1/1 0:00:00 • 0:00:00 0.00it/s 
[2024-06-24 10:17:13,016][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-24-10-06-32/advice_yourself/0.1/3>
[2024-06-24 10:17:13,016][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-24 10:17:13,019][HYDRA] 	#24 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=False data.question=advice_yourself data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=saits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=True seed=4
[rank: 0] Seed set to 4
[2024-06-24 10:17:13,214][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-24 10:17:13,215][train.py][INFO] - Instantiating model <src.methods.saits.lightningmodule.SAITSLightningModule>
[2024-06-24 10:17:13,217][train.py][INFO] - Instantiating callbacks...
[2024-06-24 10:17:13,217][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-24 10:17:13,220][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-24 10:17:13,220][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-24 10:17:13,221][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-24 10:17:13,223][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-24 10:17:13,223][train.py][INFO] - Instantiating loggers...
[2024-06-24 10:17:13,223][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-24 10:17:13,225][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-24 10:17:13,294][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-24-10-06-32/advice_yourself/0.1/4/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳┳━┓
┃┃ Name                                                                     ┃┃ ┃
┡╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇╇━┩
││ model                                                                    ││ │
││ model.layer_stack_for_first_block                                        ││ │
││ model.layer_stack_for_first_block.0                                      ││ │
││ model.layer_stack_for_first_block.0.slf_attn                             ││ │
││ model.layer_stack_for_first_block.0.slf_attn.w_qs                        ││ │
││ model.layer_stack_for_first_block.0.slf_attn.w_ks                        ││ │
││ model.layer_stack_for_first_block.0.slf_attn.w_vs                        ││ │
││ model.layer_stack_for_first_block.0.slf_attn.attention_operator          ││ │
││ model.layer_stack_for_first_block.0.slf_attn.attention_operator.dropout  ││ │
││ model.layer_stack_for_first_block.0.slf_attn.fc                          ││ │
││ model.layer_stack_for_first_block.0.dropout                              ││ │
││ model.layer_stack_for_first_block.0.layer_norm                           ││ │
││ model.layer_stack_for_first_block.0.pos_ffn                              ││ │
││ model.layer_stack_for_first_block.0.pos_ffn.linear_1                     ││ │
││ model.layer_stack_for_first_block.0.pos_ffn.linear_2                     ││ │
││ model.layer_stack_for_first_block.0.pos_ffn.layer_norm                   ││ │
││ model.layer_stack_for_first_block.0.pos_ffn.dropout                      ││ │
││ model.layer_stack_for_second_block                                       ││ │
││ model.layer_stack_for_second_block.0                                     ││ │
││ model.layer_stack_for_second_block.0.slf_attn                            ││ │
││ model.layer_stack_for_second_block.0.slf_attn.w_qs                       ││ │
││ model.layer_stack_for_second_block.0.slf_attn.w_ks                       ││ │
││ model.layer_stack_for_second_block.0.slf_attn.w_vs                       ││ │
││ model.layer_stack_for_second_block.0.slf_attn.attention_operator         ││ │
││ model.layer_stack_for_second_block.0.slf_attn.attention_operator.dropout ││ │
││ model.layer_stack_for_second_block.0.slf_attn.fc                         ││ │
││ model.layer_stack_for_second_block.0.dropout                             ││ │
││ model.layer_stack_for_second_block.0.layer_norm                          ││ │
││ model.layer_stack_for_second_block.0.pos_ffn                             ││ │
││ model.layer_stack_for_second_block.0.pos_ffn.linear_1                    ││ │
││ model.layer_stack_for_second_block.0.pos_ffn.linear_2                    ││ │
││ model.layer_stack_for_second_block.0.pos_ffn.layer_norm                  ││ │
││ model.layer_stack_for_second_block.0.pos_ffn.dropout                     ││ │
││ model.dropout                                                            ││ │
││ model.position_enc                                                       ││ │
││ model.embedding_1                                                        ││ │
││ model.reduce_dim_z                                                       ││ │
││ model.embedding_2                                                        ││ │
││ model.reduce_dim_beta                                                    ││ │
││ model.reduce_dim_gamma                                                   ││ │
││ model.weight_combine                                                     ││ │
││ model.output_projection                                                  ││ │
││ model.self_attention                                                     ││ │
││ model.output_net                                                         ││ │
││ model.output_net.0                                                       ││ │
││ model.output_net.1                                                       ││ │
││ model.output_net.2                                                       ││ │
││ model.output_net.3                                                       ││ │
││ model.output_net.4                                                       ││ │
││ clf_criterion                                                            ││ │
││ saits_loss_func                                                          ││ │
└┴──────────────────────────────────────────────────────────────────────────┴┴─┘
Trainable params: 107 K                                                         
Non-trainable params: 0                                                         
Total params: 107 K                                                             
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 28.25it/s v_num: 0.000      
                                                              val/auc: 1.000    
                                                              val/f1: 1.000     
                                                              val/precision:    
                                                              1.000 val/recall: 
                                                              1.000 val/mre:    
                                                              0.060 train/auc:  
                                                              0.949 train/f1:   
                                                              0.945             
                                                              train/precision:  
                                                              0.956             
                                                              train/recall:     
                                                              0.935 train/mre:  
                                                              0.073             
[2024-06-24 10:17:33,981][train.py][INFO] - Starting testing...
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
Restoring states from the checkpoint path at /home/khickey/test_impute/logs/debug/multiruns/06-24-10-06-32/advice_yourself/0.1/4/checkpoints/47-0.00.ckpt
Loaded model weights from the checkpoint at /home/khickey/test_impute/logs/debug/multiruns/06-24-10-06-32/advice_yourself/0.1/4/checkpoints/47-0.00.ckpt
SLURM auto-requeueing enabled. Setting signal handlers.
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃        Test metric        ┃       DataLoader 0        ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
│       test/auc_mean       │    0.4554140269756317     │
│       test/auc_std        │    0.08335398137569427    │
│       test/f1_mean        │    0.15579357743263245    │
│        test/f1_std        │    0.14944429695606232    │
│         test/mre          │    0.07203461229801178    │
│        test/mre_ci        │   0.004137228708714247    │
│    test/precision_mean    │    0.30586111545562744    │
│    test/precision_std     │    0.3080989420413971     │
│     test/recall_mean      │    0.1108376756310463     │
│      test/recall_std      │    0.10737348347902298    │
└───────────────────────────┴───────────────────────────┘
Testing ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1/1 0:00:00 • 0:00:00 0.00it/s 
[2024-06-24 10:17:48,726][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-24-10-06-32/advice_yourself/0.1/4>
[2024-06-24 10:17:48,726][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-24 10:17:48,728][HYDRA] 	#25 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=False data.question=advice_yourself data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=saits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=True seed=5
[rank: 0] Seed set to 5
[2024-06-24 10:17:48,919][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-24 10:17:48,921][train.py][INFO] - Instantiating model <src.methods.saits.lightningmodule.SAITSLightningModule>
[2024-06-24 10:17:48,922][train.py][INFO] - Instantiating callbacks...
[2024-06-24 10:17:48,923][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-24 10:17:48,925][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-24 10:17:48,926][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-24 10:17:48,926][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-24 10:17:48,928][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-24 10:17:48,928][train.py][INFO] - Instantiating loggers...
[2024-06-24 10:17:48,928][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-24 10:17:48,930][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-24 10:17:48,961][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-24-10-06-32/advice_yourself/0.1/5/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳┳━┓
┃┃ Name                                                                     ┃┃ ┃
┡╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇╇━┩
││ model                                                                    ││ │
││ model.layer_stack_for_first_block                                        ││ │
││ model.layer_stack_for_first_block.0                                      ││ │
││ model.layer_stack_for_first_block.0.slf_attn                             ││ │
││ model.layer_stack_for_first_block.0.slf_attn.w_qs                        ││ │
││ model.layer_stack_for_first_block.0.slf_attn.w_ks                        ││ │
││ model.layer_stack_for_first_block.0.slf_attn.w_vs                        ││ │
││ model.layer_stack_for_first_block.0.slf_attn.attention_operator          ││ │
││ model.layer_stack_for_first_block.0.slf_attn.attention_operator.dropout  ││ │
││ model.layer_stack_for_first_block.0.slf_attn.fc                          ││ │
││ model.layer_stack_for_first_block.0.dropout                              ││ │
││ model.layer_stack_for_first_block.0.layer_norm                           ││ │
││ model.layer_stack_for_first_block.0.pos_ffn                              ││ │
││ model.layer_stack_for_first_block.0.pos_ffn.linear_1                     ││ │
││ model.layer_stack_for_first_block.0.pos_ffn.linear_2                     ││ │
││ model.layer_stack_for_first_block.0.pos_ffn.layer_norm                   ││ │
││ model.layer_stack_for_first_block.0.pos_ffn.dropout                      ││ │
││ model.layer_stack_for_second_block                                       ││ │
││ model.layer_stack_for_second_block.0                                     ││ │
││ model.layer_stack_for_second_block.0.slf_attn                            ││ │
││ model.layer_stack_for_second_block.0.slf_attn.w_qs                       ││ │
││ model.layer_stack_for_second_block.0.slf_attn.w_ks                       ││ │
││ model.layer_stack_for_second_block.0.slf_attn.w_vs                       ││ │
││ model.layer_stack_for_second_block.0.slf_attn.attention_operator         ││ │
││ model.layer_stack_for_second_block.0.slf_attn.attention_operator.dropout ││ │
││ model.layer_stack_for_second_block.0.slf_attn.fc                         ││ │
││ model.layer_stack_for_second_block.0.dropout                             ││ │
││ model.layer_stack_for_second_block.0.layer_norm                          ││ │
││ model.layer_stack_for_second_block.0.pos_ffn                             ││ │
││ model.layer_stack_for_second_block.0.pos_ffn.linear_1                    ││ │
││ model.layer_stack_for_second_block.0.pos_ffn.linear_2                    ││ │
││ model.layer_stack_for_second_block.0.pos_ffn.layer_norm                  ││ │
││ model.layer_stack_for_second_block.0.pos_ffn.dropout                     ││ │
││ model.dropout                                                            ││ │
││ model.position_enc                                                       ││ │
││ model.embedding_1                                                        ││ │
││ model.reduce_dim_z                                                       ││ │
││ model.embedding_2                                                        ││ │
││ model.reduce_dim_beta                                                    ││ │
││ model.reduce_dim_gamma                                                   ││ │
││ model.weight_combine                                                     ││ │
││ model.output_projection                                                  ││ │
││ model.self_attention                                                     ││ │
││ model.output_net                                                         ││ │
││ model.output_net.0                                                       ││ │
││ model.output_net.1                                                       ││ │
││ model.output_net.2                                                       ││ │
││ model.output_net.3                                                       ││ │
││ model.output_net.4                                                       ││ │
││ clf_criterion                                                            ││ │
││ saits_loss_func                                                          ││ │
└┴──────────────────────────────────────────────────────────────────────────┴┴─┘
Trainable params: 107 K                                                         
Non-trainable params: 0                                                         
Total params: 107 K                                                             
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 29.24it/s v_num: 0.000      
                                                              val/auc: 0.423    
                                                              val/f1: 0.733     
                                                              val/precision:    
                                                              0.647 val/recall: 
                                                              0.846 val/mre:    
                                                              0.059 train/auc:  
                                                              0.553 train/f1:   
                                                              0.695             
                                                              train/precision:  
                                                              0.544             
                                                              train/recall:     
                                                              0.961 train/mre:  
                                                              0.074             
[2024-06-24 10:18:10,889][train.py][INFO] - Starting testing...
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
Restoring states from the checkpoint path at /home/khickey/test_impute/logs/debug/multiruns/06-24-10-06-32/advice_yourself/0.1/5/checkpoints/13-0.00.ckpt
Loaded model weights from the checkpoint at /home/khickey/test_impute/logs/debug/multiruns/06-24-10-06-32/advice_yourself/0.1/5/checkpoints/13-0.00.ckpt
SLURM auto-requeueing enabled. Setting signal handlers.
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃        Test metric        ┃       DataLoader 0        ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
│       test/auc_mean       │    0.7001746892929077     │
│       test/auc_std        │    0.0792091116309166     │
│       test/f1_mean        │    0.7392142415046692     │
│        test/f1_std        │    0.10578273236751556    │
│         test/mre          │    0.07482097297906876    │
│        test/mre_ci        │   0.003836342366412282    │
│    test/precision_mean    │    0.6052387952804565     │
│    test/precision_std     │    0.12658926844596863    │
│     test/recall_mean      │            1.0            │
│      test/recall_std      │            0.0            │
└───────────────────────────┴───────────────────────────┘
Testing ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1/1 0:00:00 • 0:00:00 0.00it/s 
[2024-06-24 10:18:25,531][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-24-10-06-32/advice_yourself/0.1/5>
[2024-06-24 10:18:25,531][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-24 10:18:25,534][HYDRA] 	#26 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=False data.question=advice_yourself data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=saits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=True seed=6
[rank: 0] Seed set to 6
[2024-06-24 10:18:25,726][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-24 10:18:25,728][train.py][INFO] - Instantiating model <src.methods.saits.lightningmodule.SAITSLightningModule>
[2024-06-24 10:18:25,730][train.py][INFO] - Instantiating callbacks...
[2024-06-24 10:18:25,730][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-24 10:18:25,733][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-24 10:18:25,733][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-24 10:18:25,734][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-24 10:18:25,736][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-24 10:18:25,736][train.py][INFO] - Instantiating loggers...
[2024-06-24 10:18:25,736][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-24 10:18:25,738][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-24 10:18:25,809][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-24-10-06-32/advice_yourself/0.1/6/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳┳━┓
┃┃ Name                                                                     ┃┃ ┃
┡╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇╇━┩
││ model                                                                    ││ │
││ model.layer_stack_for_first_block                                        ││ │
││ model.layer_stack_for_first_block.0                                      ││ │
││ model.layer_stack_for_first_block.0.slf_attn                             ││ │
││ model.layer_stack_for_first_block.0.slf_attn.w_qs                        ││ │
││ model.layer_stack_for_first_block.0.slf_attn.w_ks                        ││ │
││ model.layer_stack_for_first_block.0.slf_attn.w_vs                        ││ │
││ model.layer_stack_for_first_block.0.slf_attn.attention_operator          ││ │
││ model.layer_stack_for_first_block.0.slf_attn.attention_operator.dropout  ││ │
││ model.layer_stack_for_first_block.0.slf_attn.fc                          ││ │
││ model.layer_stack_for_first_block.0.dropout                              ││ │
││ model.layer_stack_for_first_block.0.layer_norm                           ││ │
││ model.layer_stack_for_first_block.0.pos_ffn                              ││ │
││ model.layer_stack_for_first_block.0.pos_ffn.linear_1                     ││ │
││ model.layer_stack_for_first_block.0.pos_ffn.linear_2                     ││ │
││ model.layer_stack_for_first_block.0.pos_ffn.layer_norm                   ││ │
││ model.layer_stack_for_first_block.0.pos_ffn.dropout                      ││ │
││ model.layer_stack_for_second_block                                       ││ │
││ model.layer_stack_for_second_block.0                                     ││ │
││ model.layer_stack_for_second_block.0.slf_attn                            ││ │
││ model.layer_stack_for_second_block.0.slf_attn.w_qs                       ││ │
││ model.layer_stack_for_second_block.0.slf_attn.w_ks                       ││ │
││ model.layer_stack_for_second_block.0.slf_attn.w_vs                       ││ │
││ model.layer_stack_for_second_block.0.slf_attn.attention_operator         ││ │
││ model.layer_stack_for_second_block.0.slf_attn.attention_operator.dropout ││ │
││ model.layer_stack_for_second_block.0.slf_attn.fc                         ││ │
││ model.layer_stack_for_second_block.0.dropout                             ││ │
││ model.layer_stack_for_second_block.0.layer_norm                          ││ │
││ model.layer_stack_for_second_block.0.pos_ffn                             ││ │
││ model.layer_stack_for_second_block.0.pos_ffn.linear_1                    ││ │
││ model.layer_stack_for_second_block.0.pos_ffn.linear_2                    ││ │
││ model.layer_stack_for_second_block.0.pos_ffn.layer_norm                  ││ │
││ model.layer_stack_for_second_block.0.pos_ffn.dropout                     ││ │
││ model.dropout                                                            ││ │
││ model.position_enc                                                       ││ │
││ model.embedding_1                                                        ││ │
││ model.reduce_dim_z                                                       ││ │
││ model.embedding_2                                                        ││ │
││ model.reduce_dim_beta                                                    ││ │
││ model.reduce_dim_gamma                                                   ││ │
││ model.weight_combine                                                     ││ │
││ model.output_projection                                                  ││ │
││ model.self_attention                                                     ││ │
││ model.output_net                                                         ││ │
││ model.output_net.0                                                       ││ │
││ model.output_net.1                                                       ││ │
││ model.output_net.2                                                       ││ │
││ model.output_net.3                                                       ││ │
││ model.output_net.4                                                       ││ │
││ clf_criterion                                                            ││ │
││ saits_loss_func                                                          ││ │
└┴──────────────────────────────────────────────────────────────────────────┴┴─┘
Trainable params: 107 K                                                         
Non-trainable params: 0                                                         
Total params: 107 K                                                             
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 29.77it/s v_num: 0.000      
                                                              val/auc: 0.857    
                                                              val/f1: 0.923     
                                                              val/precision:    
                                                              0.857 val/recall: 
                                                              1.000 val/mre:    
                                                              0.093 train/auc:  
                                                              0.867 train/f1:   
                                                              0.879             
                                                              train/precision:  
                                                              0.870             
                                                              train/recall:     
                                                              0.887 train/mre:  
                                                              0.081             
[2024-06-24 10:18:46,357][train.py][INFO] - Starting testing...
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
Restoring states from the checkpoint path at /home/khickey/test_impute/logs/debug/multiruns/06-24-10-06-32/advice_yourself/0.1/6/checkpoints/42-0.00.ckpt
Loaded model weights from the checkpoint at /home/khickey/test_impute/logs/debug/multiruns/06-24-10-06-32/advice_yourself/0.1/6/checkpoints/42-0.00.ckpt
SLURM auto-requeueing enabled. Setting signal handlers.
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃        Test metric        ┃       DataLoader 0        ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
│       test/auc_mean       │    0.7364585995674133     │
│       test/auc_std        │    0.10310223698616028    │
│       test/f1_mean        │    0.7259644865989685     │
│        test/f1_std        │    0.12062918394804001    │
│         test/mre          │    0.07342700660228729    │
│        test/mre_ci        │   0.003816769691184163    │
│    test/precision_mean    │    0.7035792469978333     │
│    test/precision_std     │    0.14285573363304138    │
│     test/recall_mean      │    0.7762196063995361     │
│      test/recall_std      │    0.14533273875713348    │
└───────────────────────────┴───────────────────────────┘
Testing ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1/1 0:00:00 • 0:00:00 0.00it/s 
[2024-06-24 10:19:00,854][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-24-10-06-32/advice_yourself/0.1/6>
[2024-06-24 10:19:00,854][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-24 10:19:00,857][HYDRA] 	#27 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=False data.question=advice_yourself data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=saits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=True seed=7
[rank: 0] Seed set to 7
[2024-06-24 10:19:01,069][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-24 10:19:01,071][train.py][INFO] - Instantiating model <src.methods.saits.lightningmodule.SAITSLightningModule>
[2024-06-24 10:19:01,073][train.py][INFO] - Instantiating callbacks...
[2024-06-24 10:19:01,073][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-24 10:19:01,076][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-24 10:19:01,077][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-24 10:19:01,077][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-24 10:19:01,079][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-24 10:19:01,079][train.py][INFO] - Instantiating loggers...
[2024-06-24 10:19:01,079][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-24 10:19:01,081][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-24 10:19:01,111][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-24-10-06-32/advice_yourself/0.1/7/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳┳━┓
┃┃ Name                                                                     ┃┃ ┃
┡╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇╇━┩
││ model                                                                    ││ │
││ model.layer_stack_for_first_block                                        ││ │
││ model.layer_stack_for_first_block.0                                      ││ │
││ model.layer_stack_for_first_block.0.slf_attn                             ││ │
││ model.layer_stack_for_first_block.0.slf_attn.w_qs                        ││ │
││ model.layer_stack_for_first_block.0.slf_attn.w_ks                        ││ │
││ model.layer_stack_for_first_block.0.slf_attn.w_vs                        ││ │
││ model.layer_stack_for_first_block.0.slf_attn.attention_operator          ││ │
││ model.layer_stack_for_first_block.0.slf_attn.attention_operator.dropout  ││ │
││ model.layer_stack_for_first_block.0.slf_attn.fc                          ││ │
││ model.layer_stack_for_first_block.0.dropout                              ││ │
││ model.layer_stack_for_first_block.0.layer_norm                           ││ │
││ model.layer_stack_for_first_block.0.pos_ffn                              ││ │
││ model.layer_stack_for_first_block.0.pos_ffn.linear_1                     ││ │
││ model.layer_stack_for_first_block.0.pos_ffn.linear_2                     ││ │
││ model.layer_stack_for_first_block.0.pos_ffn.layer_norm                   ││ │
││ model.layer_stack_for_first_block.0.pos_ffn.dropout                      ││ │
││ model.layer_stack_for_second_block                                       ││ │
││ model.layer_stack_for_second_block.0                                     ││ │
││ model.layer_stack_for_second_block.0.slf_attn                            ││ │
││ model.layer_stack_for_second_block.0.slf_attn.w_qs                       ││ │
││ model.layer_stack_for_second_block.0.slf_attn.w_ks                       ││ │
││ model.layer_stack_for_second_block.0.slf_attn.w_vs                       ││ │
││ model.layer_stack_for_second_block.0.slf_attn.attention_operator         ││ │
││ model.layer_stack_for_second_block.0.slf_attn.attention_operator.dropout ││ │
││ model.layer_stack_for_second_block.0.slf_attn.fc                         ││ │
││ model.layer_stack_for_second_block.0.dropout                             ││ │
││ model.layer_stack_for_second_block.0.layer_norm                          ││ │
││ model.layer_stack_for_second_block.0.pos_ffn                             ││ │
││ model.layer_stack_for_second_block.0.pos_ffn.linear_1                    ││ │
││ model.layer_stack_for_second_block.0.pos_ffn.linear_2                    ││ │
││ model.layer_stack_for_second_block.0.pos_ffn.layer_norm                  ││ │
││ model.layer_stack_for_second_block.0.pos_ffn.dropout                     ││ │
││ model.dropout                                                            ││ │
││ model.position_enc                                                       ││ │
││ model.embedding_1                                                        ││ │
││ model.reduce_dim_z                                                       ││ │
││ model.embedding_2                                                        ││ │
││ model.reduce_dim_beta                                                    ││ │
││ model.reduce_dim_gamma                                                   ││ │
││ model.weight_combine                                                     ││ │
││ model.output_projection                                                  ││ │
││ model.self_attention                                                     ││ │
││ model.output_net                                                         ││ │
││ model.output_net.0                                                       ││ │
││ model.output_net.1                                                       ││ │
││ model.output_net.2                                                       ││ │
││ model.output_net.3                                                       ││ │
││ model.output_net.4                                                       ││ │
││ clf_criterion                                                            ││ │
││ saits_loss_func                                                          ││ │
└┴──────────────────────────────────────────────────────────────────────────┴┴─┘
Trainable params: 107 K                                                         
Non-trainable params: 0                                                         
Total params: 107 K                                                             
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 28.76it/s v_num: 0.000      
                                                              val/auc: 0.545    
                                                              val/f1: 0.167     
                                                              val/precision:    
                                                              1.000 val/recall: 
                                                              0.091 val/mre:    
                                                              0.065 train/auc:  
                                                              0.877 train/f1:   
                                                              0.889             
                                                              train/precision:  
                                                              0.842             
                                                              train/recall:     
                                                              0.941 train/mre:  
                                                              0.074             
[2024-06-24 10:19:21,965][train.py][INFO] - Starting testing...
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
Restoring states from the checkpoint path at /home/khickey/test_impute/logs/debug/multiruns/06-24-10-06-32/advice_yourself/0.1/7/checkpoints/47-0.00.ckpt
Loaded model weights from the checkpoint at /home/khickey/test_impute/logs/debug/multiruns/06-24-10-06-32/advice_yourself/0.1/7/checkpoints/47-0.00.ckpt
SLURM auto-requeueing enabled. Setting signal handlers.
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃        Test metric        ┃       DataLoader 0        ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
│       test/auc_mean       │    0.6816011071205139     │
│       test/auc_std        │    0.1131119355559349     │
│       test/f1_mean        │    0.6499436497688293     │
│        test/f1_std        │    0.13657964766025543    │
│         test/mre          │    0.07761204987764359    │
│        test/mre_ci        │   0.004064603243023157    │
│    test/precision_mean    │    0.6650873422622681     │
│    test/precision_std     │    0.15846271812915802    │
│     test/recall_mean      │    0.6671318411827087     │
│      test/recall_std      │    0.15705004334449768    │
└───────────────────────────┴───────────────────────────┘
Testing ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1/1 0:00:00 • 0:00:00 0.00it/s 
[2024-06-24 10:19:36,795][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-24-10-06-32/advice_yourself/0.1/7>
[2024-06-24 10:19:36,796][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-24 10:19:36,798][HYDRA] 	#28 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=False data.question=advice_yourself data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=saits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=True seed=8
[rank: 0] Seed set to 8
[2024-06-24 10:19:36,994][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-24 10:19:36,996][train.py][INFO] - Instantiating model <src.methods.saits.lightningmodule.SAITSLightningModule>
[2024-06-24 10:19:36,998][train.py][INFO] - Instantiating callbacks...
[2024-06-24 10:19:36,998][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-24 10:19:37,000][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-24 10:19:37,001][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-24 10:19:37,001][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-24 10:19:37,004][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-24 10:19:37,004][train.py][INFO] - Instantiating loggers...
[2024-06-24 10:19:37,004][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-24 10:19:37,006][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-24 10:19:37,072][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-24-10-06-32/advice_yourself/0.1/8/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳┳━┓
┃┃ Name                                                                     ┃┃ ┃
┡╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇╇━┩
││ model                                                                    ││ │
││ model.layer_stack_for_first_block                                        ││ │
││ model.layer_stack_for_first_block.0                                      ││ │
││ model.layer_stack_for_first_block.0.slf_attn                             ││ │
││ model.layer_stack_for_first_block.0.slf_attn.w_qs                        ││ │
││ model.layer_stack_for_first_block.0.slf_attn.w_ks                        ││ │
││ model.layer_stack_for_first_block.0.slf_attn.w_vs                        ││ │
││ model.layer_stack_for_first_block.0.slf_attn.attention_operator          ││ │
││ model.layer_stack_for_first_block.0.slf_attn.attention_operator.dropout  ││ │
││ model.layer_stack_for_first_block.0.slf_attn.fc                          ││ │
││ model.layer_stack_for_first_block.0.dropout                              ││ │
││ model.layer_stack_for_first_block.0.layer_norm                           ││ │
││ model.layer_stack_for_first_block.0.pos_ffn                              ││ │
││ model.layer_stack_for_first_block.0.pos_ffn.linear_1                     ││ │
││ model.layer_stack_for_first_block.0.pos_ffn.linear_2                     ││ │
││ model.layer_stack_for_first_block.0.pos_ffn.layer_norm                   ││ │
││ model.layer_stack_for_first_block.0.pos_ffn.dropout                      ││ │
││ model.layer_stack_for_second_block                                       ││ │
││ model.layer_stack_for_second_block.0                                     ││ │
││ model.layer_stack_for_second_block.0.slf_attn                            ││ │
││ model.layer_stack_for_second_block.0.slf_attn.w_qs                       ││ │
││ model.layer_stack_for_second_block.0.slf_attn.w_ks                       ││ │
││ model.layer_stack_for_second_block.0.slf_attn.w_vs                       ││ │
││ model.layer_stack_for_second_block.0.slf_attn.attention_operator         ││ │
││ model.layer_stack_for_second_block.0.slf_attn.attention_operator.dropout ││ │
││ model.layer_stack_for_second_block.0.slf_attn.fc                         ││ │
││ model.layer_stack_for_second_block.0.dropout                             ││ │
││ model.layer_stack_for_second_block.0.layer_norm                          ││ │
││ model.layer_stack_for_second_block.0.pos_ffn                             ││ │
││ model.layer_stack_for_second_block.0.pos_ffn.linear_1                    ││ │
││ model.layer_stack_for_second_block.0.pos_ffn.linear_2                    ││ │
││ model.layer_stack_for_second_block.0.pos_ffn.layer_norm                  ││ │
││ model.layer_stack_for_second_block.0.pos_ffn.dropout                     ││ │
││ model.dropout                                                            ││ │
││ model.position_enc                                                       ││ │
││ model.embedding_1                                                        ││ │
││ model.reduce_dim_z                                                       ││ │
││ model.embedding_2                                                        ││ │
││ model.reduce_dim_beta                                                    ││ │
││ model.reduce_dim_gamma                                                   ││ │
││ model.weight_combine                                                     ││ │
││ model.output_projection                                                  ││ │
││ model.self_attention                                                     ││ │
││ model.output_net                                                         ││ │
││ model.output_net.0                                                       ││ │
││ model.output_net.1                                                       ││ │
││ model.output_net.2                                                       ││ │
││ model.output_net.3                                                       ││ │
││ model.output_net.4                                                       ││ │
││ clf_criterion                                                            ││ │
││ saits_loss_func                                                          ││ │
└┴──────────────────────────────────────────────────────────────────────────┴┴─┘
Trainable params: 107 K                                                         
Non-trainable params: 0                                                         
Total params: 107 K                                                             
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 28.48it/s v_num: 0.000      
                                                              val/auc: 0.556    
                                                              val/f1: 0.200     
                                                              val/precision:    
                                                              1.000 val/recall: 
                                                              0.111 val/mre:    
                                                              0.068 train/auc:  
                                                              0.657 train/f1:   
                                                              0.492             
                                                              train/precision:  
                                                              0.938             
                                                              train/recall:     
                                                              0.333 train/mre:  
                                                              0.075             
[2024-06-24 10:19:57,680][train.py][INFO] - Starting testing...
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
Restoring states from the checkpoint path at /home/khickey/test_impute/logs/debug/multiruns/06-24-10-06-32/advice_yourself/0.1/8/checkpoints/41-0.00.ckpt
Loaded model weights from the checkpoint at /home/khickey/test_impute/logs/debug/multiruns/06-24-10-06-32/advice_yourself/0.1/8/checkpoints/41-0.00.ckpt
SLURM auto-requeueing enabled. Setting signal handlers.
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃        Test metric        ┃       DataLoader 0        ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
│       test/auc_mean       │    0.5354315042495728     │
│       test/auc_std        │    0.10006149858236313    │
│       test/f1_mean        │    0.6036673784255981     │
│        test/f1_std        │    0.12278153747320175    │
│         test/mre          │    0.07437016069889069    │
│        test/mre_ci        │   0.004069583956152201    │
│    test/precision_mean    │    0.49325570464134216    │
│    test/precision_std     │    0.13400399684906006    │
│     test/recall_mean      │    0.7885454893112183     │
│      test/recall_std      │    0.14108942449092865    │
└───────────────────────────┴───────────────────────────┘
Testing ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1/1 0:00:00 • 0:00:00 0.00it/s 
[2024-06-24 10:20:12,575][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-24-10-06-32/advice_yourself/0.1/8>
[2024-06-24 10:20:12,576][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-24 10:20:12,578][HYDRA] 	#29 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=False data.question=advice_yourself data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=saits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=True seed=9
[rank: 0] Seed set to 9
[2024-06-24 10:20:12,772][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-24 10:20:12,774][train.py][INFO] - Instantiating model <src.methods.saits.lightningmodule.SAITSLightningModule>
[2024-06-24 10:20:12,776][train.py][INFO] - Instantiating callbacks...
[2024-06-24 10:20:12,776][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-24 10:20:12,778][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-24 10:20:12,783][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-24 10:20:12,784][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-24 10:20:12,785][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-24 10:20:12,786][train.py][INFO] - Instantiating loggers...
[2024-06-24 10:20:12,786][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-24 10:20:12,787][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-24 10:20:12,819][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-24-10-06-32/advice_yourself/0.1/9/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳┳━┓
┃┃ Name                                                                     ┃┃ ┃
┡╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇╇━┩
││ model                                                                    ││ │
││ model.layer_stack_for_first_block                                        ││ │
││ model.layer_stack_for_first_block.0                                      ││ │
││ model.layer_stack_for_first_block.0.slf_attn                             ││ │
││ model.layer_stack_for_first_block.0.slf_attn.w_qs                        ││ │
││ model.layer_stack_for_first_block.0.slf_attn.w_ks                        ││ │
││ model.layer_stack_for_first_block.0.slf_attn.w_vs                        ││ │
││ model.layer_stack_for_first_block.0.slf_attn.attention_operator          ││ │
││ model.layer_stack_for_first_block.0.slf_attn.attention_operator.dropout  ││ │
││ model.layer_stack_for_first_block.0.slf_attn.fc                          ││ │
││ model.layer_stack_for_first_block.0.dropout                              ││ │
││ model.layer_stack_for_first_block.0.layer_norm                           ││ │
││ model.layer_stack_for_first_block.0.pos_ffn                              ││ │
││ model.layer_stack_for_first_block.0.pos_ffn.linear_1                     ││ │
││ model.layer_stack_for_first_block.0.pos_ffn.linear_2                     ││ │
││ model.layer_stack_for_first_block.0.pos_ffn.layer_norm                   ││ │
││ model.layer_stack_for_first_block.0.pos_ffn.dropout                      ││ │
││ model.layer_stack_for_second_block                                       ││ │
││ model.layer_stack_for_second_block.0                                     ││ │
││ model.layer_stack_for_second_block.0.slf_attn                            ││ │
││ model.layer_stack_for_second_block.0.slf_attn.w_qs                       ││ │
││ model.layer_stack_for_second_block.0.slf_attn.w_ks                       ││ │
││ model.layer_stack_for_second_block.0.slf_attn.w_vs                       ││ │
││ model.layer_stack_for_second_block.0.slf_attn.attention_operator         ││ │
││ model.layer_stack_for_second_block.0.slf_attn.attention_operator.dropout ││ │
││ model.layer_stack_for_second_block.0.slf_attn.fc                         ││ │
││ model.layer_stack_for_second_block.0.dropout                             ││ │
││ model.layer_stack_for_second_block.0.layer_norm                          ││ │
││ model.layer_stack_for_second_block.0.pos_ffn                             ││ │
││ model.layer_stack_for_second_block.0.pos_ffn.linear_1                    ││ │
││ model.layer_stack_for_second_block.0.pos_ffn.linear_2                    ││ │
││ model.layer_stack_for_second_block.0.pos_ffn.layer_norm                  ││ │
││ model.layer_stack_for_second_block.0.pos_ffn.dropout                     ││ │
││ model.dropout                                                            ││ │
││ model.position_enc                                                       ││ │
││ model.embedding_1                                                        ││ │
││ model.reduce_dim_z                                                       ││ │
││ model.embedding_2                                                        ││ │
││ model.reduce_dim_beta                                                    ││ │
││ model.reduce_dim_gamma                                                   ││ │
││ model.weight_combine                                                     ││ │
││ model.output_projection                                                  ││ │
││ model.self_attention                                                     ││ │
││ model.output_net                                                         ││ │
││ model.output_net.0                                                       ││ │
││ model.output_net.1                                                       ││ │
││ model.output_net.2                                                       ││ │
││ model.output_net.3                                                       ││ │
││ model.output_net.4                                                       ││ │
││ clf_criterion                                                            ││ │
││ saits_loss_func                                                          ││ │
└┴──────────────────────────────────────────────────────────────────────────┴┴─┘
Trainable params: 107 K                                                         
Non-trainable params: 0                                                         
Total params: 107 K                                                             
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 28.74it/s v_num: 0.000      
                                                              val/auc: 0.962    
                                                              val/f1: 0.923     
                                                              val/precision:    
                                                              0.857 val/recall: 
                                                              1.000 val/mre:    
                                                              0.065 train/auc:  
                                                              0.971 train/f1:   
                                                              0.969             
                                                              train/precision:  
                                                              0.940             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.074             
[2024-06-24 10:20:33,574][train.py][INFO] - Starting testing...
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
Restoring states from the checkpoint path at /home/khickey/test_impute/logs/debug/multiruns/06-24-10-06-32/advice_yourself/0.1/9/checkpoints/46-0.00.ckpt
Loaded model weights from the checkpoint at /home/khickey/test_impute/logs/debug/multiruns/06-24-10-06-32/advice_yourself/0.1/9/checkpoints/46-0.00.ckpt
SLURM auto-requeueing enabled. Setting signal handlers.
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃        Test metric        ┃       DataLoader 0        ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
│       test/auc_mean       │    0.6811265349388123     │
│       test/auc_std        │    0.10987415909767151    │
│       test/f1_mean        │    0.6513603925704956     │
│        test/f1_std        │    0.13004988431930542    │
│         test/mre          │    0.07339136302471161    │
│        test/mre_ci        │   0.003971179481595755    │
│    test/precision_mean    │    0.6672093272209167     │
│    test/precision_std     │    0.16438527405261993    │
│     test/recall_mean      │    0.6663178205490112     │
│      test/recall_std      │    0.16686539351940155    │
└───────────────────────────┴───────────────────────────┘
Testing ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1/1 0:00:00 • 0:00:00 0.00it/s 
[2024-06-24 10:20:48,425][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-24-10-06-32/advice_yourself/0.1/9>
[2024-06-24 10:20:48,426][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-24 10:20:48,428][HYDRA] 	#30 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=False data.question=advice_yourself data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=saits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=True seed=0
[2024-06-24 10:20:48,627][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-24 10:20:48,629][train.py][INFO] - Instantiating model <src.methods.saits.lightningmodule.SAITSLightningModule>
[2024-06-24 10:20:48,631][train.py][INFO] - Instantiating callbacks...
[2024-06-24 10:20:48,631][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-24 10:20:48,635][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-24 10:20:48,635][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-24 10:20:48,636][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-24 10:20:48,638][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-24 10:20:48,638][train.py][INFO] - Instantiating loggers...
[2024-06-24 10:20:48,638][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-24 10:20:48,640][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-24 10:20:48,708][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-24-10-06-32/advice_yourself/0.15/0/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳┳━┓
┃┃ Name                                                                     ┃┃ ┃
┡╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇╇━┩
││ model                                                                    ││ │
││ model.layer_stack_for_first_block                                        ││ │
││ model.layer_stack_for_first_block.0                                      ││ │
││ model.layer_stack_for_first_block.0.slf_attn                             ││ │
││ model.layer_stack_for_first_block.0.slf_attn.w_qs                        ││ │
││ model.layer_stack_for_first_block.0.slf_attn.w_ks                        ││ │
││ model.layer_stack_for_first_block.0.slf_attn.w_vs                        ││ │
││ model.layer_stack_for_first_block.0.slf_attn.attention_operator          ││ │
││ model.layer_stack_for_first_block.0.slf_attn.attention_operator.dropout  ││ │
││ model.layer_stack_for_first_block.0.slf_attn.fc                          ││ │
││ model.layer_stack_for_first_block.0.dropout                              ││ │
││ model.layer_stack_for_first_block.0.layer_norm                           ││ │
││ model.layer_stack_for_first_block.0.pos_ffn                              ││ │
││ model.layer_stack_for_first_block.0.pos_ffn.linear_1                     ││ │
││ model.layer_stack_for_first_block.0.pos_ffn.linear_2                     ││ │
││ model.layer_stack_for_first_block.0.pos_ffn.layer_norm                   ││ │
││ model.layer_stack_for_first_block.0.pos_ffn.dropout                      ││ │
││ model.layer_stack_for_second_block                                       ││ │
││ model.layer_stack_for_second_block.0                                     ││ │
││ model.layer_stack_for_second_block.0.slf_attn                            ││ │
││ model.layer_stack_for_second_block.0.slf_attn.w_qs                       ││ │
││ model.layer_stack_for_second_block.0.slf_attn.w_ks                       ││ │
││ model.layer_stack_for_second_block.0.slf_attn.w_vs                       ││ │
││ model.layer_stack_for_second_block.0.slf_attn.attention_operator         ││ │
││ model.layer_stack_for_second_block.0.slf_attn.attention_operator.dropout ││ │
││ model.layer_stack_for_second_block.0.slf_attn.fc                         ││ │
││ model.layer_stack_for_second_block.0.dropout                             ││ │
││ model.layer_stack_for_second_block.0.layer_norm                          ││ │
││ model.layer_stack_for_second_block.0.pos_ffn                             ││ │
││ model.layer_stack_for_second_block.0.pos_ffn.linear_1                    ││ │
││ model.layer_stack_for_second_block.0.pos_ffn.linear_2                    ││ │
││ model.layer_stack_for_second_block.0.pos_ffn.layer_norm                  ││ │
││ model.layer_stack_for_second_block.0.pos_ffn.dropout                     ││ │
││ model.dropout                                                            ││ │
││ model.position_enc                                                       ││ │
││ model.embedding_1                                                        ││ │
││ model.reduce_dim_z                                                       ││ │
││ model.embedding_2                                                        ││ │
││ model.reduce_dim_beta                                                    ││ │
││ model.reduce_dim_gamma                                                   ││ │
││ model.weight_combine                                                     ││ │
││ model.output_projection                                                  ││ │
││ model.self_attention                                                     ││ │
││ model.output_net                                                         ││ │
││ model.output_net.0                                                       ││ │
││ model.output_net.1                                                       ││ │
││ model.output_net.2                                                       ││ │
││ model.output_net.3                                                       ││ │
││ model.output_net.4                                                       ││ │
││ clf_criterion                                                            ││ │
││ saits_loss_func                                                          ││ │
└┴──────────────────────────────────────────────────────────────────────────┴┴─┘
Trainable params: 107 K                                                         
Non-trainable params: 0                                                         
Total params: 107 K                                                             
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 28.52it/s v_num: 0.000      
                                                              val/auc: 0.714    
                                                              val/f1: 0.857     
                                                              val/precision:    
                                                              0.750 val/recall: 
                                                              1.000 val/mre:    
                                                              0.080 train/auc:  
                                                              0.744 train/f1:   
                                                              0.750             
                                                              train/precision:  
                                                              0.812             
                                                              train/recall:     
                                                              0.696 train/mre:  
                                                              0.072             
[2024-06-24 10:21:09,487][train.py][INFO] - Starting testing...
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
Restoring states from the checkpoint path at /home/khickey/test_impute/logs/debug/multiruns/06-24-10-06-32/advice_yourself/0.15/0/checkpoints/49-0.00.ckpt
Loaded model weights from the checkpoint at /home/khickey/test_impute/logs/debug/multiruns/06-24-10-06-32/advice_yourself/0.15/0/checkpoints/49-0.00.ckpt
SLURM auto-requeueing enabled. Setting signal handlers.
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃        Test metric        ┃       DataLoader 0        ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
│       test/auc_mean       │    0.42828184366226196    │
│       test/auc_std        │    0.10640884190797806    │
│       test/f1_mean        │     0.508467435836792     │
│        test/f1_std        │    0.1255996823310852     │
│         test/mre          │    0.08718165010213852    │
│        test/mre_ci        │   0.004738891031593084    │
│    test/precision_mean    │    0.43195611238479614    │
│    test/precision_std     │    0.1298297494649887     │
│     test/recall_mean      │    0.6669328808784485     │
│      test/recall_std      │    0.16512581706047058    │
└───────────────────────────┴───────────────────────────┘
Testing ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1/1 0:00:00 • 0:00:00 0.00it/s 
[2024-06-24 10:21:24,458][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-24-10-06-32/advice_yourself/0.15/0>
[2024-06-24 10:21:24,458][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-24 10:21:24,461][HYDRA] 	#31 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=False data.question=advice_yourself data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=saits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=True seed=1
[rank: 0] Seed set to 1
[2024-06-24 10:21:24,660][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-24 10:21:24,662][train.py][INFO] - Instantiating model <src.methods.saits.lightningmodule.SAITSLightningModule>
[2024-06-24 10:21:24,664][train.py][INFO] - Instantiating callbacks...
[2024-06-24 10:21:24,664][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-24 10:21:24,667][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-24 10:21:24,668][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-24 10:21:24,668][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-24 10:21:24,670][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-24 10:21:24,671][train.py][INFO] - Instantiating loggers...
[2024-06-24 10:21:24,671][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-24 10:21:24,672][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-24 10:21:24,706][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-24-10-06-32/advice_yourself/0.15/1/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳┳━┓
┃┃ Name                                                                     ┃┃ ┃
┡╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇╇━┩
││ model                                                                    ││ │
││ model.layer_stack_for_first_block                                        ││ │
││ model.layer_stack_for_first_block.0                                      ││ │
││ model.layer_stack_for_first_block.0.slf_attn                             ││ │
││ model.layer_stack_for_first_block.0.slf_attn.w_qs                        ││ │
││ model.layer_stack_for_first_block.0.slf_attn.w_ks                        ││ │
││ model.layer_stack_for_first_block.0.slf_attn.w_vs                        ││ │
││ model.layer_stack_for_first_block.0.slf_attn.attention_operator          ││ │
││ model.layer_stack_for_first_block.0.slf_attn.attention_operator.dropout  ││ │
││ model.layer_stack_for_first_block.0.slf_attn.fc                          ││ │
││ model.layer_stack_for_first_block.0.dropout                              ││ │
││ model.layer_stack_for_first_block.0.layer_norm                           ││ │
││ model.layer_stack_for_first_block.0.pos_ffn                              ││ │
││ model.layer_stack_for_first_block.0.pos_ffn.linear_1                     ││ │
││ model.layer_stack_for_first_block.0.pos_ffn.linear_2                     ││ │
││ model.layer_stack_for_first_block.0.pos_ffn.layer_norm                   ││ │
││ model.layer_stack_for_first_block.0.pos_ffn.dropout                      ││ │
││ model.layer_stack_for_second_block                                       ││ │
││ model.layer_stack_for_second_block.0                                     ││ │
││ model.layer_stack_for_second_block.0.slf_attn                            ││ │
││ model.layer_stack_for_second_block.0.slf_attn.w_qs                       ││ │
││ model.layer_stack_for_second_block.0.slf_attn.w_ks                       ││ │
││ model.layer_stack_for_second_block.0.slf_attn.w_vs                       ││ │
││ model.layer_stack_for_second_block.0.slf_attn.attention_operator         ││ │
││ model.layer_stack_for_second_block.0.slf_attn.attention_operator.dropout ││ │
││ model.layer_stack_for_second_block.0.slf_attn.fc                         ││ │
││ model.layer_stack_for_second_block.0.dropout                             ││ │
││ model.layer_stack_for_second_block.0.layer_norm                          ││ │
││ model.layer_stack_for_second_block.0.pos_ffn                             ││ │
││ model.layer_stack_for_second_block.0.pos_ffn.linear_1                    ││ │
││ model.layer_stack_for_second_block.0.pos_ffn.linear_2                    ││ │
││ model.layer_stack_for_second_block.0.pos_ffn.layer_norm                  ││ │
││ model.layer_stack_for_second_block.0.pos_ffn.dropout                     ││ │
││ model.dropout                                                            ││ │
││ model.position_enc                                                       ││ │
││ model.embedding_1                                                        ││ │
││ model.reduce_dim_z                                                       ││ │
││ model.embedding_2                                                        ││ │
││ model.reduce_dim_beta                                                    ││ │
││ model.reduce_dim_gamma                                                   ││ │
││ model.weight_combine                                                     ││ │
││ model.output_projection                                                  ││ │
││ model.self_attention                                                     ││ │
││ model.output_net                                                         ││ │
││ model.output_net.0                                                       ││ │
││ model.output_net.1                                                       ││ │
││ model.output_net.2                                                       ││ │
││ model.output_net.3                                                       ││ │
││ model.output_net.4                                                       ││ │
││ clf_criterion                                                            ││ │
││ saits_loss_func                                                          ││ │
└┴──────────────────────────────────────────────────────────────────────────┴┴─┘
Trainable params: 107 K                                                         
Non-trainable params: 0                                                         
Total params: 107 K                                                             
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 29.45it/s v_num: 0.000      
                                                              val/auc: 1.000    
                                                              val/f1: 1.000     
                                                              val/precision:    
                                                              1.000 val/recall: 
                                                              1.000 val/mre:    
                                                              0.062 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.072             
[2024-06-24 10:21:45,151][train.py][INFO] - Starting testing...
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
Restoring states from the checkpoint path at /home/khickey/test_impute/logs/debug/multiruns/06-24-10-06-32/advice_yourself/0.15/1/checkpoints/36-0.00.ckpt
Loaded model weights from the checkpoint at /home/khickey/test_impute/logs/debug/multiruns/06-24-10-06-32/advice_yourself/0.15/1/checkpoints/36-0.00.ckpt
SLURM auto-requeueing enabled. Setting signal handlers.
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃        Test metric        ┃       DataLoader 0        ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
│       test/auc_mean       │    0.5190794467926025     │
│       test/auc_std        │    0.10836151242256165    │
│       test/f1_mean        │    0.37992870807647705    │
│        test/f1_std        │    0.15495388209819794    │
│         test/mre          │    0.0754721537232399     │
│        test/mre_ci        │   0.004196919966489077    │
│    test/precision_mean    │    0.4981119930744171     │
│    test/precision_std     │    0.21809393167495728    │
│     test/recall_mean      │    0.33471551537513733    │
│      test/recall_std      │    0.16529683768749237    │
└───────────────────────────┴───────────────────────────┘
Testing ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1/1 0:00:00 • 0:00:00 0.00it/s 
[2024-06-24 10:22:00,025][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-24-10-06-32/advice_yourself/0.15/1>
[2024-06-24 10:22:00,026][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-24 10:22:00,030][HYDRA] 	#32 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=False data.question=advice_yourself data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=saits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=True seed=2
[rank: 0] Seed set to 2
[2024-06-24 10:22:00,236][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-24 10:22:00,237][train.py][INFO] - Instantiating model <src.methods.saits.lightningmodule.SAITSLightningModule>
[2024-06-24 10:22:00,239][train.py][INFO] - Instantiating callbacks...
[2024-06-24 10:22:00,239][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-24 10:22:00,242][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-24 10:22:00,242][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-24 10:22:00,243][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-24 10:22:00,245][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-24 10:22:00,246][train.py][INFO] - Instantiating loggers...
[2024-06-24 10:22:00,246][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-24 10:22:00,247][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-24 10:22:00,320][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-24-10-06-32/advice_yourself/0.15/2/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳┳━┓
┃┃ Name                                                                     ┃┃ ┃
┡╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇╇━┩
││ model                                                                    ││ │
││ model.layer_stack_for_first_block                                        ││ │
││ model.layer_stack_for_first_block.0                                      ││ │
││ model.layer_stack_for_first_block.0.slf_attn                             ││ │
││ model.layer_stack_for_first_block.0.slf_attn.w_qs                        ││ │
││ model.layer_stack_for_first_block.0.slf_attn.w_ks                        ││ │
││ model.layer_stack_for_first_block.0.slf_attn.w_vs                        ││ │
││ model.layer_stack_for_first_block.0.slf_attn.attention_operator          ││ │
││ model.layer_stack_for_first_block.0.slf_attn.attention_operator.dropout  ││ │
││ model.layer_stack_for_first_block.0.slf_attn.fc                          ││ │
││ model.layer_stack_for_first_block.0.dropout                              ││ │
││ model.layer_stack_for_first_block.0.layer_norm                           ││ │
││ model.layer_stack_for_first_block.0.pos_ffn                              ││ │
││ model.layer_stack_for_first_block.0.pos_ffn.linear_1                     ││ │
││ model.layer_stack_for_first_block.0.pos_ffn.linear_2                     ││ │
││ model.layer_stack_for_first_block.0.pos_ffn.layer_norm                   ││ │
││ model.layer_stack_for_first_block.0.pos_ffn.dropout                      ││ │
││ model.layer_stack_for_second_block                                       ││ │
││ model.layer_stack_for_second_block.0                                     ││ │
││ model.layer_stack_for_second_block.0.slf_attn                            ││ │
││ model.layer_stack_for_second_block.0.slf_attn.w_qs                       ││ │
││ model.layer_stack_for_second_block.0.slf_attn.w_ks                       ││ │
││ model.layer_stack_for_second_block.0.slf_attn.w_vs                       ││ │
││ model.layer_stack_for_second_block.0.slf_attn.attention_operator         ││ │
││ model.layer_stack_for_second_block.0.slf_attn.attention_operator.dropout ││ │
││ model.layer_stack_for_second_block.0.slf_attn.fc                         ││ │
││ model.layer_stack_for_second_block.0.dropout                             ││ │
││ model.layer_stack_for_second_block.0.layer_norm                          ││ │
││ model.layer_stack_for_second_block.0.pos_ffn                             ││ │
││ model.layer_stack_for_second_block.0.pos_ffn.linear_1                    ││ │
││ model.layer_stack_for_second_block.0.pos_ffn.linear_2                    ││ │
││ model.layer_stack_for_second_block.0.pos_ffn.layer_norm                  ││ │
││ model.layer_stack_for_second_block.0.pos_ffn.dropout                     ││ │
││ model.dropout                                                            ││ │
││ model.position_enc                                                       ││ │
││ model.embedding_1                                                        ││ │
││ model.reduce_dim_z                                                       ││ │
││ model.embedding_2                                                        ││ │
││ model.reduce_dim_beta                                                    ││ │
││ model.reduce_dim_gamma                                                   ││ │
││ model.weight_combine                                                     ││ │
││ model.output_projection                                                  ││ │
││ model.self_attention                                                     ││ │
││ model.output_net                                                         ││ │
││ model.output_net.0                                                       ││ │
││ model.output_net.1                                                       ││ │
││ model.output_net.2                                                       ││ │
││ model.output_net.3                                                       ││ │
││ model.output_net.4                                                       ││ │
││ clf_criterion                                                            ││ │
││ saits_loss_func                                                          ││ │
└┴──────────────────────────────────────────────────────────────────────────┴┴─┘
Trainable params: 107 K                                                         
Non-trainable params: 0                                                         
Total params: 107 K                                                             
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 28.71it/s v_num: 0.000      
                                                              val/auc: 1.000    
                                                              val/f1: 1.000     
                                                              val/precision:    
                                                              1.000 val/recall: 
                                                              1.000 val/mre:    
                                                              0.063 train/auc:  
                                                              0.990 train/f1:   
                                                              0.990             
                                                              train/precision:  
                                                              0.981             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.072             
[2024-06-24 10:22:21,100][train.py][INFO] - Starting testing...
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
Restoring states from the checkpoint path at /home/khickey/test_impute/logs/debug/multiruns/06-24-10-06-32/advice_yourself/0.15/2/checkpoints/37-0.00.ckpt
Loaded model weights from the checkpoint at /home/khickey/test_impute/logs/debug/multiruns/06-24-10-06-32/advice_yourself/0.15/2/checkpoints/37-0.00.ckpt
SLURM auto-requeueing enabled. Setting signal handlers.
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃        Test metric        ┃       DataLoader 0        ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
│       test/auc_mean       │    0.6167556047439575     │
│       test/auc_std        │    0.10545352846384048    │
│       test/f1_mean        │    0.5233840346336365     │
│        test/f1_std        │    0.16722068190574646    │
│         test/mre          │    0.07602111995220184    │
│        test/mre_ci        │   0.004254443570971489    │
│    test/precision_mean    │    0.6516639590263367     │
│    test/precision_std     │    0.21419373154640198    │
│     test/recall_mean      │    0.4421881437301636     │
│      test/recall_std      │     0.16364686191082      │
└───────────────────────────┴───────────────────────────┘
Testing ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1/1 0:00:00 • 0:00:00 0.00it/s 
[2024-06-24 10:22:35,994][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-24-10-06-32/advice_yourself/0.15/2>
[2024-06-24 10:22:35,994][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-24 10:22:35,997][HYDRA] 	#33 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=False data.question=advice_yourself data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=saits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=True seed=3
[rank: 0] Seed set to 3
[2024-06-24 10:22:36,190][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-24 10:22:36,192][train.py][INFO] - Instantiating model <src.methods.saits.lightningmodule.SAITSLightningModule>
[2024-06-24 10:22:36,193][train.py][INFO] - Instantiating callbacks...
[2024-06-24 10:22:36,194][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-24 10:22:36,196][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-24 10:22:36,197][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-24 10:22:36,197][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-24 10:22:36,199][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-24 10:22:36,199][train.py][INFO] - Instantiating loggers...
[2024-06-24 10:22:36,200][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-24 10:22:36,201][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-24 10:22:36,233][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-24-10-06-32/advice_yourself/0.15/3/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳┳━┓
┃┃ Name                                                                     ┃┃ ┃
┡╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇╇━┩
││ model                                                                    ││ │
││ model.layer_stack_for_first_block                                        ││ │
││ model.layer_stack_for_first_block.0                                      ││ │
││ model.layer_stack_for_first_block.0.slf_attn                             ││ │
││ model.layer_stack_for_first_block.0.slf_attn.w_qs                        ││ │
││ model.layer_stack_for_first_block.0.slf_attn.w_ks                        ││ │
││ model.layer_stack_for_first_block.0.slf_attn.w_vs                        ││ │
││ model.layer_stack_for_first_block.0.slf_attn.attention_operator          ││ │
││ model.layer_stack_for_first_block.0.slf_attn.attention_operator.dropout  ││ │
││ model.layer_stack_for_first_block.0.slf_attn.fc                          ││ │
││ model.layer_stack_for_first_block.0.dropout                              ││ │
││ model.layer_stack_for_first_block.0.layer_norm                           ││ │
││ model.layer_stack_for_first_block.0.pos_ffn                              ││ │
││ model.layer_stack_for_first_block.0.pos_ffn.linear_1                     ││ │
││ model.layer_stack_for_first_block.0.pos_ffn.linear_2                     ││ │
││ model.layer_stack_for_first_block.0.pos_ffn.layer_norm                   ││ │
││ model.layer_stack_for_first_block.0.pos_ffn.dropout                      ││ │
││ model.layer_stack_for_second_block                                       ││ │
││ model.layer_stack_for_second_block.0                                     ││ │
││ model.layer_stack_for_second_block.0.slf_attn                            ││ │
││ model.layer_stack_for_second_block.0.slf_attn.w_qs                       ││ │
││ model.layer_stack_for_second_block.0.slf_attn.w_ks                       ││ │
││ model.layer_stack_for_second_block.0.slf_attn.w_vs                       ││ │
││ model.layer_stack_for_second_block.0.slf_attn.attention_operator         ││ │
││ model.layer_stack_for_second_block.0.slf_attn.attention_operator.dropout ││ │
││ model.layer_stack_for_second_block.0.slf_attn.fc                         ││ │
││ model.layer_stack_for_second_block.0.dropout                             ││ │
││ model.layer_stack_for_second_block.0.layer_norm                          ││ │
││ model.layer_stack_for_second_block.0.pos_ffn                             ││ │
││ model.layer_stack_for_second_block.0.pos_ffn.linear_1                    ││ │
││ model.layer_stack_for_second_block.0.pos_ffn.linear_2                    ││ │
││ model.layer_stack_for_second_block.0.pos_ffn.layer_norm                  ││ │
││ model.layer_stack_for_second_block.0.pos_ffn.dropout                     ││ │
││ model.dropout                                                            ││ │
││ model.position_enc                                                       ││ │
││ model.embedding_1                                                        ││ │
││ model.reduce_dim_z                                                       ││ │
││ model.embedding_2                                                        ││ │
││ model.reduce_dim_beta                                                    ││ │
││ model.reduce_dim_gamma                                                   ││ │
││ model.weight_combine                                                     ││ │
││ model.output_projection                                                  ││ │
││ model.self_attention                                                     ││ │
││ model.output_net                                                         ││ │
││ model.output_net.0                                                       ││ │
││ model.output_net.1                                                       ││ │
││ model.output_net.2                                                       ││ │
││ model.output_net.3                                                       ││ │
││ model.output_net.4                                                       ││ │
││ clf_criterion                                                            ││ │
││ saits_loss_func                                                          ││ │
└┴──────────────────────────────────────────────────────────────────────────┴┴─┘
Trainable params: 107 K                                                         
Non-trainable params: 0                                                         
Total params: 107 K                                                             
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 27.64it/s v_num: 0.000      
                                                              val/auc: 0.875    
                                                              val/f1: 0.857     
                                                              val/precision:    
                                                              1.000 val/recall: 
                                                              0.750 val/mre:    
                                                              0.068 train/auc:  
                                                              0.971 train/f1:   
                                                              0.970             
                                                              train/precision:  
                                                              0.941             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.079             
[2024-06-24 10:22:57,031][train.py][INFO] - Starting testing...
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
Restoring states from the checkpoint path at /home/khickey/test_impute/logs/debug/multiruns/06-24-10-06-32/advice_yourself/0.15/3/checkpoints/48-0.00.ckpt
Loaded model weights from the checkpoint at /home/khickey/test_impute/logs/debug/multiruns/06-24-10-06-32/advice_yourself/0.15/3/checkpoints/48-0.00.ckpt
SLURM auto-requeueing enabled. Setting signal handlers.
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃        Test metric        ┃       DataLoader 0        ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
│       test/auc_mean       │    0.5093585252761841     │
│       test/auc_std        │    0.09579689055681229    │
│       test/f1_mean        │    0.2900541126728058     │
│        test/f1_std        │    0.17084576189517975    │
│         test/mre          │    0.0729927122592926     │
│        test/mre_ci        │   0.004064498469233513    │
│    test/precision_mean    │    0.4884605407714844     │
│    test/precision_std     │     0.29079669713974      │
│     test/recall_mean      │    0.22679434716701508    │
│      test/recall_std      │    0.14174595475196838    │
└───────────────────────────┴───────────────────────────┘
Testing ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1/1 0:00:00 • 0:00:00 0.00it/s 
[2024-06-24 10:23:11,867][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-24-10-06-32/advice_yourself/0.15/3>
[2024-06-24 10:23:11,867][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-24 10:23:11,870][HYDRA] 	#34 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=False data.question=advice_yourself data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=saits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=True seed=4
[rank: 0] Seed set to 4
[2024-06-24 10:23:12,070][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-24 10:23:12,072][train.py][INFO] - Instantiating model <src.methods.saits.lightningmodule.SAITSLightningModule>
[2024-06-24 10:23:12,073][train.py][INFO] - Instantiating callbacks...
[2024-06-24 10:23:12,074][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-24 10:23:12,077][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-24 10:23:12,077][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-24 10:23:12,078][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-24 10:23:12,080][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-24 10:23:12,080][train.py][INFO] - Instantiating loggers...
[2024-06-24 10:23:12,081][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-24 10:23:12,082][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-24 10:23:12,150][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-24-10-06-32/advice_yourself/0.15/4/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳┳━┓
┃┃ Name                                                                     ┃┃ ┃
┡╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇╇━┩
││ model                                                                    ││ │
││ model.layer_stack_for_first_block                                        ││ │
││ model.layer_stack_for_first_block.0                                      ││ │
││ model.layer_stack_for_first_block.0.slf_attn                             ││ │
││ model.layer_stack_for_first_block.0.slf_attn.w_qs                        ││ │
││ model.layer_stack_for_first_block.0.slf_attn.w_ks                        ││ │
││ model.layer_stack_for_first_block.0.slf_attn.w_vs                        ││ │
││ model.layer_stack_for_first_block.0.slf_attn.attention_operator          ││ │
││ model.layer_stack_for_first_block.0.slf_attn.attention_operator.dropout  ││ │
││ model.layer_stack_for_first_block.0.slf_attn.fc                          ││ │
││ model.layer_stack_for_first_block.0.dropout                              ││ │
││ model.layer_stack_for_first_block.0.layer_norm                           ││ │
││ model.layer_stack_for_first_block.0.pos_ffn                              ││ │
││ model.layer_stack_for_first_block.0.pos_ffn.linear_1                     ││ │
││ model.layer_stack_for_first_block.0.pos_ffn.linear_2                     ││ │
││ model.layer_stack_for_first_block.0.pos_ffn.layer_norm                   ││ │
││ model.layer_stack_for_first_block.0.pos_ffn.dropout                      ││ │
││ model.layer_stack_for_second_block                                       ││ │
││ model.layer_stack_for_second_block.0                                     ││ │
││ model.layer_stack_for_second_block.0.slf_attn                            ││ │
││ model.layer_stack_for_second_block.0.slf_attn.w_qs                       ││ │
││ model.layer_stack_for_second_block.0.slf_attn.w_ks                       ││ │
││ model.layer_stack_for_second_block.0.slf_attn.w_vs                       ││ │
││ model.layer_stack_for_second_block.0.slf_attn.attention_operator         ││ │
││ model.layer_stack_for_second_block.0.slf_attn.attention_operator.dropout ││ │
││ model.layer_stack_for_second_block.0.slf_attn.fc                         ││ │
││ model.layer_stack_for_second_block.0.dropout                             ││ │
││ model.layer_stack_for_second_block.0.layer_norm                          ││ │
││ model.layer_stack_for_second_block.0.pos_ffn                             ││ │
││ model.layer_stack_for_second_block.0.pos_ffn.linear_1                    ││ │
││ model.layer_stack_for_second_block.0.pos_ffn.linear_2                    ││ │
││ model.layer_stack_for_second_block.0.pos_ffn.layer_norm                  ││ │
││ model.layer_stack_for_second_block.0.pos_ffn.dropout                     ││ │
││ model.dropout                                                            ││ │
││ model.position_enc                                                       ││ │
││ model.embedding_1                                                        ││ │
││ model.reduce_dim_z                                                       ││ │
││ model.embedding_2                                                        ││ │
││ model.reduce_dim_beta                                                    ││ │
││ model.reduce_dim_gamma                                                   ││ │
││ model.weight_combine                                                     ││ │
││ model.output_projection                                                  ││ │
││ model.self_attention                                                     ││ │
││ model.output_net                                                         ││ │
││ model.output_net.0                                                       ││ │
││ model.output_net.1                                                       ││ │
││ model.output_net.2                                                       ││ │
││ model.output_net.3                                                       ││ │
││ model.output_net.4                                                       ││ │
││ clf_criterion                                                            ││ │
││ saits_loss_func                                                          ││ │
└┴──────────────────────────────────────────────────────────────────────────┴┴─┘
Trainable params: 107 K                                                         
Non-trainable params: 0                                                         
Total params: 107 K                                                             
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 29.21it/s v_num: 0.000      
                                                              val/auc: 1.000    
                                                              val/f1: 1.000     
                                                              val/precision:    
                                                              1.000 val/recall: 
                                                              1.000 val/mre:    
                                                              0.062 train/auc:  
                                                              0.941 train/f1:   
                                                              0.936             
                                                              train/precision:  
                                                              0.917             
                                                              train/recall:     
                                                              0.957 train/mre:  
                                                              0.074             
[2024-06-24 10:23:32,828][train.py][INFO] - Starting testing...
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
Restoring states from the checkpoint path at /home/khickey/test_impute/logs/debug/multiruns/06-24-10-06-32/advice_yourself/0.15/4/checkpoints/45-0.00.ckpt
Loaded model weights from the checkpoint at /home/khickey/test_impute/logs/debug/multiruns/06-24-10-06-32/advice_yourself/0.15/4/checkpoints/45-0.00.ckpt
SLURM auto-requeueing enabled. Setting signal handlers.
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃        Test metric        ┃       DataLoader 0        ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
│       test/auc_mean       │    0.47051477432250977    │
│       test/auc_std        │    0.11654036492109299    │
│       test/f1_mean        │    0.42706403136253357    │
│        test/f1_std        │    0.14730970561504364    │
│         test/mre          │    0.0748446136713028     │
│        test/mre_ci        │   0.004054004792124033    │
│    test/precision_mean    │    0.4337531626224518     │
│    test/precision_std     │    0.1729276478290558     │
│     test/recall_mean      │    0.44720715284347534    │
│      test/recall_std      │    0.17437024414539337    │
└───────────────────────────┴───────────────────────────┘
Testing ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1/1 0:00:00 • 0:00:00 0.00it/s 
[2024-06-24 10:23:47,296][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-24-10-06-32/advice_yourself/0.15/4>
[2024-06-24 10:23:47,297][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-24 10:23:47,299][HYDRA] 	#35 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=False data.question=advice_yourself data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=saits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=True seed=5
[rank: 0] Seed set to 5
[2024-06-24 10:23:47,485][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-24 10:23:47,487][train.py][INFO] - Instantiating model <src.methods.saits.lightningmodule.SAITSLightningModule>
[2024-06-24 10:23:47,488][train.py][INFO] - Instantiating callbacks...
[2024-06-24 10:23:47,488][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-24 10:23:47,491][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-24 10:23:47,491][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-24 10:23:47,492][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-24 10:23:47,493][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-24 10:23:47,494][train.py][INFO] - Instantiating loggers...
[2024-06-24 10:23:47,494][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-24 10:23:47,495][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-24 10:23:47,526][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-24-10-06-32/advice_yourself/0.15/5/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳┳━┓
┃┃ Name                                                                     ┃┃ ┃
┡╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇╇━┩
││ model                                                                    ││ │
││ model.layer_stack_for_first_block                                        ││ │
││ model.layer_stack_for_first_block.0                                      ││ │
││ model.layer_stack_for_first_block.0.slf_attn                             ││ │
││ model.layer_stack_for_first_block.0.slf_attn.w_qs                        ││ │
││ model.layer_stack_for_first_block.0.slf_attn.w_ks                        ││ │
││ model.layer_stack_for_first_block.0.slf_attn.w_vs                        ││ │
││ model.layer_stack_for_first_block.0.slf_attn.attention_operator          ││ │
││ model.layer_stack_for_first_block.0.slf_attn.attention_operator.dropout  ││ │
││ model.layer_stack_for_first_block.0.slf_attn.fc                          ││ │
││ model.layer_stack_for_first_block.0.dropout                              ││ │
││ model.layer_stack_for_first_block.0.layer_norm                           ││ │
││ model.layer_stack_for_first_block.0.pos_ffn                              ││ │
││ model.layer_stack_for_first_block.0.pos_ffn.linear_1                     ││ │
││ model.layer_stack_for_first_block.0.pos_ffn.linear_2                     ││ │
││ model.layer_stack_for_first_block.0.pos_ffn.layer_norm                   ││ │
││ model.layer_stack_for_first_block.0.pos_ffn.dropout                      ││ │
││ model.layer_stack_for_second_block                                       ││ │
││ model.layer_stack_for_second_block.0                                     ││ │
││ model.layer_stack_for_second_block.0.slf_attn                            ││ │
││ model.layer_stack_for_second_block.0.slf_attn.w_qs                       ││ │
││ model.layer_stack_for_second_block.0.slf_attn.w_ks                       ││ │
││ model.layer_stack_for_second_block.0.slf_attn.w_vs                       ││ │
││ model.layer_stack_for_second_block.0.slf_attn.attention_operator         ││ │
││ model.layer_stack_for_second_block.0.slf_attn.attention_operator.dropout ││ │
││ model.layer_stack_for_second_block.0.slf_attn.fc                         ││ │
││ model.layer_stack_for_second_block.0.dropout                             ││ │
││ model.layer_stack_for_second_block.0.layer_norm                          ││ │
││ model.layer_stack_for_second_block.0.pos_ffn                             ││ │
││ model.layer_stack_for_second_block.0.pos_ffn.linear_1                    ││ │
││ model.layer_stack_for_second_block.0.pos_ffn.linear_2                    ││ │
││ model.layer_stack_for_second_block.0.pos_ffn.layer_norm                  ││ │
││ model.layer_stack_for_second_block.0.pos_ffn.dropout                     ││ │
││ model.dropout                                                            ││ │
││ model.position_enc                                                       ││ │
││ model.embedding_1                                                        ││ │
││ model.reduce_dim_z                                                       ││ │
││ model.embedding_2                                                        ││ │
││ model.reduce_dim_beta                                                    ││ │
││ model.reduce_dim_gamma                                                   ││ │
││ model.weight_combine                                                     ││ │
││ model.output_projection                                                  ││ │
││ model.self_attention                                                     ││ │
││ model.output_net                                                         ││ │
││ model.output_net.0                                                       ││ │
││ model.output_net.1                                                       ││ │
││ model.output_net.2                                                       ││ │
││ model.output_net.3                                                       ││ │
││ model.output_net.4                                                       ││ │
││ clf_criterion                                                            ││ │
││ saits_loss_func                                                          ││ │
└┴──────────────────────────────────────────────────────────────────────────┴┴─┘
Trainable params: 107 K                                                         
Non-trainable params: 0                                                         
Total params: 107 K                                                             
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 30.75it/s v_num: 0.000      
                                                              val/auc: 0.840    
                                                              val/f1: 0.880     
                                                              val/precision:    
                                                              0.917 val/recall: 
                                                              0.846 val/mre:    
                                                              0.072 train/auc:  
                                                              0.930 train/f1:   
                                                              0.931             
                                                              train/precision:  
                                                              0.940             
                                                              train/recall:     
                                                              0.922 train/mre:  
                                                              0.080             
[2024-06-24 10:24:07,443][train.py][INFO] - Starting testing...
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
Restoring states from the checkpoint path at /home/khickey/test_impute/logs/debug/multiruns/06-24-10-06-32/advice_yourself/0.15/5/checkpoints/49-0.00.ckpt
Loaded model weights from the checkpoint at /home/khickey/test_impute/logs/debug/multiruns/06-24-10-06-32/advice_yourself/0.15/5/checkpoints/49-0.00.ckpt
SLURM auto-requeueing enabled. Setting signal handlers.
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃        Test metric        ┃       DataLoader 0        ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
│       test/auc_mean       │    0.5217763781547546     │
│       test/auc_std        │    0.11787982285022736    │
│       test/f1_mean        │    0.5133406519889832     │
│        test/f1_std        │    0.14381571114063263    │
│         test/mre          │    0.08816101402044296    │
│        test/mre_ci        │   0.005706749856472015    │
│    test/precision_mean    │    0.5036163330078125     │
│    test/precision_std     │    0.16251340508460999    │
│     test/recall_mean      │    0.5498371124267578     │
│      test/recall_std      │    0.1714402586221695     │
└───────────────────────────┴───────────────────────────┘
Testing ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1/1 0:00:00 • 0:00:00 0.00it/s 
[2024-06-24 10:24:22,075][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-24-10-06-32/advice_yourself/0.15/5>
[2024-06-24 10:24:22,076][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-24 10:24:22,078][HYDRA] 	#36 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=False data.question=advice_yourself data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=saits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=True seed=6
[rank: 0] Seed set to 6
[2024-06-24 10:24:22,271][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-24 10:24:22,273][train.py][INFO] - Instantiating model <src.methods.saits.lightningmodule.SAITSLightningModule>
[2024-06-24 10:24:22,274][train.py][INFO] - Instantiating callbacks...
[2024-06-24 10:24:22,274][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-24 10:24:22,277][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-24 10:24:22,277][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-24 10:24:22,278][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-24 10:24:22,280][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-24 10:24:22,281][train.py][INFO] - Instantiating loggers...
[2024-06-24 10:24:22,281][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-24 10:24:22,282][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-24 10:24:22,352][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-24-10-06-32/advice_yourself/0.15/6/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳┳━┓
┃┃ Name                                                                     ┃┃ ┃
┡╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇╇━┩
││ model                                                                    ││ │
││ model.layer_stack_for_first_block                                        ││ │
││ model.layer_stack_for_first_block.0                                      ││ │
││ model.layer_stack_for_first_block.0.slf_attn                             ││ │
││ model.layer_stack_for_first_block.0.slf_attn.w_qs                        ││ │
││ model.layer_stack_for_first_block.0.slf_attn.w_ks                        ││ │
││ model.layer_stack_for_first_block.0.slf_attn.w_vs                        ││ │
││ model.layer_stack_for_first_block.0.slf_attn.attention_operator          ││ │
││ model.layer_stack_for_first_block.0.slf_attn.attention_operator.dropout  ││ │
││ model.layer_stack_for_first_block.0.slf_attn.fc                          ││ │
││ model.layer_stack_for_first_block.0.dropout                              ││ │
││ model.layer_stack_for_first_block.0.layer_norm                           ││ │
││ model.layer_stack_for_first_block.0.pos_ffn                              ││ │
││ model.layer_stack_for_first_block.0.pos_ffn.linear_1                     ││ │
││ model.layer_stack_for_first_block.0.pos_ffn.linear_2                     ││ │
││ model.layer_stack_for_first_block.0.pos_ffn.layer_norm                   ││ │
││ model.layer_stack_for_first_block.0.pos_ffn.dropout                      ││ │
││ model.layer_stack_for_second_block                                       ││ │
││ model.layer_stack_for_second_block.0                                     ││ │
││ model.layer_stack_for_second_block.0.slf_attn                            ││ │
││ model.layer_stack_for_second_block.0.slf_attn.w_qs                       ││ │
││ model.layer_stack_for_second_block.0.slf_attn.w_ks                       ││ │
││ model.layer_stack_for_second_block.0.slf_attn.w_vs                       ││ │
││ model.layer_stack_for_second_block.0.slf_attn.attention_operator         ││ │
││ model.layer_stack_for_second_block.0.slf_attn.attention_operator.dropout ││ │
││ model.layer_stack_for_second_block.0.slf_attn.fc                         ││ │
││ model.layer_stack_for_second_block.0.dropout                             ││ │
││ model.layer_stack_for_second_block.0.layer_norm                          ││ │
││ model.layer_stack_for_second_block.0.pos_ffn                             ││ │
││ model.layer_stack_for_second_block.0.pos_ffn.linear_1                    ││ │
││ model.layer_stack_for_second_block.0.pos_ffn.linear_2                    ││ │
││ model.layer_stack_for_second_block.0.pos_ffn.layer_norm                  ││ │
││ model.layer_stack_for_second_block.0.pos_ffn.dropout                     ││ │
││ model.dropout                                                            ││ │
││ model.position_enc                                                       ││ │
││ model.embedding_1                                                        ││ │
││ model.reduce_dim_z                                                       ││ │
││ model.embedding_2                                                        ││ │
││ model.reduce_dim_beta                                                    ││ │
││ model.reduce_dim_gamma                                                   ││ │
││ model.weight_combine                                                     ││ │
││ model.output_projection                                                  ││ │
││ model.self_attention                                                     ││ │
││ model.output_net                                                         ││ │
││ model.output_net.0                                                       ││ │
││ model.output_net.1                                                       ││ │
││ model.output_net.2                                                       ││ │
││ model.output_net.3                                                       ││ │
││ model.output_net.4                                                       ││ │
││ clf_criterion                                                            ││ │
││ saits_loss_func                                                          ││ │
└┴──────────────────────────────────────────────────────────────────────────┴┴─┘
Trainable params: 107 K                                                         
Non-trainable params: 0                                                         
Total params: 107 K                                                             
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 28.75it/s v_num: 0.000      
                                                              val/auc: 0.875    
                                                              val/f1: 0.857     
                                                              val/precision:    
                                                              1.000 val/recall: 
                                                              0.750 val/mre:    
                                                              0.075 train/auc:  
                                                              0.906 train/f1:   
                                                              0.917             
                                                              train/precision:  
                                                              0.893             
                                                              train/recall:     
                                                              0.943 train/mre:  
                                                              0.074             
[2024-06-24 10:24:42,984][train.py][INFO] - Starting testing...
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
Restoring states from the checkpoint path at /home/khickey/test_impute/logs/debug/multiruns/06-24-10-06-32/advice_yourself/0.15/6/checkpoints/46-0.00.ckpt
Loaded model weights from the checkpoint at /home/khickey/test_impute/logs/debug/multiruns/06-24-10-06-32/advice_yourself/0.15/6/checkpoints/46-0.00.ckpt
SLURM auto-requeueing enabled. Setting signal handlers.
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃        Test metric        ┃       DataLoader 0        ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
│       test/auc_mean       │     0.56968092918396      │
│       test/auc_std        │    0.11418068408966064    │
│       test/f1_mean        │    0.4891955554485321     │
│        test/f1_std        │    0.16251643002033234    │
│         test/mre          │    0.07983719557523727    │
│        test/mre_ci        │   0.004864790011197329    │
│    test/precision_mean    │     0.578046977519989     │
│    test/precision_std     │    0.1926705241203308     │
│     test/recall_mean      │    0.44797396659851074    │
│      test/recall_std      │    0.16702131927013397    │
└───────────────────────────┴───────────────────────────┘
Testing ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1/1 0:00:00 • 0:00:00 0.00it/s 
[2024-06-24 10:24:57,926][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-24-10-06-32/advice_yourself/0.15/6>
[2024-06-24 10:24:57,926][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-24 10:24:57,928][HYDRA] 	#37 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=False data.question=advice_yourself data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=saits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=True seed=7
[rank: 0] Seed set to 7
[2024-06-24 10:24:58,125][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-24 10:24:58,127][train.py][INFO] - Instantiating model <src.methods.saits.lightningmodule.SAITSLightningModule>
[2024-06-24 10:24:58,129][train.py][INFO] - Instantiating callbacks...
[2024-06-24 10:24:58,129][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-24 10:24:58,132][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-24 10:24:58,132][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-24 10:24:58,133][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-24 10:24:58,134][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-24 10:24:58,135][train.py][INFO] - Instantiating loggers...
[2024-06-24 10:24:58,135][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-24 10:24:58,136][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-24 10:24:58,167][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-24-10-06-32/advice_yourself/0.15/7/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳┳━┓
┃┃ Name                                                                     ┃┃ ┃
┡╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇╇━┩
││ model                                                                    ││ │
││ model.layer_stack_for_first_block                                        ││ │
││ model.layer_stack_for_first_block.0                                      ││ │
││ model.layer_stack_for_first_block.0.slf_attn                             ││ │
││ model.layer_stack_for_first_block.0.slf_attn.w_qs                        ││ │
││ model.layer_stack_for_first_block.0.slf_attn.w_ks                        ││ │
││ model.layer_stack_for_first_block.0.slf_attn.w_vs                        ││ │
││ model.layer_stack_for_first_block.0.slf_attn.attention_operator          ││ │
││ model.layer_stack_for_first_block.0.slf_attn.attention_operator.dropout  ││ │
││ model.layer_stack_for_first_block.0.slf_attn.fc                          ││ │
││ model.layer_stack_for_first_block.0.dropout                              ││ │
││ model.layer_stack_for_first_block.0.layer_norm                           ││ │
││ model.layer_stack_for_first_block.0.pos_ffn                              ││ │
││ model.layer_stack_for_first_block.0.pos_ffn.linear_1                     ││ │
││ model.layer_stack_for_first_block.0.pos_ffn.linear_2                     ││ │
││ model.layer_stack_for_first_block.0.pos_ffn.layer_norm                   ││ │
││ model.layer_stack_for_first_block.0.pos_ffn.dropout                      ││ │
││ model.layer_stack_for_second_block                                       ││ │
││ model.layer_stack_for_second_block.0                                     ││ │
││ model.layer_stack_for_second_block.0.slf_attn                            ││ │
││ model.layer_stack_for_second_block.0.slf_attn.w_qs                       ││ │
││ model.layer_stack_for_second_block.0.slf_attn.w_ks                       ││ │
││ model.layer_stack_for_second_block.0.slf_attn.w_vs                       ││ │
││ model.layer_stack_for_second_block.0.slf_attn.attention_operator         ││ │
││ model.layer_stack_for_second_block.0.slf_attn.attention_operator.dropout ││ │
││ model.layer_stack_for_second_block.0.slf_attn.fc                         ││ │
││ model.layer_stack_for_second_block.0.dropout                             ││ │
││ model.layer_stack_for_second_block.0.layer_norm                          ││ │
││ model.layer_stack_for_second_block.0.pos_ffn                             ││ │
││ model.layer_stack_for_second_block.0.pos_ffn.linear_1                    ││ │
││ model.layer_stack_for_second_block.0.pos_ffn.linear_2                    ││ │
││ model.layer_stack_for_second_block.0.pos_ffn.layer_norm                  ││ │
││ model.layer_stack_for_second_block.0.pos_ffn.dropout                     ││ │
││ model.dropout                                                            ││ │
││ model.position_enc                                                       ││ │
││ model.embedding_1                                                        ││ │
││ model.reduce_dim_z                                                       ││ │
││ model.embedding_2                                                        ││ │
││ model.reduce_dim_beta                                                    ││ │
││ model.reduce_dim_gamma                                                   ││ │
││ model.weight_combine                                                     ││ │
││ model.output_projection                                                  ││ │
││ model.self_attention                                                     ││ │
││ model.output_net                                                         ││ │
││ model.output_net.0                                                       ││ │
││ model.output_net.1                                                       ││ │
││ model.output_net.2                                                       ││ │
││ model.output_net.3                                                       ││ │
││ model.output_net.4                                                       ││ │
││ clf_criterion                                                            ││ │
││ saits_loss_func                                                          ││ │
└┴──────────────────────────────────────────────────────────────────────────┴┴─┘
Trainable params: 107 K                                                         
Non-trainable params: 0                                                         
Total params: 107 K                                                             
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 28.76it/s v_num: 0.000      
                                                              val/auc: 0.682    
                                                              val/f1: 0.533     
                                                              val/precision:    
                                                              1.000 val/recall: 
                                                              0.364 val/mre:    
                                                              0.068 train/auc:  
                                                              0.938 train/f1:   
                                                              0.943             
                                                              train/precision:  
                                                              0.909             
                                                              train/recall:     
                                                              0.980 train/mre:  
                                                              0.077             
[2024-06-24 10:25:19,028][train.py][INFO] - Starting testing...
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
Restoring states from the checkpoint path at /home/khickey/test_impute/logs/debug/multiruns/06-24-10-06-32/advice_yourself/0.15/7/checkpoints/46-0.00.ckpt
Loaded model weights from the checkpoint at /home/khickey/test_impute/logs/debug/multiruns/06-24-10-06-32/advice_yourself/0.15/7/checkpoints/46-0.00.ckpt
SLURM auto-requeueing enabled. Setting signal handlers.
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃        Test metric        ┃       DataLoader 0        ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
│       test/auc_mean       │    0.5710203051567078     │
│       test/auc_std        │    0.11280128359794617    │
│       test/f1_mean        │    0.48442238569259644    │
│        test/f1_std        │    0.1569477915763855     │
│         test/mre          │    0.07519011944532394    │
│        test/mre_ci        │    0.00424274243414402    │
│    test/precision_mean    │    0.5683151483535767     │
│    test/precision_std     │    0.19390027225017548    │
│     test/recall_mean      │    0.44295212626457214    │
│      test/recall_std      │    0.16891945898532867    │
└───────────────────────────┴───────────────────────────┘
Testing ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1/1 0:00:00 • 0:00:00 0.00it/s 
[2024-06-24 10:25:34,046][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-24-10-06-32/advice_yourself/0.15/7>
[2024-06-24 10:25:34,046][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-24 10:25:34,049][HYDRA] 	#38 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=False data.question=advice_yourself data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=saits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=True seed=8
[rank: 0] Seed set to 8
[2024-06-24 10:25:34,239][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-24 10:25:34,241][train.py][INFO] - Instantiating model <src.methods.saits.lightningmodule.SAITSLightningModule>
[2024-06-24 10:25:34,242][train.py][INFO] - Instantiating callbacks...
[2024-06-24 10:25:34,242][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-24 10:25:34,245][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-24 10:25:34,245][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-24 10:25:34,246][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-24 10:25:34,248][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-24 10:25:34,249][train.py][INFO] - Instantiating loggers...
[2024-06-24 10:25:34,249][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-24 10:25:34,250][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-24 10:25:34,319][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-24-10-06-32/advice_yourself/0.15/8/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳┳━┓
┃┃ Name                                                                     ┃┃ ┃
┡╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇╇━┩
││ model                                                                    ││ │
││ model.layer_stack_for_first_block                                        ││ │
││ model.layer_stack_for_first_block.0                                      ││ │
││ model.layer_stack_for_first_block.0.slf_attn                             ││ │
││ model.layer_stack_for_first_block.0.slf_attn.w_qs                        ││ │
││ model.layer_stack_for_first_block.0.slf_attn.w_ks                        ││ │
││ model.layer_stack_for_first_block.0.slf_attn.w_vs                        ││ │
││ model.layer_stack_for_first_block.0.slf_attn.attention_operator          ││ │
││ model.layer_stack_for_first_block.0.slf_attn.attention_operator.dropout  ││ │
││ model.layer_stack_for_first_block.0.slf_attn.fc                          ││ │
││ model.layer_stack_for_first_block.0.dropout                              ││ │
││ model.layer_stack_for_first_block.0.layer_norm                           ││ │
││ model.layer_stack_for_first_block.0.pos_ffn                              ││ │
││ model.layer_stack_for_first_block.0.pos_ffn.linear_1                     ││ │
││ model.layer_stack_for_first_block.0.pos_ffn.linear_2                     ││ │
││ model.layer_stack_for_first_block.0.pos_ffn.layer_norm                   ││ │
││ model.layer_stack_for_first_block.0.pos_ffn.dropout                      ││ │
││ model.layer_stack_for_second_block                                       ││ │
││ model.layer_stack_for_second_block.0                                     ││ │
││ model.layer_stack_for_second_block.0.slf_attn                            ││ │
││ model.layer_stack_for_second_block.0.slf_attn.w_qs                       ││ │
││ model.layer_stack_for_second_block.0.slf_attn.w_ks                       ││ │
││ model.layer_stack_for_second_block.0.slf_attn.w_vs                       ││ │
││ model.layer_stack_for_second_block.0.slf_attn.attention_operator         ││ │
││ model.layer_stack_for_second_block.0.slf_attn.attention_operator.dropout ││ │
││ model.layer_stack_for_second_block.0.slf_attn.fc                         ││ │
││ model.layer_stack_for_second_block.0.dropout                             ││ │
││ model.layer_stack_for_second_block.0.layer_norm                          ││ │
││ model.layer_stack_for_second_block.0.pos_ffn                             ││ │
││ model.layer_stack_for_second_block.0.pos_ffn.linear_1                    ││ │
││ model.layer_stack_for_second_block.0.pos_ffn.linear_2                    ││ │
││ model.layer_stack_for_second_block.0.pos_ffn.layer_norm                  ││ │
││ model.layer_stack_for_second_block.0.pos_ffn.dropout                     ││ │
││ model.dropout                                                            ││ │
││ model.position_enc                                                       ││ │
││ model.embedding_1                                                        ││ │
││ model.reduce_dim_z                                                       ││ │
││ model.embedding_2                                                        ││ │
││ model.reduce_dim_beta                                                    ││ │
││ model.reduce_dim_gamma                                                   ││ │
││ model.weight_combine                                                     ││ │
││ model.output_projection                                                  ││ │
││ model.self_attention                                                     ││ │
││ model.output_net                                                         ││ │
││ model.output_net.0                                                       ││ │
││ model.output_net.1                                                       ││ │
││ model.output_net.2                                                       ││ │
││ model.output_net.3                                                       ││ │
││ model.output_net.4                                                       ││ │
││ clf_criterion                                                            ││ │
││ saits_loss_func                                                          ││ │
└┴──────────────────────────────────────────────────────────────────────────┴┴─┘
Trainable params: 107 K                                                         
Non-trainable params: 0                                                         
Total params: 107 K                                                             
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 29.34it/s v_num: 0.000      
                                                              val/auc: 0.667    
                                                              val/f1: 0.500     
                                                              val/precision:    
                                                              1.000 val/recall: 
                                                              0.333 val/mre:    
                                                              0.073 train/auc:  
                                                              0.700 train/f1:   
                                                              0.571             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              0.400 train/mre:  
                                                              0.082             
[2024-06-24 10:25:54,906][train.py][INFO] - Starting testing...
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
Restoring states from the checkpoint path at /home/khickey/test_impute/logs/debug/multiruns/06-24-10-06-32/advice_yourself/0.15/8/checkpoints/29-0.00.ckpt
Loaded model weights from the checkpoint at /home/khickey/test_impute/logs/debug/multiruns/06-24-10-06-32/advice_yourself/0.15/8/checkpoints/29-0.00.ckpt
SLURM auto-requeueing enabled. Setting signal handlers.
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃        Test metric        ┃       DataLoader 0        ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
│       test/auc_mean       │    0.5970260500907898     │
│       test/auc_std        │    0.0664883702993393     │
│       test/f1_mean        │    0.6885389089584351     │
│        test/f1_std        │    0.10458104312419891    │
│         test/mre          │    0.06980149447917938    │
│        test/mre_ci        │   0.003642148105427623    │
│    test/precision_mean    │    0.5227286219596863     │
│    test/precision_std     │    0.12214256078004837    │
│     test/recall_mean      │            1.0            │
│      test/recall_std      │            0.0            │
└───────────────────────────┴───────────────────────────┘
Testing ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1/1 0:00:00 • 0:00:00 0.00it/s 
[2024-06-24 10:26:09,865][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-24-10-06-32/advice_yourself/0.15/8>
[2024-06-24 10:26:09,866][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-24 10:26:09,869][HYDRA] 	#39 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=False data.question=advice_yourself data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=saits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=True seed=9
[rank: 0] Seed set to 9
[2024-06-24 10:26:10,071][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-24 10:26:10,073][train.py][INFO] - Instantiating model <src.methods.saits.lightningmodule.SAITSLightningModule>
[2024-06-24 10:26:10,074][train.py][INFO] - Instantiating callbacks...
[2024-06-24 10:26:10,075][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-24 10:26:10,077][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-24 10:26:10,078][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-24 10:26:10,078][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-24 10:26:10,080][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-24 10:26:10,080][train.py][INFO] - Instantiating loggers...
[2024-06-24 10:26:10,080][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-24 10:26:10,082][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-24 10:26:10,113][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-24-10-06-32/advice_yourself/0.15/9/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳┳━┓
┃┃ Name                                                                     ┃┃ ┃
┡╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇╇━┩
││ model                                                                    ││ │
││ model.layer_stack_for_first_block                                        ││ │
││ model.layer_stack_for_first_block.0                                      ││ │
││ model.layer_stack_for_first_block.0.slf_attn                             ││ │
││ model.layer_stack_for_first_block.0.slf_attn.w_qs                        ││ │
││ model.layer_stack_for_first_block.0.slf_attn.w_ks                        ││ │
││ model.layer_stack_for_first_block.0.slf_attn.w_vs                        ││ │
││ model.layer_stack_for_first_block.0.slf_attn.attention_operator          ││ │
││ model.layer_stack_for_first_block.0.slf_attn.attention_operator.dropout  ││ │
││ model.layer_stack_for_first_block.0.slf_attn.fc                          ││ │
││ model.layer_stack_for_first_block.0.dropout                              ││ │
││ model.layer_stack_for_first_block.0.layer_norm                           ││ │
││ model.layer_stack_for_first_block.0.pos_ffn                              ││ │
││ model.layer_stack_for_first_block.0.pos_ffn.linear_1                     ││ │
││ model.layer_stack_for_first_block.0.pos_ffn.linear_2                     ││ │
││ model.layer_stack_for_first_block.0.pos_ffn.layer_norm                   ││ │
││ model.layer_stack_for_first_block.0.pos_ffn.dropout                      ││ │
││ model.layer_stack_for_second_block                                       ││ │
││ model.layer_stack_for_second_block.0                                     ││ │
││ model.layer_stack_for_second_block.0.slf_attn                            ││ │
││ model.layer_stack_for_second_block.0.slf_attn.w_qs                       ││ │
││ model.layer_stack_for_second_block.0.slf_attn.w_ks                       ││ │
││ model.layer_stack_for_second_block.0.slf_attn.w_vs                       ││ │
││ model.layer_stack_for_second_block.0.slf_attn.attention_operator         ││ │
││ model.layer_stack_for_second_block.0.slf_attn.attention_operator.dropout ││ │
││ model.layer_stack_for_second_block.0.slf_attn.fc                         ││ │
││ model.layer_stack_for_second_block.0.dropout                             ││ │
││ model.layer_stack_for_second_block.0.layer_norm                          ││ │
││ model.layer_stack_for_second_block.0.pos_ffn                             ││ │
││ model.layer_stack_for_second_block.0.pos_ffn.linear_1                    ││ │
││ model.layer_stack_for_second_block.0.pos_ffn.linear_2                    ││ │
││ model.layer_stack_for_second_block.0.pos_ffn.layer_norm                  ││ │
││ model.layer_stack_for_second_block.0.pos_ffn.dropout                     ││ │
││ model.dropout                                                            ││ │
││ model.position_enc                                                       ││ │
││ model.embedding_1                                                        ││ │
││ model.reduce_dim_z                                                       ││ │
││ model.embedding_2                                                        ││ │
││ model.reduce_dim_beta                                                    ││ │
││ model.reduce_dim_gamma                                                   ││ │
││ model.weight_combine                                                     ││ │
││ model.output_projection                                                  ││ │
││ model.self_attention                                                     ││ │
││ model.output_net                                                         ││ │
││ model.output_net.0                                                       ││ │
││ model.output_net.1                                                       ││ │
││ model.output_net.2                                                       ││ │
││ model.output_net.3                                                       ││ │
││ model.output_net.4                                                       ││ │
││ clf_criterion                                                            ││ │
││ saits_loss_func                                                          ││ │
└┴──────────────────────────────────────────────────────────────────────────┴┴─┘
Trainable params: 107 K                                                         
Non-trainable params: 0                                                         
Total params: 107 K                                                             
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 28.68it/s v_num: 0.000      
                                                              val/auc: 0.962    
                                                              val/f1: 0.923     
                                                              val/precision:    
                                                              0.857 val/recall: 
                                                              1.000 val/mre:    
                                                              0.073 train/auc:  
                                                              0.990 train/f1:   
                                                              0.989             
                                                              train/precision:  
                                                              0.979             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.078             
[2024-06-24 10:26:30,926][train.py][INFO] - Starting testing...
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
Restoring states from the checkpoint path at /home/khickey/test_impute/logs/debug/multiruns/06-24-10-06-32/advice_yourself/0.15/9/checkpoints/47-0.00.ckpt
Loaded model weights from the checkpoint at /home/khickey/test_impute/logs/debug/multiruns/06-24-10-06-32/advice_yourself/0.15/9/checkpoints/47-0.00.ckpt
SLURM auto-requeueing enabled. Setting signal handlers.
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃        Test metric        ┃       DataLoader 0        ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
│       test/auc_mean       │    0.6748610138893127     │
│       test/auc_std        │    0.10559702664613724    │
│       test/f1_mean        │    0.6094318628311157     │
│        test/f1_std        │    0.14665119349956512    │
│         test/mre          │    0.07271527498960495    │
│        test/mre_ci        │   0.004061663523316383    │
│    test/precision_mean    │    0.7231091260910034     │
│    test/precision_std     │    0.17646583914756775    │
│     test/recall_mean      │    0.5575182437896729     │
│      test/recall_std      │    0.1741323173046112     │
└───────────────────────────┴───────────────────────────┘
Testing ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1/1 0:00:00 • 0:00:00 0.00it/s 
[2024-06-24 10:26:45,832][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-24-10-06-32/advice_yourself/0.15/9>
[2024-06-24 10:26:45,833][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-24 10:26:45,834][HYDRA] 	#40 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=False data.question=advice_yourself data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=saits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=True seed=0
[2024-06-24 10:26:46,028][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-24 10:26:46,030][train.py][INFO] - Instantiating model <src.methods.saits.lightningmodule.SAITSLightningModule>
[2024-06-24 10:26:46,032][train.py][INFO] - Instantiating callbacks...
[2024-06-24 10:26:46,032][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-24 10:26:46,035][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-24 10:26:46,036][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-24 10:26:46,036][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-24 10:26:46,039][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-24 10:26:46,039][train.py][INFO] - Instantiating loggers...
[2024-06-24 10:26:46,039][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-24 10:26:46,041][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-24 10:26:46,111][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-24-10-06-32/advice_yourself/0.2/0/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳┳━┓
┃┃ Name                                                                     ┃┃ ┃
┡╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇╇━┩
││ model                                                                    ││ │
││ model.layer_stack_for_first_block                                        ││ │
││ model.layer_stack_for_first_block.0                                      ││ │
││ model.layer_stack_for_first_block.0.slf_attn                             ││ │
││ model.layer_stack_for_first_block.0.slf_attn.w_qs                        ││ │
││ model.layer_stack_for_first_block.0.slf_attn.w_ks                        ││ │
││ model.layer_stack_for_first_block.0.slf_attn.w_vs                        ││ │
││ model.layer_stack_for_first_block.0.slf_attn.attention_operator          ││ │
││ model.layer_stack_for_first_block.0.slf_attn.attention_operator.dropout  ││ │
││ model.layer_stack_for_first_block.0.slf_attn.fc                          ││ │
││ model.layer_stack_for_first_block.0.dropout                              ││ │
││ model.layer_stack_for_first_block.0.layer_norm                           ││ │
││ model.layer_stack_for_first_block.0.pos_ffn                              ││ │
││ model.layer_stack_for_first_block.0.pos_ffn.linear_1                     ││ │
││ model.layer_stack_for_first_block.0.pos_ffn.linear_2                     ││ │
││ model.layer_stack_for_first_block.0.pos_ffn.layer_norm                   ││ │
││ model.layer_stack_for_first_block.0.pos_ffn.dropout                      ││ │
││ model.layer_stack_for_second_block                                       ││ │
││ model.layer_stack_for_second_block.0                                     ││ │
││ model.layer_stack_for_second_block.0.slf_attn                            ││ │
││ model.layer_stack_for_second_block.0.slf_attn.w_qs                       ││ │
││ model.layer_stack_for_second_block.0.slf_attn.w_ks                       ││ │
││ model.layer_stack_for_second_block.0.slf_attn.w_vs                       ││ │
││ model.layer_stack_for_second_block.0.slf_attn.attention_operator         ││ │
││ model.layer_stack_for_second_block.0.slf_attn.attention_operator.dropout ││ │
││ model.layer_stack_for_second_block.0.slf_attn.fc                         ││ │
││ model.layer_stack_for_second_block.0.dropout                             ││ │
││ model.layer_stack_for_second_block.0.layer_norm                          ││ │
││ model.layer_stack_for_second_block.0.pos_ffn                             ││ │
││ model.layer_stack_for_second_block.0.pos_ffn.linear_1                    ││ │
││ model.layer_stack_for_second_block.0.pos_ffn.linear_2                    ││ │
││ model.layer_stack_for_second_block.0.pos_ffn.layer_norm                  ││ │
││ model.layer_stack_for_second_block.0.pos_ffn.dropout                     ││ │
││ model.dropout                                                            ││ │
││ model.position_enc                                                       ││ │
││ model.embedding_1                                                        ││ │
││ model.reduce_dim_z                                                       ││ │
││ model.embedding_2                                                        ││ │
││ model.reduce_dim_beta                                                    ││ │
││ model.reduce_dim_gamma                                                   ││ │
││ model.weight_combine                                                     ││ │
││ model.output_projection                                                  ││ │
││ model.self_attention                                                     ││ │
││ model.output_net                                                         ││ │
││ model.output_net.0                                                       ││ │
││ model.output_net.1                                                       ││ │
││ model.output_net.2                                                       ││ │
││ model.output_net.3                                                       ││ │
││ model.output_net.4                                                       ││ │
││ clf_criterion                                                            ││ │
││ saits_loss_func                                                          ││ │
└┴──────────────────────────────────────────────────────────────────────────┴┴─┘
Trainable params: 107 K                                                         
Non-trainable params: 0                                                         
Total params: 107 K                                                             
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 29.10it/s v_num: 0.000      
                                                              val/auc: 0.929    
                                                              val/f1: 0.960     
                                                              val/precision:    
                                                              0.923 val/recall: 
                                                              1.000 val/mre:    
                                                              0.067 train/auc:  
                                                              0.933 train/f1:   
                                                              0.948             
                                                              train/precision:  
                                                              0.917             
                                                              train/recall:     
                                                              0.982 train/mre:  
                                                              0.079             
[2024-06-24 10:27:07,514][train.py][INFO] - Starting testing...
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
Restoring states from the checkpoint path at /home/khickey/test_impute/logs/debug/multiruns/06-24-10-06-32/advice_yourself/0.2/0/checkpoints/45-0.00.ckpt
Loaded model weights from the checkpoint at /home/khickey/test_impute/logs/debug/multiruns/06-24-10-06-32/advice_yourself/0.2/0/checkpoints/45-0.00.ckpt
SLURM auto-requeueing enabled. Setting signal handlers.
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃        Test metric        ┃       DataLoader 0        ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
│       test/auc_mean       │    0.49295639991760254    │
│       test/auc_std        │    0.09855624288320541    │
│       test/f1_mean        │     0.572862982749939     │
│        test/f1_std        │    0.12250109761953354    │
│         test/mre          │    0.08505641669034958    │
│        test/mre_ci        │   0.005311512388288975    │
│    test/precision_mean    │    0.46826186776161194    │
│    test/precision_std     │    0.12676528096199036    │
│     test/recall_mean      │    0.7784302234649658     │
│      test/recall_std      │    0.14888830482959747    │
└───────────────────────────┴───────────────────────────┘
Testing ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1/1 0:00:00 • 0:00:00 0.00it/s 
[2024-06-24 10:27:22,473][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-24-10-06-32/advice_yourself/0.2/0>
[2024-06-24 10:27:22,474][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-24 10:27:22,477][HYDRA] 	#41 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=False data.question=advice_yourself data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=saits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=True seed=1
[rank: 0] Seed set to 1
[2024-06-24 10:27:22,672][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-24 10:27:22,674][train.py][INFO] - Instantiating model <src.methods.saits.lightningmodule.SAITSLightningModule>
[2024-06-24 10:27:22,676][train.py][INFO] - Instantiating callbacks...
[2024-06-24 10:27:22,676][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-24 10:27:22,678][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-24 10:27:22,679][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-24 10:27:22,680][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-24 10:27:22,681][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-24 10:27:22,681][train.py][INFO] - Instantiating loggers...
[2024-06-24 10:27:22,682][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-24 10:27:22,683][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-24 10:27:22,715][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-24-10-06-32/advice_yourself/0.2/1/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳┳━┓
┃┃ Name                                                                     ┃┃ ┃
┡╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇╇━┩
││ model                                                                    ││ │
││ model.layer_stack_for_first_block                                        ││ │
││ model.layer_stack_for_first_block.0                                      ││ │
││ model.layer_stack_for_first_block.0.slf_attn                             ││ │
││ model.layer_stack_for_first_block.0.slf_attn.w_qs                        ││ │
││ model.layer_stack_for_first_block.0.slf_attn.w_ks                        ││ │
││ model.layer_stack_for_first_block.0.slf_attn.w_vs                        ││ │
││ model.layer_stack_for_first_block.0.slf_attn.attention_operator          ││ │
││ model.layer_stack_for_first_block.0.slf_attn.attention_operator.dropout  ││ │
││ model.layer_stack_for_first_block.0.slf_attn.fc                          ││ │
││ model.layer_stack_for_first_block.0.dropout                              ││ │
││ model.layer_stack_for_first_block.0.layer_norm                           ││ │
││ model.layer_stack_for_first_block.0.pos_ffn                              ││ │
││ model.layer_stack_for_first_block.0.pos_ffn.linear_1                     ││ │
││ model.layer_stack_for_first_block.0.pos_ffn.linear_2                     ││ │
││ model.layer_stack_for_first_block.0.pos_ffn.layer_norm                   ││ │
││ model.layer_stack_for_first_block.0.pos_ffn.dropout                      ││ │
││ model.layer_stack_for_second_block                                       ││ │
││ model.layer_stack_for_second_block.0                                     ││ │
││ model.layer_stack_for_second_block.0.slf_attn                            ││ │
││ model.layer_stack_for_second_block.0.slf_attn.w_qs                       ││ │
││ model.layer_stack_for_second_block.0.slf_attn.w_ks                       ││ │
││ model.layer_stack_for_second_block.0.slf_attn.w_vs                       ││ │
││ model.layer_stack_for_second_block.0.slf_attn.attention_operator         ││ │
││ model.layer_stack_for_second_block.0.slf_attn.attention_operator.dropout ││ │
││ model.layer_stack_for_second_block.0.slf_attn.fc                         ││ │
││ model.layer_stack_for_second_block.0.dropout                             ││ │
││ model.layer_stack_for_second_block.0.layer_norm                          ││ │
││ model.layer_stack_for_second_block.0.pos_ffn                             ││ │
││ model.layer_stack_for_second_block.0.pos_ffn.linear_1                    ││ │
││ model.layer_stack_for_second_block.0.pos_ffn.linear_2                    ││ │
││ model.layer_stack_for_second_block.0.pos_ffn.layer_norm                  ││ │
││ model.layer_stack_for_second_block.0.pos_ffn.dropout                     ││ │
││ model.dropout                                                            ││ │
││ model.position_enc                                                       ││ │
││ model.embedding_1                                                        ││ │
││ model.reduce_dim_z                                                       ││ │
││ model.embedding_2                                                        ││ │
││ model.reduce_dim_beta                                                    ││ │
││ model.reduce_dim_gamma                                                   ││ │
││ model.weight_combine                                                     ││ │
││ model.output_projection                                                  ││ │
││ model.self_attention                                                     ││ │
││ model.output_net                                                         ││ │
││ model.output_net.0                                                       ││ │
││ model.output_net.1                                                       ││ │
││ model.output_net.2                                                       ││ │
││ model.output_net.3                                                       ││ │
││ model.output_net.4                                                       ││ │
││ clf_criterion                                                            ││ │
││ saits_loss_func                                                          ││ │
└┴──────────────────────────────────────────────────────────────────────────┴┴─┘
Trainable params: 107 K                                                         
Non-trainable params: 0                                                         
Total params: 107 K                                                             
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 29.24it/s v_num: 0.000      
                                                              val/auc: 0.944    
                                                              val/f1: 0.941     
                                                              val/precision:    
                                                              1.000 val/recall: 
                                                              0.889 val/mre:    
                                                              0.063 train/auc:  
                                                              0.940 train/f1:   
                                                              0.941             
                                                              train/precision:  
                                                              0.960             
                                                              train/recall:     
                                                              0.923 train/mre:  
                                                              0.077             
[2024-06-24 10:27:43,493][train.py][INFO] - Starting testing...
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
Restoring states from the checkpoint path at /home/khickey/test_impute/logs/debug/multiruns/06-24-10-06-32/advice_yourself/0.2/1/checkpoints/37-0.00.ckpt
Loaded model weights from the checkpoint at /home/khickey/test_impute/logs/debug/multiruns/06-24-10-06-32/advice_yourself/0.2/1/checkpoints/37-0.00.ckpt
SLURM auto-requeueing enabled. Setting signal handlers.
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃        Test metric        ┃       DataLoader 0        ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
│       test/auc_mean       │    0.6301200985908508     │
│       test/auc_std        │    0.11297918856143951    │
│       test/f1_mean        │    0.5732418298721313     │
│        test/f1_std        │    0.14766624569892883    │
│         test/mre          │    0.07415104657411575    │
│        test/mre_ci        │   0.004328844603151083    │
│    test/precision_mean    │    0.6234394907951355     │
│    test/precision_std     │    0.1714116483926773     │
│     test/recall_mean      │    0.5564596652984619     │
│      test/recall_std      │    0.17628036439418793    │
└───────────────────────────┴───────────────────────────┘
Testing ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1/1 0:00:00 • 0:00:00 0.00it/s 
[2024-06-24 10:27:58,392][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-24-10-06-32/advice_yourself/0.2/1>
[2024-06-24 10:27:58,392][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-24 10:27:58,397][HYDRA] 	#42 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=False data.question=advice_yourself data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=saits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=True seed=2
[rank: 0] Seed set to 2
[2024-06-24 10:27:58,587][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-24 10:27:58,588][train.py][INFO] - Instantiating model <src.methods.saits.lightningmodule.SAITSLightningModule>
[2024-06-24 10:27:58,590][train.py][INFO] - Instantiating callbacks...
[2024-06-24 10:27:58,590][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-24 10:27:58,592][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-24 10:27:58,593][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-24 10:27:58,593][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-24 10:27:58,595][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-24 10:27:58,596][train.py][INFO] - Instantiating loggers...
[2024-06-24 10:27:58,596][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-24 10:27:58,597][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-24 10:27:58,664][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-24-10-06-32/advice_yourself/0.2/2/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳┳━┓
┃┃ Name                                                                     ┃┃ ┃
┡╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇╇━┩
││ model                                                                    ││ │
││ model.layer_stack_for_first_block                                        ││ │
││ model.layer_stack_for_first_block.0                                      ││ │
││ model.layer_stack_for_first_block.0.slf_attn                             ││ │
││ model.layer_stack_for_first_block.0.slf_attn.w_qs                        ││ │
││ model.layer_stack_for_first_block.0.slf_attn.w_ks                        ││ │
││ model.layer_stack_for_first_block.0.slf_attn.w_vs                        ││ │
││ model.layer_stack_for_first_block.0.slf_attn.attention_operator          ││ │
││ model.layer_stack_for_first_block.0.slf_attn.attention_operator.dropout  ││ │
││ model.layer_stack_for_first_block.0.slf_attn.fc                          ││ │
││ model.layer_stack_for_first_block.0.dropout                              ││ │
││ model.layer_stack_for_first_block.0.layer_norm                           ││ │
││ model.layer_stack_for_first_block.0.pos_ffn                              ││ │
││ model.layer_stack_for_first_block.0.pos_ffn.linear_1                     ││ │
││ model.layer_stack_for_first_block.0.pos_ffn.linear_2                     ││ │
││ model.layer_stack_for_first_block.0.pos_ffn.layer_norm                   ││ │
││ model.layer_stack_for_first_block.0.pos_ffn.dropout                      ││ │
││ model.layer_stack_for_second_block                                       ││ │
││ model.layer_stack_for_second_block.0                                     ││ │
││ model.layer_stack_for_second_block.0.slf_attn                            ││ │
││ model.layer_stack_for_second_block.0.slf_attn.w_qs                       ││ │
││ model.layer_stack_for_second_block.0.slf_attn.w_ks                       ││ │
││ model.layer_stack_for_second_block.0.slf_attn.w_vs                       ││ │
││ model.layer_stack_for_second_block.0.slf_attn.attention_operator         ││ │
││ model.layer_stack_for_second_block.0.slf_attn.attention_operator.dropout ││ │
││ model.layer_stack_for_second_block.0.slf_attn.fc                         ││ │
││ model.layer_stack_for_second_block.0.dropout                             ││ │
││ model.layer_stack_for_second_block.0.layer_norm                          ││ │
││ model.layer_stack_for_second_block.0.pos_ffn                             ││ │
││ model.layer_stack_for_second_block.0.pos_ffn.linear_1                    ││ │
││ model.layer_stack_for_second_block.0.pos_ffn.linear_2                    ││ │
││ model.layer_stack_for_second_block.0.pos_ffn.layer_norm                  ││ │
││ model.layer_stack_for_second_block.0.pos_ffn.dropout                     ││ │
││ model.dropout                                                            ││ │
││ model.position_enc                                                       ││ │
││ model.embedding_1                                                        ││ │
││ model.reduce_dim_z                                                       ││ │
││ model.embedding_2                                                        ││ │
││ model.reduce_dim_beta                                                    ││ │
││ model.reduce_dim_gamma                                                   ││ │
││ model.weight_combine                                                     ││ │
││ model.output_projection                                                  ││ │
││ model.self_attention                                                     ││ │
││ model.output_net                                                         ││ │
││ model.output_net.0                                                       ││ │
││ model.output_net.1                                                       ││ │
││ model.output_net.2                                                       ││ │
││ model.output_net.3                                                       ││ │
││ model.output_net.4                                                       ││ │
││ clf_criterion                                                            ││ │
││ saits_loss_func                                                          ││ │
└┴──────────────────────────────────────────────────────────────────────────┴┴─┘
Trainable params: 107 K                                                         
Non-trainable params: 0                                                         
Total params: 107 K                                                             
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 27.92it/s v_num: 0.000      
                                                              val/auc: 1.000    
                                                              val/f1: 1.000     
                                                              val/precision:    
                                                              1.000 val/recall: 
                                                              1.000 val/mre:    
                                                              0.065 train/auc:  
                                                              0.990 train/f1:   
                                                              0.990             
                                                              train/precision:  
                                                              0.981             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.072             
[2024-06-24 10:28:20,746][train.py][INFO] - Starting testing...
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
Restoring states from the checkpoint path at /home/khickey/test_impute/logs/debug/multiruns/06-24-10-06-32/advice_yourself/0.2/2/checkpoints/42-0.00.ckpt
Loaded model weights from the checkpoint at /home/khickey/test_impute/logs/debug/multiruns/06-24-10-06-32/advice_yourself/0.2/2/checkpoints/42-0.00.ckpt
SLURM auto-requeueing enabled. Setting signal handlers.
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃        Test metric        ┃       DataLoader 0        ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
│       test/auc_mean       │    0.5050559043884277     │
│       test/auc_std        │    0.09938480705022812    │
│       test/f1_mean        │    0.29091784358024597    │
│        test/f1_std        │    0.17163534462451935    │
│         test/mre          │    0.07710215449333191    │
│        test/mre_ci        │   0.003966344054788351    │
│    test/precision_mean    │    0.4845426678657532     │
│    test/precision_std     │    0.30088329315185547    │
│     test/recall_mean      │    0.22146622836589813    │
│      test/recall_std      │    0.14289964735507965    │
└───────────────────────────┴───────────────────────────┘
Testing ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1/1 0:00:00 • 0:00:00 0.00it/s 
[2024-06-24 10:28:34,894][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-24-10-06-32/advice_yourself/0.2/2>
[2024-06-24 10:28:34,895][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-24 10:28:34,897][HYDRA] 	#43 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=False data.question=advice_yourself data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=saits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=True seed=3
[rank: 0] Seed set to 3
[2024-06-24 10:28:35,881][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-24 10:28:35,883][train.py][INFO] - Instantiating model <src.methods.saits.lightningmodule.SAITSLightningModule>
[2024-06-24 10:28:35,884][train.py][INFO] - Instantiating callbacks...
[2024-06-24 10:28:35,885][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-24 10:28:35,887][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-24 10:28:35,888][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-24 10:28:35,888][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-24 10:28:35,890][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-24 10:28:35,890][train.py][INFO] - Instantiating loggers...
[2024-06-24 10:28:35,890][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-24 10:28:35,891][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-24 10:28:35,924][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-24-10-06-32/advice_yourself/0.2/3/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳┳━┓
┃┃ Name                                                                     ┃┃ ┃
┡╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇╇━┩
││ model                                                                    ││ │
││ model.layer_stack_for_first_block                                        ││ │
││ model.layer_stack_for_first_block.0                                      ││ │
││ model.layer_stack_for_first_block.0.slf_attn                             ││ │
││ model.layer_stack_for_first_block.0.slf_attn.w_qs                        ││ │
││ model.layer_stack_for_first_block.0.slf_attn.w_ks                        ││ │
││ model.layer_stack_for_first_block.0.slf_attn.w_vs                        ││ │
││ model.layer_stack_for_first_block.0.slf_attn.attention_operator          ││ │
││ model.layer_stack_for_first_block.0.slf_attn.attention_operator.dropout  ││ │
││ model.layer_stack_for_first_block.0.slf_attn.fc                          ││ │
││ model.layer_stack_for_first_block.0.dropout                              ││ │
││ model.layer_stack_for_first_block.0.layer_norm                           ││ │
││ model.layer_stack_for_first_block.0.pos_ffn                              ││ │
││ model.layer_stack_for_first_block.0.pos_ffn.linear_1                     ││ │
││ model.layer_stack_for_first_block.0.pos_ffn.linear_2                     ││ │
││ model.layer_stack_for_first_block.0.pos_ffn.layer_norm                   ││ │
││ model.layer_stack_for_first_block.0.pos_ffn.dropout                      ││ │
││ model.layer_stack_for_second_block                                       ││ │
││ model.layer_stack_for_second_block.0                                     ││ │
││ model.layer_stack_for_second_block.0.slf_attn                            ││ │
││ model.layer_stack_for_second_block.0.slf_attn.w_qs                       ││ │
││ model.layer_stack_for_second_block.0.slf_attn.w_ks                       ││ │
││ model.layer_stack_for_second_block.0.slf_attn.w_vs                       ││ │
││ model.layer_stack_for_second_block.0.slf_attn.attention_operator         ││ │
││ model.layer_stack_for_second_block.0.slf_attn.attention_operator.dropout ││ │
││ model.layer_stack_for_second_block.0.slf_attn.fc                         ││ │
││ model.layer_stack_for_second_block.0.dropout                             ││ │
││ model.layer_stack_for_second_block.0.layer_norm                          ││ │
││ model.layer_stack_for_second_block.0.pos_ffn                             ││ │
││ model.layer_stack_for_second_block.0.pos_ffn.linear_1                    ││ │
││ model.layer_stack_for_second_block.0.pos_ffn.linear_2                    ││ │
││ model.layer_stack_for_second_block.0.pos_ffn.layer_norm                  ││ │
││ model.layer_stack_for_second_block.0.pos_ffn.dropout                     ││ │
││ model.dropout                                                            ││ │
││ model.position_enc                                                       ││ │
││ model.embedding_1                                                        ││ │
││ model.reduce_dim_z                                                       ││ │
││ model.embedding_2                                                        ││ │
││ model.reduce_dim_beta                                                    ││ │
││ model.reduce_dim_gamma                                                   ││ │
││ model.weight_combine                                                     ││ │
││ model.output_projection                                                  ││ │
││ model.self_attention                                                     ││ │
││ model.output_net                                                         ││ │
││ model.output_net.0                                                       ││ │
││ model.output_net.1                                                       ││ │
││ model.output_net.2                                                       ││ │
││ model.output_net.3                                                       ││ │
││ model.output_net.4                                                       ││ │
││ clf_criterion                                                            ││ │
││ saits_loss_func                                                          ││ │
└┴──────────────────────────────────────────────────────────────────────────┴┴─┘
Trainable params: 107 K                                                         
Non-trainable params: 0                                                         
Total params: 107 K                                                             
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 28.95it/s v_num: 0.000      
                                                              val/auc: 0.955    
                                                              val/f1: 0.941     
                                                              val/precision:    
                                                              0.889 val/recall: 
                                                              1.000 val/mre:    
                                                              0.077 train/auc:  
                                                              0.863 train/f1:   
                                                              0.873             
                                                              train/precision:  
                                                              0.774             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.082             
[2024-06-24 10:28:56,237][train.py][INFO] - Starting testing...
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
Restoring states from the checkpoint path at /home/khickey/test_impute/logs/debug/multiruns/06-24-10-06-32/advice_yourself/0.2/3/checkpoints/41-0.00.ckpt
Loaded model weights from the checkpoint at /home/khickey/test_impute/logs/debug/multiruns/06-24-10-06-32/advice_yourself/0.2/3/checkpoints/41-0.00.ckpt
SLURM auto-requeueing enabled. Setting signal handlers.
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃        Test metric        ┃       DataLoader 0        ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
│       test/auc_mean       │     0.561588704586029     │
│       test/auc_std        │    0.08881955593824387    │
│       test/f1_mean        │    0.30523261427879333    │
│        test/f1_std        │    0.18431466817855835    │
│         test/mre          │    0.08122563362121582    │
│        test/mre_ci        │   0.003509294707328081    │
│    test/precision_mean    │    0.6416071653366089     │
│    test/precision_std     │    0.3271532654762268     │
│     test/recall_mean      │    0.22376638650894165    │
│      test/recall_std      │    0.14314225316047668    │
└───────────────────────────┴───────────────────────────┘
Testing ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1/1 0:00:01 • 0:00:00 0.00it/s 
[2024-06-24 10:29:10,467][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-24-10-06-32/advice_yourself/0.2/3>
[2024-06-24 10:29:10,468][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-24 10:29:10,470][HYDRA] 	#44 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=False data.question=advice_yourself data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=saits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=True seed=4
[rank: 0] Seed set to 4
[2024-06-24 10:29:10,657][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-24 10:29:10,658][train.py][INFO] - Instantiating model <src.methods.saits.lightningmodule.SAITSLightningModule>
[2024-06-24 10:29:10,660][train.py][INFO] - Instantiating callbacks...
[2024-06-24 10:29:10,660][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-24 10:29:10,670][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-24 10:29:10,670][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-24 10:29:10,671][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-24 10:29:10,673][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-24 10:29:10,674][train.py][INFO] - Instantiating loggers...
[2024-06-24 10:29:10,674][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-24 10:29:10,675][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-24 10:29:10,743][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-24-10-06-32/advice_yourself/0.2/4/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳┳━┓
┃┃ Name                                                                     ┃┃ ┃
┡╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇╇━┩
││ model                                                                    ││ │
││ model.layer_stack_for_first_block                                        ││ │
││ model.layer_stack_for_first_block.0                                      ││ │
││ model.layer_stack_for_first_block.0.slf_attn                             ││ │
││ model.layer_stack_for_first_block.0.slf_attn.w_qs                        ││ │
││ model.layer_stack_for_first_block.0.slf_attn.w_ks                        ││ │
││ model.layer_stack_for_first_block.0.slf_attn.w_vs                        ││ │
││ model.layer_stack_for_first_block.0.slf_attn.attention_operator          ││ │
││ model.layer_stack_for_first_block.0.slf_attn.attention_operator.dropout  ││ │
││ model.layer_stack_for_first_block.0.slf_attn.fc                          ││ │
││ model.layer_stack_for_first_block.0.dropout                              ││ │
││ model.layer_stack_for_first_block.0.layer_norm                           ││ │
││ model.layer_stack_for_first_block.0.pos_ffn                              ││ │
││ model.layer_stack_for_first_block.0.pos_ffn.linear_1                     ││ │
││ model.layer_stack_for_first_block.0.pos_ffn.linear_2                     ││ │
││ model.layer_stack_for_first_block.0.pos_ffn.layer_norm                   ││ │
││ model.layer_stack_for_first_block.0.pos_ffn.dropout                      ││ │
││ model.layer_stack_for_second_block                                       ││ │
││ model.layer_stack_for_second_block.0                                     ││ │
││ model.layer_stack_for_second_block.0.slf_attn                            ││ │
││ model.layer_stack_for_second_block.0.slf_attn.w_qs                       ││ │
││ model.layer_stack_for_second_block.0.slf_attn.w_ks                       ││ │
││ model.layer_stack_for_second_block.0.slf_attn.w_vs                       ││ │
││ model.layer_stack_for_second_block.0.slf_attn.attention_operator         ││ │
││ model.layer_stack_for_second_block.0.slf_attn.attention_operator.dropout ││ │
││ model.layer_stack_for_second_block.0.slf_attn.fc                         ││ │
││ model.layer_stack_for_second_block.0.dropout                             ││ │
││ model.layer_stack_for_second_block.0.layer_norm                          ││ │
││ model.layer_stack_for_second_block.0.pos_ffn                             ││ │
││ model.layer_stack_for_second_block.0.pos_ffn.linear_1                    ││ │
││ model.layer_stack_for_second_block.0.pos_ffn.linear_2                    ││ │
││ model.layer_stack_for_second_block.0.pos_ffn.layer_norm                  ││ │
││ model.layer_stack_for_second_block.0.pos_ffn.dropout                     ││ │
││ model.dropout                                                            ││ │
││ model.position_enc                                                       ││ │
││ model.embedding_1                                                        ││ │
││ model.reduce_dim_z                                                       ││ │
││ model.embedding_2                                                        ││ │
││ model.reduce_dim_beta                                                    ││ │
││ model.reduce_dim_gamma                                                   ││ │
││ model.weight_combine                                                     ││ │
││ model.output_projection                                                  ││ │
││ model.self_attention                                                     ││ │
││ model.output_net                                                         ││ │
││ model.output_net.0                                                       ││ │
││ model.output_net.1                                                       ││ │
││ model.output_net.2                                                       ││ │
││ model.output_net.3                                                       ││ │
││ model.output_net.4                                                       ││ │
││ clf_criterion                                                            ││ │
││ saits_loss_func                                                          ││ │
└┴──────────────────────────────────────────────────────────────────────────┴┴─┘
Trainable params: 107 K                                                         
Non-trainable params: 0                                                         
Total params: 107 K                                                             
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 27.54it/s v_num: 0.000      
                                                              val/auc: 1.000    
                                                              val/f1: 1.000     
                                                              val/precision:    
                                                              1.000 val/recall: 
                                                              1.000 val/mre:    
                                                              0.070 train/auc:  
                                                              0.936 train/f1:   
                                                              0.932             
                                                              train/precision:  
                                                              0.976             
                                                              train/recall:     
                                                              0.891 train/mre:  
                                                              0.077             
[2024-06-24 10:29:30,765][train.py][INFO] - Starting testing...
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
Restoring states from the checkpoint path at /home/khickey/test_impute/logs/debug/multiruns/06-24-10-06-32/advice_yourself/0.2/4/checkpoints/44-0.00.ckpt
Loaded model weights from the checkpoint at /home/khickey/test_impute/logs/debug/multiruns/06-24-10-06-32/advice_yourself/0.2/4/checkpoints/44-0.00.ckpt
SLURM auto-requeueing enabled. Setting signal handlers.
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃        Test metric        ┃       DataLoader 0        ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
│       test/auc_mean       │    0.5011845827102661     │
│       test/auc_std        │    0.07238326221704483    │
│       test/f1_mean        │    0.16716885566711426    │
│        test/f1_std        │    0.14830224215984344    │
│         test/mre          │    0.07476085424423218    │
│        test/mre_ci        │    0.00433430215343833    │
│    test/precision_mean    │    0.43502143025398254    │
│    test/precision_std     │    0.3892185688018799     │
│     test/recall_mean      │    0.11152765899896622    │
│      test/recall_std      │    0.11000829190015793    │
└───────────────────────────┴───────────────────────────┘
Testing ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1/1 0:00:00 • 0:00:00 0.00it/s 
[2024-06-24 10:29:45,390][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-24-10-06-32/advice_yourself/0.2/4>
[2024-06-24 10:29:45,390][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-24 10:29:45,392][HYDRA] 	#45 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=False data.question=advice_yourself data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=saits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=True seed=5
[rank: 0] Seed set to 5
[2024-06-24 10:29:45,587][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-24 10:29:45,590][train.py][INFO] - Instantiating model <src.methods.saits.lightningmodule.SAITSLightningModule>
[2024-06-24 10:29:45,591][train.py][INFO] - Instantiating callbacks...
[2024-06-24 10:29:45,591][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-24 10:29:45,595][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-24 10:29:45,595][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-24 10:29:45,596][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-24 10:29:45,598][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-24 10:29:45,599][train.py][INFO] - Instantiating loggers...
[2024-06-24 10:29:45,599][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-24 10:29:45,600][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-24 10:29:45,632][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-24-10-06-32/advice_yourself/0.2/5/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳┳━┓
┃┃ Name                                                                     ┃┃ ┃
┡╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇╇━┩
││ model                                                                    ││ │
││ model.layer_stack_for_first_block                                        ││ │
││ model.layer_stack_for_first_block.0                                      ││ │
││ model.layer_stack_for_first_block.0.slf_attn                             ││ │
││ model.layer_stack_for_first_block.0.slf_attn.w_qs                        ││ │
││ model.layer_stack_for_first_block.0.slf_attn.w_ks                        ││ │
││ model.layer_stack_for_first_block.0.slf_attn.w_vs                        ││ │
││ model.layer_stack_for_first_block.0.slf_attn.attention_operator          ││ │
││ model.layer_stack_for_first_block.0.slf_attn.attention_operator.dropout  ││ │
││ model.layer_stack_for_first_block.0.slf_attn.fc                          ││ │
││ model.layer_stack_for_first_block.0.dropout                              ││ │
││ model.layer_stack_for_first_block.0.layer_norm                           ││ │
││ model.layer_stack_for_first_block.0.pos_ffn                              ││ │
││ model.layer_stack_for_first_block.0.pos_ffn.linear_1                     ││ │
││ model.layer_stack_for_first_block.0.pos_ffn.linear_2                     ││ │
││ model.layer_stack_for_first_block.0.pos_ffn.layer_norm                   ││ │
││ model.layer_stack_for_first_block.0.pos_ffn.dropout                      ││ │
││ model.layer_stack_for_second_block                                       ││ │
││ model.layer_stack_for_second_block.0                                     ││ │
││ model.layer_stack_for_second_block.0.slf_attn                            ││ │
││ model.layer_stack_for_second_block.0.slf_attn.w_qs                       ││ │
││ model.layer_stack_for_second_block.0.slf_attn.w_ks                       ││ │
││ model.layer_stack_for_second_block.0.slf_attn.w_vs                       ││ │
││ model.layer_stack_for_second_block.0.slf_attn.attention_operator         ││ │
││ model.layer_stack_for_second_block.0.slf_attn.attention_operator.dropout ││ │
││ model.layer_stack_for_second_block.0.slf_attn.fc                         ││ │
││ model.layer_stack_for_second_block.0.dropout                             ││ │
││ model.layer_stack_for_second_block.0.layer_norm                          ││ │
││ model.layer_stack_for_second_block.0.pos_ffn                             ││ │
││ model.layer_stack_for_second_block.0.pos_ffn.linear_1                    ││ │
││ model.layer_stack_for_second_block.0.pos_ffn.linear_2                    ││ │
││ model.layer_stack_for_second_block.0.pos_ffn.layer_norm                  ││ │
││ model.layer_stack_for_second_block.0.pos_ffn.dropout                     ││ │
││ model.dropout                                                            ││ │
││ model.position_enc                                                       ││ │
││ model.embedding_1                                                        ││ │
││ model.reduce_dim_z                                                       ││ │
││ model.embedding_2                                                        ││ │
││ model.reduce_dim_beta                                                    ││ │
││ model.reduce_dim_gamma                                                   ││ │
││ model.weight_combine                                                     ││ │
││ model.output_projection                                                  ││ │
││ model.self_attention                                                     ││ │
││ model.output_net                                                         ││ │
││ model.output_net.0                                                       ││ │
││ model.output_net.1                                                       ││ │
││ model.output_net.2                                                       ││ │
││ model.output_net.3                                                       ││ │
││ model.output_net.4                                                       ││ │
││ clf_criterion                                                            ││ │
││ saits_loss_func                                                          ││ │
└┴──────────────────────────────────────────────────────────────────────────┴┴─┘
Trainable params: 107 K                                                         
Non-trainable params: 0                                                         
Total params: 107 K                                                             
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 28.44it/s v_num: 0.000      
                                                              val/auc: 0.468    
                                                              val/f1: 0.714     
                                                              val/precision:    
                                                              0.667 val/recall: 
                                                              0.769 val/mre:    
                                                              0.057 train/auc:  
                                                              0.670 train/f1:   
                                                              0.738             
                                                              train/precision:  
                                                              0.634             
                                                              train/recall:     
                                                              0.882 train/mre:  
                                                              0.067             
[2024-06-24 10:30:06,686][train.py][INFO] - Starting testing...
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
Restoring states from the checkpoint path at /home/khickey/test_impute/logs/debug/multiruns/06-24-10-06-32/advice_yourself/0.2/5/checkpoints/0-0.00.ckpt
Loaded model weights from the checkpoint at /home/khickey/test_impute/logs/debug/multiruns/06-24-10-06-32/advice_yourself/0.2/5/checkpoints/0-0.00.ckpt
SLURM auto-requeueing enabled. Setting signal handlers.
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃        Test metric        ┃       DataLoader 0        ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
│       test/auc_mean       │            0.5            │
│       test/auc_std        │            0.0            │
│       test/f1_mean        │     0.629287838935852     │
│        test/f1_std        │    0.10991019010543823    │
│         test/mre          │    0.2736746072769165     │
│        test/mre_ci        │   0.004869766533374786    │
│    test/precision_mean    │    0.4773157835006714     │
│    test/precision_std     │    0.11232612282037735    │
│     test/recall_mean      │            1.0            │
│      test/recall_std      │            0.0            │
└───────────────────────────┴───────────────────────────┘
Testing ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1/1 0:00:00 • 0:00:00 0.00it/s 
[2024-06-24 10:30:21,619][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-24-10-06-32/advice_yourself/0.2/5>
[2024-06-24 10:30:21,620][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-24 10:30:21,623][HYDRA] 	#46 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=False data.question=advice_yourself data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=saits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=True seed=6
[rank: 0] Seed set to 6
[2024-06-24 10:30:21,815][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-24 10:30:21,817][train.py][INFO] - Instantiating model <src.methods.saits.lightningmodule.SAITSLightningModule>
[2024-06-24 10:30:21,819][train.py][INFO] - Instantiating callbacks...
[2024-06-24 10:30:21,819][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-24 10:30:21,821][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-24 10:30:21,821][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-24 10:30:21,822][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-24 10:30:21,824][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-24 10:30:21,825][train.py][INFO] - Instantiating loggers...
[2024-06-24 10:30:21,825][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-24 10:30:21,826][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-24 10:30:21,914][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-24-10-06-32/advice_yourself/0.2/6/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳┳━┓
┃┃ Name                                                                     ┃┃ ┃
┡╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇╇━┩
││ model                                                                    ││ │
││ model.layer_stack_for_first_block                                        ││ │
││ model.layer_stack_for_first_block.0                                      ││ │
││ model.layer_stack_for_first_block.0.slf_attn                             ││ │
││ model.layer_stack_for_first_block.0.slf_attn.w_qs                        ││ │
││ model.layer_stack_for_first_block.0.slf_attn.w_ks                        ││ │
││ model.layer_stack_for_first_block.0.slf_attn.w_vs                        ││ │
││ model.layer_stack_for_first_block.0.slf_attn.attention_operator          ││ │
││ model.layer_stack_for_first_block.0.slf_attn.attention_operator.dropout  ││ │
││ model.layer_stack_for_first_block.0.slf_attn.fc                          ││ │
││ model.layer_stack_for_first_block.0.dropout                              ││ │
││ model.layer_stack_for_first_block.0.layer_norm                           ││ │
││ model.layer_stack_for_first_block.0.pos_ffn                              ││ │
││ model.layer_stack_for_first_block.0.pos_ffn.linear_1                     ││ │
││ model.layer_stack_for_first_block.0.pos_ffn.linear_2                     ││ │
││ model.layer_stack_for_first_block.0.pos_ffn.layer_norm                   ││ │
││ model.layer_stack_for_first_block.0.pos_ffn.dropout                      ││ │
││ model.layer_stack_for_second_block                                       ││ │
││ model.layer_stack_for_second_block.0                                     ││ │
││ model.layer_stack_for_second_block.0.slf_attn                            ││ │
││ model.layer_stack_for_second_block.0.slf_attn.w_qs                       ││ │
││ model.layer_stack_for_second_block.0.slf_attn.w_ks                       ││ │
││ model.layer_stack_for_second_block.0.slf_attn.w_vs                       ││ │
││ model.layer_stack_for_second_block.0.slf_attn.attention_operator         ││ │
││ model.layer_stack_for_second_block.0.slf_attn.attention_operator.dropout ││ │
││ model.layer_stack_for_second_block.0.slf_attn.fc                         ││ │
││ model.layer_stack_for_second_block.0.dropout                             ││ │
││ model.layer_stack_for_second_block.0.layer_norm                          ││ │
││ model.layer_stack_for_second_block.0.pos_ffn                             ││ │
││ model.layer_stack_for_second_block.0.pos_ffn.linear_1                    ││ │
││ model.layer_stack_for_second_block.0.pos_ffn.linear_2                    ││ │
││ model.layer_stack_for_second_block.0.pos_ffn.layer_norm                  ││ │
││ model.layer_stack_for_second_block.0.pos_ffn.dropout                     ││ │
││ model.dropout                                                            ││ │
││ model.position_enc                                                       ││ │
││ model.embedding_1                                                        ││ │
││ model.reduce_dim_z                                                       ││ │
││ model.embedding_2                                                        ││ │
││ model.reduce_dim_beta                                                    ││ │
││ model.reduce_dim_gamma                                                   ││ │
││ model.weight_combine                                                     ││ │
││ model.output_projection                                                  ││ │
││ model.self_attention                                                     ││ │
││ model.output_net                                                         ││ │
││ model.output_net.0                                                       ││ │
││ model.output_net.1                                                       ││ │
││ model.output_net.2                                                       ││ │
││ model.output_net.3                                                       ││ │
││ model.output_net.4                                                       ││ │
││ clf_criterion                                                            ││ │
││ saits_loss_func                                                          ││ │
└┴──────────────────────────────────────────────────────────────────────────┴┴─┘
Trainable params: 107 K                                                         
Non-trainable params: 0                                                         
Total params: 107 K                                                             
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 28.54it/s v_num: 0.000      
                                                              val/auc: 0.958    
                                                              val/f1: 0.957     
                                                              val/precision:    
                                                              1.000 val/recall: 
                                                              0.917 val/mre:    
                                                              0.076 train/auc:  
                                                              0.949 train/f1:   
                                                              0.953             
                                                              train/precision:  
                                                              0.944             
                                                              train/recall:     
                                                              0.962 train/mre:  
                                                              0.077             
[2024-06-24 10:30:42,045][train.py][INFO] - Starting testing...
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
Restoring states from the checkpoint path at /home/khickey/test_impute/logs/debug/multiruns/06-24-10-06-32/advice_yourself/0.2/6/checkpoints/45-0.00.ckpt
Loaded model weights from the checkpoint at /home/khickey/test_impute/logs/debug/multiruns/06-24-10-06-32/advice_yourself/0.2/6/checkpoints/45-0.00.ckpt
SLURM auto-requeueing enabled. Setting signal handlers.
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃        Test metric        ┃       DataLoader 0        ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
│       test/auc_mean       │    0.6254868507385254     │
│       test/auc_std        │    0.11281658709049225    │
│       test/f1_mean        │    0.5777689814567566     │
│        test/f1_std        │    0.1505921185016632     │
│         test/mre          │    0.07491601258516312    │
│        test/mre_ci        │   0.003981451969593763    │
│    test/precision_mean    │    0.6300273537635803     │
│    test/precision_std     │    0.17638777196407318    │
│     test/recall_mean      │    0.5535420775413513     │
│      test/recall_std      │    0.17197874188423157    │
└───────────────────────────┴───────────────────────────┘
Testing ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1/1 0:00:00 • 0:00:00 0.00it/s 
[2024-06-24 10:30:57,090][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-24-10-06-32/advice_yourself/0.2/6>
[2024-06-24 10:30:57,091][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-24 10:30:57,093][HYDRA] 	#47 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=False data.question=advice_yourself data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=saits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=True seed=7
[rank: 0] Seed set to 7
[2024-06-24 10:30:57,285][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-24 10:30:57,287][train.py][INFO] - Instantiating model <src.methods.saits.lightningmodule.SAITSLightningModule>
[2024-06-24 10:30:57,288][train.py][INFO] - Instantiating callbacks...
[2024-06-24 10:30:57,289][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-24 10:30:57,291][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-24 10:30:57,292][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-24 10:30:57,292][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-24 10:30:57,294][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-24 10:30:57,294][train.py][INFO] - Instantiating loggers...
[2024-06-24 10:30:57,295][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-24 10:30:57,296][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-24 10:30:57,332][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-24-10-06-32/advice_yourself/0.2/7/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳┳━┓
┃┃ Name                                                                     ┃┃ ┃
┡╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇╇━┩
││ model                                                                    ││ │
││ model.layer_stack_for_first_block                                        ││ │
││ model.layer_stack_for_first_block.0                                      ││ │
││ model.layer_stack_for_first_block.0.slf_attn                             ││ │
││ model.layer_stack_for_first_block.0.slf_attn.w_qs                        ││ │
││ model.layer_stack_for_first_block.0.slf_attn.w_ks                        ││ │
││ model.layer_stack_for_first_block.0.slf_attn.w_vs                        ││ │
││ model.layer_stack_for_first_block.0.slf_attn.attention_operator          ││ │
││ model.layer_stack_for_first_block.0.slf_attn.attention_operator.dropout  ││ │
││ model.layer_stack_for_first_block.0.slf_attn.fc                          ││ │
││ model.layer_stack_for_first_block.0.dropout                              ││ │
││ model.layer_stack_for_first_block.0.layer_norm                           ││ │
││ model.layer_stack_for_first_block.0.pos_ffn                              ││ │
││ model.layer_stack_for_first_block.0.pos_ffn.linear_1                     ││ │
││ model.layer_stack_for_first_block.0.pos_ffn.linear_2                     ││ │
││ model.layer_stack_for_first_block.0.pos_ffn.layer_norm                   ││ │
││ model.layer_stack_for_first_block.0.pos_ffn.dropout                      ││ │
││ model.layer_stack_for_second_block                                       ││ │
││ model.layer_stack_for_second_block.0                                     ││ │
││ model.layer_stack_for_second_block.0.slf_attn                            ││ │
││ model.layer_stack_for_second_block.0.slf_attn.w_qs                       ││ │
││ model.layer_stack_for_second_block.0.slf_attn.w_ks                       ││ │
││ model.layer_stack_for_second_block.0.slf_attn.w_vs                       ││ │
││ model.layer_stack_for_second_block.0.slf_attn.attention_operator         ││ │
││ model.layer_stack_for_second_block.0.slf_attn.attention_operator.dropout ││ │
││ model.layer_stack_for_second_block.0.slf_attn.fc                         ││ │
││ model.layer_stack_for_second_block.0.dropout                             ││ │
││ model.layer_stack_for_second_block.0.layer_norm                          ││ │
││ model.layer_stack_for_second_block.0.pos_ffn                             ││ │
││ model.layer_stack_for_second_block.0.pos_ffn.linear_1                    ││ │
││ model.layer_stack_for_second_block.0.pos_ffn.linear_2                    ││ │
││ model.layer_stack_for_second_block.0.pos_ffn.layer_norm                  ││ │
││ model.layer_stack_for_second_block.0.pos_ffn.dropout                     ││ │
││ model.dropout                                                            ││ │
││ model.position_enc                                                       ││ │
││ model.embedding_1                                                        ││ │
││ model.reduce_dim_z                                                       ││ │
││ model.embedding_2                                                        ││ │
││ model.reduce_dim_beta                                                    ││ │
││ model.reduce_dim_gamma                                                   ││ │
││ model.weight_combine                                                     ││ │
││ model.output_projection                                                  ││ │
││ model.self_attention                                                     ││ │
││ model.output_net                                                         ││ │
││ model.output_net.0                                                       ││ │
││ model.output_net.1                                                       ││ │
││ model.output_net.2                                                       ││ │
││ model.output_net.3                                                       ││ │
││ model.output_net.4                                                       ││ │
││ clf_criterion                                                            ││ │
││ saits_loss_func                                                          ││ │
└┴──────────────────────────────────────────────────────────────────────────┴┴─┘
Trainable params: 107 K                                                         
Non-trainable params: 0                                                         
Total params: 107 K                                                             
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 29.33it/s v_num: 0.000      
                                                              val/auc: 1.000    
                                                              val/f1: 1.000     
                                                              val/precision:    
                                                              1.000 val/recall: 
                                                              1.000 val/mre:    
                                                              0.067 train/auc:  
                                                              0.990 train/f1:   
                                                              0.990             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              0.980 train/mre:  
                                                              0.079             
[2024-06-24 10:31:18,314][train.py][INFO] - Starting testing...
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
Restoring states from the checkpoint path at /home/khickey/test_impute/logs/debug/multiruns/06-24-10-06-32/advice_yourself/0.2/7/checkpoints/43-0.00.ckpt
Loaded model weights from the checkpoint at /home/khickey/test_impute/logs/debug/multiruns/06-24-10-06-32/advice_yourself/0.2/7/checkpoints/43-0.00.ckpt
SLURM auto-requeueing enabled. Setting signal handlers.
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃        Test metric        ┃       DataLoader 0        ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
│       test/auc_mean       │    0.7341049909591675     │
│       test/auc_std        │    0.10396222770214081    │
│       test/f1_mean        │    0.6925066113471985     │
│        test/f1_std        │    0.13347573578357697    │
│         test/mre          │    0.07802867889404297    │
│        test/mre_ci        │   0.004456621129065752    │
│    test/precision_mean    │    0.7475796341896057     │
│    test/precision_std     │    0.15379421412944794    │
│     test/recall_mean      │    0.6697952747344971     │
│      test/recall_std      │    0.1617266684770584     │
└───────────────────────────┴───────────────────────────┘
Testing ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1/1 0:00:01 • 0:00:00 0.00it/s 
[2024-06-24 10:31:33,545][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-24-10-06-32/advice_yourself/0.2/7>
[2024-06-24 10:31:33,546][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-24 10:31:33,549][HYDRA] 	#48 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=False data.question=advice_yourself data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=saits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=True seed=8
[rank: 0] Seed set to 8
[2024-06-24 10:31:33,767][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-24 10:31:33,769][train.py][INFO] - Instantiating model <src.methods.saits.lightningmodule.SAITSLightningModule>
[2024-06-24 10:31:33,771][train.py][INFO] - Instantiating callbacks...
[2024-06-24 10:31:33,771][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-24 10:31:33,774][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-24 10:31:33,774][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-24 10:31:33,775][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-24 10:31:33,780][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-24 10:31:33,780][train.py][INFO] - Instantiating loggers...
[2024-06-24 10:31:33,781][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-24 10:31:33,782][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-24 10:31:33,897][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-24-10-06-32/advice_yourself/0.2/8/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳┳━┓
┃┃ Name                                                                     ┃┃ ┃
┡╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇╇━┩
││ model                                                                    ││ │
││ model.layer_stack_for_first_block                                        ││ │
││ model.layer_stack_for_first_block.0                                      ││ │
││ model.layer_stack_for_first_block.0.slf_attn                             ││ │
││ model.layer_stack_for_first_block.0.slf_attn.w_qs                        ││ │
││ model.layer_stack_for_first_block.0.slf_attn.w_ks                        ││ │
││ model.layer_stack_for_first_block.0.slf_attn.w_vs                        ││ │
││ model.layer_stack_for_first_block.0.slf_attn.attention_operator          ││ │
││ model.layer_stack_for_first_block.0.slf_attn.attention_operator.dropout  ││ │
││ model.layer_stack_for_first_block.0.slf_attn.fc                          ││ │
││ model.layer_stack_for_first_block.0.dropout                              ││ │
││ model.layer_stack_for_first_block.0.layer_norm                           ││ │
││ model.layer_stack_for_first_block.0.pos_ffn                              ││ │
││ model.layer_stack_for_first_block.0.pos_ffn.linear_1                     ││ │
││ model.layer_stack_for_first_block.0.pos_ffn.linear_2                     ││ │
││ model.layer_stack_for_first_block.0.pos_ffn.layer_norm                   ││ │
││ model.layer_stack_for_first_block.0.pos_ffn.dropout                      ││ │
││ model.layer_stack_for_second_block                                       ││ │
││ model.layer_stack_for_second_block.0                                     ││ │
││ model.layer_stack_for_second_block.0.slf_attn                            ││ │
││ model.layer_stack_for_second_block.0.slf_attn.w_qs                       ││ │
││ model.layer_stack_for_second_block.0.slf_attn.w_ks                       ││ │
││ model.layer_stack_for_second_block.0.slf_attn.w_vs                       ││ │
││ model.layer_stack_for_second_block.0.slf_attn.attention_operator         ││ │
││ model.layer_stack_for_second_block.0.slf_attn.attention_operator.dropout ││ │
││ model.layer_stack_for_second_block.0.slf_attn.fc                         ││ │
││ model.layer_stack_for_second_block.0.dropout                             ││ │
││ model.layer_stack_for_second_block.0.layer_norm                          ││ │
││ model.layer_stack_for_second_block.0.pos_ffn                             ││ │
││ model.layer_stack_for_second_block.0.pos_ffn.linear_1                    ││ │
││ model.layer_stack_for_second_block.0.pos_ffn.linear_2                    ││ │
││ model.layer_stack_for_second_block.0.pos_ffn.layer_norm                  ││ │
││ model.layer_stack_for_second_block.0.pos_ffn.dropout                     ││ │
││ model.dropout                                                            ││ │
││ model.position_enc                                                       ││ │
││ model.embedding_1                                                        ││ │
││ model.reduce_dim_z                                                       ││ │
││ model.embedding_2                                                        ││ │
││ model.reduce_dim_beta                                                    ││ │
││ model.reduce_dim_gamma                                                   ││ │
││ model.weight_combine                                                     ││ │
││ model.output_projection                                                  ││ │
││ model.self_attention                                                     ││ │
││ model.output_net                                                         ││ │
││ model.output_net.0                                                       ││ │
││ model.output_net.1                                                       ││ │
││ model.output_net.2                                                       ││ │
││ model.output_net.3                                                       ││ │
││ model.output_net.4                                                       ││ │
││ clf_criterion                                                            ││ │
││ saits_loss_func                                                          ││ │
└┴──────────────────────────────────────────────────────────────────────────┴┴─┘
Trainable params: 107 K                                                         
Non-trainable params: 0                                                         
Total params: 107 K                                                             
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 29.41it/s v_num: 0.000      
                                                              val/auc: 0.667    
                                                              val/f1: 0.500     
                                                              val/precision:    
                                                              1.000 val/recall: 
                                                              0.333 val/mre:    
                                                              0.075 train/auc:  
                                                              0.772 train/f1:   
                                                              0.720             
                                                              train/precision:  
                                                              0.900             
                                                              train/recall:     
                                                              0.600 train/mre:  
                                                              0.091             
[2024-06-24 10:31:54,778][train.py][INFO] - Starting testing...
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
Restoring states from the checkpoint path at /home/khickey/test_impute/logs/debug/multiruns/06-24-10-06-32/advice_yourself/0.2/8/checkpoints/28-0.00.ckpt
Loaded model weights from the checkpoint at /home/khickey/test_impute/logs/debug/multiruns/06-24-10-06-32/advice_yourself/0.2/8/checkpoints/28-0.00.ckpt
SLURM auto-requeueing enabled. Setting signal handlers.
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃        Test metric        ┃       DataLoader 0        ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
│       test/auc_mean       │            0.5            │
│       test/auc_std        │            0.0            │
│       test/f1_mean        │    0.6407393217086792     │
│        test/f1_std        │    0.10555462539196014    │
│         test/mre          │    0.06926365941762924    │
│        test/mre_ci        │   0.0037894879933446646   │
│    test/precision_mean    │    0.46805262565612793    │
│    test/precision_std     │    0.11608115583658218    │
│     test/recall_mean      │            1.0            │
│      test/recall_std      │            0.0            │
└───────────────────────────┴───────────────────────────┘
Testing ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1/1 0:00:00 • 0:00:00 0.00it/s 
[2024-06-24 10:32:09,372][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-24-10-06-32/advice_yourself/0.2/8>
[2024-06-24 10:32:09,372][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-24 10:32:09,374][HYDRA] 	#49 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=False data.question=advice_yourself data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=saits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=True seed=9
[rank: 0] Seed set to 9
[2024-06-24 10:32:09,572][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-24 10:32:09,575][train.py][INFO] - Instantiating model <src.methods.saits.lightningmodule.SAITSLightningModule>
[2024-06-24 10:32:09,576][train.py][INFO] - Instantiating callbacks...
[2024-06-24 10:32:09,576][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-24 10:32:09,579][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-24 10:32:09,580][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-24 10:32:09,580][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-24 10:32:09,581][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-24 10:32:09,582][train.py][INFO] - Instantiating loggers...
[2024-06-24 10:32:09,582][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-24 10:32:09,583][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-24 10:32:09,614][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-24-10-06-32/advice_yourself/0.2/9/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳┳━┓
┃┃ Name                                                                     ┃┃ ┃
┡╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇╇━┩
││ model                                                                    ││ │
││ model.layer_stack_for_first_block                                        ││ │
││ model.layer_stack_for_first_block.0                                      ││ │
││ model.layer_stack_for_first_block.0.slf_attn                             ││ │
││ model.layer_stack_for_first_block.0.slf_attn.w_qs                        ││ │
││ model.layer_stack_for_first_block.0.slf_attn.w_ks                        ││ │
││ model.layer_stack_for_first_block.0.slf_attn.w_vs                        ││ │
││ model.layer_stack_for_first_block.0.slf_attn.attention_operator          ││ │
││ model.layer_stack_for_first_block.0.slf_attn.attention_operator.dropout  ││ │
││ model.layer_stack_for_first_block.0.slf_attn.fc                          ││ │
││ model.layer_stack_for_first_block.0.dropout                              ││ │
││ model.layer_stack_for_first_block.0.layer_norm                           ││ │
││ model.layer_stack_for_first_block.0.pos_ffn                              ││ │
││ model.layer_stack_for_first_block.0.pos_ffn.linear_1                     ││ │
││ model.layer_stack_for_first_block.0.pos_ffn.linear_2                     ││ │
││ model.layer_stack_for_first_block.0.pos_ffn.layer_norm                   ││ │
││ model.layer_stack_for_first_block.0.pos_ffn.dropout                      ││ │
││ model.layer_stack_for_second_block                                       ││ │
││ model.layer_stack_for_second_block.0                                     ││ │
││ model.layer_stack_for_second_block.0.slf_attn                            ││ │
││ model.layer_stack_for_second_block.0.slf_attn.w_qs                       ││ │
││ model.layer_stack_for_second_block.0.slf_attn.w_ks                       ││ │
││ model.layer_stack_for_second_block.0.slf_attn.w_vs                       ││ │
││ model.layer_stack_for_second_block.0.slf_attn.attention_operator         ││ │
││ model.layer_stack_for_second_block.0.slf_attn.attention_operator.dropout ││ │
││ model.layer_stack_for_second_block.0.slf_attn.fc                         ││ │
││ model.layer_stack_for_second_block.0.dropout                             ││ │
││ model.layer_stack_for_second_block.0.layer_norm                          ││ │
││ model.layer_stack_for_second_block.0.pos_ffn                             ││ │
││ model.layer_stack_for_second_block.0.pos_ffn.linear_1                    ││ │
││ model.layer_stack_for_second_block.0.pos_ffn.linear_2                    ││ │
││ model.layer_stack_for_second_block.0.pos_ffn.layer_norm                  ││ │
││ model.layer_stack_for_second_block.0.pos_ffn.dropout                     ││ │
││ model.dropout                                                            ││ │
││ model.position_enc                                                       ││ │
││ model.embedding_1                                                        ││ │
││ model.reduce_dim_z                                                       ││ │
││ model.embedding_2                                                        ││ │
││ model.reduce_dim_beta                                                    ││ │
││ model.reduce_dim_gamma                                                   ││ │
││ model.weight_combine                                                     ││ │
││ model.output_projection                                                  ││ │
││ model.self_attention                                                     ││ │
││ model.output_net                                                         ││ │
││ model.output_net.0                                                       ││ │
││ model.output_net.1                                                       ││ │
││ model.output_net.2                                                       ││ │
││ model.output_net.3                                                       ││ │
││ model.output_net.4                                                       ││ │
││ clf_criterion                                                            ││ │
││ saits_loss_func                                                          ││ │
└┴──────────────────────────────────────────────────────────────────────────┴┴─┘
Trainable params: 107 K                                                         
Non-trainable params: 0                                                         
Total params: 107 K                                                             
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 28.95it/s v_num: 0.000      
                                                              val/auc: 1.000    
                                                              val/f1: 1.000     
                                                              val/precision:    
                                                              1.000 val/recall: 
                                                              1.000 val/mre:    
                                                              0.067 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.070             
[2024-06-24 10:32:30,722][train.py][INFO] - Starting testing...
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
Restoring states from the checkpoint path at /home/khickey/test_impute/logs/debug/multiruns/06-24-10-06-32/advice_yourself/0.2/9/checkpoints/41-0.00.ckpt
Loaded model weights from the checkpoint at /home/khickey/test_impute/logs/debug/multiruns/06-24-10-06-32/advice_yourself/0.2/9/checkpoints/41-0.00.ckpt
SLURM auto-requeueing enabled. Setting signal handlers.
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃        Test metric        ┃       DataLoader 0        ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
│       test/auc_mean       │    0.6257603764533997     │
│       test/auc_std        │    0.11145513504743576    │
│       test/f1_mean        │    0.5779182314872742     │
│        test/f1_std        │    0.14800256490707397    │
│         test/mre          │    0.07260826230049133    │
│        test/mre_ci        │   0.004123708698898554    │
│    test/precision_mean    │     0.630111575126648     │
│    test/precision_std     │    0.1728653460741043     │
│     test/recall_mean      │    0.5551047325134277     │
│      test/recall_std      │    0.1675412803888321     │
└───────────────────────────┴───────────────────────────┘
Testing ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1/1 0:00:00 • 0:00:00 0.00it/s 
[2024-06-24 10:32:45,283][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-24-10-06-32/advice_yourself/0.2/9>
[2024-06-24 10:32:45,284][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-24 10:32:45,286][HYDRA] 	#50 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=False data.question=advice_yourself data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=saits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=True seed=0
[2024-06-24 10:32:45,472][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-24 10:32:45,474][train.py][INFO] - Instantiating model <src.methods.saits.lightningmodule.SAITSLightningModule>
[2024-06-24 10:32:45,476][train.py][INFO] - Instantiating callbacks...
[2024-06-24 10:32:45,476][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-24 10:32:45,478][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-24 10:32:45,478][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-24 10:32:45,479][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-24 10:32:45,481][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-24 10:32:45,481][train.py][INFO] - Instantiating loggers...
[2024-06-24 10:32:45,482][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-24 10:32:45,483][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-24 10:32:45,553][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-24-10-06-32/advice_yourself/0.25/0/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳┳━┓
┃┃ Name                                                                     ┃┃ ┃
┡╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇╇━┩
││ model                                                                    ││ │
││ model.layer_stack_for_first_block                                        ││ │
││ model.layer_stack_for_first_block.0                                      ││ │
││ model.layer_stack_for_first_block.0.slf_attn                             ││ │
││ model.layer_stack_for_first_block.0.slf_attn.w_qs                        ││ │
││ model.layer_stack_for_first_block.0.slf_attn.w_ks                        ││ │
││ model.layer_stack_for_first_block.0.slf_attn.w_vs                        ││ │
││ model.layer_stack_for_first_block.0.slf_attn.attention_operator          ││ │
││ model.layer_stack_for_first_block.0.slf_attn.attention_operator.dropout  ││ │
││ model.layer_stack_for_first_block.0.slf_attn.fc                          ││ │
││ model.layer_stack_for_first_block.0.dropout                              ││ │
││ model.layer_stack_for_first_block.0.layer_norm                           ││ │
││ model.layer_stack_for_first_block.0.pos_ffn                              ││ │
││ model.layer_stack_for_first_block.0.pos_ffn.linear_1                     ││ │
││ model.layer_stack_for_first_block.0.pos_ffn.linear_2                     ││ │
││ model.layer_stack_for_first_block.0.pos_ffn.layer_norm                   ││ │
││ model.layer_stack_for_first_block.0.pos_ffn.dropout                      ││ │
││ model.layer_stack_for_second_block                                       ││ │
││ model.layer_stack_for_second_block.0                                     ││ │
││ model.layer_stack_for_second_block.0.slf_attn                            ││ │
││ model.layer_stack_for_second_block.0.slf_attn.w_qs                       ││ │
││ model.layer_stack_for_second_block.0.slf_attn.w_ks                       ││ │
││ model.layer_stack_for_second_block.0.slf_attn.w_vs                       ││ │
││ model.layer_stack_for_second_block.0.slf_attn.attention_operator         ││ │
││ model.layer_stack_for_second_block.0.slf_attn.attention_operator.dropout ││ │
││ model.layer_stack_for_second_block.0.slf_attn.fc                         ││ │
││ model.layer_stack_for_second_block.0.dropout                             ││ │
││ model.layer_stack_for_second_block.0.layer_norm                          ││ │
││ model.layer_stack_for_second_block.0.pos_ffn                             ││ │
││ model.layer_stack_for_second_block.0.pos_ffn.linear_1                    ││ │
││ model.layer_stack_for_second_block.0.pos_ffn.linear_2                    ││ │
││ model.layer_stack_for_second_block.0.pos_ffn.layer_norm                  ││ │
││ model.layer_stack_for_second_block.0.pos_ffn.dropout                     ││ │
││ model.dropout                                                            ││ │
││ model.position_enc                                                       ││ │
││ model.embedding_1                                                        ││ │
││ model.reduce_dim_z                                                       ││ │
││ model.embedding_2                                                        ││ │
││ model.reduce_dim_beta                                                    ││ │
││ model.reduce_dim_gamma                                                   ││ │
││ model.weight_combine                                                     ││ │
││ model.output_projection                                                  ││ │
││ model.self_attention                                                     ││ │
││ model.output_net                                                         ││ │
││ model.output_net.0                                                       ││ │
││ model.output_net.1                                                       ││ │
││ model.output_net.2                                                       ││ │
││ model.output_net.3                                                       ││ │
││ model.output_net.4                                                       ││ │
││ clf_criterion                                                            ││ │
││ saits_loss_func                                                          ││ │
└┴──────────────────────────────────────────────────────────────────────────┴┴─┘
Trainable params: 107 K                                                         
Non-trainable params: 0                                                         
Total params: 107 K                                                             
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 28.94it/s v_num: 0.000      
                                                              val/auc: 0.786    
                                                              val/f1: 0.889     
                                                              val/precision:    
                                                              0.800 val/recall: 
                                                              1.000 val/mre:    
                                                              0.074 train/auc:  
                                                              0.868 train/f1:   
                                                              0.883             
                                                              train/precision:  
                                                              0.891             
                                                              train/recall:     
                                                              0.875 train/mre:  
                                                              0.077             
[2024-06-24 10:33:06,070][train.py][INFO] - Starting testing...
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
Restoring states from the checkpoint path at /home/khickey/test_impute/logs/debug/multiruns/06-24-10-06-32/advice_yourself/0.25/0/checkpoints/46-0.00.ckpt
Loaded model weights from the checkpoint at /home/khickey/test_impute/logs/debug/multiruns/06-24-10-06-32/advice_yourself/0.25/0/checkpoints/46-0.00.ckpt
SLURM auto-requeueing enabled. Setting signal handlers.
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃        Test metric        ┃       DataLoader 0        ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
│       test/auc_mean       │    0.6895872354507446     │
│       test/auc_std        │    0.11124872416257858    │
│       test/f1_mean        │    0.6897587180137634     │
│        test/f1_std        │    0.12280480563640594    │
│         test/mre          │    0.08097750693559647    │
│        test/mre_ci        │   0.004173749126493931    │
│    test/precision_mean    │    0.6390928626060486     │
│    test/precision_std     │    0.14208561182022095    │
│     test/recall_mean      │    0.7784302234649658     │
│      test/recall_std      │    0.14888830482959747    │
└───────────────────────────┴───────────────────────────┘
Testing ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1/1 0:00:00 • 0:00:00 0.00it/s 
[2024-06-24 10:33:21,192][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-24-10-06-32/advice_yourself/0.25/0>
[2024-06-24 10:33:21,192][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-24 10:33:21,195][HYDRA] 	#51 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=False data.question=advice_yourself data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=saits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=True seed=1
[rank: 0] Seed set to 1
[2024-06-24 10:33:21,389][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-24 10:33:21,391][train.py][INFO] - Instantiating model <src.methods.saits.lightningmodule.SAITSLightningModule>
[2024-06-24 10:33:21,393][train.py][INFO] - Instantiating callbacks...
[2024-06-24 10:33:21,393][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-24 10:33:21,395][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-24 10:33:21,396][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-24 10:33:21,396][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-24 10:33:21,398][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-24 10:33:21,398][train.py][INFO] - Instantiating loggers...
[2024-06-24 10:33:21,399][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-24 10:33:21,400][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-24 10:33:21,437][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-24-10-06-32/advice_yourself/0.25/1/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳┳━┓
┃┃ Name                                                                     ┃┃ ┃
┡╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇╇━┩
││ model                                                                    ││ │
││ model.layer_stack_for_first_block                                        ││ │
││ model.layer_stack_for_first_block.0                                      ││ │
││ model.layer_stack_for_first_block.0.slf_attn                             ││ │
││ model.layer_stack_for_first_block.0.slf_attn.w_qs                        ││ │
││ model.layer_stack_for_first_block.0.slf_attn.w_ks                        ││ │
││ model.layer_stack_for_first_block.0.slf_attn.w_vs                        ││ │
││ model.layer_stack_for_first_block.0.slf_attn.attention_operator          ││ │
││ model.layer_stack_for_first_block.0.slf_attn.attention_operator.dropout  ││ │
││ model.layer_stack_for_first_block.0.slf_attn.fc                          ││ │
││ model.layer_stack_for_first_block.0.dropout                              ││ │
││ model.layer_stack_for_first_block.0.layer_norm                           ││ │
││ model.layer_stack_for_first_block.0.pos_ffn                              ││ │
││ model.layer_stack_for_first_block.0.pos_ffn.linear_1                     ││ │
││ model.layer_stack_for_first_block.0.pos_ffn.linear_2                     ││ │
││ model.layer_stack_for_first_block.0.pos_ffn.layer_norm                   ││ │
││ model.layer_stack_for_first_block.0.pos_ffn.dropout                      ││ │
││ model.layer_stack_for_second_block                                       ││ │
││ model.layer_stack_for_second_block.0                                     ││ │
││ model.layer_stack_for_second_block.0.slf_attn                            ││ │
││ model.layer_stack_for_second_block.0.slf_attn.w_qs                       ││ │
││ model.layer_stack_for_second_block.0.slf_attn.w_ks                       ││ │
││ model.layer_stack_for_second_block.0.slf_attn.w_vs                       ││ │
││ model.layer_stack_for_second_block.0.slf_attn.attention_operator         ││ │
││ model.layer_stack_for_second_block.0.slf_attn.attention_operator.dropout ││ │
││ model.layer_stack_for_second_block.0.slf_attn.fc                         ││ │
││ model.layer_stack_for_second_block.0.dropout                             ││ │
││ model.layer_stack_for_second_block.0.layer_norm                          ││ │
││ model.layer_stack_for_second_block.0.pos_ffn                             ││ │
││ model.layer_stack_for_second_block.0.pos_ffn.linear_1                    ││ │
││ model.layer_stack_for_second_block.0.pos_ffn.linear_2                    ││ │
││ model.layer_stack_for_second_block.0.pos_ffn.layer_norm                  ││ │
││ model.layer_stack_for_second_block.0.pos_ffn.dropout                     ││ │
││ model.dropout                                                            ││ │
││ model.position_enc                                                       ││ │
││ model.embedding_1                                                        ││ │
││ model.reduce_dim_z                                                       ││ │
││ model.embedding_2                                                        ││ │
││ model.reduce_dim_beta                                                    ││ │
││ model.reduce_dim_gamma                                                   ││ │
││ model.weight_combine                                                     ││ │
││ model.output_projection                                                  ││ │
││ model.self_attention                                                     ││ │
││ model.output_net                                                         ││ │
││ model.output_net.0                                                       ││ │
││ model.output_net.1                                                       ││ │
││ model.output_net.2                                                       ││ │
││ model.output_net.3                                                       ││ │
││ model.output_net.4                                                       ││ │
││ clf_criterion                                                            ││ │
││ saits_loss_func                                                          ││ │
└┴──────────────────────────────────────────────────────────────────────────┴┴─┘
Trainable params: 107 K                                                         
Non-trainable params: 0                                                         
Total params: 107 K                                                             
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 28.15it/s v_num: 0.000      
                                                              val/auc: 1.000    
                                                              val/f1: 1.000     
                                                              val/precision:    
                                                              1.000 val/recall: 
                                                              1.000 val/mre:    
                                                              0.065 train/auc:  
                                                              0.875 train/f1:   
                                                              0.857             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              0.750 train/mre:  
                                                              0.082             
[2024-06-24 10:33:42,433][train.py][INFO] - Starting testing...
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
Restoring states from the checkpoint path at /home/khickey/test_impute/logs/debug/multiruns/06-24-10-06-32/advice_yourself/0.25/1/checkpoints/34-0.00.ckpt
Loaded model weights from the checkpoint at /home/khickey/test_impute/logs/debug/multiruns/06-24-10-06-32/advice_yourself/0.25/1/checkpoints/34-0.00.ckpt
SLURM auto-requeueing enabled. Setting signal handlers.
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃        Test metric        ┃       DataLoader 0        ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
│       test/auc_mean       │    0.5299639701843262     │
│       test/auc_std        │    0.11601349711418152    │
│       test/f1_mean        │    0.5133618116378784     │
│        test/f1_std        │    0.14025041460990906    │
│         test/mre          │    0.07701074331998825    │
│        test/mre_ci        │   0.004358834121376276    │
│    test/precision_mean    │      0.4998779296875      │
│    test/precision_std     │    0.15962335467338562    │
│     test/recall_mean      │    0.5564596652984619     │
│      test/recall_std      │    0.17628036439418793    │
└───────────────────────────┴───────────────────────────┘
Testing ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1/1 0:00:01 • 0:00:00 0.00it/s 
[2024-06-24 10:33:57,667][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-24-10-06-32/advice_yourself/0.25/1>
[2024-06-24 10:33:57,667][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-24 10:33:57,670][HYDRA] 	#52 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=False data.question=advice_yourself data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=saits callbacks=[default,imputation_metrics] callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=True seed=2
[rank: 0] Seed set to 2
[2024-06-24 10:33:57,966][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-24 10:33:57,968][train.py][INFO] - Instantiating model <src.methods.saits.lightningmodule.SAITSLightningModule>
[2024-06-24 10:33:57,970][train.py][INFO] - Instantiating callbacks...
[2024-06-24 10:33:57,970][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-24 10:33:57,973][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-24 10:33:57,973][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-24 10:33:57,974][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-24 10:33:57,978][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-24 10:33:57,978][train.py][INFO] - Instantiating loggers...
[2024-06-24 10:33:57,979][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-24 10:33:57,980][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-24 10:33:58,077][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-24-10-06-32/advice_yourself/0.25/2/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
slurmstepd: error: *** STEP 652365.0 ON compute-2-01 CANCELLED AT 2024-06-24T10:34:00 ***
slurmstepd: error: *** JOB 652365 ON compute-2-01 CANCELLED AT 2024-06-24T10:34:00 ***
srun: Job step aborted: Waiting up to 32 seconds for job step to finish.
