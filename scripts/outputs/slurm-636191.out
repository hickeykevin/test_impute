[2024-05-30 11:05:50,656][HYDRA] Launching 490 jobs locally
[2024-05-30 11:05:50,657][HYDRA] 	#0 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=advice_yourself data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=0
[2024-05-30 11:05:50,941][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 11:05:52,594][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
2024-05-30 11:05:54 [ERROR]: ❌ No module named 'torch_geometric'
Note torch_geometric is missing, please install it with 'pip install torch_geometric torch_scatter torch_sparse' or 'conda install -c pyg pyg pytorch-scatter pytorch-sparse'
2024-05-30 11:05:54 [ERROR]: ❌ name 'MessagePassing' is not defined
Note torch_geometric is missing, please install it with 'pip install torch_geometric torch_scatter torch_sparse' or 'conda install -c pyg pyg pytorch-scatter pytorch-sparse'
[2024-05-30 11:05:54,124][train.py][INFO] - Instantiating callbacks...
[2024-05-30 11:05:54,125][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 11:05:54,151][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 11:05:54,153][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 11:05:54,161][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 11:05:54,162][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 11:05:54,363][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 11:05:54,364][train.py][INFO] - Instantiating loggers...
[2024-05-30 11:05:54,365][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 11:05:54,367][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
/home/khickey/test_impute/env/lib/python3.12/site-packages/torch/cuda/__init__.py:628: UserWarning: Can't initialize NVML
  warnings.warn("Can't initialize NVML")
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 11:05:55,112][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/advice_yourself/0.0/0/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.37it/s v_num: 0.000      
                                                              val/auc: 0.567    
                                                              val/f1: 0.429     
                                                              val/precision:    
                                                              0.600 val/recall: 
                                                              0.333 val/mre: nan
                                                              train/auc: 0.855  
                                                              train/f1: 0.847   
                                                              train/precision:  
                                                              0.893             
                                                              train/recall:     
                                                              0.806 train/mre:  
                                                              nan               
[2024-05-30 11:07:44,379][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/advice_yourself/0.0/0>
[2024-05-30 11:07:44,380][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 11:07:44,384][HYDRA] 	#1 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=advice_yourself data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=1
[rank: 0] Seed set to 1
[2024-05-30 11:07:44,697][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 11:07:44,700][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 11:07:44,702][train.py][INFO] - Instantiating callbacks...
[2024-05-30 11:07:44,702][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 11:07:44,706][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 11:07:44,707][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 11:07:44,708][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 11:07:44,708][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 11:07:44,710][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 11:07:44,729][train.py][INFO] - Instantiating loggers...
[2024-05-30 11:07:44,730][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 11:07:44,733][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 11:07:44,795][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/advice_yourself/0.0/1/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.45it/s v_num: 0.000      
                                                              val/auc: 0.561    
                                                              val/f1: 0.333     
                                                              val/precision:    
                                                              0.667 val/recall: 
                                                              0.222 val/mre: nan
                                                              train/auc: 0.895  
                                                              train/f1: 0.896   
                                                              train/precision:  
                                                              0.889             
                                                              train/recall:     
                                                              0.903 train/mre:  
                                                              nan               
[2024-05-30 11:09:32,932][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/advice_yourself/0.0/1>
[2024-05-30 11:09:32,933][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 11:09:32,937][HYDRA] 	#2 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=advice_yourself data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=2
[rank: 0] Seed set to 2
[2024-05-30 11:09:33,234][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 11:09:33,236][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 11:09:33,238][train.py][INFO] - Instantiating callbacks...
[2024-05-30 11:09:33,239][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 11:09:33,242][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 11:09:33,243][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 11:09:33,244][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 11:09:33,245][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 11:09:33,247][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 11:09:33,247][train.py][INFO] - Instantiating loggers...
[2024-05-30 11:09:33,248][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 11:09:33,250][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 11:09:33,294][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/advice_yourself/0.0/2/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.56it/s v_num: 0.000      
                                                              val/auc: 0.617    
                                                              val/f1: 0.462     
                                                              val/precision:    
                                                              0.750 val/recall: 
                                                              0.333 val/mre: nan
                                                              train/auc: 0.863  
                                                              train/f1: 0.862   
                                                              train/precision:  
                                                              0.869             
                                                              train/recall:     
                                                              0.855 train/mre:  
                                                              nan               
[2024-05-30 11:11:21,733][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/advice_yourself/0.0/2>
[2024-05-30 11:11:21,734][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 11:11:21,737][HYDRA] 	#3 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=advice_yourself data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=3
[rank: 0] Seed set to 3
[2024-05-30 11:11:22,018][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 11:11:22,021][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 11:11:22,023][train.py][INFO] - Instantiating callbacks...
[2024-05-30 11:11:22,023][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 11:11:22,027][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 11:11:22,027][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 11:11:22,028][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 11:11:22,029][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 11:11:22,031][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 11:11:22,032][train.py][INFO] - Instantiating loggers...
[2024-05-30 11:11:22,032][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 11:11:22,034][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 11:11:22,076][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/advice_yourself/0.0/3/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.45it/s v_num: 0.000      
                                                              val/auc: 0.733    
                                                              val/f1: 0.706     
                                                              val/precision:    
                                                              0.750 val/recall: 
                                                              0.667 val/mre: nan
                                                              train/auc: 0.839  
                                                              train/f1: 0.833   
                                                              train/precision:  
                                                              0.862             
                                                              train/recall:     
                                                              0.806 train/mre:  
                                                              nan               
[2024-05-30 11:13:09,698][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/advice_yourself/0.0/3>
[2024-05-30 11:13:09,699][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 11:13:09,702][HYDRA] 	#4 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=advice_yourself data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=4
[rank: 0] Seed set to 4
[2024-05-30 11:13:09,996][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 11:13:09,999][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 11:13:10,001][train.py][INFO] - Instantiating callbacks...
[2024-05-30 11:13:10,001][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 11:13:10,005][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 11:13:10,006][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 11:13:10,006][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 11:13:10,007][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 11:13:10,009][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 11:13:10,010][train.py][INFO] - Instantiating loggers...
[2024-05-30 11:13:10,010][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 11:13:10,013][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 11:13:10,057][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/advice_yourself/0.0/4/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.51it/s v_num: 0.000      
                                                              val/auc: 0.611    
                                                              val/f1: 0.364     
                                                              val/precision:    
                                                              1.000 val/recall: 
                                                              0.222 val/mre: nan
                                                              train/auc: 0.927  
                                                              train/f1: 0.927   
                                                              train/precision:  
                                                              0.934             
                                                              train/recall:     
                                                              0.919 train/mre:  
                                                              nan               
[2024-05-30 11:14:57,238][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/advice_yourself/0.0/4>
[2024-05-30 11:14:57,239][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 11:14:57,242][HYDRA] 	#5 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=advice_yourself data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=5
[rank: 0] Seed set to 5
[2024-05-30 11:14:57,527][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 11:14:57,529][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 11:14:57,532][train.py][INFO] - Instantiating callbacks...
[2024-05-30 11:14:57,532][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 11:14:57,536][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 11:14:57,536][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 11:14:57,537][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 11:14:57,538][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 11:14:57,540][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 11:14:57,541][train.py][INFO] - Instantiating loggers...
[2024-05-30 11:14:57,541][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 11:14:57,543][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 11:14:57,587][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/advice_yourself/0.0/5/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.47it/s v_num: 0.000      
                                                              val/auc: 0.572    
                                                              val/f1: 0.500     
                                                              val/precision:    
                                                              0.571 val/recall: 
                                                              0.444 val/mre: nan
                                                              train/auc: 0.935  
                                                              train/f1: 0.935   
                                                              train/precision:  
                                                              0.935             
                                                              train/recall:     
                                                              0.935 train/mre:  
                                                              nan               
[2024-05-30 11:16:44,408][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/advice_yourself/0.0/5>
[2024-05-30 11:16:44,409][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 11:16:44,412][HYDRA] 	#6 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=advice_yourself data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=6
[rank: 0] Seed set to 6
[2024-05-30 11:16:44,709][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 11:16:44,711][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 11:16:44,713][train.py][INFO] - Instantiating callbacks...
[2024-05-30 11:16:44,714][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 11:16:44,717][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 11:16:44,718][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 11:16:44,719][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 11:16:44,719][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 11:16:44,721][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 11:16:44,722][train.py][INFO] - Instantiating loggers...
[2024-05-30 11:16:44,722][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 11:16:44,724][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 11:16:44,778][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/advice_yourself/0.0/6/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.61it/s v_num: 0.000      
                                                              val/auc: 0.567    
                                                              val/f1: 0.429     
                                                              val/precision:    
                                                              0.600 val/recall: 
                                                              0.333 val/mre: nan
                                                              train/auc: 0.895  
                                                              train/f1: 0.891   
                                                              train/precision:  
                                                              0.930             
                                                              train/recall:     
                                                              0.855 train/mre:  
                                                              nan               
[2024-05-30 11:18:28,714][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/advice_yourself/0.0/6>
[2024-05-30 11:18:28,715][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 11:18:28,718][HYDRA] 	#7 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=advice_yourself data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=7
[rank: 0] Seed set to 7
[2024-05-30 11:18:29,000][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 11:18:29,002][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 11:18:29,004][train.py][INFO] - Instantiating callbacks...
[2024-05-30 11:18:29,005][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 11:18:29,008][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 11:18:29,009][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 11:18:29,010][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 11:18:29,010][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 11:18:29,012][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 11:18:29,013][train.py][INFO] - Instantiating loggers...
[2024-05-30 11:18:29,014][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 11:18:29,016][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 11:18:29,063][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/advice_yourself/0.0/7/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.55it/s v_num: 0.000      
                                                              val/auc: 0.628    
                                                              val/f1: 0.588     
                                                              val/precision:    
                                                              0.625 val/recall: 
                                                              0.556 val/mre: nan
                                                              train/auc: 0.895  
                                                              train/f1: 0.901   
                                                              train/precision:  
                                                              0.855             
                                                              train/recall:     
                                                              0.952 train/mre:  
                                                              nan               
[2024-05-30 11:20:13,387][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/advice_yourself/0.0/7>
[2024-05-30 11:20:13,388][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 11:20:13,391][HYDRA] 	#8 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=advice_yourself data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=8
[rank: 0] Seed set to 8
[2024-05-30 11:20:13,687][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 11:20:13,689][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 11:20:13,691][train.py][INFO] - Instantiating callbacks...
[2024-05-30 11:20:13,692][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 11:20:13,695][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 11:20:13,696][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 11:20:13,696][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 11:20:13,697][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 11:20:13,699][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 11:20:13,701][train.py][INFO] - Instantiating loggers...
[2024-05-30 11:20:13,702][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 11:20:13,704][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 11:20:13,750][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/advice_yourself/0.0/8/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.20it/s v_num: 0.000      
                                                              val/auc: 0.672    
                                                              val/f1: 0.571     
                                                              val/precision:    
                                                              0.800 val/recall: 
                                                              0.444 val/mre: nan
                                                              train/auc: 0.960  
                                                              train/f1: 0.959   
                                                              train/precision:  
                                                              0.983             
                                                              train/recall:     
                                                              0.935 train/mre:  
                                                              nan               
[2024-05-30 11:21:57,414][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/advice_yourself/0.0/8>
[2024-05-30 11:21:57,415][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 11:21:57,418][HYDRA] 	#9 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=advice_yourself data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=9
[rank: 0] Seed set to 9
[2024-05-30 11:21:57,703][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 11:21:57,706][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 11:21:57,708][train.py][INFO] - Instantiating callbacks...
[2024-05-30 11:21:57,708][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 11:21:57,711][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 11:21:57,712][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 11:21:57,713][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 11:21:57,714][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 11:21:57,715][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 11:21:57,716][train.py][INFO] - Instantiating loggers...
[2024-05-30 11:21:57,717][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 11:21:57,719][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 11:21:57,760][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/advice_yourself/0.0/9/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.58it/s v_num: 0.000      
                                                              val/auc: 0.456    
                                                              val/f1: 0.167     
                                                              val/precision:    
                                                              0.333 val/recall: 
                                                              0.111 val/mre: nan
                                                              train/auc: 0.847  
                                                              train/f1: 0.855   
                                                              train/precision:  
                                                              0.812             
                                                              train/recall:     
                                                              0.903 train/mre:  
                                                              nan               
[2024-05-30 11:23:41,349][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/advice_yourself/0.0/9>
[2024-05-30 11:23:41,350][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 11:23:41,353][HYDRA] 	#10 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=advice_yourself data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=0
[2024-05-30 11:23:41,654][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 11:23:41,656][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 11:23:41,658][train.py][INFO] - Instantiating callbacks...
[2024-05-30 11:23:41,659][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 11:23:41,662][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 11:23:41,663][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 11:23:41,664][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 11:23:41,664][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 11:23:41,666][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 11:23:41,669][train.py][INFO] - Instantiating loggers...
[2024-05-30 11:23:41,669][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 11:23:41,671][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 11:23:41,716][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/advice_yourself/0.05/0/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.60it/s v_num: 0.000      
                                                              val/auc: 0.567    
                                                              val/f1: 0.429     
                                                              val/precision:    
                                                              0.600 val/recall: 
                                                              0.333 val/mre:    
                                                              0.121 train/auc:  
                                                              0.879 train/f1:   
                                                              0.882             
                                                              train/precision:  
                                                              0.862             
                                                              train/recall:     
                                                              0.903 train/mre:  
                                                              0.125             
[2024-05-30 11:25:26,198][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/advice_yourself/0.05/0>
[2024-05-30 11:25:26,198][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 11:25:26,201][HYDRA] 	#11 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=advice_yourself data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=1
[rank: 0] Seed set to 1
[2024-05-30 11:25:26,489][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 11:25:26,491][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 11:25:26,493][train.py][INFO] - Instantiating callbacks...
[2024-05-30 11:25:26,493][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 11:25:26,497][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 11:25:26,497][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 11:25:26,498][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 11:25:26,499][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 11:25:26,501][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 11:25:26,502][train.py][INFO] - Instantiating loggers...
[2024-05-30 11:25:26,502][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 11:25:26,504][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 11:25:26,546][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/advice_yourself/0.05/1/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.55it/s v_num: 0.000      
                                                              val/auc: 0.617    
                                                              val/f1: 0.462     
                                                              val/precision:    
                                                              0.750 val/recall: 
                                                              0.333 val/mre:    
                                                              0.120 train/auc:  
                                                              0.863 train/f1:   
                                                              0.862             
                                                              train/precision:  
                                                              0.869             
                                                              train/recall:     
                                                              0.855 train/mre:  
                                                              0.122             
[2024-05-30 11:27:10,648][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/advice_yourself/0.05/1>
[2024-05-30 11:27:10,649][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 11:27:10,652][HYDRA] 	#12 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=advice_yourself data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=2
[rank: 0] Seed set to 2
[2024-05-30 11:27:10,947][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 11:27:10,949][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 11:27:10,951][train.py][INFO] - Instantiating callbacks...
[2024-05-30 11:27:10,952][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 11:27:10,955][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 11:27:10,956][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 11:27:10,957][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 11:27:10,957][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 11:27:10,959][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 11:27:10,962][train.py][INFO] - Instantiating loggers...
[2024-05-30 11:27:10,963][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 11:27:10,965][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 11:27:11,012][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/advice_yourself/0.05/2/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.59it/s v_num: 0.000      
                                                              val/auc: 0.722    
                                                              val/f1: 0.615     
                                                              val/precision:    
                                                              1.000 val/recall: 
                                                              0.444 val/mre:    
                                                              0.114 train/auc:  
                                                              0.879 train/f1:   
                                                              0.876             
                                                              train/precision:  
                                                              0.898             
                                                              train/recall:     
                                                              0.855 train/mre:  
                                                              0.122             
[2024-05-30 11:28:55,278][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/advice_yourself/0.05/2>
[2024-05-30 11:28:55,279][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 11:28:55,282][HYDRA] 	#13 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=advice_yourself data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=3
[rank: 0] Seed set to 3
[2024-05-30 11:28:55,608][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 11:28:55,611][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 11:28:55,613][train.py][INFO] - Instantiating callbacks...
[2024-05-30 11:28:55,614][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 11:28:55,618][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 11:28:55,619][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 11:28:55,620][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 11:28:55,621][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 11:28:55,622][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 11:28:55,623][train.py][INFO] - Instantiating loggers...
[2024-05-30 11:28:55,624][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 11:28:55,627][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 11:28:55,689][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/advice_yourself/0.05/3/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.49it/s v_num: 0.000      
                                                              val/auc: 0.678    
                                                              val/f1: 0.625     
                                                              val/precision:    
                                                              0.714 val/recall: 
                                                              0.556 val/mre:    
                                                              0.113 train/auc:  
                                                              0.871 train/f1:   
                                                              0.877             
                                                              train/precision:  
                                                              0.838             
                                                              train/recall:     
                                                              0.919 train/mre:  
                                                              0.119             
[2024-05-30 11:30:39,059][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/advice_yourself/0.05/3>
[2024-05-30 11:30:39,059][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 11:30:39,063][HYDRA] 	#14 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=advice_yourself data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=4
[rank: 0] Seed set to 4
[2024-05-30 11:30:39,353][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 11:30:39,355][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 11:30:39,357][train.py][INFO] - Instantiating callbacks...
[2024-05-30 11:30:39,358][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 11:30:39,361][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 11:30:39,362][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 11:30:39,363][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 11:30:39,363][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 11:30:39,365][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 11:30:39,367][train.py][INFO] - Instantiating loggers...
[2024-05-30 11:30:39,368][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 11:30:39,370][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 11:30:39,420][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/advice_yourself/0.05/4/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.60it/s v_num: 0.000      
                                                              val/auc: 0.611    
                                                              val/f1: 0.364     
                                                              val/precision:    
                                                              1.000 val/recall: 
                                                              0.222 val/mre:    
                                                              0.122 train/auc:  
                                                              0.895 train/f1:   
                                                              0.893             
                                                              train/precision:  
                                                              0.915             
                                                              train/recall:     
                                                              0.871 train/mre:  
                                                              0.126             
[2024-05-30 11:32:23,189][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/advice_yourself/0.05/4>
[2024-05-30 11:32:23,190][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 11:32:23,194][HYDRA] 	#15 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=advice_yourself data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=5
[rank: 0] Seed set to 5
[2024-05-30 11:32:23,482][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 11:32:23,484][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 11:32:23,486][train.py][INFO] - Instantiating callbacks...
[2024-05-30 11:32:23,486][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 11:32:23,490][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 11:32:23,491][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 11:32:23,491][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 11:32:23,492][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 11:32:23,494][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 11:32:23,495][train.py][INFO] - Instantiating loggers...
[2024-05-30 11:32:23,495][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 11:32:23,497][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 11:32:23,538][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/advice_yourself/0.05/5/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.64it/s v_num: 0.000      
                                                              val/auc: 0.572    
                                                              val/f1: 0.500     
                                                              val/precision:    
                                                              0.571 val/recall: 
                                                              0.444 val/mre:    
                                                              0.119 train/auc:  
                                                              0.944 train/f1:   
                                                              0.944             
                                                              train/precision:  
                                                              0.937             
                                                              train/recall:     
                                                              0.952 train/mre:  
                                                              0.121             
[2024-05-30 11:34:07,333][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/advice_yourself/0.05/5>
[2024-05-30 11:34:07,334][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 11:34:07,336][HYDRA] 	#16 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=advice_yourself data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=6
[rank: 0] Seed set to 6
[2024-05-30 11:34:07,627][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 11:34:07,630][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 11:34:07,632][train.py][INFO] - Instantiating callbacks...
[2024-05-30 11:34:07,632][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 11:34:07,636][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 11:34:07,636][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 11:34:07,637][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 11:34:07,638][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 11:34:07,640][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 11:34:07,642][train.py][INFO] - Instantiating loggers...
[2024-05-30 11:34:07,643][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 11:34:07,645][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 11:34:07,693][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/advice_yourself/0.05/6/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.61it/s v_num: 0.000      
                                                              val/auc: 0.411    
                                                              val/f1: 0.267     
                                                              val/precision:    
                                                              0.333 val/recall: 
                                                              0.222 val/mre:    
                                                              0.114 train/auc:  
                                                              0.879 train/f1:   
                                                              0.874             
                                                              train/precision:  
                                                              0.912             
                                                              train/recall:     
                                                              0.839 train/mre:  
                                                              0.118             
[2024-05-30 11:35:51,139][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/advice_yourself/0.05/6>
[2024-05-30 11:35:51,140][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 11:35:51,143][HYDRA] 	#17 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=advice_yourself data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=7
[rank: 0] Seed set to 7
[2024-05-30 11:35:51,429][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 11:35:51,431][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 11:35:51,433][train.py][INFO] - Instantiating callbacks...
[2024-05-30 11:35:51,434][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 11:35:51,437][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 11:35:51,438][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 11:35:51,439][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 11:35:51,439][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 11:35:51,441][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 11:35:51,442][train.py][INFO] - Instantiating loggers...
[2024-05-30 11:35:51,442][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 11:35:51,445][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 11:35:51,487][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/advice_yourself/0.05/7/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.61it/s v_num: 0.000      
                                                              val/auc: 0.628    
                                                              val/f1: 0.588     
                                                              val/precision:    
                                                              0.625 val/recall: 
                                                              0.556 val/mre:    
                                                              0.113 train/auc:  
                                                              0.863 train/f1:   
                                                              0.872             
                                                              train/precision:  
                                                              0.817             
                                                              train/recall:     
                                                              0.935 train/mre:  
                                                              0.119             
[2024-05-30 11:37:35,111][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/advice_yourself/0.05/7>
[2024-05-30 11:37:35,112][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 11:37:35,116][HYDRA] 	#18 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=advice_yourself data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=8
[rank: 0] Seed set to 8
[2024-05-30 11:37:35,405][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 11:37:35,407][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 11:37:35,409][train.py][INFO] - Instantiating callbacks...
[2024-05-30 11:37:35,410][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 11:37:35,413][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 11:37:35,414][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 11:37:35,415][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 11:37:35,415][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 11:37:35,417][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 11:37:35,418][train.py][INFO] - Instantiating loggers...
[2024-05-30 11:37:35,418][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 11:37:35,421][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 11:37:35,465][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/advice_yourself/0.05/8/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.56it/s v_num: 0.000      
                                                              val/auc: 0.672    
                                                              val/f1: 0.571     
                                                              val/precision:    
                                                              0.800 val/recall: 
                                                              0.444 val/mre:    
                                                              0.120 train/auc:  
                                                              0.935 train/f1:   
                                                              0.935             
                                                              train/precision:  
                                                              0.935             
                                                              train/recall:     
                                                              0.935 train/mre:  
                                                              0.127             
[2024-05-30 11:39:18,850][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/advice_yourself/0.05/8>
[2024-05-30 11:39:18,851][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 11:39:18,854][HYDRA] 	#19 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=advice_yourself data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=9
[rank: 0] Seed set to 9
[2024-05-30 11:39:19,148][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 11:39:19,151][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 11:39:19,153][train.py][INFO] - Instantiating callbacks...
[2024-05-30 11:39:19,153][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 11:39:19,157][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 11:39:19,158][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 11:39:19,158][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 11:39:19,159][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 11:39:19,161][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 11:39:19,163][train.py][INFO] - Instantiating loggers...
[2024-05-30 11:39:19,163][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 11:39:19,165][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 11:39:19,249][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/advice_yourself/0.05/9/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.60it/s v_num: 0.000      
                                                              val/auc: 0.456    
                                                              val/f1: 0.167     
                                                              val/precision:    
                                                              0.333 val/recall: 
                                                              0.111 val/mre:    
                                                              0.116 train/auc:  
                                                              0.863 train/f1:   
                                                              0.870             
                                                              train/precision:  
                                                              0.826             
                                                              train/recall:     
                                                              0.919 train/mre:  
                                                              0.120             
[2024-05-30 11:41:03,199][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/advice_yourself/0.05/9>
[2024-05-30 11:41:03,199][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 11:41:03,202][HYDRA] 	#20 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=advice_yourself data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=0
[2024-05-30 11:41:03,499][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 11:41:03,501][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 11:41:03,504][train.py][INFO] - Instantiating callbacks...
[2024-05-30 11:41:03,504][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 11:41:03,507][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 11:41:03,508][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 11:41:03,509][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 11:41:03,510][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 11:41:03,511][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 11:41:03,514][train.py][INFO] - Instantiating loggers...
[2024-05-30 11:41:03,514][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 11:41:03,516][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 11:41:03,562][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/advice_yourself/0.1/0/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.56it/s v_num: 0.000      
                                                              val/auc: 0.678    
                                                              val/f1: 0.625     
                                                              val/precision:    
                                                              0.714 val/recall: 
                                                              0.556 val/mre:    
                                                              0.128 train/auc:  
                                                              0.879 train/f1:   
                                                              0.885             
                                                              train/precision:  
                                                              0.841             
                                                              train/recall:     
                                                              0.935 train/mre:  
                                                              0.132             
[2024-05-30 11:42:47,756][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/advice_yourself/0.1/0>
[2024-05-30 11:42:47,757][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 11:42:47,759][HYDRA] 	#21 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=advice_yourself data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=1
[rank: 0] Seed set to 1
[2024-05-30 11:42:48,073][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 11:42:48,075][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 11:42:48,077][train.py][INFO] - Instantiating callbacks...
[2024-05-30 11:42:48,078][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 11:42:48,081][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 11:42:48,082][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 11:42:48,082][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 11:42:48,083][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 11:42:48,085][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 11:42:48,087][train.py][INFO] - Instantiating loggers...
[2024-05-30 11:42:48,087][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 11:42:48,089][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 11:42:48,174][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/advice_yourself/0.1/1/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.59it/s v_num: 0.000      
                                                              val/auc: 0.517    
                                                              val/f1: 0.400     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.333 val/mre:    
                                                              0.123 train/auc:  
                                                              0.855 train/f1:   
                                                              0.859             
                                                              train/precision:  
                                                              0.833             
                                                              train/recall:     
                                                              0.887 train/mre:  
                                                              0.127             
[2024-05-30 11:44:32,322][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/advice_yourself/0.1/1>
[2024-05-30 11:44:32,323][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 11:44:32,326][HYDRA] 	#22 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=advice_yourself data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=2
[rank: 0] Seed set to 2
[2024-05-30 11:44:32,608][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 11:44:32,610][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 11:44:32,612][train.py][INFO] - Instantiating callbacks...
[2024-05-30 11:44:32,612][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 11:44:32,616][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 11:44:32,616][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 11:44:32,617][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 11:44:32,618][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 11:44:32,620][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 11:44:32,621][train.py][INFO] - Instantiating loggers...
[2024-05-30 11:44:32,621][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 11:44:32,623][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 11:44:32,665][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/advice_yourself/0.1/2/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.64it/s v_num: 0.000      
                                                              val/auc: 0.667    
                                                              val/f1: 0.500     
                                                              val/precision:    
                                                              1.000 val/recall: 
                                                              0.333 val/mre:    
                                                              0.124 train/auc:  
                                                              0.919 train/f1:   
                                                              0.922             
                                                              train/precision:  
                                                              0.894             
                                                              train/recall:     
                                                              0.952 train/mre:  
                                                              0.130             
[2024-05-30 11:46:16,612][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/advice_yourself/0.1/2>
[2024-05-30 11:46:16,613][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 11:46:16,616][HYDRA] 	#23 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=advice_yourself data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=3
[rank: 0] Seed set to 3
[2024-05-30 11:46:16,898][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 11:46:16,900][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 11:46:16,902][train.py][INFO] - Instantiating callbacks...
[2024-05-30 11:46:16,903][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 11:46:16,906][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 11:46:16,907][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 11:46:16,908][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 11:46:16,908][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 11:46:16,910][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 11:46:16,911][train.py][INFO] - Instantiating loggers...
[2024-05-30 11:46:16,911][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 11:46:16,913][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 11:46:16,955][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/advice_yourself/0.1/3/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.60it/s v_num: 0.000      
                                                              val/auc: 0.522    
                                                              val/f1: 0.471     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.444 val/mre:    
                                                              0.119 train/auc:  
                                                              0.855 train/f1:   
                                                              0.852             
                                                              train/precision:  
                                                              0.867             
                                                              train/recall:     
                                                              0.839 train/mre:  
                                                              0.125             
[2024-05-30 11:48:00,509][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/advice_yourself/0.1/3>
[2024-05-30 11:48:00,510][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 11:48:00,514][HYDRA] 	#24 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=advice_yourself data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=4
[rank: 0] Seed set to 4
[2024-05-30 11:48:00,797][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 11:48:00,799][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 11:48:00,801][train.py][INFO] - Instantiating callbacks...
[2024-05-30 11:48:00,802][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 11:48:00,805][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 11:48:00,806][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 11:48:00,807][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 11:48:00,807][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 11:48:00,809][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 11:48:00,810][train.py][INFO] - Instantiating loggers...
[2024-05-30 11:48:00,810][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 11:48:00,813][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 11:48:00,855][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/advice_yourself/0.1/4/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.59it/s v_num: 0.000      
                                                              val/auc: 0.622    
                                                              val/f1: 0.533     
                                                              val/precision:    
                                                              0.667 val/recall: 
                                                              0.444 val/mre:    
                                                              0.125 train/auc:  
                                                              0.879 train/f1:   
                                                              0.889             
                                                              train/precision:  
                                                              0.822             
                                                              train/recall:     
                                                              0.968 train/mre:  
                                                              0.131             
[2024-05-30 11:49:44,488][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/advice_yourself/0.1/4>
[2024-05-30 11:49:44,489][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 11:49:44,492][HYDRA] 	#25 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=advice_yourself data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=5
[rank: 0] Seed set to 5
[2024-05-30 11:49:44,775][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 11:49:44,777][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 11:49:44,779][train.py][INFO] - Instantiating callbacks...
[2024-05-30 11:49:44,779][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 11:49:44,783][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 11:49:44,783][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 11:49:44,784][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 11:49:44,785][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 11:49:44,787][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 11:49:44,788][train.py][INFO] - Instantiating loggers...
[2024-05-30 11:49:44,788][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 11:49:44,790][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 11:49:44,832][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/advice_yourself/0.1/5/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.60it/s v_num: 0.000      
                                                              val/auc: 0.683    
                                                              val/f1: 0.667     
                                                              val/precision:    
                                                              0.667 val/recall: 
                                                              0.667 val/mre:    
                                                              0.123 train/auc:  
                                                              0.895 train/f1:   
                                                              0.898             
                                                              train/precision:  
                                                              0.877             
                                                              train/recall:     
                                                              0.919 train/mre:  
                                                              0.128             
[2024-05-30 11:51:28,672][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/advice_yourself/0.1/5>
[2024-05-30 11:51:28,672][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 11:51:28,675][HYDRA] 	#26 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=advice_yourself data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=6
[rank: 0] Seed set to 6
[2024-05-30 11:51:28,963][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 11:51:28,966][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 11:51:28,968][train.py][INFO] - Instantiating callbacks...
[2024-05-30 11:51:28,968][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 11:51:28,972][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 11:51:28,972][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 11:51:28,973][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 11:51:28,974][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 11:51:28,976][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 11:51:28,977][train.py][INFO] - Instantiating loggers...
[2024-05-30 11:51:28,978][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 11:51:28,980][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 11:51:29,063][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/advice_yourself/0.1/6/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.62it/s v_num: 0.000      
                                                              val/auc: 0.472    
                                                              val/f1: 0.444     
                                                              val/precision:    
                                                              0.444 val/recall: 
                                                              0.444 val/mre:    
                                                              0.123 train/auc:  
                                                              0.887 train/f1:   
                                                              0.885             
                                                              train/precision:  
                                                              0.900             
                                                              train/recall:     
                                                              0.871 train/mre:  
                                                              0.126             
[2024-05-30 11:53:12,471][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/advice_yourself/0.1/6>
[2024-05-30 11:53:12,472][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 11:53:12,476][HYDRA] 	#27 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=advice_yourself data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=7
[rank: 0] Seed set to 7
[2024-05-30 11:53:12,762][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 11:53:12,765][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 11:53:12,767][train.py][INFO] - Instantiating callbacks...
[2024-05-30 11:53:12,767][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 11:53:12,770][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 11:53:12,771][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 11:53:12,772][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 11:53:12,773][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 11:53:12,774][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 11:53:12,777][train.py][INFO] - Instantiating loggers...
[2024-05-30 11:53:12,777][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 11:53:12,780][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 11:53:12,825][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/advice_yourself/0.1/7/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.61it/s v_num: 0.000      
                                                              val/auc: 0.628    
                                                              val/f1: 0.588     
                                                              val/precision:    
                                                              0.625 val/recall: 
                                                              0.556 val/mre:    
                                                              0.119 train/auc:  
                                                              0.863 train/f1:   
                                                              0.868             
                                                              train/precision:  
                                                              0.836             
                                                              train/recall:     
                                                              0.903 train/mre:  
                                                              0.124             
[2024-05-30 11:54:56,138][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/advice_yourself/0.1/7>
[2024-05-30 11:54:56,139][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 11:54:56,142][HYDRA] 	#28 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=advice_yourself data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=8
[rank: 0] Seed set to 8
[2024-05-30 11:54:56,433][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 11:54:56,436][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 11:54:56,438][train.py][INFO] - Instantiating callbacks...
[2024-05-30 11:54:56,438][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 11:54:56,441][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 11:54:56,442][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 11:54:56,443][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 11:54:56,444][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 11:54:56,446][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 11:54:56,447][train.py][INFO] - Instantiating loggers...
[2024-05-30 11:54:56,448][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 11:54:56,450][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 11:54:56,536][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/advice_yourself/0.1/8/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.64it/s v_num: 0.000      
                                                              val/auc: 0.572    
                                                              val/f1: 0.500     
                                                              val/precision:    
                                                              0.571 val/recall: 
                                                              0.444 val/mre:    
                                                              0.124 train/auc:  
                                                              0.919 train/f1:   
                                                              0.918             
                                                              train/precision:  
                                                              0.933             
                                                              train/recall:     
                                                              0.903 train/mre:  
                                                              0.130             
[2024-05-30 11:56:39,785][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/advice_yourself/0.1/8>
[2024-05-30 11:56:39,786][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 11:56:39,789][HYDRA] 	#29 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=advice_yourself data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=9
[rank: 0] Seed set to 9
[2024-05-30 11:56:40,075][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 11:56:40,077][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 11:56:40,079][train.py][INFO] - Instantiating callbacks...
[2024-05-30 11:56:40,079][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 11:56:40,083][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 11:56:40,083][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 11:56:40,084][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 11:56:40,085][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 11:56:40,087][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 11:56:40,090][train.py][INFO] - Instantiating loggers...
[2024-05-30 11:56:40,090][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 11:56:40,092][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 11:56:40,137][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/advice_yourself/0.1/9/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.61it/s v_num: 0.000      
                                                              val/auc: 0.567    
                                                              val/f1: 0.429     
                                                              val/precision:    
                                                              0.600 val/recall: 
                                                              0.333 val/mre:    
                                                              0.120 train/auc:  
                                                              0.839 train/f1:   
                                                              0.851             
                                                              train/precision:  
                                                              0.792             
                                                              train/recall:     
                                                              0.919 train/mre:  
                                                              0.125             
[2024-05-30 11:58:24,050][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/advice_yourself/0.1/9>
[2024-05-30 11:58:24,051][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 11:58:24,054][HYDRA] 	#30 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=advice_yourself data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=0
[2024-05-30 11:58:24,346][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 11:58:24,348][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 11:58:24,350][train.py][INFO] - Instantiating callbacks...
[2024-05-30 11:58:24,351][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 11:58:24,354][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 11:58:24,355][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 11:58:24,356][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 11:58:24,356][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 11:58:24,358][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 11:58:24,360][train.py][INFO] - Instantiating loggers...
[2024-05-30 11:58:24,360][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 11:58:24,362][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 11:58:24,444][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/advice_yourself/0.15/0/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.58it/s v_num: 0.000      
                                                              val/auc: 0.622    
                                                              val/f1: 0.533     
                                                              val/precision:    
                                                              0.667 val/recall: 
                                                              0.444 val/mre:    
                                                              0.130 train/auc:  
                                                              0.919 train/f1:   
                                                              0.921             
                                                              train/precision:  
                                                              0.906             
                                                              train/recall:     
                                                              0.935 train/mre:  
                                                              0.135             
[2024-05-30 12:00:08,300][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/advice_yourself/0.15/0>
[2024-05-30 12:00:08,301][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 12:00:08,304][HYDRA] 	#31 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=advice_yourself data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=1
[rank: 0] Seed set to 1
[2024-05-30 12:00:08,590][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 12:00:08,593][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 12:00:08,595][train.py][INFO] - Instantiating callbacks...
[2024-05-30 12:00:08,595][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 12:00:08,598][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 12:00:08,599][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 12:00:08,600][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 12:00:08,601][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 12:00:08,602][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 12:00:08,606][train.py][INFO] - Instantiating loggers...
[2024-05-30 12:00:08,606][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 12:00:08,608][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 12:00:08,655][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/advice_yourself/0.15/1/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.59it/s v_num: 0.000      
                                                              val/auc: 0.517    
                                                              val/f1: 0.400     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.333 val/mre:    
                                                              0.125 train/auc:  
                                                              0.863 train/f1:   
                                                              0.860             
                                                              train/precision:  
                                                              0.881             
                                                              train/recall:     
                                                              0.839 train/mre:  
                                                              0.131             
[2024-05-30 12:01:52,035][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/advice_yourself/0.15/1>
[2024-05-30 12:01:52,036][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 12:01:52,039][HYDRA] 	#32 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=advice_yourself data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=2
[rank: 0] Seed set to 2
[2024-05-30 12:01:52,326][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 12:01:52,328][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 12:01:52,330][train.py][INFO] - Instantiating callbacks...
[2024-05-30 12:01:52,331][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 12:01:52,334][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 12:01:52,335][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 12:01:52,336][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 12:01:52,336][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 12:01:52,338][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 12:01:52,340][train.py][INFO] - Instantiating loggers...
[2024-05-30 12:01:52,340][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 12:01:52,342][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 12:01:52,425][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/advice_yourself/0.15/2/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.61it/s v_num: 0.000      
                                                              val/auc: 0.672    
                                                              val/f1: 0.571     
                                                              val/precision:    
                                                              0.800 val/recall: 
                                                              0.444 val/mre:    
                                                              0.126 train/auc:  
                                                              0.879 train/f1:   
                                                              0.885             
                                                              train/precision:  
                                                              0.841             
                                                              train/recall:     
                                                              0.935 train/mre:  
                                                              0.132             
[2024-05-30 12:03:36,613][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/advice_yourself/0.15/2>
[2024-05-30 12:03:36,614][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 12:03:36,617][HYDRA] 	#33 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=advice_yourself data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=3
[rank: 0] Seed set to 3
[2024-05-30 12:03:36,913][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 12:03:36,916][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 12:03:36,918][train.py][INFO] - Instantiating callbacks...
[2024-05-30 12:03:36,918][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 12:03:36,922][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 12:03:36,922][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 12:03:36,923][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 12:03:36,924][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 12:03:36,926][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 12:03:36,928][train.py][INFO] - Instantiating loggers...
[2024-05-30 12:03:36,928][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 12:03:36,930][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 12:03:36,977][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/advice_yourself/0.15/3/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.56it/s v_num: 0.000      
                                                              val/auc: 0.356    
                                                              val/f1: 0.143     
                                                              val/precision:    
                                                              0.200 val/recall: 
                                                              0.111 val/mre:    
                                                              0.124 train/auc:  
                                                              0.871 train/f1:   
                                                              0.867             
                                                              train/precision:  
                                                              0.897             
                                                              train/recall:     
                                                              0.839 train/mre:  
                                                              0.132             
[2024-05-30 12:05:20,542][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/advice_yourself/0.15/3>
[2024-05-30 12:05:20,543][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 12:05:20,546][HYDRA] 	#34 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=advice_yourself data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=4
[rank: 0] Seed set to 4
[2024-05-30 12:05:20,837][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 12:05:20,839][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 12:05:20,841][train.py][INFO] - Instantiating callbacks...
[2024-05-30 12:05:20,842][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 12:05:20,845][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 12:05:20,846][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 12:05:20,847][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 12:05:20,847][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 12:05:20,849][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 12:05:20,851][train.py][INFO] - Instantiating loggers...
[2024-05-30 12:05:20,851][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 12:05:20,855][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 12:05:20,936][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/advice_yourself/0.15/4/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.56it/s v_num: 0.000      
                                                              val/auc: 0.517    
                                                              val/f1: 0.400     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.333 val/mre:    
                                                              0.131 train/auc:  
                                                              0.903 train/f1:   
                                                              0.905             
                                                              train/precision:  
                                                              0.891             
                                                              train/recall:     
                                                              0.919 train/mre:  
                                                              0.137             
[2024-05-30 12:07:05,261][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/advice_yourself/0.15/4>
[2024-05-30 12:07:05,261][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 12:07:05,265][HYDRA] 	#35 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=advice_yourself data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=5
[rank: 0] Seed set to 5
[2024-05-30 12:07:05,553][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 12:07:05,556][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 12:07:05,558][train.py][INFO] - Instantiating callbacks...
[2024-05-30 12:07:05,558][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 12:07:05,562][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 12:07:05,562][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 12:07:05,563][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 12:07:05,564][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 12:07:05,566][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 12:07:05,567][train.py][INFO] - Instantiating loggers...
[2024-05-30 12:07:05,567][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 12:07:05,569][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 12:07:05,612][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/advice_yourself/0.15/5/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.57it/s v_num: 0.000      
                                                              val/auc: 0.517    
                                                              val/f1: 0.400     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.333 val/mre:    
                                                              0.128 train/auc:  
                                                              0.944 train/f1:   
                                                              0.942             
                                                              train/precision:  
                                                              0.966             
                                                              train/recall:     
                                                              0.919 train/mre:  
                                                              0.134             
[2024-05-30 12:08:49,137][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/advice_yourself/0.15/5>
[2024-05-30 12:08:49,138][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 12:08:49,141][HYDRA] 	#36 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=advice_yourself data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=6
[rank: 0] Seed set to 6
[2024-05-30 12:08:49,430][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 12:08:49,432][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 12:08:49,435][train.py][INFO] - Instantiating callbacks...
[2024-05-30 12:08:49,435][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 12:08:49,438][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 12:08:49,439][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 12:08:49,440][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 12:08:49,441][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 12:08:49,442][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 12:08:49,443][train.py][INFO] - Instantiating loggers...
[2024-05-30 12:08:49,444][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 12:08:49,447][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 12:08:49,493][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/advice_yourself/0.15/6/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.56it/s v_num: 0.000      
                                                              val/auc: 0.422    
                                                              val/f1: 0.421     
                                                              val/precision:    
                                                              0.400 val/recall: 
                                                              0.444 val/mre:    
                                                              0.126 train/auc:  
                                                              0.903 train/f1:   
                                                              0.902             
                                                              train/precision:  
                                                              0.917             
                                                              train/recall:     
                                                              0.887 train/mre:  
                                                              0.131             
[2024-05-30 12:10:33,374][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/advice_yourself/0.15/6>
[2024-05-30 12:10:33,375][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 12:10:33,380][HYDRA] 	#37 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=advice_yourself data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=7
[rank: 0] Seed set to 7
[2024-05-30 12:10:33,721][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 12:10:33,723][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 12:10:33,725][train.py][INFO] - Instantiating callbacks...
[2024-05-30 12:10:33,726][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 12:10:33,730][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 12:10:33,731][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 12:10:33,731][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 12:10:33,732][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 12:10:33,734][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 12:10:33,736][train.py][INFO] - Instantiating loggers...
[2024-05-30 12:10:33,736][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 12:10:33,739][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 12:10:33,916][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/advice_yourself/0.15/7/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.55it/s v_num: 0.000      
                                                              val/auc: 0.683    
                                                              val/f1: 0.667     
                                                              val/precision:    
                                                              0.667 val/recall: 
                                                              0.667 val/mre:    
                                                              0.122 train/auc:  
                                                              0.839 train/f1:   
                                                              0.853             
                                                              train/precision:  
                                                              0.784             
                                                              train/recall:     
                                                              0.935 train/mre:  
                                                              0.129             
[2024-05-30 12:12:17,871][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/advice_yourself/0.15/7>
[2024-05-30 12:12:17,872][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 12:12:17,875][HYDRA] 	#38 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=advice_yourself data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=8
[rank: 0] Seed set to 8
[2024-05-30 12:12:18,169][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 12:12:18,172][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 12:12:18,175][train.py][INFO] - Instantiating callbacks...
[2024-05-30 12:12:18,175][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 12:12:18,179][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 12:12:18,180][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 12:12:18,181][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 12:12:18,181][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 12:12:18,183][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 12:12:18,186][train.py][INFO] - Instantiating loggers...
[2024-05-30 12:12:18,186][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 12:12:18,188][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 12:12:18,236][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/advice_yourself/0.15/8/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.57it/s v_num: 0.000      
                                                              val/auc: 0.622    
                                                              val/f1: 0.533     
                                                              val/precision:    
                                                              0.667 val/recall: 
                                                              0.444 val/mre:    
                                                              0.128 train/auc:  
                                                              0.919 train/f1:   
                                                              0.918             
                                                              train/precision:  
                                                              0.933             
                                                              train/recall:     
                                                              0.903 train/mre:  
                                                              0.135             
[2024-05-30 12:14:01,654][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/advice_yourself/0.15/8>
[2024-05-30 12:14:01,655][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 12:14:01,658][HYDRA] 	#39 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=advice_yourself data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=9
[rank: 0] Seed set to 9
[2024-05-30 12:14:01,946][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 12:14:01,949][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 12:14:01,951][train.py][INFO] - Instantiating callbacks...
[2024-05-30 12:14:01,951][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 12:14:01,955][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 12:14:01,955][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 12:14:01,956][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 12:14:01,957][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 12:14:01,959][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 12:14:01,961][train.py][INFO] - Instantiating loggers...
[2024-05-30 12:14:01,961][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 12:14:01,965][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 12:14:02,046][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/advice_yourself/0.15/9/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.60it/s v_num: 0.000      
                                                              val/auc: 0.511    
                                                              val/f1: 0.308     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.222 val/mre:    
                                                              0.124 train/auc:  
                                                              0.831 train/f1:   
                                                              0.829             
                                                              train/precision:  
                                                              0.836             
                                                              train/recall:     
                                                              0.823 train/mre:  
                                                              0.129             
[2024-05-30 12:15:46,503][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/advice_yourself/0.15/9>
[2024-05-30 12:15:46,504][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 12:15:46,507][HYDRA] 	#40 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=advice_yourself data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=0
[2024-05-30 12:15:46,793][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 12:15:46,795][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 12:15:46,797][train.py][INFO] - Instantiating callbacks...
[2024-05-30 12:15:46,798][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 12:15:46,801][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 12:15:46,802][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 12:15:46,803][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 12:15:46,804][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 12:15:46,805][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 12:15:46,806][train.py][INFO] - Instantiating loggers...
[2024-05-30 12:15:46,806][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 12:15:46,808][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 12:15:46,850][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/advice_yourself/0.2/0/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.62it/s v_num: 0.000      
                                                              val/auc: 0.844    
                                                              val/f1: 0.842     
                                                              val/precision:    
                                                              0.800 val/recall: 
                                                              0.889 val/mre:    
                                                              0.133 train/auc:  
                                                              0.855 train/f1:   
                                                              0.866             
                                                              train/precision:  
                                                              0.806             
                                                              train/recall:     
                                                              0.935 train/mre:  
                                                              0.138             
[2024-05-30 12:17:30,451][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/advice_yourself/0.2/0>
[2024-05-30 12:17:30,452][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 12:17:30,455][HYDRA] 	#41 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=advice_yourself data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=1
[rank: 0] Seed set to 1
[2024-05-30 12:17:30,740][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 12:17:30,743][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 12:17:30,745][train.py][INFO] - Instantiating callbacks...
[2024-05-30 12:17:30,745][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 12:17:30,748][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 12:17:30,749][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 12:17:30,750][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 12:17:30,751][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 12:17:30,752][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 12:17:30,753][train.py][INFO] - Instantiating loggers...
[2024-05-30 12:17:30,754][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 12:17:30,756][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 12:17:30,798][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/advice_yourself/0.2/1/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.60it/s v_num: 0.000      
                                                              val/auc: 0.628    
                                                              val/f1: 0.588     
                                                              val/precision:    
                                                              0.625 val/recall: 
                                                              0.556 val/mre:    
                                                              0.126 train/auc:  
                                                              0.823 train/f1:   
                                                              0.820             
                                                              train/precision:  
                                                              0.833             
                                                              train/recall:     
                                                              0.806 train/mre:  
                                                              0.133             
[2024-05-30 12:19:14,808][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/advice_yourself/0.2/1>
[2024-05-30 12:19:14,809][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 12:19:14,812][HYDRA] 	#42 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=advice_yourself data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=2
[rank: 0] Seed set to 2
[2024-05-30 12:19:15,096][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 12:19:15,099][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 12:19:15,101][train.py][INFO] - Instantiating callbacks...
[2024-05-30 12:19:15,101][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 12:19:15,104][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 12:19:15,105][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 12:19:15,108][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 12:19:15,108][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 12:19:15,110][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 12:19:15,111][train.py][INFO] - Instantiating loggers...
[2024-05-30 12:19:15,111][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 12:19:15,113][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 12:19:15,156][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/advice_yourself/0.2/2/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.58it/s v_num: 0.000      
                                                              val/auc: 0.467    
                                                              val/f1: 0.375     
                                                              val/precision:    
                                                              0.429 val/recall: 
                                                              0.333 val/mre:    
                                                              0.133 train/auc:  
                                                              0.911 train/f1:   
                                                              0.916             
                                                              train/precision:  
                                                              0.870             
                                                              train/recall:     
                                                              0.968 train/mre:  
                                                              0.138             
[2024-05-30 12:20:58,453][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/advice_yourself/0.2/2>
[2024-05-30 12:20:58,454][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 12:20:58,457][HYDRA] 	#43 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=advice_yourself data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=3
[rank: 0] Seed set to 3
[2024-05-30 12:20:58,748][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 12:20:58,751][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 12:20:58,753][train.py][INFO] - Instantiating callbacks...
[2024-05-30 12:20:58,754][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 12:20:58,757][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 12:20:58,758][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 12:20:58,759][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 12:20:58,759][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 12:20:58,761][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 12:20:58,764][train.py][INFO] - Instantiating loggers...
[2024-05-30 12:20:58,764][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 12:20:58,767][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 12:20:58,815][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/advice_yourself/0.2/3/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.59it/s v_num: 0.000      
                                                              val/auc: 0.461    
                                                              val/f1: 0.286     
                                                              val/precision:    
                                                              0.400 val/recall: 
                                                              0.222 val/mre:    
                                                              0.129 train/auc:  
                                                              0.887 train/f1:   
                                                              0.887             
                                                              train/precision:  
                                                              0.887             
                                                              train/recall:     
                                                              0.887 train/mre:  
                                                              0.136             
[2024-05-30 12:22:42,841][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/advice_yourself/0.2/3>
[2024-05-30 12:22:42,842][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 12:22:42,845][HYDRA] 	#44 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=advice_yourself data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=4
[rank: 0] Seed set to 4
[2024-05-30 12:22:43,134][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 12:22:43,136][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 12:22:43,138][train.py][INFO] - Instantiating callbacks...
[2024-05-30 12:22:43,139][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 12:22:43,142][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 12:22:43,143][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 12:22:43,144][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 12:22:43,144][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 12:22:43,146][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 12:22:43,147][train.py][INFO] - Instantiating loggers...
[2024-05-30 12:22:43,147][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 12:22:43,150][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 12:22:43,197][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/advice_yourself/0.2/4/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.61it/s v_num: 0.000      
                                                              val/auc: 0.622    
                                                              val/f1: 0.533     
                                                              val/precision:    
                                                              0.667 val/recall: 
                                                              0.444 val/mre:    
                                                              0.132 train/auc:  
                                                              0.887 train/f1:   
                                                              0.891             
                                                              train/precision:  
                                                              0.864             
                                                              train/recall:     
                                                              0.919 train/mre:  
                                                              0.139             
[2024-05-30 12:24:26,273][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/advice_yourself/0.2/4>
[2024-05-30 12:24:26,273][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 12:24:26,277][HYDRA] 	#45 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=advice_yourself data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=5
[rank: 0] Seed set to 5
[2024-05-30 12:24:26,562][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 12:24:26,565][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 12:24:26,567][train.py][INFO] - Instantiating callbacks...
[2024-05-30 12:24:26,567][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 12:24:26,571][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 12:24:26,571][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 12:24:26,572][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 12:24:26,573][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 12:24:26,575][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 12:24:26,576][train.py][INFO] - Instantiating loggers...
[2024-05-30 12:24:26,576][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 12:24:26,579][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 12:24:26,622][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/advice_yourself/0.2/5/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.61it/s v_num: 0.000      
                                                              val/auc: 0.572    
                                                              val/f1: 0.500     
                                                              val/precision:    
                                                              0.571 val/recall: 
                                                              0.444 val/mre:    
                                                              0.128 train/auc:  
                                                              0.911 train/f1:   
                                                              0.912             
                                                              train/precision:  
                                                              0.905             
                                                              train/recall:     
                                                              0.919 train/mre:  
                                                              0.136             
[2024-05-30 12:26:10,451][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/advice_yourself/0.2/5>
[2024-05-30 12:26:10,451][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 12:26:10,455][HYDRA] 	#46 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=advice_yourself data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=6
[rank: 0] Seed set to 6
[2024-05-30 12:26:10,743][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 12:26:10,745][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 12:26:10,747][train.py][INFO] - Instantiating callbacks...
[2024-05-30 12:26:10,747][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 12:26:10,751][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 12:26:10,752][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 12:26:10,752][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 12:26:10,753][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 12:26:10,755][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 12:26:10,757][train.py][INFO] - Instantiating loggers...
[2024-05-30 12:26:10,757][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 12:26:10,759][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 12:26:10,844][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/advice_yourself/0.2/6/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.13it/s v_num: 0.000      
                                                              val/auc: 0.522    
                                                              val/f1: 0.471     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.444 val/mre:    
                                                              0.131 train/auc:  
                                                              0.879 train/f1:   
                                                              0.865             
                                                              train/precision:  
                                                              0.980             
                                                              train/recall:     
                                                              0.774 train/mre:  
                                                              0.137             
[2024-05-30 12:27:54,190][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/advice_yourself/0.2/6>
[2024-05-30 12:27:54,190][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 12:27:54,193][HYDRA] 	#47 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=advice_yourself data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=7
[rank: 0] Seed set to 7
[2024-05-30 12:27:54,492][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 12:27:54,495][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 12:27:54,497][train.py][INFO] - Instantiating callbacks...
[2024-05-30 12:27:54,497][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 12:27:54,501][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 12:27:54,501][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 12:27:54,502][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 12:27:54,503][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 12:27:54,505][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 12:27:54,506][train.py][INFO] - Instantiating loggers...
[2024-05-30 12:27:54,506][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 12:27:54,508][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 12:27:54,552][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/advice_yourself/0.2/7/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.62it/s v_num: 0.000      
                                                              val/auc: 0.633    
                                                              val/f1: 0.632     
                                                              val/precision:    
                                                              0.600 val/recall: 
                                                              0.667 val/mre:    
                                                              0.129 train/auc:  
                                                              0.847 train/f1:   
                                                              0.863             
                                                              train/precision:  
                                                              0.779             
                                                              train/recall:     
                                                              0.968 train/mre:  
                                                              0.134             
[2024-05-30 12:29:37,912][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/advice_yourself/0.2/7>
[2024-05-30 12:29:37,913][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 12:29:37,916][HYDRA] 	#48 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=advice_yourself data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=8
[rank: 0] Seed set to 8
[2024-05-30 12:29:38,198][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 12:29:38,200][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 12:29:38,202][train.py][INFO] - Instantiating callbacks...
[2024-05-30 12:29:38,203][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 12:29:38,206][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 12:29:38,207][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 12:29:38,207][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 12:29:38,208][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 12:29:38,210][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 12:29:38,211][train.py][INFO] - Instantiating loggers...
[2024-05-30 12:29:38,211][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 12:29:38,213][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 12:29:38,261][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/advice_yourself/0.2/8/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.66it/s v_num: 0.000      
                                                              val/auc: 0.667    
                                                              val/f1: 0.500     
                                                              val/precision:    
                                                              1.000 val/recall: 
                                                              0.333 val/mre:    
                                                              0.133 train/auc:  
                                                              0.911 train/f1:   
                                                              0.911             
                                                              train/precision:  
                                                              0.918             
                                                              train/recall:     
                                                              0.903 train/mre:  
                                                              0.140             
[2024-05-30 12:31:22,119][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/advice_yourself/0.2/8>
[2024-05-30 12:31:22,120][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 12:31:22,123][HYDRA] 	#49 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=advice_yourself data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=9
[rank: 0] Seed set to 9
[2024-05-30 12:31:22,409][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 12:31:22,411][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 12:31:22,413][train.py][INFO] - Instantiating callbacks...
[2024-05-30 12:31:22,414][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 12:31:22,417][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 12:31:22,418][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 12:31:22,419][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 12:31:22,419][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 12:31:22,421][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 12:31:22,422][train.py][INFO] - Instantiating loggers...
[2024-05-30 12:31:22,422][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 12:31:22,424][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 12:31:22,467][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/advice_yourself/0.2/9/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.62it/s v_num: 0.000      
                                                              val/auc: 0.406    
                                                              val/f1: 0.154     
                                                              val/precision:    
                                                              0.250 val/recall: 
                                                              0.111 val/mre:    
                                                              0.131 train/auc:  
                                                              0.847 train/f1:   
                                                              0.850             
                                                              train/precision:  
                                                              0.831             
                                                              train/recall:     
                                                              0.871 train/mre:  
                                                              0.135             
[2024-05-30 12:33:06,016][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/advice_yourself/0.2/9>
[2024-05-30 12:33:06,017][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 12:33:06,020][HYDRA] 	#50 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=advice_yourself data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=0
[2024-05-30 12:33:06,314][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 12:33:06,317][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 12:33:06,319][train.py][INFO] - Instantiating callbacks...
[2024-05-30 12:33:06,319][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 12:33:06,323][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 12:33:06,323][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 12:33:06,324][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 12:33:06,325][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 12:33:06,327][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 12:33:06,329][train.py][INFO] - Instantiating loggers...
[2024-05-30 12:33:06,329][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 12:33:06,332][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 12:33:06,384][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/advice_yourself/0.25/0/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.56it/s v_num: 0.000      
                                                              val/auc: 0.517    
                                                              val/f1: 0.400     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.333 val/mre:    
                                                              0.137 train/auc:  
                                                              0.879 train/f1:   
                                                              0.884             
                                                              train/precision:  
                                                              0.851             
                                                              train/recall:     
                                                              0.919 train/mre:  
                                                              0.142             
[2024-05-30 12:34:50,832][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/advice_yourself/0.25/0>
[2024-05-30 12:34:50,833][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 12:34:50,837][HYDRA] 	#51 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=advice_yourself data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=1
[rank: 0] Seed set to 1
[2024-05-30 12:34:51,130][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 12:34:51,133][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 12:34:51,135][train.py][INFO] - Instantiating callbacks...
[2024-05-30 12:34:51,136][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 12:34:51,140][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 12:34:51,141][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 12:34:51,141][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 12:34:51,142][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 12:34:51,144][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 12:34:51,146][train.py][INFO] - Instantiating loggers...
[2024-05-30 12:34:51,146][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 12:34:51,148][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 12:34:51,233][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/advice_yourself/0.25/1/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.56it/s v_num: 0.000      
                                                              val/auc: 0.678    
                                                              val/f1: 0.625     
                                                              val/precision:    
                                                              0.714 val/recall: 
                                                              0.556 val/mre:    
                                                              0.130 train/auc:  
                                                              0.790 train/f1:   
                                                              0.787             
                                                              train/precision:  
                                                              0.800             
                                                              train/recall:     
                                                              0.774 train/mre:  
                                                              0.137             
[2024-05-30 12:36:34,731][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/advice_yourself/0.25/1>
[2024-05-30 12:36:34,732][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 12:36:34,735][HYDRA] 	#52 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=advice_yourself data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=2
[rank: 0] Seed set to 2
[2024-05-30 12:36:35,033][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 12:36:35,035][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 12:36:35,037][train.py][INFO] - Instantiating callbacks...
[2024-05-30 12:36:35,038][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 12:36:35,041][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 12:36:35,042][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 12:36:35,043][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 12:36:35,044][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 12:36:35,045][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 12:36:35,048][train.py][INFO] - Instantiating loggers...
[2024-05-30 12:36:35,048][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 12:36:35,050][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 12:36:35,098][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/advice_yourself/0.25/2/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.57it/s v_num: 0.000      
                                                              val/auc: 0.572    
                                                              val/f1: 0.500     
                                                              val/precision:    
                                                              0.571 val/recall: 
                                                              0.444 val/mre:    
                                                              0.138 train/auc:  
                                                              0.879 train/f1:   
                                                              0.885             
                                                              train/precision:  
                                                              0.841             
                                                              train/recall:     
                                                              0.935 train/mre:  
                                                              0.145             
[2024-05-30 12:38:18,849][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/advice_yourself/0.25/2>
[2024-05-30 12:38:18,850][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 12:38:18,853][HYDRA] 	#53 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=advice_yourself data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=3
[rank: 0] Seed set to 3
[2024-05-30 12:38:19,143][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 12:38:19,145][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 12:38:19,148][train.py][INFO] - Instantiating callbacks...
[2024-05-30 12:38:19,148][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 12:38:19,151][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 12:38:19,152][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 12:38:19,153][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 12:38:19,154][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 12:38:19,155][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 12:38:19,156][train.py][INFO] - Instantiating loggers...
[2024-05-30 12:38:19,157][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 12:38:19,159][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 12:38:19,203][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/advice_yourself/0.25/3/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.60it/s v_num: 0.000      
                                                              val/auc: 0.356    
                                                              val/f1: 0.143     
                                                              val/precision:    
                                                              0.200 val/recall: 
                                                              0.111 val/mre:    
                                                              0.134 train/auc:  
                                                              0.887 train/f1:   
                                                              0.885             
                                                              train/precision:  
                                                              0.900             
                                                              train/recall:     
                                                              0.871 train/mre:  
                                                              0.141             
[2024-05-30 12:40:02,623][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/advice_yourself/0.25/3>
[2024-05-30 12:40:02,623][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 12:40:02,626][HYDRA] 	#54 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=advice_yourself data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=4
[rank: 0] Seed set to 4
[2024-05-30 12:40:02,922][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 12:40:02,925][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 12:40:02,927][train.py][INFO] - Instantiating callbacks...
[2024-05-30 12:40:02,927][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 12:40:02,930][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 12:40:02,931][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 12:40:02,932][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 12:40:02,933][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 12:40:02,934][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 12:40:02,937][train.py][INFO] - Instantiating loggers...
[2024-05-30 12:40:02,937][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 12:40:02,940][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 12:40:02,987][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/advice_yourself/0.25/4/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.64it/s v_num: 0.000      
                                                              val/auc: 0.567    
                                                              val/f1: 0.429     
                                                              val/precision:    
                                                              0.600 val/recall: 
                                                              0.333 val/mre:    
                                                              0.133 train/auc:  
                                                              0.887 train/f1:   
                                                              0.894             
                                                              train/precision:  
                                                              0.843             
                                                              train/recall:     
                                                              0.952 train/mre:  
                                                              0.140             
[2024-05-30 12:41:47,146][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/advice_yourself/0.25/4>
[2024-05-30 12:41:47,146][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 12:41:47,149][HYDRA] 	#55 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=advice_yourself data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=5
[rank: 0] Seed set to 5
[2024-05-30 12:41:47,439][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 12:41:47,441][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 12:41:47,443][train.py][INFO] - Instantiating callbacks...
[2024-05-30 12:41:47,444][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 12:41:47,447][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 12:41:47,448][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 12:41:47,449][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 12:41:47,449][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 12:41:47,451][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 12:41:47,452][train.py][INFO] - Instantiating loggers...
[2024-05-30 12:41:47,452][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 12:41:47,454][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 12:41:47,496][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/advice_yourself/0.25/5/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.56it/s v_num: 0.000      
                                                              val/auc: 0.567    
                                                              val/f1: 0.429     
                                                              val/precision:    
                                                              0.600 val/recall: 
                                                              0.333 val/mre:    
                                                              0.129 train/auc:  
                                                              0.903 train/f1:   
                                                              0.902             
                                                              train/precision:  
                                                              0.917             
                                                              train/recall:     
                                                              0.887 train/mre:  
                                                              0.137             
[2024-05-30 12:43:30,957][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/advice_yourself/0.25/5>
[2024-05-30 12:43:30,958][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 12:43:30,962][HYDRA] 	#56 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=advice_yourself data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=6
[rank: 0] Seed set to 6
[2024-05-30 12:43:31,263][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 12:43:31,266][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 12:43:31,269][train.py][INFO] - Instantiating callbacks...
[2024-05-30 12:43:31,269][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 12:43:31,274][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 12:43:31,275][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 12:43:31,276][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 12:43:31,277][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 12:43:31,278][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 12:43:31,283][train.py][INFO] - Instantiating loggers...
[2024-05-30 12:43:31,283][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 12:43:31,285][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 12:43:31,384][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/advice_yourself/0.25/6/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.60it/s v_num: 0.000      
                                                              val/auc: 0.478    
                                                              val/f1: 0.500     
                                                              val/precision:    
                                                              0.455 val/recall: 
                                                              0.556 val/mre:    
                                                              0.136 train/auc:  
                                                              0.895 train/f1:   
                                                              0.883             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              0.790 train/mre:  
                                                              0.143             
[2024-05-30 12:45:14,990][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/advice_yourself/0.25/6>
[2024-05-30 12:45:14,991][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 12:45:14,995][HYDRA] 	#57 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=advice_yourself data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=7
[rank: 0] Seed set to 7
[2024-05-30 12:45:15,279][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 12:45:15,281][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 12:45:15,283][train.py][INFO] - Instantiating callbacks...
[2024-05-30 12:45:15,283][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 12:45:15,287][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 12:45:15,287][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 12:45:15,288][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 12:45:15,289][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 12:45:15,291][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 12:45:15,292][train.py][INFO] - Instantiating loggers...
[2024-05-30 12:45:15,292][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 12:45:15,294][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 12:45:15,340][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/advice_yourself/0.25/7/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.60it/s v_num: 0.000      
                                                              val/auc: 0.583    
                                                              val/f1: 0.600     
                                                              val/precision:    
                                                              0.545 val/recall: 
                                                              0.667 val/mre:    
                                                              0.132 train/auc:  
                                                              0.847 train/f1:   
                                                              0.861             
                                                              train/precision:  
                                                              0.787             
                                                              train/recall:     
                                                              0.952 train/mre:  
                                                              0.138             
[2024-05-30 12:46:58,503][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/advice_yourself/0.25/7>
[2024-05-30 12:46:58,504][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 12:46:58,506][HYDRA] 	#58 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=advice_yourself data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=8
[rank: 0] Seed set to 8
[2024-05-30 12:46:58,790][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 12:46:58,792][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 12:46:58,794][train.py][INFO] - Instantiating callbacks...
[2024-05-30 12:46:58,795][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 12:46:58,798][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 12:46:58,799][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 12:46:58,800][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 12:46:58,801][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 12:46:58,802][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 12:46:58,803][train.py][INFO] - Instantiating loggers...
[2024-05-30 12:46:58,803][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 12:46:58,805][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 12:46:58,848][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/advice_yourself/0.25/8/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.63it/s v_num: 0.000      
                                                              val/auc: 0.672    
                                                              val/f1: 0.571     
                                                              val/precision:    
                                                              0.800 val/recall: 
                                                              0.444 val/mre:    
                                                              0.135 train/auc:  
                                                              0.919 train/f1:   
                                                              0.918             
                                                              train/precision:  
                                                              0.933             
                                                              train/recall:     
                                                              0.903 train/mre:  
                                                              0.142             
[2024-05-30 12:48:41,834][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/advice_yourself/0.25/8>
[2024-05-30 12:48:41,834][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 12:48:41,837][HYDRA] 	#59 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=advice_yourself data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=9
[rank: 0] Seed set to 9
[2024-05-30 12:48:42,122][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 12:48:42,124][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 12:48:42,126][train.py][INFO] - Instantiating callbacks...
[2024-05-30 12:48:42,127][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 12:48:42,130][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 12:48:42,131][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 12:48:42,132][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 12:48:42,132][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 12:48:42,134][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 12:48:42,135][train.py][INFO] - Instantiating loggers...
[2024-05-30 12:48:42,135][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 12:48:42,138][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 12:48:42,179][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/advice_yourself/0.25/9/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.52it/s v_num: 0.000      
                                                              val/auc: 0.561    
                                                              val/f1: 0.333     
                                                              val/precision:    
                                                              0.667 val/recall: 
                                                              0.222 val/mre:    
                                                              0.135 train/auc:  
                                                              0.839 train/f1:   
                                                              0.846             
                                                              train/precision:  
                                                              0.809             
                                                              train/recall:     
                                                              0.887 train/mre:  
                                                              0.141             
[2024-05-30 12:50:27,974][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/advice_yourself/0.25/9>
[2024-05-30 12:50:27,974][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 12:50:27,978][HYDRA] 	#60 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=advice_yourself data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=0
[2024-05-30 12:50:28,274][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 12:50:28,277][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 12:50:28,279][train.py][INFO] - Instantiating callbacks...
[2024-05-30 12:50:28,280][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 12:50:28,283][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 12:50:28,284][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 12:50:28,285][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 12:50:28,286][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 12:50:28,288][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 12:50:28,289][train.py][INFO] - Instantiating loggers...
[2024-05-30 12:50:28,289][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 12:50:28,291][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 12:50:28,334][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/advice_yourself/0.3/0/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.48it/s v_num: 0.000      
                                                              val/auc: 0.633    
                                                              val/f1: 0.632     
                                                              val/precision:    
                                                              0.600 val/recall: 
                                                              0.667 val/mre:    
                                                              0.139 train/auc:  
                                                              0.863 train/f1:   
                                                              0.868             
                                                              train/precision:  
                                                              0.836             
                                                              train/recall:     
                                                              0.903 train/mre:  
                                                              0.145             
[2024-05-30 12:52:15,436][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/advice_yourself/0.3/0>
[2024-05-30 12:52:15,437][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 12:52:15,440][HYDRA] 	#61 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=advice_yourself data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=1
[rank: 0] Seed set to 1
[2024-05-30 12:52:15,736][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 12:52:15,739][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 12:52:15,741][train.py][INFO] - Instantiating callbacks...
[2024-05-30 12:52:15,741][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 12:52:15,745][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 12:52:15,746][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 12:52:15,746][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 12:52:15,747][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 12:52:15,749][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 12:52:15,750][train.py][INFO] - Instantiating loggers...
[2024-05-30 12:52:15,750][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 12:52:15,753][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 12:52:15,795][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/advice_yourself/0.3/1/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.47it/s v_num: 0.000      
                                                              val/auc: 0.678    
                                                              val/f1: 0.625     
                                                              val/precision:    
                                                              0.714 val/recall: 
                                                              0.556 val/mre:    
                                                              0.132 train/auc:  
                                                              0.823 train/f1:   
                                                              0.841             
                                                              train/precision:  
                                                              0.763             
                                                              train/recall:     
                                                              0.935 train/mre:  
                                                              0.138             
[2024-05-30 12:54:03,413][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/advice_yourself/0.3/1>
[2024-05-30 12:54:03,413][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 12:54:03,417][HYDRA] 	#62 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=advice_yourself data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=2
[rank: 0] Seed set to 2
[2024-05-30 12:54:03,713][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 12:54:03,716][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 12:54:03,718][train.py][INFO] - Instantiating callbacks...
[2024-05-30 12:54:03,718][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 12:54:03,722][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 12:54:03,722][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 12:54:03,723][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 12:54:03,724][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 12:54:03,726][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 12:54:03,727][train.py][INFO] - Instantiating loggers...
[2024-05-30 12:54:03,727][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 12:54:03,729][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 12:54:03,772][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/advice_yourself/0.3/2/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.48it/s v_num: 0.000      
                                                              val/auc: 0.622    
                                                              val/f1: 0.533     
                                                              val/precision:    
                                                              0.667 val/recall: 
                                                              0.444 val/mre:    
                                                              0.140 train/auc:  
                                                              0.879 train/f1:   
                                                              0.884             
                                                              train/precision:  
                                                              0.851             
                                                              train/recall:     
                                                              0.919 train/mre:  
                                                              0.148             
[2024-05-30 12:55:51,027][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/advice_yourself/0.3/2>
[2024-05-30 12:55:51,027][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 12:55:51,031][HYDRA] 	#63 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=advice_yourself data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=3
[rank: 0] Seed set to 3
[2024-05-30 12:55:51,320][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 12:55:51,323][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 12:55:51,325][train.py][INFO] - Instantiating callbacks...
[2024-05-30 12:55:51,325][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 12:55:51,329][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 12:55:51,329][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 12:55:51,330][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 12:55:51,331][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 12:55:51,333][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 12:55:51,334][train.py][INFO] - Instantiating loggers...
[2024-05-30 12:55:51,334][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 12:55:51,336][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 12:55:51,379][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/advice_yourself/0.3/3/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.46it/s v_num: 0.000      
                                                              val/auc: 0.411    
                                                              val/f1: 0.267     
                                                              val/precision:    
                                                              0.333 val/recall: 
                                                              0.222 val/mre:    
                                                              0.138 train/auc:  
                                                              0.911 train/f1:   
                                                              0.911             
                                                              train/precision:  
                                                              0.918             
                                                              train/recall:     
                                                              0.903 train/mre:  
                                                              0.147             
[2024-05-30 12:57:38,577][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/advice_yourself/0.3/3>
[2024-05-30 12:57:38,578][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 12:57:38,581][HYDRA] 	#64 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=advice_yourself data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=4
[rank: 0] Seed set to 4
[2024-05-30 12:57:38,879][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 12:57:38,881][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 12:57:38,883][train.py][INFO] - Instantiating callbacks...
[2024-05-30 12:57:38,884][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 12:57:38,887][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 12:57:38,888][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 12:57:38,891][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 12:57:38,891][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 12:57:38,893][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 12:57:38,897][train.py][INFO] - Instantiating loggers...
[2024-05-30 12:57:38,897][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 12:57:38,899][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 12:57:38,984][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/advice_yourself/0.3/4/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.52it/s v_num: 0.000      
                                                              val/auc: 0.672    
                                                              val/f1: 0.571     
                                                              val/precision:    
                                                              0.800 val/recall: 
                                                              0.444 val/mre:    
                                                              0.138 train/auc:  
                                                              0.879 train/f1:   
                                                              0.880             
                                                              train/precision:  
                                                              0.873             
                                                              train/recall:     
                                                              0.887 train/mre:  
                                                              0.146             
[2024-05-30 12:59:25,082][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/advice_yourself/0.3/4>
[2024-05-30 12:59:25,083][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 12:59:25,090][HYDRA] 	#65 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=advice_yourself data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=5
[rank: 0] Seed set to 5
[2024-05-30 12:59:25,392][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 12:59:25,395][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 12:59:25,397][train.py][INFO] - Instantiating callbacks...
[2024-05-30 12:59:25,397][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 12:59:25,401][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 12:59:25,402][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 12:59:25,402][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 12:59:25,403][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 12:59:25,405][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 12:59:25,406][train.py][INFO] - Instantiating loggers...
[2024-05-30 12:59:25,406][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 12:59:25,409][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 12:59:25,452][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/advice_yourself/0.3/5/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.50it/s v_num: 0.000      
                                                              val/auc: 0.689    
                                                              val/f1: 0.700     
                                                              val/precision:    
                                                              0.636 val/recall: 
                                                              0.778 val/mre:    
                                                              0.132 train/auc:  
                                                              0.839 train/f1:   
                                                              0.846             
                                                              train/precision:  
                                                              0.809             
                                                              train/recall:     
                                                              0.887 train/mre:  
                                                              0.141             
[2024-05-30 13:01:12,870][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/advice_yourself/0.3/5>
[2024-05-30 13:01:12,871][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 13:01:12,874][HYDRA] 	#66 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=advice_yourself data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=6
[rank: 0] Seed set to 6
[2024-05-30 13:01:13,180][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 13:01:13,183][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 13:01:13,186][train.py][INFO] - Instantiating callbacks...
[2024-05-30 13:01:13,186][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 13:01:13,190][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 13:01:13,190][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 13:01:13,191][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 13:01:13,192][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 13:01:13,194][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 13:01:13,198][train.py][INFO] - Instantiating loggers...
[2024-05-30 13:01:13,198][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 13:01:13,200][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 13:01:13,297][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/advice_yourself/0.3/6/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.47it/s v_num: 0.000      
                                                              val/auc: 0.633    
                                                              val/f1: 0.632     
                                                              val/precision:    
                                                              0.600 val/recall: 
                                                              0.667 val/mre:    
                                                              0.140 train/auc:  
                                                              0.919 train/f1:   
                                                              0.912             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              0.839 train/mre:  
                                                              0.147             
[2024-05-30 13:02:59,694][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/advice_yourself/0.3/6>
[2024-05-30 13:02:59,694][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 13:02:59,698][HYDRA] 	#67 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=advice_yourself data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=7
[rank: 0] Seed set to 7
[2024-05-30 13:02:59,987][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 13:02:59,989][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 13:02:59,991][train.py][INFO] - Instantiating callbacks...
[2024-05-30 13:02:59,992][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 13:02:59,995][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 13:02:59,996][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 13:02:59,997][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 13:02:59,997][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 13:02:59,999][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 13:03:00,000][train.py][INFO] - Instantiating loggers...
[2024-05-30 13:03:00,000][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 13:03:00,003][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 13:03:00,045][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/advice_yourself/0.3/7/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.51it/s v_num: 0.000      
                                                              val/auc: 0.633    
                                                              val/f1: 0.632     
                                                              val/precision:    
                                                              0.600 val/recall: 
                                                              0.667 val/mre:    
                                                              0.135 train/auc:  
                                                              0.847 train/f1:   
                                                              0.857             
                                                              train/precision:  
                                                              0.803             
                                                              train/recall:     
                                                              0.919 train/mre:  
                                                              0.140             
[2024-05-30 13:04:46,721][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/advice_yourself/0.3/7>
[2024-05-30 13:04:46,722][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 13:04:46,725][HYDRA] 	#68 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=advice_yourself data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=8
[rank: 0] Seed set to 8
[2024-05-30 13:04:47,022][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 13:04:47,025][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 13:04:47,027][train.py][INFO] - Instantiating callbacks...
[2024-05-30 13:04:47,027][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 13:04:47,031][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 13:04:47,031][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 13:04:47,032][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 13:04:47,033][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 13:04:47,035][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 13:04:47,036][train.py][INFO] - Instantiating loggers...
[2024-05-30 13:04:47,036][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 13:04:47,038][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 13:04:47,088][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/advice_yourself/0.3/8/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.51it/s v_num: 0.000      
                                                              val/auc: 0.617    
                                                              val/f1: 0.462     
                                                              val/precision:    
                                                              0.750 val/recall: 
                                                              0.333 val/mre:    
                                                              0.138 train/auc:  
                                                              0.871 train/f1:   
                                                              0.875             
                                                              train/precision:  
                                                              0.848             
                                                              train/recall:     
                                                              0.903 train/mre:  
                                                              0.146             
[2024-05-30 13:06:33,527][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/advice_yourself/0.3/8>
[2024-05-30 13:06:33,528][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 13:06:33,532][HYDRA] 	#69 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=advice_yourself data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=9
[rank: 0] Seed set to 9
[2024-05-30 13:06:33,818][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 13:06:33,821][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 13:06:33,823][train.py][INFO] - Instantiating callbacks...
[2024-05-30 13:06:33,823][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 13:06:33,827][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 13:06:33,828][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 13:06:33,828][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 13:06:33,829][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 13:06:33,831][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 13:06:33,832][train.py][INFO] - Instantiating loggers...
[2024-05-30 13:06:33,832][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 13:06:33,835][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 13:06:33,876][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/advice_yourself/0.3/9/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.51it/s v_num: 0.000      
                                                              val/auc: 0.561    
                                                              val/f1: 0.333     
                                                              val/precision:    
                                                              0.667 val/recall: 
                                                              0.222 val/mre:    
                                                              0.141 train/auc:  
                                                              0.911 train/f1:   
                                                              0.912             
                                                              train/precision:  
                                                              0.905             
                                                              train/recall:     
                                                              0.919 train/mre:  
                                                              0.146             
[2024-05-30 13:08:20,719][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/advice_yourself/0.3/9>
[2024-05-30 13:08:20,720][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 13:08:20,727][HYDRA] 	#70 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=anything_regret data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=0
[2024-05-30 13:08:21,031][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 13:08:21,034][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 13:08:21,036][train.py][INFO] - Instantiating callbacks...
[2024-05-30 13:08:21,036][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 13:08:21,040][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 13:08:21,041][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 13:08:21,041][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 13:08:21,042][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 13:08:21,044][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 13:08:21,045][train.py][INFO] - Instantiating loggers...
[2024-05-30 13:08:21,046][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 13:08:21,048][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 13:08:21,091][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/anything_regret/0.0/0/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.70it/s v_num: 0.000      
                                                              val/auc: 0.611    
                                                              val/f1: 0.632     
                                                              val/precision:    
                                                              0.600 val/recall: 
                                                              0.667 val/mre: nan
                                                              train/auc: 0.705  
                                                              train/f1: 0.752   
                                                              train/precision:  
                                                              0.649             
                                                              train/recall:     
                                                              0.893 train/mre:  
                                                              nan               
[2024-05-30 13:10:03,946][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/anything_regret/0.0/0>
[2024-05-30 13:10:03,946][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 13:10:03,949][HYDRA] 	#71 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=anything_regret data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=1
[rank: 0] Seed set to 1
[2024-05-30 13:10:04,241][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 13:10:04,243][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 13:10:04,246][train.py][INFO] - Instantiating callbacks...
[2024-05-30 13:10:04,246][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 13:10:04,249][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 13:10:04,250][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 13:10:04,251][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 13:10:04,252][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 13:10:04,253][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 13:10:04,256][train.py][INFO] - Instantiating loggers...
[2024-05-30 13:10:04,257][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 13:10:04,259][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 13:10:04,360][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/anything_regret/0.0/1/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.62it/s v_num: 0.000      
                                                              val/auc: 0.500    
                                                              val/f1: 0.400     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.333 val/mre: nan
                                                              train/auc: 0.830  
                                                              train/f1: 0.822   
                                                              train/precision:  
                                                              0.863             
                                                              train/recall:     
                                                              0.786 train/mre:  
                                                              nan               
[2024-05-30 13:11:47,487][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/anything_regret/0.0/1>
[2024-05-30 13:11:47,488][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 13:11:47,491][HYDRA] 	#72 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=anything_regret data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=2
[rank: 0] Seed set to 2
[2024-05-30 13:11:47,792][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 13:11:47,795][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 13:11:47,797][train.py][INFO] - Instantiating callbacks...
[2024-05-30 13:11:47,797][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 13:11:47,801][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 13:11:47,801][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 13:11:47,802][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 13:11:47,803][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 13:11:47,805][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 13:11:47,806][train.py][INFO] - Instantiating loggers...
[2024-05-30 13:11:47,806][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 13:11:47,808][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 13:11:47,853][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/anything_regret/0.0/2/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.61it/s v_num: 0.000      
                                                              val/auc: 0.556    
                                                              val/f1: 0.600     
                                                              val/precision:    
                                                              0.545 val/recall: 
                                                              0.667 val/mre: nan
                                                              train/auc: 0.821  
                                                              train/f1: 0.848   
                                                              train/precision:  
                                                              0.737             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              nan               
[2024-05-30 13:13:30,084][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/anything_regret/0.0/2>
[2024-05-30 13:13:30,084][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 13:13:30,088][HYDRA] 	#73 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=anything_regret data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=3
[rank: 0] Seed set to 3
[2024-05-30 13:13:30,383][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 13:13:30,386][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 13:13:30,388][train.py][INFO] - Instantiating callbacks...
[2024-05-30 13:13:30,388][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 13:13:30,392][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 13:13:30,392][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 13:13:30,393][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 13:13:30,394][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 13:13:30,396][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 13:13:30,399][train.py][INFO] - Instantiating loggers...
[2024-05-30 13:13:30,399][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 13:13:30,401][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 13:13:30,487][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/anything_regret/0.0/3/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.55it/s v_num: 0.000      
                                                              val/auc: 0.611    
                                                              val/f1: 0.632     
                                                              val/precision:    
                                                              0.600 val/recall: 
                                                              0.667 val/mre: nan
                                                              train/auc: 0.777  
                                                              train/f1: 0.803   
                                                              train/precision:  
                                                              0.718             
                                                              train/recall:     
                                                              0.911 train/mre:  
                                                              nan               
[2024-05-30 13:15:13,832][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/anything_regret/0.0/3>
[2024-05-30 13:15:13,833][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 13:15:13,836][HYDRA] 	#74 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=anything_regret data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=4
[rank: 0] Seed set to 4
[2024-05-30 13:15:14,136][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 13:15:14,139][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 13:15:14,141][train.py][INFO] - Instantiating callbacks...
[2024-05-30 13:15:14,141][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 13:15:14,145][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 13:15:14,146][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 13:15:14,147][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 13:15:14,147][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 13:15:14,149][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 13:15:14,150][train.py][INFO] - Instantiating loggers...
[2024-05-30 13:15:14,151][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 13:15:14,153][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 13:15:14,198][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/anything_regret/0.0/4/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.61it/s v_num: 0.000      
                                                              val/auc: 0.556    
                                                              val/f1: 0.600     
                                                              val/precision:    
                                                              0.545 val/recall: 
                                                              0.667 val/mre: nan
                                                              train/auc: 0.839  
                                                              train/f1: 0.845   
                                                              train/precision:  
                                                              0.817             
                                                              train/recall:     
                                                              0.875 train/mre:  
                                                              nan               
[2024-05-30 13:16:56,540][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/anything_regret/0.0/4>
[2024-05-30 13:16:56,541][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 13:16:56,544][HYDRA] 	#75 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=anything_regret data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=5
[rank: 0] Seed set to 5
[2024-05-30 13:16:56,837][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 13:16:56,839][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 13:16:56,841][train.py][INFO] - Instantiating callbacks...
[2024-05-30 13:16:56,842][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 13:16:56,845][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 13:16:56,846][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 13:16:56,847][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 13:16:56,847][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 13:16:56,849][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 13:16:56,852][train.py][INFO] - Instantiating loggers...
[2024-05-30 13:16:56,853][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 13:16:56,855][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 13:16:56,938][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/anything_regret/0.0/5/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.61it/s v_num: 0.000      
                                                              val/auc: 0.556    
                                                              val/f1: 0.556     
                                                              val/precision:    
                                                              0.556 val/recall: 
                                                              0.556 val/mre: nan
                                                              train/auc: 0.786  
                                                              train/f1: 0.818   
                                                              train/precision:  
                                                              0.711             
                                                              train/recall:     
                                                              0.964 train/mre:  
                                                              nan               
[2024-05-30 13:18:38,967][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/anything_regret/0.0/5>
[2024-05-30 13:18:38,967][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 13:18:38,971][HYDRA] 	#76 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=anything_regret data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=6
[rank: 0] Seed set to 6
[2024-05-30 13:18:39,265][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 13:18:39,268][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 13:18:39,270][train.py][INFO] - Instantiating callbacks...
[2024-05-30 13:18:39,270][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 13:18:39,274][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 13:18:39,275][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 13:18:39,275][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 13:18:39,276][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 13:18:39,278][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 13:18:39,279][train.py][INFO] - Instantiating loggers...
[2024-05-30 13:18:39,280][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 13:18:39,282][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 13:18:39,368][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/anything_regret/0.0/6/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.61it/s v_num: 0.000      
                                                              val/auc: 0.556    
                                                              val/f1: 0.600     
                                                              val/precision:    
                                                              0.545 val/recall: 
                                                              0.667 val/mre: nan
                                                              train/auc: 0.670  
                                                              train/f1: 0.730   
                                                              train/precision:  
                                                              0.617             
                                                              train/recall:     
                                                              0.893 train/mre:  
                                                              nan               
[2024-05-30 13:20:21,202][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/anything_regret/0.0/6>
[2024-05-30 13:20:21,203][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 13:20:21,206][HYDRA] 	#77 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=anything_regret data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=7
[rank: 0] Seed set to 7
[2024-05-30 13:20:21,502][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 13:20:21,504][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 13:20:21,506][train.py][INFO] - Instantiating callbacks...
[2024-05-30 13:20:21,507][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 13:20:21,510][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 13:20:21,511][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 13:20:21,512][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 13:20:21,513][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 13:20:21,515][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 13:20:21,518][train.py][INFO] - Instantiating loggers...
[2024-05-30 13:20:21,518][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 13:20:21,520][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 13:20:21,821][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/anything_regret/0.0/7/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.61it/s v_num: 0.000      
                                                              val/auc: 0.500    
                                                              val/f1: 0.571     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.667 val/mre: nan
                                                              train/auc: 0.750  
                                                              train/f1: 0.781   
                                                              train/precision:  
                                                              0.694             
                                                              train/recall:     
                                                              0.893 train/mre:  
                                                              nan               
[2024-05-30 13:22:03,823][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/anything_regret/0.0/7>
[2024-05-30 13:22:03,824][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 13:22:03,827][HYDRA] 	#78 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=anything_regret data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=8
[rank: 0] Seed set to 8
[2024-05-30 13:22:04,115][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 13:22:04,117][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 13:22:04,119][train.py][INFO] - Instantiating callbacks...
[2024-05-30 13:22:04,120][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 13:22:04,123][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 13:22:04,124][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 13:22:04,124][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 13:22:04,125][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 13:22:04,127][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 13:22:04,128][train.py][INFO] - Instantiating loggers...
[2024-05-30 13:22:04,128][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 13:22:04,130][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 13:22:04,172][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/anything_regret/0.0/8/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.64it/s v_num: 0.000      
                                                              val/auc: 0.611    
                                                              val/f1: 0.588     
                                                              val/precision:    
                                                              0.625 val/recall: 
                                                              0.556 val/mre: nan
                                                              train/auc: 0.696  
                                                              train/f1: 0.673   
                                                              train/precision:  
                                                              0.729             
                                                              train/recall:     
                                                              0.625 train/mre:  
                                                              nan               
[2024-05-30 13:23:45,591][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/anything_regret/0.0/8>
[2024-05-30 13:23:45,593][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 13:23:45,596][HYDRA] 	#79 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=anything_regret data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=9
[rank: 0] Seed set to 9
[2024-05-30 13:23:45,904][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 13:23:45,906][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 13:23:45,909][train.py][INFO] - Instantiating callbacks...
[2024-05-30 13:23:45,909][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 13:23:45,912][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 13:23:45,913][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 13:23:45,914][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 13:23:45,915][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 13:23:45,916][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 13:23:45,917][train.py][INFO] - Instantiating loggers...
[2024-05-30 13:23:45,918][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 13:23:45,920][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 13:23:45,963][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/anything_regret/0.0/9/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.62it/s v_num: 0.000      
                                                              val/auc: 0.611    
                                                              val/f1: 0.632     
                                                              val/precision:    
                                                              0.600 val/recall: 
                                                              0.667 val/mre: nan
                                                              train/auc: 0.741  
                                                              train/f1: 0.760   
                                                              train/precision:  
                                                              0.708             
                                                              train/recall:     
                                                              0.821 train/mre:  
                                                              nan               
[2024-05-30 13:25:28,490][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/anything_regret/0.0/9>
[2024-05-30 13:25:28,491][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 13:25:28,495][HYDRA] 	#80 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=anything_regret data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=0
[2024-05-30 13:25:28,795][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 13:25:28,798][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 13:25:28,800][train.py][INFO] - Instantiating callbacks...
[2024-05-30 13:25:28,800][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 13:25:28,804][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 13:25:28,805][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 13:25:28,806][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 13:25:28,806][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 13:25:28,808][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 13:25:28,809][train.py][INFO] - Instantiating loggers...
[2024-05-30 13:25:28,809][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 13:25:28,811][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 13:25:28,854][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/anything_regret/0.05/0/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.60it/s v_num: 0.000      
                                                              val/auc: 0.556    
                                                              val/f1: 0.600     
                                                              val/precision:    
                                                              0.545 val/recall: 
                                                              0.667 val/mre:    
                                                              0.091 train/auc:  
                                                              0.759 train/f1:   
                                                              0.800             
                                                              train/precision:  
                                                              0.684             
                                                              train/recall:     
                                                              0.964 train/mre:  
                                                              0.088             
[2024-05-30 13:27:11,268][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/anything_regret/0.05/0>
[2024-05-30 13:27:11,269][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 13:27:11,273][HYDRA] 	#81 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=anything_regret data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=1
[rank: 0] Seed set to 1
[2024-05-30 13:27:11,635][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 13:27:11,637][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 13:27:11,640][train.py][INFO] - Instantiating callbacks...
[2024-05-30 13:27:11,640][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 13:27:11,646][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 13:27:11,647][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 13:27:11,648][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 13:27:11,649][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 13:27:11,651][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 13:27:11,652][train.py][INFO] - Instantiating loggers...
[2024-05-30 13:27:11,652][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 13:27:11,654][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 13:27:11,792][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/anything_regret/0.05/1/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.61it/s v_num: 0.000      
                                                              val/auc: 0.611    
                                                              val/f1: 0.632     
                                                              val/precision:    
                                                              0.600 val/recall: 
                                                              0.667 val/mre:    
                                                              0.100 train/auc:  
                                                              0.875 train/f1:   
                                                              0.881             
                                                              train/precision:  
                                                              0.839             
                                                              train/recall:     
                                                              0.929 train/mre:  
                                                              0.095             
[2024-05-30 13:28:54,651][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/anything_regret/0.05/1>
[2024-05-30 13:28:54,652][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 13:28:54,655][HYDRA] 	#82 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=anything_regret data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=2
[rank: 0] Seed set to 2
[2024-05-30 13:28:54,950][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 13:28:54,953][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 13:28:54,956][train.py][INFO] - Instantiating callbacks...
[2024-05-30 13:28:54,956][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 13:28:54,960][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 13:28:54,961][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 13:28:54,962][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 13:28:54,963][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 13:28:54,965][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 13:28:54,966][train.py][INFO] - Instantiating loggers...
[2024-05-30 13:28:54,966][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 13:28:54,968][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 13:28:55,014][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/anything_regret/0.05/2/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.56it/s v_num: 0.000      
                                                              val/auc: 0.556    
                                                              val/f1: 0.600     
                                                              val/precision:    
                                                              0.545 val/recall: 
                                                              0.667 val/mre:    
                                                              0.099 train/auc:  
                                                              0.830 train/f1:   
                                                              0.855             
                                                              train/precision:  
                                                              0.747             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.095             
[2024-05-30 13:30:37,915][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/anything_regret/0.05/2>
[2024-05-30 13:30:37,916][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 13:30:37,919][HYDRA] 	#83 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=anything_regret data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=3
[rank: 0] Seed set to 3
[2024-05-30 13:30:38,212][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 13:30:38,214][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 13:30:38,217][train.py][INFO] - Instantiating callbacks...
[2024-05-30 13:30:38,217][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 13:30:38,220][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 13:30:38,221][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 13:30:38,222][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 13:30:38,223][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 13:30:38,224][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 13:30:38,225][train.py][INFO] - Instantiating loggers...
[2024-05-30 13:30:38,226][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 13:30:38,228][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 13:30:38,334][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/anything_regret/0.05/3/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.55it/s v_num: 0.000      
                                                              val/auc: 0.556    
                                                              val/f1: 0.556     
                                                              val/precision:    
                                                              0.556 val/recall: 
                                                              0.556 val/mre:    
                                                              0.095 train/auc:  
                                                              0.768 train/f1:   
                                                              0.790             
                                                              train/precision:  
                                                              0.721             
                                                              train/recall:     
                                                              0.875 train/mre:  
                                                              0.091             
[2024-05-30 13:32:22,011][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/anything_regret/0.05/3>
[2024-05-30 13:32:22,012][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 13:32:22,015][HYDRA] 	#84 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=anything_regret data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=4
[rank: 0] Seed set to 4
[2024-05-30 13:32:22,310][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 13:32:22,313][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 13:32:22,315][train.py][INFO] - Instantiating callbacks...
[2024-05-30 13:32:22,315][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 13:32:22,318][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 13:32:22,319][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 13:32:22,320][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 13:32:22,321][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 13:32:22,323][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 13:32:22,323][train.py][INFO] - Instantiating loggers...
[2024-05-30 13:32:22,324][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 13:32:22,326][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 13:32:22,373][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/anything_regret/0.05/4/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.63it/s v_num: 0.000      
                                                              val/auc: 0.500    
                                                              val/f1: 0.571     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.667 val/mre:    
                                                              0.100 train/auc:  
                                                              0.857 train/f1:   
                                                              0.862             
                                                              train/precision:  
                                                              0.833             
                                                              train/recall:     
                                                              0.893 train/mre:  
                                                              0.094             
[2024-05-30 13:34:05,188][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/anything_regret/0.05/4>
[2024-05-30 13:34:05,188][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 13:34:05,191][HYDRA] 	#85 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=anything_regret data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=5
[rank: 0] Seed set to 5
[2024-05-30 13:34:05,480][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 13:34:05,483][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 13:34:05,485][train.py][INFO] - Instantiating callbacks...
[2024-05-30 13:34:05,485][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 13:34:05,489][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 13:34:05,490][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 13:34:05,490][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 13:34:05,491][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 13:34:05,493][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 13:34:05,494][train.py][INFO] - Instantiating loggers...
[2024-05-30 13:34:05,494][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 13:34:05,498][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 13:34:05,540][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/anything_regret/0.05/5/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.59it/s v_num: 0.000      
                                                              val/auc: 0.500    
                                                              val/f1: 0.526     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.556 val/mre:    
                                                              0.093 train/auc:  
                                                              0.795 train/f1:   
                                                              0.824             
                                                              train/precision:  
                                                              0.720             
                                                              train/recall:     
                                                              0.964 train/mre:  
                                                              0.087             
[2024-05-30 13:35:48,463][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/anything_regret/0.05/5>
[2024-05-30 13:35:48,464][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 13:35:48,467][HYDRA] 	#86 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=anything_regret data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=6
[rank: 0] Seed set to 6
[2024-05-30 13:35:48,758][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 13:35:48,760][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 13:35:48,762][train.py][INFO] - Instantiating callbacks...
[2024-05-30 13:35:48,763][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 13:35:48,766][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 13:35:48,767][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 13:35:48,768][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 13:35:48,768][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 13:35:48,770][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 13:35:48,771][train.py][INFO] - Instantiating loggers...
[2024-05-30 13:35:48,771][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 13:35:48,773][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 13:35:48,816][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/anything_regret/0.05/6/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.60it/s v_num: 0.000      
                                                              val/auc: 0.500    
                                                              val/f1: 0.571     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.667 val/mre:    
                                                              0.092 train/auc:  
                                                              0.705 train/f1:   
                                                              0.759             
                                                              train/precision:  
                                                              0.642             
                                                              train/recall:     
                                                              0.929 train/mre:  
                                                              0.087             
[2024-05-30 13:37:31,208][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/anything_regret/0.05/6>
[2024-05-30 13:37:31,209][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 13:37:31,212][HYDRA] 	#87 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=anything_regret data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=7
[rank: 0] Seed set to 7
[2024-05-30 13:37:31,514][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 13:37:31,517][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 13:37:31,519][train.py][INFO] - Instantiating callbacks...
[2024-05-30 13:37:31,520][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 13:37:31,523][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 13:37:31,524][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 13:37:31,525][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 13:37:31,526][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 13:37:31,527][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 13:37:31,528][train.py][INFO] - Instantiating loggers...
[2024-05-30 13:37:31,529][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 13:37:31,531][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 13:37:31,576][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/anything_regret/0.05/7/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.60it/s v_num: 0.000      
                                                              val/auc: 0.444    
                                                              val/f1: 0.500     
                                                              val/precision:    
                                                              0.455 val/recall: 
                                                              0.556 val/mre:    
                                                              0.095 train/auc:  
                                                              0.759 train/f1:   
                                                              0.794             
                                                              train/precision:  
                                                              0.693             
                                                              train/recall:     
                                                              0.929 train/mre:  
                                                              0.088             
[2024-05-30 13:39:14,402][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/anything_regret/0.05/7>
[2024-05-30 13:39:14,402][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 13:39:14,406][HYDRA] 	#88 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=anything_regret data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=8
[rank: 0] Seed set to 8
[2024-05-30 13:39:14,737][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 13:39:14,740][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 13:39:14,742][train.py][INFO] - Instantiating callbacks...
[2024-05-30 13:39:14,742][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 13:39:14,746][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 13:39:14,747][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 13:39:14,748][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 13:39:14,748][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 13:39:14,750][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 13:39:14,751][train.py][INFO] - Instantiating loggers...
[2024-05-30 13:39:14,751][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 13:39:14,754][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 13:39:14,915][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/anything_regret/0.05/8/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.57it/s v_num: 0.000      
                                                              val/auc: 0.500    
                                                              val/f1: 0.526     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.556 val/mre:    
                                                              0.097 train/auc:  
                                                              0.804 train/f1:   
                                                              0.800             
                                                              train/precision:  
                                                              0.815             
                                                              train/recall:     
                                                              0.786 train/mre:  
                                                              0.094             
[2024-05-30 13:40:57,686][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/anything_regret/0.05/8>
[2024-05-30 13:40:57,687][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 13:40:57,690][HYDRA] 	#89 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=anything_regret data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=9
[rank: 0] Seed set to 9
[2024-05-30 13:40:57,981][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 13:40:57,984][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 13:40:57,986][train.py][INFO] - Instantiating callbacks...
[2024-05-30 13:40:57,986][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 13:40:57,990][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 13:40:57,990][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 13:40:57,991][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 13:40:57,992][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 13:40:57,994][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 13:40:57,995][train.py][INFO] - Instantiating loggers...
[2024-05-30 13:40:57,995][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 13:40:57,997][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 13:40:58,040][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/anything_regret/0.05/9/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.54it/s v_num: 0.000      
                                                              val/auc: 0.556    
                                                              val/f1: 0.600     
                                                              val/precision:    
                                                              0.545 val/recall: 
                                                              0.667 val/mre:    
                                                              0.098 train/auc:  
                                                              0.804 train/f1:   
                                                              0.820             
                                                              train/precision:  
                                                              0.758             
                                                              train/recall:     
                                                              0.893 train/mre:  
                                                              0.092             
[2024-05-30 13:42:41,455][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/anything_regret/0.05/9>
[2024-05-30 13:42:41,456][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 13:42:41,459][HYDRA] 	#90 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=anything_regret data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=0
[2024-05-30 13:42:41,754][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 13:42:41,757][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 13:42:41,759][train.py][INFO] - Instantiating callbacks...
[2024-05-30 13:42:41,759][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 13:42:41,763][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 13:42:41,763][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 13:42:41,764][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 13:42:41,765][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 13:42:41,767][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 13:42:41,768][train.py][INFO] - Instantiating loggers...
[2024-05-30 13:42:41,768][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 13:42:41,770][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 13:42:41,813][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/anything_regret/0.1/0/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.59it/s v_num: 0.000      
                                                              val/auc: 0.611    
                                                              val/f1: 0.632     
                                                              val/precision:    
                                                              0.600 val/recall: 
                                                              0.667 val/mre:    
                                                              0.096 train/auc:  
                                                              0.768 train/f1:   
                                                              0.809             
                                                              train/precision:  
                                                              0.688             
                                                              train/recall:     
                                                              0.982 train/mre:  
                                                              0.093             
[2024-05-30 13:44:25,122][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/anything_regret/0.1/0>
[2024-05-30 13:44:25,123][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 13:44:25,126][HYDRA] 	#91 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=anything_regret data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=1
[rank: 0] Seed set to 1
[2024-05-30 13:44:25,414][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 13:44:25,416][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 13:44:25,418][train.py][INFO] - Instantiating callbacks...
[2024-05-30 13:44:25,419][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 13:44:25,422][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 13:44:25,423][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 13:44:25,424][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 13:44:25,425][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 13:44:25,427][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 13:44:25,427][train.py][INFO] - Instantiating loggers...
[2024-05-30 13:44:25,428][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 13:44:25,430][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 13:44:25,478][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/anything_regret/0.1/1/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.54it/s v_num: 0.000      
                                                              val/auc: 0.611    
                                                              val/f1: 0.588     
                                                              val/precision:    
                                                              0.625 val/recall: 
                                                              0.556 val/mre:    
                                                              0.101 train/auc:  
                                                              0.821 train/f1:   
                                                              0.796             
                                                              train/precision:  
                                                              0.929             
                                                              train/recall:     
                                                              0.696 train/mre:  
                                                              0.098             
[2024-05-30 13:46:08,888][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/anything_regret/0.1/1>
[2024-05-30 13:46:08,889][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 13:46:08,892][HYDRA] 	#92 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=anything_regret data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=2
[rank: 0] Seed set to 2
[2024-05-30 13:46:09,199][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 13:46:09,201][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 13:46:09,203][train.py][INFO] - Instantiating callbacks...
[2024-05-30 13:46:09,204][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 13:46:09,207][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 13:46:09,208][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 13:46:09,209][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 13:46:09,210][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 13:46:09,211][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 13:46:09,215][train.py][INFO] - Instantiating loggers...
[2024-05-30 13:46:09,215][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 13:46:09,217][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 13:46:09,281][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/anything_regret/0.1/2/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.54it/s v_num: 0.000      
                                                              val/auc: 0.611    
                                                              val/f1: 0.667     
                                                              val/precision:    
                                                              0.583 val/recall: 
                                                              0.778 val/mre:    
                                                              0.102 train/auc:  
                                                              0.866 train/f1:   
                                                              0.880             
                                                              train/precision:  
                                                              0.797             
                                                              train/recall:     
                                                              0.982 train/mre:  
                                                              0.099             
[2024-05-30 13:47:52,534][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/anything_regret/0.1/2>
[2024-05-30 13:47:52,535][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 13:47:52,539][HYDRA] 	#93 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=anything_regret data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=3
[rank: 0] Seed set to 3
[2024-05-30 13:47:52,840][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 13:47:52,842][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 13:47:52,845][train.py][INFO] - Instantiating callbacks...
[2024-05-30 13:47:52,845][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 13:47:52,848][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 13:47:52,849][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 13:47:52,850][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 13:47:52,851][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 13:47:52,853][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 13:47:52,854][train.py][INFO] - Instantiating loggers...
[2024-05-30 13:47:52,854][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 13:47:52,857][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 13:47:52,941][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/anything_regret/0.1/3/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.54it/s v_num: 0.000      
                                                              val/auc: 0.667    
                                                              val/f1: 0.667     
                                                              val/precision:    
                                                              0.667 val/recall: 
                                                              0.667 val/mre:    
                                                              0.097 train/auc:  
                                                              0.741 train/f1:   
                                                              0.782             
                                                              train/precision:  
                                                              0.675             
                                                              train/recall:     
                                                              0.929 train/mre:  
                                                              0.094             
[2024-05-30 13:49:37,103][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/anything_regret/0.1/3>
[2024-05-30 13:49:37,104][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 13:49:37,107][HYDRA] 	#94 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=anything_regret data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=4
[rank: 0] Seed set to 4
[2024-05-30 13:49:37,409][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 13:49:37,411][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 13:49:37,413][train.py][INFO] - Instantiating callbacks...
[2024-05-30 13:49:37,414][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 13:49:37,417][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 13:49:37,418][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 13:49:37,419][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 13:49:37,420][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 13:49:37,422][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 13:49:37,430][train.py][INFO] - Instantiating loggers...
[2024-05-30 13:49:37,431][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 13:49:37,433][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 13:49:37,481][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/anything_regret/0.1/4/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.66it/s v_num: 0.000      
                                                              val/auc: 0.556    
                                                              val/f1: 0.600     
                                                              val/precision:    
                                                              0.545 val/recall: 
                                                              0.667 val/mre:    
                                                              0.101 train/auc:  
                                                              0.839 train/f1:   
                                                              0.852             
                                                              train/precision:  
                                                              0.788             
                                                              train/recall:     
                                                              0.929 train/mre:  
                                                              0.097             
[2024-05-30 13:51:19,909][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/anything_regret/0.1/4>
[2024-05-30 13:51:19,910][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 13:51:19,914][HYDRA] 	#95 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=anything_regret data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=5
[rank: 0] Seed set to 5
[2024-05-30 13:51:20,205][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 13:51:20,208][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 13:51:20,210][train.py][INFO] - Instantiating callbacks...
[2024-05-30 13:51:20,210][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 13:51:20,214][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 13:51:20,215][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 13:51:20,215][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 13:51:20,216][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 13:51:20,218][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 13:51:20,219][train.py][INFO] - Instantiating loggers...
[2024-05-30 13:51:20,219][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 13:51:20,222][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 13:51:20,265][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/anything_regret/0.1/5/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.58it/s v_num: 0.000      
                                                              val/auc: 0.611    
                                                              val/f1: 0.588     
                                                              val/precision:    
                                                              0.625 val/recall: 
                                                              0.556 val/mre:    
                                                              0.095 train/auc:  
                                                              0.768 train/f1:   
                                                              0.806             
                                                              train/precision:  
                                                              0.692             
                                                              train/recall:     
                                                              0.964 train/mre:  
                                                              0.092             
[2024-05-30 13:53:03,723][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/anything_regret/0.1/5>
[2024-05-30 13:53:03,724][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 13:53:03,728][HYDRA] 	#96 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=anything_regret data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=6
[rank: 0] Seed set to 6
[2024-05-30 13:53:04,020][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 13:53:04,022][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 13:53:04,024][train.py][INFO] - Instantiating callbacks...
[2024-05-30 13:53:04,025][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 13:53:04,028][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 13:53:04,029][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 13:53:04,030][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 13:53:04,031][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 13:53:04,033][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 13:53:04,033][train.py][INFO] - Instantiating loggers...
[2024-05-30 13:53:04,034][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 13:53:04,038][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 13:53:04,080][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/anything_regret/0.1/6/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.61it/s v_num: 0.000      
                                                              val/auc: 0.556    
                                                              val/f1: 0.556     
                                                              val/precision:    
                                                              0.556 val/recall: 
                                                              0.556 val/mre:    
                                                              0.100 train/auc:  
                                                              0.768 train/f1:   
                                                              0.794             
                                                              train/precision:  
                                                              0.714             
                                                              train/recall:     
                                                              0.893 train/mre:  
                                                              0.094             
[2024-05-30 13:54:47,511][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/anything_regret/0.1/6>
[2024-05-30 13:54:47,511][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 13:54:47,515][HYDRA] 	#97 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=anything_regret data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=7
[rank: 0] Seed set to 7
[2024-05-30 13:54:47,805][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 13:54:47,807][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 13:54:47,809][train.py][INFO] - Instantiating callbacks...
[2024-05-30 13:54:47,810][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 13:54:47,813][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 13:54:47,814][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 13:54:47,815][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 13:54:47,815][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 13:54:47,817][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 13:54:47,818][train.py][INFO] - Instantiating loggers...
[2024-05-30 13:54:47,818][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 13:54:47,821][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 13:54:47,865][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/anything_regret/0.1/7/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.63it/s v_num: 0.000      
                                                              val/auc: 0.389    
                                                              val/f1: 0.476     
                                                              val/precision:    
                                                              0.417 val/recall: 
                                                              0.556 val/mre:    
                                                              0.096 train/auc:  
                                                              0.759 train/f1:   
                                                              0.794             
                                                              train/precision:  
                                                              0.693             
                                                              train/recall:     
                                                              0.929 train/mre:  
                                                              0.091             
[2024-05-30 13:56:30,686][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/anything_regret/0.1/7>
[2024-05-30 13:56:30,687][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 13:56:30,691][HYDRA] 	#98 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=anything_regret data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=8
[rank: 0] Seed set to 8
[2024-05-30 13:56:30,983][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 13:56:30,985][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 13:56:30,987][train.py][INFO] - Instantiating callbacks...
[2024-05-30 13:56:30,988][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 13:56:30,991][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 13:56:30,992][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 13:56:30,993][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 13:56:30,994][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 13:56:30,996][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 13:56:30,996][train.py][INFO] - Instantiating loggers...
[2024-05-30 13:56:30,997][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 13:56:30,999][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 13:56:31,041][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/anything_regret/0.1/8/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.57it/s v_num: 0.000      
                                                              val/auc: 0.500    
                                                              val/f1: 0.526     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.556 val/mre:    
                                                              0.100 train/auc:  
                                                              0.848 train/f1:   
                                                              0.852             
                                                              train/precision:  
                                                              0.831             
                                                              train/recall:     
                                                              0.875 train/mre:  
                                                              0.095             
[2024-05-30 13:58:14,340][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/anything_regret/0.1/8>
[2024-05-30 13:58:14,341][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 13:58:14,346][HYDRA] 	#99 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=anything_regret data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=9
[rank: 0] Seed set to 9
[2024-05-30 13:58:14,639][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 13:58:14,641][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 13:58:14,643][train.py][INFO] - Instantiating callbacks...
[2024-05-30 13:58:14,644][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 13:58:14,647][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 13:58:14,648][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 13:58:14,649][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 13:58:14,649][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 13:58:14,651][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 13:58:14,652][train.py][INFO] - Instantiating loggers...
[2024-05-30 13:58:14,652][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 13:58:14,656][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 13:58:14,699][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/anything_regret/0.1/9/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.50it/s v_num: 0.000      
                                                              val/auc: 0.556    
                                                              val/f1: 0.600     
                                                              val/precision:    
                                                              0.545 val/recall: 
                                                              0.667 val/mre:    
                                                              0.103 train/auc:  
                                                              0.830 train/f1:   
                                                              0.846             
                                                              train/precision:  
                                                              0.776             
                                                              train/recall:     
                                                              0.929 train/mre:  
                                                              0.099             
[2024-05-30 13:59:58,382][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/anything_regret/0.1/9>
[2024-05-30 13:59:58,382][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 13:59:58,386][HYDRA] 	#100 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=anything_regret data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=0
[2024-05-30 13:59:58,693][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 13:59:58,696][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 13:59:58,698][train.py][INFO] - Instantiating callbacks...
[2024-05-30 13:59:58,698][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 13:59:58,702][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 13:59:58,703][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 13:59:58,704][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 13:59:58,704][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 13:59:58,706][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 13:59:58,707][train.py][INFO] - Instantiating loggers...
[2024-05-30 13:59:58,707][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 13:59:58,710][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 13:59:58,755][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/anything_regret/0.15/0/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.56it/s v_num: 0.000      
                                                              val/auc: 0.611    
                                                              val/f1: 0.588     
                                                              val/precision:    
                                                              0.625 val/recall: 
                                                              0.556 val/mre:    
                                                              0.101 train/auc:  
                                                              0.732 train/f1:   
                                                              0.779             
                                                              train/precision:  
                                                              0.663             
                                                              train/recall:     
                                                              0.946 train/mre:  
                                                              0.098             
[2024-05-30 14:01:42,594][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/anything_regret/0.15/0>
[2024-05-30 14:01:42,595][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 14:01:42,598][HYDRA] 	#101 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=anything_regret data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=1
[rank: 0] Seed set to 1
[2024-05-30 14:01:42,900][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 14:01:42,902][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 14:01:42,904][train.py][INFO] - Instantiating callbacks...
[2024-05-30 14:01:42,905][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 14:01:42,908][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 14:01:42,909][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 14:01:42,910][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 14:01:42,911][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 14:01:42,913][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 14:01:42,916][train.py][INFO] - Instantiating loggers...
[2024-05-30 14:01:42,916][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 14:01:42,919][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 14:01:43,017][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/anything_regret/0.15/1/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.58it/s v_num: 0.000      
                                                              val/auc: 0.611    
                                                              val/f1: 0.632     
                                                              val/precision:    
                                                              0.600 val/recall: 
                                                              0.667 val/mre:    
                                                              0.101 train/auc:  
                                                              0.705 train/f1:   
                                                              0.713             
                                                              train/precision:  
                                                              0.695             
                                                              train/recall:     
                                                              0.732 train/mre:  
                                                              0.098             
[2024-05-30 14:03:26,404][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/anything_regret/0.15/1>
[2024-05-30 14:03:26,405][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 14:03:26,408][HYDRA] 	#102 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=anything_regret data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=2
[rank: 0] Seed set to 2
[2024-05-30 14:03:26,710][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 14:03:26,712][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 14:03:26,714][train.py][INFO] - Instantiating callbacks...
[2024-05-30 14:03:26,715][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 14:03:26,718][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 14:03:26,719][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 14:03:26,720][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 14:03:26,721][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 14:03:26,723][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 14:03:26,724][train.py][INFO] - Instantiating loggers...
[2024-05-30 14:03:26,725][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 14:03:26,727][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 14:03:26,770][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/anything_regret/0.15/2/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.57it/s v_num: 0.000      
                                                              val/auc: 0.444    
                                                              val/f1: 0.545     
                                                              val/precision:    
                                                              0.462 val/recall: 
                                                              0.667 val/mre:    
                                                              0.104 train/auc:  
                                                              0.830 train/f1:   
                                                              0.853             
                                                              train/precision:  
                                                              0.753             
                                                              train/recall:     
                                                              0.982 train/mre:  
                                                              0.101             
[2024-05-30 14:05:10,762][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/anything_regret/0.15/2>
[2024-05-30 14:05:10,763][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 14:05:10,767][HYDRA] 	#103 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=anything_regret data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=3
[rank: 0] Seed set to 3
[2024-05-30 14:05:11,062][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 14:05:11,065][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 14:05:11,067][train.py][INFO] - Instantiating callbacks...
[2024-05-30 14:05:11,067][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 14:05:11,071][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 14:05:11,072][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 14:05:11,072][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 14:05:11,073][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 14:05:11,075][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 14:05:11,076][train.py][INFO] - Instantiating loggers...
[2024-05-30 14:05:11,076][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 14:05:11,079][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 14:05:11,121][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/anything_regret/0.15/3/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.58it/s v_num: 0.000      
                                                              val/auc: 0.611    
                                                              val/f1: 0.632     
                                                              val/precision:    
                                                              0.600 val/recall: 
                                                              0.667 val/mre:    
                                                              0.100 train/auc:  
                                                              0.812 train/f1:   
                                                              0.829             
                                                              train/precision:  
                                                              0.761             
                                                              train/recall:     
                                                              0.911 train/mre:  
                                                              0.097             
[2024-05-30 14:06:55,169][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/anything_regret/0.15/3>
[2024-05-30 14:06:55,170][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 14:06:55,173][HYDRA] 	#104 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=anything_regret data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=4
[rank: 0] Seed set to 4
[2024-05-30 14:06:55,472][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 14:06:55,474][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 14:06:55,476][train.py][INFO] - Instantiating callbacks...
[2024-05-30 14:06:55,477][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 14:06:55,481][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 14:06:55,481][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 14:06:55,482][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 14:06:55,483][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 14:06:55,485][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 14:06:55,488][train.py][INFO] - Instantiating loggers...
[2024-05-30 14:06:55,488][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 14:06:55,491][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 14:06:55,574][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/anything_regret/0.15/4/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.66it/s v_num: 0.000      
                                                              val/auc: 0.556    
                                                              val/f1: 0.600     
                                                              val/precision:    
                                                              0.545 val/recall: 
                                                              0.667 val/mre:    
                                                              0.101 train/auc:  
                                                              0.768 train/f1:   
                                                              0.794             
                                                              train/precision:  
                                                              0.714             
                                                              train/recall:     
                                                              0.893 train/mre:  
                                                              0.097             
[2024-05-30 14:08:37,814][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/anything_regret/0.15/4>
[2024-05-30 14:08:37,815][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 14:08:37,818][HYDRA] 	#105 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=anything_regret data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=5
[rank: 0] Seed set to 5
[2024-05-30 14:08:38,113][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 14:08:38,115][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 14:08:38,118][train.py][INFO] - Instantiating callbacks...
[2024-05-30 14:08:38,118][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 14:08:38,122][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 14:08:38,122][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 14:08:38,123][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 14:08:38,124][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 14:08:38,126][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 14:08:38,127][train.py][INFO] - Instantiating loggers...
[2024-05-30 14:08:38,127][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 14:08:38,129][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 14:08:38,174][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/anything_regret/0.15/5/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.56it/s v_num: 0.000      
                                                              val/auc: 0.500    
                                                              val/f1: 0.526     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.556 val/mre:    
                                                              0.100 train/auc:  
                                                              0.795 train/f1:   
                                                              0.819             
                                                              train/precision:  
                                                              0.732             
                                                              train/recall:     
                                                              0.929 train/mre:  
                                                              0.095             
[2024-05-30 14:10:23,047][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/anything_regret/0.15/5>
[2024-05-30 14:10:23,048][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 14:10:23,052][HYDRA] 	#106 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=anything_regret data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=6
[rank: 0] Seed set to 6
[2024-05-30 14:10:23,356][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 14:10:23,359][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 14:10:23,361][train.py][INFO] - Instantiating callbacks...
[2024-05-30 14:10:23,361][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 14:10:23,365][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 14:10:23,366][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 14:10:23,366][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 14:10:23,367][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 14:10:23,369][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 14:10:23,370][train.py][INFO] - Instantiating loggers...
[2024-05-30 14:10:23,370][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 14:10:23,372][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 14:10:23,418][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/anything_regret/0.15/6/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.56it/s v_num: 0.000      
                                                              val/auc: 0.556    
                                                              val/f1: 0.600     
                                                              val/precision:    
                                                              0.545 val/recall: 
                                                              0.667 val/mre:    
                                                              0.103 train/auc:  
                                                              0.777 train/f1:   
                                                              0.803             
                                                              train/precision:  
                                                              0.718             
                                                              train/recall:     
                                                              0.911 train/mre:  
                                                              0.098             
[2024-05-30 14:12:05,640][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/anything_regret/0.15/6>
[2024-05-30 14:12:05,641][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 14:12:05,645][HYDRA] 	#107 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=anything_regret data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=7
[rank: 0] Seed set to 7
[2024-05-30 14:12:05,952][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 14:12:05,955][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 14:12:05,957][train.py][INFO] - Instantiating callbacks...
[2024-05-30 14:12:05,957][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 14:12:05,961][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 14:12:05,962][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 14:12:05,962][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 14:12:05,963][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 14:12:05,965][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 14:12:05,973][train.py][INFO] - Instantiating loggers...
[2024-05-30 14:12:05,973][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 14:12:05,975][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 14:12:06,067][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/anything_regret/0.15/7/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.70it/s v_num: 0.000      
                                                              val/auc: 0.389    
                                                              val/f1: 0.476     
                                                              val/precision:    
                                                              0.417 val/recall: 
                                                              0.556 val/mre:    
                                                              0.098 train/auc:  
                                                              0.732 train/f1:   
                                                              0.779             
                                                              train/precision:  
                                                              0.663             
                                                              train/recall:     
                                                              0.946 train/mre:  
                                                              0.093             
[2024-05-30 14:13:47,239][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/anything_regret/0.15/7>
[2024-05-30 14:13:47,240][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 14:13:47,244][HYDRA] 	#108 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=anything_regret data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=8
[rank: 0] Seed set to 8
[2024-05-30 14:13:47,613][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 14:13:47,617][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 14:13:47,619][train.py][INFO] - Instantiating callbacks...
[2024-05-30 14:13:47,620][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 14:13:47,624][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 14:13:47,625][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 14:13:47,626][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 14:13:47,627][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 14:13:47,628][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 14:13:47,631][train.py][INFO] - Instantiating loggers...
[2024-05-30 14:13:47,631][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 14:13:47,633][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 14:13:47,764][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/anything_regret/0.15/8/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.68it/s v_num: 0.000      
                                                              val/auc: 0.389    
                                                              val/f1: 0.353     
                                                              val/precision:    
                                                              0.375 val/recall: 
                                                              0.333 val/mre:    
                                                              0.105 train/auc:  
                                                              0.857 train/f1:   
                                                              0.857             
                                                              train/precision:  
                                                              0.857             
                                                              train/recall:     
                                                              0.857 train/mre:  
                                                              0.100             
[2024-05-30 14:15:27,723][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/anything_regret/0.15/8>
[2024-05-30 14:15:27,724][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 14:15:27,727][HYDRA] 	#109 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=anything_regret data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=9
[rank: 0] Seed set to 9
[2024-05-30 14:15:28,030][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 14:15:28,033][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 14:15:28,035][train.py][INFO] - Instantiating callbacks...
[2024-05-30 14:15:28,035][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 14:15:28,039][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 14:15:28,039][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 14:15:28,040][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 14:15:28,041][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 14:15:28,043][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 14:15:28,046][train.py][INFO] - Instantiating loggers...
[2024-05-30 14:15:28,046][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 14:15:28,049][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 14:15:28,135][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/anything_regret/0.15/9/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.66it/s v_num: 0.000      
                                                              val/auc: 0.444    
                                                              val/f1: 0.545     
                                                              val/precision:    
                                                              0.462 val/recall: 
                                                              0.667 val/mre:    
                                                              0.105 train/auc:  
                                                              0.786 train/f1:   
                                                              0.812             
                                                              train/precision:  
                                                              0.722             
                                                              train/recall:     
                                                              0.929 train/mre:  
                                                              0.101             
[2024-05-30 14:17:09,137][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/anything_regret/0.15/9>
[2024-05-30 14:17:09,138][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 14:17:09,141][HYDRA] 	#110 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=anything_regret data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=0
[2024-05-30 14:17:09,437][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 14:17:09,440][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 14:17:09,442][train.py][INFO] - Instantiating callbacks...
[2024-05-30 14:17:09,443][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 14:17:09,446][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 14:17:09,447][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 14:17:09,448][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 14:17:09,449][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 14:17:09,451][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 14:17:09,452][train.py][INFO] - Instantiating loggers...
[2024-05-30 14:17:09,452][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 14:17:09,454][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 14:17:09,497][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/anything_regret/0.2/0/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.68it/s v_num: 0.000      
                                                              val/auc: 0.556    
                                                              val/f1: 0.556     
                                                              val/precision:    
                                                              0.556 val/recall: 
                                                              0.556 val/mre:    
                                                              0.105 train/auc:  
                                                              0.750 train/f1:   
                                                              0.794             
                                                              train/precision:  
                                                              0.675             
                                                              train/recall:     
                                                              0.964 train/mre:  
                                                              0.101             
[2024-05-30 14:18:52,554][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/anything_regret/0.2/0>
[2024-05-30 14:18:52,555][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 14:18:52,558][HYDRA] 	#111 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=anything_regret data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=1
[rank: 0] Seed set to 1
[2024-05-30 14:18:52,853][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 14:18:52,855][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 14:18:52,857][train.py][INFO] - Instantiating callbacks...
[2024-05-30 14:18:52,858][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 14:18:52,861][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 14:18:52,862][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 14:18:52,863][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 14:18:52,864][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 14:18:52,866][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 14:18:52,867][train.py][INFO] - Instantiating loggers...
[2024-05-30 14:18:52,867][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 14:18:52,869][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 14:18:52,912][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/anything_regret/0.2/1/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.72it/s v_num: 0.000      
                                                              val/auc: 0.667    
                                                              val/f1: 0.625     
                                                              val/precision:    
                                                              0.714 val/recall: 
                                                              0.556 val/mre:    
                                                              0.108 train/auc:  
                                                              0.812 train/f1:   
                                                              0.796             
                                                              train/precision:  
                                                              0.872             
                                                              train/recall:     
                                                              0.732 train/mre:  
                                                              0.105             
[2024-05-30 14:20:34,879][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/anything_regret/0.2/1>
[2024-05-30 14:20:34,879][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 14:20:34,883][HYDRA] 	#112 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=anything_regret data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=2
[rank: 0] Seed set to 2
[2024-05-30 14:20:35,168][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 14:20:35,170][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 14:20:35,172][train.py][INFO] - Instantiating callbacks...
[2024-05-30 14:20:35,173][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 14:20:35,176][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 14:20:35,177][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 14:20:35,178][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 14:20:35,178][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 14:20:35,180][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 14:20:35,182][train.py][INFO] - Instantiating loggers...
[2024-05-30 14:20:35,182][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 14:20:35,184][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 14:20:35,227][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/anything_regret/0.2/2/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.69it/s v_num: 0.000      
                                                              val/auc: 0.444    
                                                              val/f1: 0.545     
                                                              val/precision:    
                                                              0.462 val/recall: 
                                                              0.667 val/mre:    
                                                              0.105 train/auc:  
                                                              0.786 train/f1:   
                                                              0.824             
                                                              train/precision:  
                                                              0.700             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.102             
[2024-05-30 14:22:16,828][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/anything_regret/0.2/2>
[2024-05-30 14:22:16,828][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 14:22:16,831][HYDRA] 	#113 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=anything_regret data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=3
[rank: 0] Seed set to 3
[2024-05-30 14:22:17,118][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 14:22:17,121][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 14:22:17,123][train.py][INFO] - Instantiating callbacks...
[2024-05-30 14:22:17,123][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 14:22:17,126][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 14:22:17,127][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 14:22:17,128][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 14:22:17,129][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 14:22:17,131][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 14:22:17,131][train.py][INFO] - Instantiating loggers...
[2024-05-30 14:22:17,132][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 14:22:17,134][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 14:22:17,176][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/anything_regret/0.2/3/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.70it/s v_num: 0.000      
                                                              val/auc: 0.667    
                                                              val/f1: 0.667     
                                                              val/precision:    
                                                              0.667 val/recall: 
                                                              0.667 val/mre:    
                                                              0.105 train/auc:  
                                                              0.750 train/f1:   
                                                              0.785             
                                                              train/precision:  
                                                              0.689             
                                                              train/recall:     
                                                              0.911 train/mre:  
                                                              0.102             
[2024-05-30 14:23:57,971][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/anything_regret/0.2/3>
[2024-05-30 14:23:57,972][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 14:23:57,975][HYDRA] 	#114 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=anything_regret data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=4
[rank: 0] Seed set to 4
[2024-05-30 14:23:58,276][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 14:23:58,278][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 14:23:58,280][train.py][INFO] - Instantiating callbacks...
[2024-05-30 14:23:58,281][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 14:23:58,284][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 14:23:58,285][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 14:23:58,286][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 14:23:58,286][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 14:23:58,288][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 14:23:58,289][train.py][INFO] - Instantiating loggers...
[2024-05-30 14:23:58,289][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 14:23:58,292][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 14:23:58,338][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/anything_regret/0.2/4/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.71it/s v_num: 0.000      
                                                              val/auc: 0.500    
                                                              val/f1: 0.571     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.667 val/mre:    
                                                              0.105 train/auc:  
                                                              0.768 train/f1:   
                                                              0.787             
                                                              train/precision:  
                                                              0.727             
                                                              train/recall:     
                                                              0.857 train/mre:  
                                                              0.101             
[2024-05-30 14:25:38,796][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/anything_regret/0.2/4>
[2024-05-30 14:25:38,797][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 14:25:38,802][HYDRA] 	#115 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=anything_regret data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=5
[rank: 0] Seed set to 5
[2024-05-30 14:25:39,126][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 14:25:39,129][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 14:25:39,131][train.py][INFO] - Instantiating callbacks...
[2024-05-30 14:25:39,131][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 14:25:39,135][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 14:25:39,136][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 14:25:39,136][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 14:25:39,137][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 14:25:39,139][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 14:25:39,142][train.py][INFO] - Instantiating loggers...
[2024-05-30 14:25:39,142][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 14:25:39,144][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 14:25:39,258][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/anything_regret/0.2/5/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.66it/s v_num: 0.000      
                                                              val/auc: 0.556    
                                                              val/f1: 0.600     
                                                              val/precision:    
                                                              0.545 val/recall: 
                                                              0.667 val/mre:    
                                                              0.106 train/auc:  
                                                              0.804 train/f1:   
                                                              0.825             
                                                              train/precision:  
                                                              0.743             
                                                              train/recall:     
                                                              0.929 train/mre:  
                                                              0.102             
[2024-05-30 14:27:18,665][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/anything_regret/0.2/5>
[2024-05-30 14:27:18,665][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 14:27:18,669][HYDRA] 	#116 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=anything_regret data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=6
[rank: 0] Seed set to 6
[2024-05-30 14:27:18,973][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 14:27:18,975][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 14:27:18,978][train.py][INFO] - Instantiating callbacks...
[2024-05-30 14:27:18,978][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 14:27:18,981][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 14:27:18,982][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 14:27:18,983][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 14:27:18,984][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 14:27:18,985][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 14:27:18,988][train.py][INFO] - Instantiating loggers...
[2024-05-30 14:27:18,989][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 14:27:18,991][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 14:27:19,077][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/anything_regret/0.2/6/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.74it/s v_num: 0.000      
                                                              val/auc: 0.500    
                                                              val/f1: 0.571     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.667 val/mre:    
                                                              0.104 train/auc:  
                                                              0.723 train/f1:   
                                                              0.770             
                                                              train/precision:  
                                                              0.658             
                                                              train/recall:     
                                                              0.929 train/mre:  
                                                              0.100             
[2024-05-30 14:28:58,844][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/anything_regret/0.2/6>
[2024-05-30 14:28:58,845][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 14:28:58,848][HYDRA] 	#117 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=anything_regret data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=7
[rank: 0] Seed set to 7
[2024-05-30 14:28:59,137][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 14:28:59,140][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 14:28:59,143][train.py][INFO] - Instantiating callbacks...
[2024-05-30 14:28:59,143][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 14:28:59,147][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 14:28:59,148][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 14:28:59,149][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 14:28:59,149][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 14:28:59,151][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 14:28:59,152][train.py][INFO] - Instantiating loggers...
[2024-05-30 14:28:59,152][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 14:28:59,156][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 14:28:59,199][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/anything_regret/0.2/7/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.61it/s v_num: 0.000      
                                                              val/auc: 0.389    
                                                              val/f1: 0.476     
                                                              val/precision:    
                                                              0.417 val/recall: 
                                                              0.556 val/mre:    
                                                              0.102 train/auc:  
                                                              0.714 train/f1:   
                                                              0.758             
                                                              train/precision:  
                                                              0.658             
                                                              train/recall:     
                                                              0.893 train/mre:  
                                                              0.097             
[2024-05-30 14:30:38,928][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/anything_regret/0.2/7>
[2024-05-30 14:30:38,929][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 14:30:38,933][HYDRA] 	#118 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=anything_regret data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=8
[rank: 0] Seed set to 8
[2024-05-30 14:30:39,282][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 14:30:39,284][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 14:30:39,286][train.py][INFO] - Instantiating callbacks...
[2024-05-30 14:30:39,287][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 14:30:39,291][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 14:30:39,292][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 14:30:39,293][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 14:30:39,294][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 14:30:39,295][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 14:30:39,297][train.py][INFO] - Instantiating loggers...
[2024-05-30 14:30:39,297][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 14:30:39,299][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 14:30:39,409][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/anything_regret/0.2/8/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.60it/s v_num: 0.000      
                                                              val/auc: 0.500    
                                                              val/f1: 0.526     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.556 val/mre:    
                                                              0.107 train/auc:  
                                                              0.839 train/f1:   
                                                              0.850             
                                                              train/precision:  
                                                              0.797             
                                                              train/recall:     
                                                              0.911 train/mre:  
                                                              0.104             
[2024-05-30 14:32:21,317][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/anything_regret/0.2/8>
[2024-05-30 14:32:21,318][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 14:32:21,321][HYDRA] 	#119 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=anything_regret data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=9
[rank: 0] Seed set to 9
[2024-05-30 14:32:21,639][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 14:32:21,642][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 14:32:21,644][train.py][INFO] - Instantiating callbacks...
[2024-05-30 14:32:21,645][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 14:32:21,648][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 14:32:21,649][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 14:32:21,650][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 14:32:21,651][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 14:32:21,653][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 14:32:21,654][train.py][INFO] - Instantiating loggers...
[2024-05-30 14:32:21,654][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 14:32:21,656][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 14:32:21,699][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/anything_regret/0.2/9/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.66it/s v_num: 0.000      
                                                              val/auc: 0.611    
                                                              val/f1: 0.588     
                                                              val/precision:    
                                                              0.625 val/recall: 
                                                              0.556 val/mre:    
                                                              0.107 train/auc:  
                                                              0.804 train/f1:   
                                                              0.820             
                                                              train/precision:  
                                                              0.758             
                                                              train/recall:     
                                                              0.893 train/mre:  
                                                              0.104             
[2024-05-30 14:34:04,970][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/anything_regret/0.2/9>
[2024-05-30 14:34:04,971][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 14:34:04,974][HYDRA] 	#120 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=anything_regret data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=0
[2024-05-30 14:34:05,263][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 14:34:05,265][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 14:34:05,267][train.py][INFO] - Instantiating callbacks...
[2024-05-30 14:34:05,268][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 14:34:05,271][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 14:34:05,272][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 14:34:05,273][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 14:34:05,274][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 14:34:05,275][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 14:34:05,276][train.py][INFO] - Instantiating loggers...
[2024-05-30 14:34:05,277][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 14:34:05,279][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 14:34:05,321][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/anything_regret/0.25/0/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.70it/s v_num: 0.000      
                                                              val/auc: 0.500    
                                                              val/f1: 0.526     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.556 val/mre:    
                                                              0.109 train/auc:  
                                                              0.786 train/f1:   
                                                              0.815             
                                                              train/precision:  
                                                              0.716             
                                                              train/recall:     
                                                              0.946 train/mre:  
                                                              0.106             
[2024-05-30 14:35:48,776][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/anything_regret/0.25/0>
[2024-05-30 14:35:48,777][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 14:35:48,781][HYDRA] 	#121 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=anything_regret data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=1
[rank: 0] Seed set to 1
[2024-05-30 14:35:49,107][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 14:35:49,109][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 14:35:49,111][train.py][INFO] - Instantiating callbacks...
[2024-05-30 14:35:49,112][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 14:35:49,115][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 14:35:49,116][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 14:35:49,117][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 14:35:49,118][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 14:35:49,120][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 14:35:49,120][train.py][INFO] - Instantiating loggers...
[2024-05-30 14:35:49,121][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 14:35:49,123][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 14:35:49,174][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/anything_regret/0.25/1/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.67it/s v_num: 0.000      
                                                              val/auc: 0.667    
                                                              val/f1: 0.667     
                                                              val/precision:    
                                                              0.667 val/recall: 
                                                              0.667 val/mre:    
                                                              0.108 train/auc:  
                                                              0.759 train/f1:   
                                                              0.773             
                                                              train/precision:  
                                                              0.730             
                                                              train/recall:     
                                                              0.821 train/mre:  
                                                              0.106             
[2024-05-30 14:37:31,369][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/anything_regret/0.25/1>
[2024-05-30 14:37:31,370][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 14:37:31,373][HYDRA] 	#122 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=anything_regret data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=2
[rank: 0] Seed set to 2
[2024-05-30 14:37:31,658][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 14:37:31,660][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 14:37:31,663][train.py][INFO] - Instantiating callbacks...
[2024-05-30 14:37:31,663][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 14:37:31,666][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 14:37:31,667][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 14:37:31,668][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 14:37:31,668][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 14:37:31,670][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 14:37:31,672][train.py][INFO] - Instantiating loggers...
[2024-05-30 14:37:31,672][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 14:37:31,674][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 14:37:31,716][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/anything_regret/0.25/2/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.57it/s v_num: 0.000      
                                                              val/auc: 0.500    
                                                              val/f1: 0.571     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.667 val/mre:    
                                                              0.108 train/auc:  
                                                              0.795 train/f1:   
                                                              0.830             
                                                              train/precision:  
                                                              0.709             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.105             
[2024-05-30 14:39:14,897][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/anything_regret/0.25/2>
[2024-05-30 14:39:14,898][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 14:39:14,901][HYDRA] 	#123 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=anything_regret data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=3
[rank: 0] Seed set to 3
[2024-05-30 14:39:15,218][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 14:39:15,221][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 14:39:15,223][train.py][INFO] - Instantiating callbacks...
[2024-05-30 14:39:15,224][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 14:39:15,228][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 14:39:15,229][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 14:39:15,230][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 14:39:15,230][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 14:39:15,232][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 14:39:15,260][train.py][INFO] - Instantiating loggers...
[2024-05-30 14:39:15,261][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 14:39:15,265][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 14:39:15,388][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/anything_regret/0.25/3/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.58it/s v_num: 0.000      
                                                              val/auc: 0.556    
                                                              val/f1: 0.600     
                                                              val/precision:    
                                                              0.545 val/recall: 
                                                              0.667 val/mre:    
                                                              0.108 train/auc:  
                                                              0.786 train/f1:   
                                                              0.818             
                                                              train/precision:  
                                                              0.711             
                                                              train/recall:     
                                                              0.964 train/mre:  
                                                              0.105             
[2024-05-30 14:40:58,317][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/anything_regret/0.25/3>
[2024-05-30 14:40:58,318][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 14:40:58,321][HYDRA] 	#124 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=anything_regret data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=4
[rank: 0] Seed set to 4
[2024-05-30 14:40:58,632][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 14:40:58,635][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 14:40:58,637][train.py][INFO] - Instantiating callbacks...
[2024-05-30 14:40:58,637][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 14:40:58,641][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 14:40:58,641][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 14:40:58,642][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 14:40:58,643][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 14:40:58,645][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 14:40:58,648][train.py][INFO] - Instantiating loggers...
[2024-05-30 14:40:58,649][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 14:40:58,651][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 14:40:58,757][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/anything_regret/0.25/4/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.68it/s v_num: 0.000      
                                                              val/auc: 0.556    
                                                              val/f1: 0.556     
                                                              val/precision:    
                                                              0.556 val/recall: 
                                                              0.556 val/mre:    
                                                              0.107 train/auc:  
                                                              0.768 train/f1:   
                                                              0.797             
                                                              train/precision:  
                                                              0.708             
                                                              train/recall:     
                                                              0.911 train/mre:  
                                                              0.104             
[2024-05-30 14:42:38,554][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/anything_regret/0.25/4>
[2024-05-30 14:42:38,555][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 14:42:38,558][HYDRA] 	#125 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=anything_regret data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=5
[rank: 0] Seed set to 5
[2024-05-30 14:42:38,856][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 14:42:38,858][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 14:42:38,860][train.py][INFO] - Instantiating callbacks...
[2024-05-30 14:42:38,861][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 14:42:38,864][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 14:42:38,865][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 14:42:38,866][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 14:42:38,866][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 14:42:38,868][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 14:42:38,872][train.py][INFO] - Instantiating loggers...
[2024-05-30 14:42:38,872][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 14:42:38,874][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 14:42:38,968][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/anything_regret/0.25/5/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.70it/s v_num: 0.000      
                                                              val/auc: 0.611    
                                                              val/f1: 0.588     
                                                              val/precision:    
                                                              0.625 val/recall: 
                                                              0.556 val/mre:    
                                                              0.107 train/auc:  
                                                              0.795 train/f1:   
                                                              0.827             
                                                              train/precision:  
                                                              0.714             
                                                              train/recall:     
                                                              0.982 train/mre:  
                                                              0.104             
[2024-05-30 14:44:18,825][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/anything_regret/0.25/5>
[2024-05-30 14:44:18,825][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 14:44:18,829][HYDRA] 	#126 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=anything_regret data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=6
[rank: 0] Seed set to 6
[2024-05-30 14:44:19,120][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 14:44:19,123][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 14:44:19,125][train.py][INFO] - Instantiating callbacks...
[2024-05-30 14:44:19,125][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 14:44:19,129][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 14:44:19,129][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 14:44:19,130][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 14:44:19,131][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 14:44:19,133][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 14:44:19,134][train.py][INFO] - Instantiating loggers...
[2024-05-30 14:44:19,134][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 14:44:19,136][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 14:44:19,178][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/anything_regret/0.25/6/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.67it/s v_num: 0.000      
                                                              val/auc: 0.556    
                                                              val/f1: 0.500     
                                                              val/precision:    
                                                              0.571 val/recall: 
                                                              0.444 val/mre:    
                                                              0.113 train/auc:  
                                                              0.893 train/f1:   
                                                              0.897             
                                                              train/precision:  
                                                              0.867             
                                                              train/recall:     
                                                              0.929 train/mre:  
                                                              0.110             
[2024-05-30 14:45:59,394][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/anything_regret/0.25/6>
[2024-05-30 14:45:59,394][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 14:45:59,397][HYDRA] 	#127 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=anything_regret data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=7
[rank: 0] Seed set to 7
[2024-05-30 14:45:59,686][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 14:45:59,688][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 14:45:59,690][train.py][INFO] - Instantiating callbacks...
[2024-05-30 14:45:59,691][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 14:45:59,694][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 14:45:59,695][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 14:45:59,696][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 14:45:59,696][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 14:45:59,698][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 14:45:59,699][train.py][INFO] - Instantiating loggers...
[2024-05-30 14:45:59,699][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 14:45:59,702][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 14:45:59,749][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/anything_regret/0.25/7/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.62it/s v_num: 0.000      
                                                              val/auc: 0.389    
                                                              val/f1: 0.476     
                                                              val/precision:    
                                                              0.417 val/recall: 
                                                              0.556 val/mre:    
                                                              0.104 train/auc:  
                                                              0.750 train/f1:   
                                                              0.788             
                                                              train/precision:  
                                                              0.684             
                                                              train/recall:     
                                                              0.929 train/mre:  
                                                              0.100             
[2024-05-30 14:47:42,818][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/anything_regret/0.25/7>
[2024-05-30 14:47:42,818][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 14:47:42,822][HYDRA] 	#128 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=anything_regret data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=8
[rank: 0] Seed set to 8
[2024-05-30 14:47:43,121][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 14:47:43,124][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 14:47:43,126][train.py][INFO] - Instantiating callbacks...
[2024-05-30 14:47:43,126][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 14:47:43,130][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 14:47:43,131][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 14:47:43,131][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 14:47:43,132][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 14:47:43,134][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 14:47:43,135][train.py][INFO] - Instantiating loggers...
[2024-05-30 14:47:43,135][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 14:47:43,137][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 14:47:43,180][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/anything_regret/0.25/8/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.67it/s v_num: 0.000      
                                                              val/auc: 0.611    
                                                              val/f1: 0.632     
                                                              val/precision:    
                                                              0.600 val/recall: 
                                                              0.667 val/mre:    
                                                              0.110 train/auc:  
                                                              0.857 train/f1:   
                                                              0.860             
                                                              train/precision:  
                                                              0.845             
                                                              train/recall:     
                                                              0.875 train/mre:  
                                                              0.107             
[2024-05-30 14:49:26,284][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/anything_regret/0.25/8>
[2024-05-30 14:49:26,285][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 14:49:26,288][HYDRA] 	#129 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=anything_regret data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=9
[rank: 0] Seed set to 9
[2024-05-30 14:49:26,741][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 14:49:26,743][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 14:49:26,746][train.py][INFO] - Instantiating callbacks...
[2024-05-30 14:49:26,746][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 14:49:26,750][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 14:49:26,751][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 14:49:26,751][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 14:49:26,752][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 14:49:26,754][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 14:49:26,755][train.py][INFO] - Instantiating loggers...
[2024-05-30 14:49:26,755][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 14:49:26,757][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 14:49:26,901][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/anything_regret/0.25/9/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.68it/s v_num: 0.000      
                                                              val/auc: 0.556    
                                                              val/f1: 0.600     
                                                              val/precision:    
                                                              0.545 val/recall: 
                                                              0.667 val/mre:    
                                                              0.110 train/auc:  
                                                              0.723 train/f1:   
                                                              0.756             
                                                              train/precision:  
                                                              0.676             
                                                              train/recall:     
                                                              0.857 train/mre:  
                                                              0.106             
[2024-05-30 14:51:10,415][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/anything_regret/0.25/9>
[2024-05-30 14:51:10,416][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 14:51:10,419][HYDRA] 	#130 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=anything_regret data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=0
[2024-05-30 14:51:10,887][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 14:51:10,890][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 14:51:10,892][train.py][INFO] - Instantiating callbacks...
[2024-05-30 14:51:10,892][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 14:51:10,898][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 14:51:10,899][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 14:51:10,900][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 14:51:10,900][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 14:51:10,902][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 14:51:10,904][train.py][INFO] - Instantiating loggers...
[2024-05-30 14:51:10,904][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 14:51:10,906][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 14:51:11,017][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/anything_regret/0.3/0/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.71it/s v_num: 0.000      
                                                              val/auc: 0.500    
                                                              val/f1: 0.526     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.556 val/mre:    
                                                              0.111 train/auc:  
                                                              0.741 train/f1:   
                                                              0.779             
                                                              train/precision:  
                                                              0.680             
                                                              train/recall:     
                                                              0.911 train/mre:  
                                                              0.108             
[2024-05-30 14:52:51,719][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/anything_regret/0.3/0>
[2024-05-30 14:52:51,720][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 14:52:51,723][HYDRA] 	#131 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=anything_regret data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=1
[rank: 0] Seed set to 1
[2024-05-30 14:52:52,006][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 14:52:52,008][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 14:52:52,010][train.py][INFO] - Instantiating callbacks...
[2024-05-30 14:52:52,010][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 14:52:52,014][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 14:52:52,015][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 14:52:52,015][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 14:52:52,016][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 14:52:52,018][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 14:52:52,019][train.py][INFO] - Instantiating loggers...
[2024-05-30 14:52:52,019][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 14:52:52,023][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 14:52:52,069][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/anything_regret/0.3/1/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.68it/s v_num: 0.000      
                                                              val/auc: 0.500    
                                                              val/f1: 0.471     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.444 val/mre:    
                                                              0.113 train/auc:  
                                                              0.821 train/f1:   
                                                              0.828             
                                                              train/precision:  
                                                              0.800             
                                                              train/recall:     
                                                              0.857 train/mre:  
                                                              0.111             
[2024-05-30 14:54:32,820][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/anything_regret/0.3/1>
[2024-05-30 14:54:32,821][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 14:54:32,825][HYDRA] 	#132 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=anything_regret data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=2
[rank: 0] Seed set to 2
[2024-05-30 14:54:33,132][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 14:54:33,134][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 14:54:33,136][train.py][INFO] - Instantiating callbacks...
[2024-05-30 14:54:33,137][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 14:54:33,140][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 14:54:33,141][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 14:54:33,142][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 14:54:33,143][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 14:54:33,144][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 14:54:33,169][train.py][INFO] - Instantiating loggers...
[2024-05-30 14:54:33,170][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 14:54:33,174][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 14:54:33,271][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/anything_regret/0.3/2/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.57it/s v_num: 0.000      
                                                              val/auc: 0.556    
                                                              val/f1: 0.600     
                                                              val/precision:    
                                                              0.545 val/recall: 
                                                              0.667 val/mre:    
                                                              0.112 train/auc:  
                                                              0.812 train/f1:   
                                                              0.842             
                                                              train/precision:  
                                                              0.727             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.110             
[2024-05-30 14:56:16,609][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/anything_regret/0.3/2>
[2024-05-30 14:56:16,610][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 14:56:16,613][HYDRA] 	#133 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=anything_regret data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=3
[rank: 0] Seed set to 3
[2024-05-30 14:56:16,923][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 14:56:16,925][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 14:56:16,927][train.py][INFO] - Instantiating callbacks...
[2024-05-30 14:56:16,928][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 14:56:16,931][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 14:56:16,932][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 14:56:16,933][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 14:56:16,934][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 14:56:16,935][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 14:56:16,939][train.py][INFO] - Instantiating loggers...
[2024-05-30 14:56:16,939][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 14:56:16,942][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 14:56:17,028][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/anything_regret/0.3/3/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.56it/s v_num: 0.000      
                                                              val/auc: 0.500    
                                                              val/f1: 0.571     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.667 val/mre:    
                                                              0.107 train/auc:  
                                                              0.670 train/f1:   
                                                              0.726             
                                                              train/precision:  
                                                              0.620             
                                                              train/recall:     
                                                              0.875 train/mre:  
                                                              0.104             
[2024-05-30 14:58:00,158][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/anything_regret/0.3/3>
[2024-05-30 14:58:00,158][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 14:58:00,161][HYDRA] 	#134 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=anything_regret data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=4
[rank: 0] Seed set to 4
[2024-05-30 14:58:00,477][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 14:58:00,479][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 14:58:00,481][train.py][INFO] - Instantiating callbacks...
[2024-05-30 14:58:00,482][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 14:58:00,485][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 14:58:00,486][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 14:58:00,487][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 14:58:00,487][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 14:58:00,489][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 14:58:00,493][train.py][INFO] - Instantiating loggers...
[2024-05-30 14:58:00,494][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 14:58:00,496][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 14:58:00,583][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/anything_regret/0.3/4/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.57it/s v_num: 0.000      
                                                              val/auc: 0.444    
                                                              val/f1: 0.545     
                                                              val/precision:    
                                                              0.462 val/recall: 
                                                              0.667 val/mre:    
                                                              0.110 train/auc:  
                                                              0.777 train/f1:   
                                                              0.803             
                                                              train/precision:  
                                                              0.718             
                                                              train/recall:     
                                                              0.911 train/mre:  
                                                              0.107             
[2024-05-30 14:59:43,657][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/anything_regret/0.3/4>
[2024-05-30 14:59:43,658][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 14:59:43,661][HYDRA] 	#135 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=anything_regret data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=5
[rank: 0] Seed set to 5
[2024-05-30 14:59:43,974][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 14:59:43,976][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 14:59:43,978][train.py][INFO] - Instantiating callbacks...
[2024-05-30 14:59:43,979][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 14:59:43,982][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 14:59:43,983][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 14:59:43,984][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 14:59:43,985][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 14:59:43,987][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 14:59:43,988][train.py][INFO] - Instantiating loggers...
[2024-05-30 14:59:43,988][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 14:59:43,990][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 14:59:44,035][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/anything_regret/0.3/5/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.59it/s v_num: 0.000      
                                                              val/auc: 0.500    
                                                              val/f1: 0.526     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.556 val/mre:    
                                                              0.115 train/auc:  
                                                              0.866 train/f1:   
                                                              0.878             
                                                              train/precision:  
                                                              0.806             
                                                              train/recall:     
                                                              0.964 train/mre:  
                                                              0.111             
[2024-05-30 15:01:26,267][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/anything_regret/0.3/5>
[2024-05-30 15:01:26,268][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 15:01:26,270][HYDRA] 	#136 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=anything_regret data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=6
[rank: 0] Seed set to 6
[2024-05-30 15:01:26,564][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 15:01:26,566][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 15:01:26,568][train.py][INFO] - Instantiating callbacks...
[2024-05-30 15:01:26,569][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 15:01:26,572][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 15:01:26,573][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 15:01:26,574][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 15:01:26,575][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 15:01:26,576][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 15:01:26,577][train.py][INFO] - Instantiating loggers...
[2024-05-30 15:01:26,578][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 15:01:26,580][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 15:01:26,625][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/anything_regret/0.3/6/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.60it/s v_num: 0.000      
                                                              val/auc: 0.611    
                                                              val/f1: 0.588     
                                                              val/precision:    
                                                              0.625 val/recall: 
                                                              0.556 val/mre:    
                                                              0.116 train/auc:  
                                                              0.893 train/f1:   
                                                              0.897             
                                                              train/precision:  
                                                              0.867             
                                                              train/recall:     
                                                              0.929 train/mre:  
                                                              0.113             
[2024-05-30 15:03:10,523][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/anything_regret/0.3/6>
[2024-05-30 15:03:10,524][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 15:03:10,527][HYDRA] 	#137 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=anything_regret data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=7
[rank: 0] Seed set to 7
[2024-05-30 15:03:10,868][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 15:03:10,871][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 15:03:10,873][train.py][INFO] - Instantiating callbacks...
[2024-05-30 15:03:10,873][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 15:03:10,877][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 15:03:10,878][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 15:03:10,879][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 15:03:10,880][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 15:03:10,882][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 15:03:10,883][train.py][INFO] - Instantiating loggers...
[2024-05-30 15:03:10,883][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 15:03:10,885][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 15:03:11,003][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/anything_regret/0.3/7/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.66it/s v_num: 0.000      
                                                              val/auc: 0.444    
                                                              val/f1: 0.500     
                                                              val/precision:    
                                                              0.455 val/recall: 
                                                              0.556 val/mre:    
                                                              0.106 train/auc:  
                                                              0.759 train/f1:   
                                                              0.794             
                                                              train/precision:  
                                                              0.693             
                                                              train/recall:     
                                                              0.929 train/mre:  
                                                              0.103             
[2024-05-30 15:04:52,932][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/anything_regret/0.3/7>
[2024-05-30 15:04:52,933][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 15:04:52,936][HYDRA] 	#138 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=anything_regret data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=8
[rank: 0] Seed set to 8
[2024-05-30 15:04:53,222][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 15:04:53,225][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 15:04:53,227][train.py][INFO] - Instantiating callbacks...
[2024-05-30 15:04:53,227][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 15:04:53,231][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 15:04:53,231][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 15:04:53,232][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 15:04:53,233][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 15:04:53,235][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 15:04:53,236][train.py][INFO] - Instantiating loggers...
[2024-05-30 15:04:53,236][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 15:04:53,238][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 15:04:53,280][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/anything_regret/0.3/8/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.62it/s v_num: 0.000      
                                                              val/auc: 0.500    
                                                              val/f1: 0.526     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.556 val/mre:    
                                                              0.113 train/auc:  
                                                              0.830 train/f1:   
                                                              0.846             
                                                              train/precision:  
                                                              0.776             
                                                              train/recall:     
                                                              0.929 train/mre:  
                                                              0.109             
[2024-05-30 15:06:36,736][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/anything_regret/0.3/8>
[2024-05-30 15:06:36,737][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 15:06:36,740][HYDRA] 	#139 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=anything_regret data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=9
[rank: 0] Seed set to 9
[2024-05-30 15:06:37,043][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 15:06:37,045][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 15:06:37,048][train.py][INFO] - Instantiating callbacks...
[2024-05-30 15:06:37,048][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 15:06:37,052][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 15:06:37,052][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 15:06:37,053][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 15:06:37,054][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 15:06:37,056][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 15:06:37,057][train.py][INFO] - Instantiating loggers...
[2024-05-30 15:06:37,058][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 15:06:37,060][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 15:06:37,105][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/anything_regret/0.3/9/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.53it/s v_num: 0.000      
                                                              val/auc: 0.556    
                                                              val/f1: 0.600     
                                                              val/precision:    
                                                              0.545 val/recall: 
                                                              0.667 val/mre:    
                                                              0.114 train/auc:  
                                                              0.786 train/f1:   
                                                              0.800             
                                                              train/precision:  
                                                              0.750             
                                                              train/recall:     
                                                              0.857 train/mre:  
                                                              0.110             
[2024-05-30 15:08:20,036][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/anything_regret/0.3/9>
[2024-05-30 15:08:20,037][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 15:08:20,040][HYDRA] 	#140 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=argued_someone data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=0
[2024-05-30 15:08:20,346][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 15:08:20,349][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 15:08:20,352][train.py][INFO] - Instantiating callbacks...
[2024-05-30 15:08:20,353][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 15:08:20,357][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 15:08:20,358][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 15:08:20,359][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 15:08:20,359][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 15:08:20,361][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 15:08:20,362][train.py][INFO] - Instantiating loggers...
[2024-05-30 15:08:20,362][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 15:08:20,367][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 15:08:20,410][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/argued_someone/0.0/0/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.58it/s v_num: 0.000      
                                                              val/auc: 0.500    
                                                              val/f1: 0.286     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.200 val/mre: nan
                                                              train/auc: 0.903  
                                                              train/f1: 0.905   
                                                              train/precision:  
                                                              0.891             
                                                              train/recall:     
                                                              0.919 train/mre:  
                                                              nan               
[2024-05-30 15:10:07,873][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/argued_someone/0.0/0>
[2024-05-30 15:10:07,874][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 15:10:07,877][HYDRA] 	#141 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=argued_someone data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=1
[rank: 0] Seed set to 1
[2024-05-30 15:10:08,185][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 15:10:08,187][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 15:10:08,190][train.py][INFO] - Instantiating callbacks...
[2024-05-30 15:10:08,190][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 15:10:08,193][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 15:10:08,194][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 15:10:08,195][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 15:10:08,196][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 15:10:08,198][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 15:10:08,201][train.py][INFO] - Instantiating loggers...
[2024-05-30 15:10:08,201][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 15:10:08,203][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 15:10:08,297][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/argued_someone/0.0/1/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.45it/s v_num: 0.000      
                                                              val/auc: 0.550    
                                                              val/f1: 0.400     
                                                              val/precision:    
                                                              0.600 val/recall: 
                                                              0.300 val/mre: nan
                                                              train/auc: 0.887  
                                                              train/f1: 0.892   
                                                              train/precision:  
                                                              0.853             
                                                              train/recall:     
                                                              0.935 train/mre:  
                                                              nan               
[2024-05-30 15:11:55,148][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/argued_someone/0.0/1>
[2024-05-30 15:11:55,149][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 15:11:55,152][HYDRA] 	#142 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=argued_someone data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=2
[rank: 0] Seed set to 2
[2024-05-30 15:11:55,463][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 15:11:55,465][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 15:11:55,468][train.py][INFO] - Instantiating callbacks...
[2024-05-30 15:11:55,468][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 15:11:55,472][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 15:11:55,473][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 15:11:55,473][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 15:11:55,474][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 15:11:55,476][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 15:11:55,479][train.py][INFO] - Instantiating loggers...
[2024-05-30 15:11:55,480][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 15:11:55,484][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 15:11:55,579][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/argued_someone/0.0/2/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.62it/s v_num: 0.000      
                                                              val/auc: 0.500    
                                                              val/f1: 0.167     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.100 val/mre: nan
                                                              train/auc: 0.903  
                                                              train/f1: 0.905   
                                                              train/precision:  
                                                              0.891             
                                                              train/recall:     
                                                              0.919 train/mre:  
                                                              nan               
[2024-05-30 15:13:42,182][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/argued_someone/0.0/2>
[2024-05-30 15:13:42,182][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 15:13:42,185][HYDRA] 	#143 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=argued_someone data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=3
[rank: 0] Seed set to 3
[2024-05-30 15:13:42,475][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 15:13:42,477][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 15:13:42,479][train.py][INFO] - Instantiating callbacks...
[2024-05-30 15:13:42,480][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 15:13:42,483][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 15:13:42,484][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 15:13:42,485][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 15:13:42,485][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 15:13:42,487][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 15:13:42,488][train.py][INFO] - Instantiating loggers...
[2024-05-30 15:13:42,488][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 15:13:42,490][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 15:13:42,544][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/argued_someone/0.0/3/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.60it/s v_num: 0.000      
                                                              val/auc: 0.550    
                                                              val/f1: 0.400     
                                                              val/precision:    
                                                              0.600 val/recall: 
                                                              0.300 val/mre: nan
                                                              train/auc: 0.879  
                                                              train/f1: 0.878   
                                                              train/precision:  
                                                              0.885             
                                                              train/recall:     
                                                              0.871 train/mre:  
                                                              nan               
[2024-05-30 15:15:26,030][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/argued_someone/0.0/3>
[2024-05-30 15:15:26,031][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 15:15:26,035][HYDRA] 	#144 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=argued_someone data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=4
[rank: 0] Seed set to 4
[2024-05-30 15:15:26,328][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 15:15:26,330][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 15:15:26,332][train.py][INFO] - Instantiating callbacks...
[2024-05-30 15:15:26,333][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 15:15:26,336][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 15:15:26,337][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 15:15:26,337][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 15:15:26,338][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 15:15:26,340][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 15:15:26,341][train.py][INFO] - Instantiating loggers...
[2024-05-30 15:15:26,341][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 15:15:26,343][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 15:15:26,387][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/argued_someone/0.0/4/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.07it/s v_num: 0.000      
                                                              val/auc: 0.600    
                                                              val/f1: 0.429     
                                                              val/precision:    
                                                              0.750 val/recall: 
                                                              0.300 val/mre: nan
                                                              train/auc: 0.944  
                                                              train/f1: 0.945   
                                                              train/precision:  
                                                              0.923             
                                                              train/recall:     
                                                              0.968 train/mre:  
                                                              nan               
[2024-05-30 15:17:10,822][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/argued_someone/0.0/4>
[2024-05-30 15:17:10,823][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 15:17:10,825][HYDRA] 	#145 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=argued_someone data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=5
[rank: 0] Seed set to 5
[2024-05-30 15:17:11,111][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 15:17:11,113][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 15:17:11,115][train.py][INFO] - Instantiating callbacks...
[2024-05-30 15:17:11,116][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 15:17:11,119][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 15:17:11,120][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 15:17:11,121][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 15:17:11,121][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 15:17:11,123][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 15:17:11,124][train.py][INFO] - Instantiating loggers...
[2024-05-30 15:17:11,124][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 15:17:11,126][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 15:17:11,168][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/argued_someone/0.0/5/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.61it/s v_num: 0.000      
                                                              val/auc: 0.550    
                                                              val/f1: 0.182     
                                                              val/precision:    
                                                              1.000 val/recall: 
                                                              0.100 val/mre: nan
                                                              train/auc: 0.944  
                                                              train/f1: 0.944   
                                                              train/precision:  
                                                              0.937             
                                                              train/recall:     
                                                              0.952 train/mre:  
                                                              nan               
[2024-05-30 15:18:54,587][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/argued_someone/0.0/5>
[2024-05-30 15:18:54,590][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 15:18:54,600][HYDRA] 	#146 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=argued_someone data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=6
[rank: 0] Seed set to 6
[2024-05-30 15:18:54,891][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 15:18:54,894][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 15:18:54,896][train.py][INFO] - Instantiating callbacks...
[2024-05-30 15:18:54,896][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 15:18:54,899][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 15:18:54,900][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 15:18:54,901][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 15:18:54,902][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 15:18:54,904][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 15:18:54,905][train.py][INFO] - Instantiating loggers...
[2024-05-30 15:18:54,905][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 15:18:54,907][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 15:18:54,950][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/argued_someone/0.0/6/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.59it/s v_num: 0.000      
                                                              val/auc: 0.550    
                                                              val/f1: 0.471     
                                                              val/precision:    
                                                              0.571 val/recall: 
                                                              0.400 val/mre: nan
                                                              train/auc: 0.895  
                                                              train/f1: 0.904   
                                                              train/precision:  
                                                              0.836             
                                                              train/recall:     
                                                              0.984 train/mre:  
                                                              nan               
[2024-05-30 15:20:38,771][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/argued_someone/0.0/6>
[2024-05-30 15:20:38,772][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 15:20:38,776][HYDRA] 	#147 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=argued_someone data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=7
[rank: 0] Seed set to 7
[2024-05-30 15:20:39,078][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 15:20:39,081][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 15:20:39,083][train.py][INFO] - Instantiating callbacks...
[2024-05-30 15:20:39,084][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 15:20:39,088][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 15:20:39,089][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 15:20:39,089][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 15:20:39,090][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 15:20:39,092][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 15:20:39,095][train.py][INFO] - Instantiating loggers...
[2024-05-30 15:20:39,096][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 15:20:39,098][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 15:20:39,203][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/argued_someone/0.0/7/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.60it/s v_num: 0.000      
                                                              val/auc: 0.550    
                                                              val/f1: 0.471     
                                                              val/precision:    
                                                              0.571 val/recall: 
                                                              0.400 val/mre: nan
                                                              train/auc: 0.935  
                                                              train/f1: 0.938   
                                                              train/precision:  
                                                              0.897             
                                                              train/recall:     
                                                              0.984 train/mre:  
                                                              nan               
[2024-05-30 15:22:23,986][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/argued_someone/0.0/7>
[2024-05-30 15:22:23,987][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 15:22:23,990][HYDRA] 	#148 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=argued_someone data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=8
[rank: 0] Seed set to 8
[2024-05-30 15:22:24,296][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 15:22:24,299][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 15:22:24,301][train.py][INFO] - Instantiating callbacks...
[2024-05-30 15:22:24,301][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 15:22:24,305][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 15:22:24,305][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 15:22:24,306][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 15:22:24,307][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 15:22:24,309][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 15:22:24,312][train.py][INFO] - Instantiating loggers...
[2024-05-30 15:22:24,313][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 15:22:24,315][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 15:22:24,400][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/argued_someone/0.0/8/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.63it/s v_num: 0.000      
                                                              val/auc: 0.450    
                                                              val/f1: 0.154     
                                                              val/precision:    
                                                              0.333 val/recall: 
                                                              0.100 val/mre: nan
                                                              train/auc: 0.903  
                                                              train/f1: 0.908   
                                                              train/precision:  
                                                              0.868             
                                                              train/recall:     
                                                              0.952 train/mre:  
                                                              nan               
[2024-05-30 15:24:07,843][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/argued_someone/0.0/8>
[2024-05-30 15:24:07,844][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 15:24:07,847][HYDRA] 	#149 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=argued_someone data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=9
[rank: 0] Seed set to 9
[2024-05-30 15:24:08,137][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 15:24:08,139][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 15:24:08,142][train.py][INFO] - Instantiating callbacks...
[2024-05-30 15:24:08,142][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 15:24:08,145][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 15:24:08,146][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 15:24:08,147][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 15:24:08,148][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 15:24:08,149][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 15:24:08,150][train.py][INFO] - Instantiating loggers...
[2024-05-30 15:24:08,151][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 15:24:08,153][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 15:24:08,196][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/argued_someone/0.0/9/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.60it/s v_num: 0.000      
                                                              val/auc: 0.550    
                                                              val/f1: 0.400     
                                                              val/precision:    
                                                              0.600 val/recall: 
                                                              0.300 val/mre: nan
                                                              train/auc: 0.903  
                                                              train/f1: 0.909   
                                                              train/precision:  
                                                              0.857             
                                                              train/recall:     
                                                              0.968 train/mre:  
                                                              nan               
[2024-05-30 15:25:52,121][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/argued_someone/0.0/9>
[2024-05-30 15:25:52,122][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 15:25:52,127][HYDRA] 	#150 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=argued_someone data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=0
[2024-05-30 15:25:52,414][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 15:25:52,417][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 15:25:52,419][train.py][INFO] - Instantiating callbacks...
[2024-05-30 15:25:52,419][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 15:25:52,422][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 15:25:52,423][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 15:25:52,424][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 15:25:52,425][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 15:25:52,427][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 15:25:52,427][train.py][INFO] - Instantiating loggers...
[2024-05-30 15:25:52,428][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 15:25:52,430][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 15:25:52,472][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/argued_someone/0.05/0/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.61it/s v_num: 0.000      
                                                              val/auc: 0.400    
                                                              val/f1: 0.250     
                                                              val/precision:    
                                                              0.333 val/recall: 
                                                              0.200 val/mre:    
                                                              0.135 train/auc:  
                                                              0.895 train/f1:   
                                                              0.901             
                                                              train/precision:  
                                                              0.855             
                                                              train/recall:     
                                                              0.952 train/mre:  
                                                              0.129             
[2024-05-30 15:27:36,204][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/argued_someone/0.05/0>
[2024-05-30 15:27:36,205][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 15:27:36,208][HYDRA] 	#151 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=argued_someone data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=1
[rank: 0] Seed set to 1
[2024-05-30 15:27:36,495][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 15:27:36,497][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 15:27:36,499][train.py][INFO] - Instantiating callbacks...
[2024-05-30 15:27:36,500][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 15:27:36,503][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 15:27:36,504][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 15:27:36,504][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 15:27:36,505][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 15:27:36,507][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 15:27:36,508][train.py][INFO] - Instantiating loggers...
[2024-05-30 15:27:36,508][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 15:27:36,510][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 15:27:36,556][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/argued_someone/0.05/1/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.58it/s v_num: 0.000      
                                                              val/auc: 0.500    
                                                              val/f1: 0.375     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.300 val/mre:    
                                                              0.136 train/auc:  
                                                              0.879 train/f1:   
                                                              0.885             
                                                              train/precision:  
                                                              0.841             
                                                              train/recall:     
                                                              0.935 train/mre:  
                                                              0.131             
[2024-05-30 15:29:21,454][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/argued_someone/0.05/1>
[2024-05-30 15:29:21,470][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 15:29:21,480][HYDRA] 	#152 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=argued_someone data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=2
[rank: 0] Seed set to 2
[2024-05-30 15:29:22,168][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 15:29:22,171][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 15:29:22,173][train.py][INFO] - Instantiating callbacks...
[2024-05-30 15:29:22,173][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 15:29:22,177][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 15:29:22,177][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 15:29:22,178][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 15:29:22,179][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 15:29:22,181][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 15:29:22,181][train.py][INFO] - Instantiating loggers...
[2024-05-30 15:29:22,182][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 15:29:22,184][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 15:29:22,231][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/argued_someone/0.05/2/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.62it/s v_num: 0.000      
                                                              val/auc: 0.550    
                                                              val/f1: 0.308     
                                                              val/precision:    
                                                              0.667 val/recall: 
                                                              0.200 val/mre:    
                                                              0.138 train/auc:  
                                                              0.895 train/f1:   
                                                              0.899             
                                                              train/precision:  
                                                              0.866             
                                                              train/recall:     
                                                              0.935 train/mre:  
                                                              0.132             
[2024-05-30 15:31:07,912][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/argued_someone/0.05/2>
[2024-05-30 15:31:07,913][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 15:31:07,917][HYDRA] 	#153 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=argued_someone data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=3
[rank: 0] Seed set to 3
[2024-05-30 15:31:08,206][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 15:31:08,208][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 15:31:08,211][train.py][INFO] - Instantiating callbacks...
[2024-05-30 15:31:08,211][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 15:31:08,214][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 15:31:08,215][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 15:31:08,216][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 15:31:08,217][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 15:31:08,218][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 15:31:08,220][train.py][INFO] - Instantiating loggers...
[2024-05-30 15:31:08,220][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 15:31:08,222][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 15:31:08,269][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/argued_someone/0.05/3/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.61it/s v_num: 0.000      
                                                              val/auc: 0.550    
                                                              val/f1: 0.471     
                                                              val/precision:    
                                                              0.571 val/recall: 
                                                              0.400 val/mre:    
                                                              0.136 train/auc:  
                                                              0.847 train/f1:   
                                                              0.850             
                                                              train/precision:  
                                                              0.831             
                                                              train/recall:     
                                                              0.871 train/mre:  
                                                              0.130             
[2024-05-30 15:32:51,117][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/argued_someone/0.05/3>
[2024-05-30 15:32:51,118][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 15:32:51,121][HYDRA] 	#154 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=argued_someone data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=4
[rank: 0] Seed set to 4
[2024-05-30 15:32:51,423][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 15:32:51,426][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 15:32:51,428][train.py][INFO] - Instantiating callbacks...
[2024-05-30 15:32:51,428][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 15:32:51,432][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 15:32:51,432][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 15:32:51,433][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 15:32:51,434][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 15:32:51,436][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 15:32:51,439][train.py][INFO] - Instantiating loggers...
[2024-05-30 15:32:51,439][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 15:32:51,441][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 15:32:51,531][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/argued_someone/0.05/4/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.61it/s v_num: 0.000      
                                                              val/auc: 0.650    
                                                              val/f1: 0.533     
                                                              val/precision:    
                                                              0.800 val/recall: 
                                                              0.400 val/mre:    
                                                              0.142 train/auc:  
                                                              0.927 train/f1:   
                                                              0.929             
                                                              train/precision:  
                                                              0.908             
                                                              train/recall:     
                                                              0.952 train/mre:  
                                                              0.134             
[2024-05-30 15:34:36,675][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/argued_someone/0.05/4>
[2024-05-30 15:34:36,676][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 15:34:36,679][HYDRA] 	#155 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=argued_someone data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=5
[rank: 0] Seed set to 5
[2024-05-30 15:34:36,979][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 15:34:36,982][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 15:34:36,984][train.py][INFO] - Instantiating callbacks...
[2024-05-30 15:34:36,984][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 15:34:36,988][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 15:34:36,988][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 15:34:36,989][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 15:34:36,990][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 15:34:36,992][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 15:34:36,995][train.py][INFO] - Instantiating loggers...
[2024-05-30 15:34:36,995][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 15:34:36,997][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 15:34:37,080][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/argued_someone/0.05/5/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.65it/s v_num: 0.000      
                                                              val/auc: 0.500    
                                                              val/f1: 0.286     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.200 val/mre:    
                                                              0.146 train/auc:  
                                                              0.952 train/f1:   
                                                              0.952             
                                                              train/precision:  
                                                              0.938             
                                                              train/recall:     
                                                              0.968 train/mre:  
                                                              0.135             
[2024-05-30 15:36:21,296][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/argued_someone/0.05/5>
[2024-05-30 15:36:21,297][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 15:36:21,300][HYDRA] 	#156 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=argued_someone data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=6
[rank: 0] Seed set to 6
[2024-05-30 15:36:21,584][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 15:36:21,586][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 15:36:21,588][train.py][INFO] - Instantiating callbacks...
[2024-05-30 15:36:21,589][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 15:36:21,592][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 15:36:21,593][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 15:36:21,594][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 15:36:21,594][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 15:36:21,596][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 15:36:21,597][train.py][INFO] - Instantiating loggers...
[2024-05-30 15:36:21,597][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 15:36:21,599][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 15:36:21,641][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/argued_someone/0.05/6/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.61it/s v_num: 0.000      
                                                              val/auc: 0.550    
                                                              val/f1: 0.471     
                                                              val/precision:    
                                                              0.571 val/recall: 
                                                              0.400 val/mre:    
                                                              0.132 train/auc:  
                                                              0.855 train/f1:   
                                                              0.864             
                                                              train/precision:  
                                                              0.814             
                                                              train/recall:     
                                                              0.919 train/mre:  
                                                              0.124             
[2024-05-30 15:38:05,112][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/argued_someone/0.05/6>
[2024-05-30 15:38:05,113][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 15:38:05,116][HYDRA] 	#157 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=argued_someone data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=7
[rank: 0] Seed set to 7
[2024-05-30 15:38:05,407][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 15:38:05,409][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 15:38:05,411][train.py][INFO] - Instantiating callbacks...
[2024-05-30 15:38:05,412][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 15:38:05,415][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 15:38:05,416][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 15:38:05,417][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 15:38:05,418][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 15:38:05,419][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 15:38:05,420][train.py][INFO] - Instantiating loggers...
[2024-05-30 15:38:05,421][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 15:38:05,423][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 15:38:05,466][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/argued_someone/0.05/7/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.58it/s v_num: 0.000      
                                                              val/auc: 0.550    
                                                              val/f1: 0.526     
                                                              val/precision:    
                                                              0.556 val/recall: 
                                                              0.500 val/mre:    
                                                              0.137 train/auc:  
                                                              0.919 train/f1:   
                                                              0.925             
                                                              train/precision:  
                                                              0.861             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.128             
[2024-05-30 15:39:48,681][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/argued_someone/0.05/7>
[2024-05-30 15:39:48,682][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 15:39:48,685][HYDRA] 	#158 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=argued_someone data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=8
[rank: 0] Seed set to 8
[2024-05-30 15:39:48,986][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 15:39:48,988][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 15:39:48,990][train.py][INFO] - Instantiating callbacks...
[2024-05-30 15:39:48,991][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 15:39:48,994][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 15:39:48,995][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 15:39:48,995][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 15:39:48,996][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 15:39:48,998][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 15:39:48,999][train.py][INFO] - Instantiating loggers...
[2024-05-30 15:39:48,999][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 15:39:49,002][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 15:39:49,045][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/argued_someone/0.05/8/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.59it/s v_num: 0.000      
                                                              val/auc: 0.450    
                                                              val/f1: 0.267     
                                                              val/precision:    
                                                              0.400 val/recall: 
                                                              0.200 val/mre:    
                                                              0.138 train/auc:  
                                                              0.895 train/f1:   
                                                              0.898             
                                                              train/precision:  
                                                              0.877             
                                                              train/recall:     
                                                              0.919 train/mre:  
                                                              0.129             
[2024-05-30 15:41:32,778][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/argued_someone/0.05/8>
[2024-05-30 15:41:32,779][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 15:41:32,782][HYDRA] 	#159 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=argued_someone data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=9
[rank: 0] Seed set to 9
[2024-05-30 15:41:33,069][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 15:41:33,071][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 15:41:33,073][train.py][INFO] - Instantiating callbacks...
[2024-05-30 15:41:33,074][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 15:41:33,077][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 15:41:33,078][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 15:41:33,079][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 15:41:33,080][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 15:41:33,081][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 15:41:33,083][train.py][INFO] - Instantiating loggers...
[2024-05-30 15:41:33,083][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 15:41:33,085][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 15:41:33,128][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/argued_someone/0.05/9/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.57it/s v_num: 0.000      
                                                              val/auc: 0.500    
                                                              val/f1: 0.375     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.300 val/mre:    
                                                              0.132 train/auc:  
                                                              0.895 train/f1:   
                                                              0.904             
                                                              train/precision:  
                                                              0.836             
                                                              train/recall:     
                                                              0.984 train/mre:  
                                                              0.126             
[2024-05-30 15:43:17,827][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/argued_someone/0.05/9>
[2024-05-30 15:43:17,827][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 15:43:17,832][HYDRA] 	#160 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=argued_someone data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=0
[2024-05-30 15:43:18,138][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 15:43:18,141][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 15:43:18,143][train.py][INFO] - Instantiating callbacks...
[2024-05-30 15:43:18,143][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 15:43:18,147][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 15:43:18,147][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 15:43:18,148][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 15:43:18,149][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 15:43:18,151][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 15:43:18,157][train.py][INFO] - Instantiating loggers...
[2024-05-30 15:43:18,157][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 15:43:18,160][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 15:43:18,253][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/argued_someone/0.1/0/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.53it/s v_num: 0.000      
                                                              val/auc: 0.350    
                                                              val/f1: 0.133     
                                                              val/precision:    
                                                              0.200 val/recall: 
                                                              0.100 val/mre:    
                                                              0.141 train/auc:  
                                                              0.903 train/f1:   
                                                              0.910             
                                                              train/precision:  
                                                              0.847             
                                                              train/recall:     
                                                              0.984 train/mre:  
                                                              0.135             
[2024-05-30 15:45:02,775][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/argued_someone/0.1/0>
[2024-05-30 15:45:02,776][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 15:45:02,780][HYDRA] 	#161 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=argued_someone data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=1
[rank: 0] Seed set to 1
[2024-05-30 15:45:03,084][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 15:45:03,086][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 15:45:03,089][train.py][INFO] - Instantiating callbacks...
[2024-05-30 15:45:03,089][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 15:45:03,092][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 15:45:03,093][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 15:45:03,094][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 15:45:03,095][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 15:45:03,096][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 15:45:03,099][train.py][INFO] - Instantiating loggers...
[2024-05-30 15:45:03,100][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 15:45:03,102][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 15:45:03,187][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/argued_someone/0.1/1/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.62it/s v_num: 0.000      
                                                              val/auc: 0.450    
                                                              val/f1: 0.267     
                                                              val/precision:    
                                                              0.400 val/recall: 
                                                              0.200 val/mre:    
                                                              0.141 train/auc:  
                                                              0.847 train/f1:   
                                                              0.855             
                                                              train/precision:  
                                                              0.812             
                                                              train/recall:     
                                                              0.903 train/mre:  
                                                              0.133             
[2024-05-30 15:46:47,984][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/argued_someone/0.1/1>
[2024-05-30 15:46:47,985][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 15:46:47,988][HYDRA] 	#162 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=argued_someone data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=2
[rank: 0] Seed set to 2
[2024-05-30 15:46:48,377][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 15:46:48,380][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 15:46:48,382][train.py][INFO] - Instantiating callbacks...
[2024-05-30 15:46:48,382][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 15:46:48,388][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 15:46:48,388][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 15:46:48,389][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 15:46:48,390][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 15:46:48,392][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 15:46:48,392][train.py][INFO] - Instantiating loggers...
[2024-05-30 15:46:48,393][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 15:46:48,395][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 15:46:48,452][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/argued_someone/0.1/2/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.63it/s v_num: 0.000      
                                                              val/auc: 0.650    
                                                              val/f1: 0.533     
                                                              val/precision:    
                                                              0.800 val/recall: 
                                                              0.400 val/mre:    
                                                              0.142 train/auc:  
                                                              0.863 train/f1:   
                                                              0.864             
                                                              train/precision:  
                                                              0.857             
                                                              train/recall:     
                                                              0.871 train/mre:  
                                                              0.136             
[2024-05-30 15:48:32,296][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/argued_someone/0.1/2>
[2024-05-30 15:48:32,297][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 15:48:32,300][HYDRA] 	#163 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=argued_someone data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=3
[rank: 0] Seed set to 3
[2024-05-30 15:48:32,588][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 15:48:32,591][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 15:48:32,593][train.py][INFO] - Instantiating callbacks...
[2024-05-30 15:48:32,593][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 15:48:32,597][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 15:48:32,598][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 15:48:32,598][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 15:48:32,599][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 15:48:32,601][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 15:48:32,602][train.py][INFO] - Instantiating loggers...
[2024-05-30 15:48:32,602][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 15:48:32,604][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 15:48:32,646][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/argued_someone/0.1/3/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.59it/s v_num: 0.000      
                                                              val/auc: 0.550    
                                                              val/f1: 0.471     
                                                              val/precision:    
                                                              0.571 val/recall: 
                                                              0.400 val/mre:    
                                                              0.145 train/auc:  
                                                              0.879 train/f1:   
                                                              0.882             
                                                              train/precision:  
                                                              0.862             
                                                              train/recall:     
                                                              0.903 train/mre:  
                                                              0.136             
[2024-05-30 15:50:17,628][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/argued_someone/0.1/3>
[2024-05-30 15:50:17,628][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 15:50:17,632][HYDRA] 	#164 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=argued_someone data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=4
[rank: 0] Seed set to 4
[2024-05-30 15:50:17,931][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 15:50:17,933][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 15:50:17,936][train.py][INFO] - Instantiating callbacks...
[2024-05-30 15:50:17,936][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 15:50:17,939][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 15:50:17,940][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 15:50:17,941][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 15:50:17,942][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 15:50:17,944][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 15:50:17,945][train.py][INFO] - Instantiating loggers...
[2024-05-30 15:50:17,945][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 15:50:17,947][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 15:50:17,989][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/argued_someone/0.1/4/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.61it/s v_num: 0.000      
                                                              val/auc: 0.700    
                                                              val/f1: 0.667     
                                                              val/precision:    
                                                              0.750 val/recall: 
                                                              0.600 val/mre:    
                                                              0.149 train/auc:  
                                                              0.944 train/f1:   
                                                              0.946             
                                                              train/precision:  
                                                              0.910             
                                                              train/recall:     
                                                              0.984 train/mre:  
                                                              0.140             
[2024-05-30 15:52:01,619][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/argued_someone/0.1/4>
[2024-05-30 15:52:01,620][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 15:52:01,623][HYDRA] 	#165 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=argued_someone data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=5
[rank: 0] Seed set to 5
[2024-05-30 15:52:01,915][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 15:52:01,918][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 15:52:01,920][train.py][INFO] - Instantiating callbacks...
[2024-05-30 15:52:01,920][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 15:52:01,923][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 15:52:01,924][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 15:52:01,925][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 15:52:01,926][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 15:52:01,927][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 15:52:01,928][train.py][INFO] - Instantiating loggers...
[2024-05-30 15:52:01,929][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 15:52:01,931][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 15:52:01,973][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/argued_someone/0.1/5/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.05it/s v_num: 0.000      
                                                              val/auc: 0.400    
                                                              val/f1: 0.250     
                                                              val/precision:    
                                                              0.333 val/recall: 
                                                              0.200 val/mre:    
                                                              0.150 train/auc:  
                                                              0.960 train/f1:   
                                                              0.961             
                                                              train/precision:  
                                                              0.938             
                                                              train/recall:     
                                                              0.984 train/mre:  
                                                              0.143             
[2024-05-30 15:53:45,750][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/argued_someone/0.1/5>
[2024-05-30 15:53:45,751][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 15:53:45,754][HYDRA] 	#166 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=argued_someone data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=6
[rank: 0] Seed set to 6
[2024-05-30 15:53:46,040][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 15:53:46,042][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 15:53:46,044][train.py][INFO] - Instantiating callbacks...
[2024-05-30 15:53:46,045][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 15:53:46,048][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 15:53:46,049][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 15:53:46,050][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 15:53:46,050][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 15:53:46,052][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 15:53:46,053][train.py][INFO] - Instantiating loggers...
[2024-05-30 15:53:46,053][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 15:53:46,055][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 15:53:46,098][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/argued_someone/0.1/6/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.59it/s v_num: 0.000      
                                                              val/auc: 0.500    
                                                              val/f1: 0.375     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.300 val/mre:    
                                                              0.136 train/auc:  
                                                              0.855 train/f1:   
                                                              0.864             
                                                              train/precision:  
                                                              0.814             
                                                              train/recall:     
                                                              0.919 train/mre:  
                                                              0.128             
[2024-05-30 15:55:29,684][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/argued_someone/0.1/6>
[2024-05-30 15:55:29,685][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 15:55:29,688][HYDRA] 	#167 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=argued_someone data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=7
[rank: 0] Seed set to 7
[2024-05-30 15:55:29,989][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 15:55:29,992][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 15:55:29,995][train.py][INFO] - Instantiating callbacks...
[2024-05-30 15:55:29,995][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 15:55:29,999][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 15:55:30,000][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 15:55:30,001][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 15:55:30,001][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 15:55:30,003][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 15:55:30,019][train.py][INFO] - Instantiating loggers...
[2024-05-30 15:55:30,020][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 15:55:30,024][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 15:55:30,113][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/argued_someone/0.1/7/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.61it/s v_num: 0.000      
                                                              val/auc: 0.550    
                                                              val/f1: 0.526     
                                                              val/precision:    
                                                              0.556 val/recall: 
                                                              0.500 val/mre:    
                                                              0.145 train/auc:  
                                                              0.919 train/f1:   
                                                              0.925             
                                                              train/precision:  
                                                              0.861             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.137             
[2024-05-30 15:57:13,493][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/argued_someone/0.1/7>
[2024-05-30 15:57:13,494][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 15:57:13,497][HYDRA] 	#168 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=argued_someone data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=8
[rank: 0] Seed set to 8
[2024-05-30 15:57:13,800][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 15:57:13,804][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 15:57:13,807][train.py][INFO] - Instantiating callbacks...
[2024-05-30 15:57:13,807][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 15:57:13,811][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 15:57:13,812][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 15:57:13,812][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 15:57:13,813][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 15:57:13,815][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 15:57:13,819][train.py][INFO] - Instantiating loggers...
[2024-05-30 15:57:13,820][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 15:57:13,822][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 15:57:13,906][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/argued_someone/0.1/8/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.64it/s v_num: 0.000      
                                                              val/auc: 0.550    
                                                              val/f1: 0.526     
                                                              val/precision:    
                                                              0.556 val/recall: 
                                                              0.500 val/mre:    
                                                              0.148 train/auc:  
                                                              0.911 train/f1:   
                                                              0.915             
                                                              train/precision:  
                                                              0.881             
                                                              train/recall:     
                                                              0.952 train/mre:  
                                                              0.138             
[2024-05-30 15:58:58,237][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/argued_someone/0.1/8>
[2024-05-30 15:58:58,238][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 15:58:58,241][HYDRA] 	#169 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=argued_someone data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=9
[rank: 0] Seed set to 9
[2024-05-30 15:58:58,526][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 15:58:58,528][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 15:58:58,530][train.py][INFO] - Instantiating callbacks...
[2024-05-30 15:58:58,530][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 15:58:58,534][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 15:58:58,534][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 15:58:58,535][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 15:58:58,536][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 15:58:58,538][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 15:58:58,539][train.py][INFO] - Instantiating loggers...
[2024-05-30 15:58:58,539][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 15:58:58,541][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 15:58:58,584][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/argued_someone/0.1/9/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.57it/s v_num: 0.000      
                                                              val/auc: 0.550    
                                                              val/f1: 0.400     
                                                              val/precision:    
                                                              0.600 val/recall: 
                                                              0.300 val/mre:    
                                                              0.140 train/auc:  
                                                              0.863 train/f1:   
                                                              0.874             
                                                              train/precision:  
                                                              0.808             
                                                              train/recall:     
                                                              0.952 train/mre:  
                                                              0.132             
[2024-05-30 16:00:42,799][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/argued_someone/0.1/9>
[2024-05-30 16:00:42,799][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 16:00:42,805][HYDRA] 	#170 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=argued_someone data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=0
[2024-05-30 16:00:43,158][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 16:00:43,162][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 16:00:43,164][train.py][INFO] - Instantiating callbacks...
[2024-05-30 16:00:43,164][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 16:00:43,168][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 16:00:43,169][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 16:00:43,169][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 16:00:43,170][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 16:00:43,172][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 16:00:43,173][train.py][INFO] - Instantiating loggers...
[2024-05-30 16:00:43,173][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 16:00:43,175][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 16:00:43,230][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/argued_someone/0.15/0/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.60it/s v_num: 0.000      
                                                              val/auc: 0.450    
                                                              val/f1: 0.267     
                                                              val/precision:    
                                                              0.400 val/recall: 
                                                              0.200 val/mre:    
                                                              0.146 train/auc:  
                                                              0.903 train/f1:   
                                                              0.908             
                                                              train/precision:  
                                                              0.868             
                                                              train/recall:     
                                                              0.952 train/mre:  
                                                              0.140             
[2024-05-30 16:02:27,697][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/argued_someone/0.15/0>
[2024-05-30 16:02:27,697][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 16:02:27,700][HYDRA] 	#171 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=argued_someone data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=1
[rank: 0] Seed set to 1
[2024-05-30 16:02:27,985][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 16:02:27,987][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 16:02:27,989][train.py][INFO] - Instantiating callbacks...
[2024-05-30 16:02:27,989][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 16:02:27,993][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 16:02:27,994][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 16:02:27,994][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 16:02:27,995][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 16:02:27,997][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 16:02:27,998][train.py][INFO] - Instantiating loggers...
[2024-05-30 16:02:27,998][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 16:02:28,000][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 16:02:28,044][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/argued_someone/0.15/1/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.60it/s v_num: 0.000      
                                                              val/auc: 0.500    
                                                              val/f1: 0.500     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.500 val/mre:    
                                                              0.143 train/auc:  
                                                              0.847 train/f1:   
                                                              0.857             
                                                              train/precision:  
                                                              0.803             
                                                              train/recall:     
                                                              0.919 train/mre:  
                                                              0.136             
[2024-05-30 16:04:11,706][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/argued_someone/0.15/1>
[2024-05-30 16:04:11,706][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 16:04:11,710][HYDRA] 	#172 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=argued_someone data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=2
[rank: 0] Seed set to 2
[2024-05-30 16:04:11,997][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 16:04:11,999][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 16:04:12,001][train.py][INFO] - Instantiating callbacks...
[2024-05-30 16:04:12,002][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 16:04:12,005][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 16:04:12,006][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 16:04:12,007][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 16:04:12,008][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 16:04:12,009][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 16:04:12,010][train.py][INFO] - Instantiating loggers...
[2024-05-30 16:04:12,011][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 16:04:12,013][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 16:04:12,056][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/argued_someone/0.15/2/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.61it/s v_num: 0.000      
                                                              val/auc: 0.550    
                                                              val/f1: 0.471     
                                                              val/precision:    
                                                              0.571 val/recall: 
                                                              0.400 val/mre:    
                                                              0.144 train/auc:  
                                                              0.831 train/f1:   
                                                              0.840             
                                                              train/precision:  
                                                              0.797             
                                                              train/recall:     
                                                              0.887 train/mre:  
                                                              0.140             
[2024-05-30 16:05:55,527][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/argued_someone/0.15/2>
[2024-05-30 16:05:55,528][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 16:05:55,533][HYDRA] 	#173 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=argued_someone data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=3
[rank: 0] Seed set to 3
[2024-05-30 16:05:55,829][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 16:05:55,831][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 16:05:55,833][train.py][INFO] - Instantiating callbacks...
[2024-05-30 16:05:55,834][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 16:05:55,837][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 16:05:55,838][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 16:05:55,839][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 16:05:55,839][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 16:05:55,841][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 16:05:55,844][train.py][INFO] - Instantiating loggers...
[2024-05-30 16:05:55,845][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 16:05:55,847][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 16:05:55,941][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/argued_someone/0.15/3/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.59it/s v_num: 0.000      
                                                              val/auc: 0.450    
                                                              val/f1: 0.353     
                                                              val/precision:    
                                                              0.429 val/recall: 
                                                              0.300 val/mre:    
                                                              0.148 train/auc:  
                                                              0.879 train/f1:   
                                                              0.882             
                                                              train/precision:  
                                                              0.862             
                                                              train/recall:     
                                                              0.903 train/mre:  
                                                              0.140             
[2024-05-30 16:07:39,946][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/argued_someone/0.15/3>
[2024-05-30 16:07:39,947][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 16:07:39,950][HYDRA] 	#174 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=argued_someone data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=4
[rank: 0] Seed set to 4
[2024-05-30 16:07:40,254][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 16:07:40,257][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 16:07:40,259][train.py][INFO] - Instantiating callbacks...
[2024-05-30 16:07:40,260][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 16:07:40,264][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 16:07:40,265][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 16:07:40,265][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 16:07:40,266][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 16:07:40,268][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 16:07:40,272][train.py][INFO] - Instantiating loggers...
[2024-05-30 16:07:40,272][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 16:07:40,274][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 16:07:40,365][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/argued_someone/0.15/4/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.63it/s v_num: 0.000      
                                                              val/auc: 0.500    
                                                              val/f1: 0.500     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.500 val/mre:    
                                                              0.152 train/auc:  
                                                              0.895 train/f1:   
                                                              0.902             
                                                              train/precision:  
                                                              0.845             
                                                              train/recall:     
                                                              0.968 train/mre:  
                                                              0.144             
[2024-05-30 16:09:25,219][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/argued_someone/0.15/4>
[2024-05-30 16:09:25,219][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 16:09:25,222][HYDRA] 	#175 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=argued_someone data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=5
[rank: 0] Seed set to 5
[2024-05-30 16:09:25,509][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 16:09:25,511][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 16:09:25,513][train.py][INFO] - Instantiating callbacks...
[2024-05-30 16:09:25,514][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 16:09:25,517][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 16:09:25,518][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 16:09:25,518][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 16:09:25,519][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 16:09:25,521][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 16:09:25,522][train.py][INFO] - Instantiating loggers...
[2024-05-30 16:09:25,522][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 16:09:25,524][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 16:09:25,567][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/argued_someone/0.15/5/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.60it/s v_num: 0.000      
                                                              val/auc: 0.550    
                                                              val/f1: 0.308     
                                                              val/precision:    
                                                              0.667 val/recall: 
                                                              0.200 val/mre:    
                                                              0.157 train/auc:  
                                                              0.944 train/f1:   
                                                              0.945             
                                                              train/precision:  
                                                              0.923             
                                                              train/recall:     
                                                              0.968 train/mre:  
                                                              0.148             
[2024-05-30 16:11:10,177][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/argued_someone/0.15/5>
[2024-05-30 16:11:10,178][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 16:11:10,181][HYDRA] 	#176 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=argued_someone data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=6
[rank: 0] Seed set to 6
[2024-05-30 16:11:10,468][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 16:11:10,470][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 16:11:10,472][train.py][INFO] - Instantiating callbacks...
[2024-05-30 16:11:10,472][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 16:11:10,476][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 16:11:10,477][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 16:11:10,477][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 16:11:10,478][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 16:11:10,480][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 16:11:10,481][train.py][INFO] - Instantiating loggers...
[2024-05-30 16:11:10,481][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 16:11:10,483][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 16:11:10,526][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/argued_someone/0.15/6/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.60it/s v_num: 0.000      
                                                              val/auc: 0.550    
                                                              val/f1: 0.471     
                                                              val/precision:    
                                                              0.571 val/recall: 
                                                              0.400 val/mre:    
                                                              0.141 train/auc:  
                                                              0.871 train/f1:   
                                                              0.879             
                                                              train/precision:  
                                                              0.829             
                                                              train/recall:     
                                                              0.935 train/mre:  
                                                              0.132             
[2024-05-30 16:12:53,887][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/argued_someone/0.15/6>
[2024-05-30 16:12:53,888][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 16:12:53,891][HYDRA] 	#177 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=argued_someone data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=7
[rank: 0] Seed set to 7
[2024-05-30 16:12:54,177][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 16:12:54,179][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 16:12:54,181][train.py][INFO] - Instantiating callbacks...
[2024-05-30 16:12:54,182][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 16:12:54,185][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 16:12:54,186][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 16:12:54,186][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 16:12:54,187][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 16:12:54,189][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 16:12:54,190][train.py][INFO] - Instantiating loggers...
[2024-05-30 16:12:54,190][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 16:12:54,192][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 16:12:54,234][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/argued_someone/0.15/7/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.61it/s v_num: 0.000      
                                                              val/auc: 0.600    
                                                              val/f1: 0.600     
                                                              val/precision:    
                                                              0.600 val/recall: 
                                                              0.600 val/mre:    
                                                              0.150 train/auc:  
                                                              0.927 train/f1:   
                                                              0.932             
                                                              train/precision:  
                                                              0.873             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.140             
[2024-05-30 16:14:37,683][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/argued_someone/0.15/7>
[2024-05-30 16:14:37,684][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 16:14:37,687][HYDRA] 	#178 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=argued_someone data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=8
[rank: 0] Seed set to 8
[2024-05-30 16:14:37,970][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 16:14:37,973][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 16:14:37,975][train.py][INFO] - Instantiating callbacks...
[2024-05-30 16:14:37,975][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 16:14:37,979][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 16:14:37,979][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 16:14:37,980][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 16:14:37,981][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 16:14:37,983][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 16:14:37,983][train.py][INFO] - Instantiating loggers...
[2024-05-30 16:14:37,984][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 16:14:37,986][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 16:14:38,032][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/argued_someone/0.15/8/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.61it/s v_num: 0.000      
                                                              val/auc: 0.600    
                                                              val/f1: 0.600     
                                                              val/precision:    
                                                              0.600 val/recall: 
                                                              0.600 val/mre:    
                                                              0.152 train/auc:  
                                                              0.927 train/f1:   
                                                              0.928             
                                                              train/precision:  
                                                              0.921             
                                                              train/recall:     
                                                              0.935 train/mre:  
                                                              0.142             
[2024-05-30 16:16:22,266][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/argued_someone/0.15/8>
[2024-05-30 16:16:22,266][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 16:16:22,270][HYDRA] 	#179 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=argued_someone data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=9
[rank: 0] Seed set to 9
[2024-05-30 16:16:22,558][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 16:16:22,560][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 16:16:22,562][train.py][INFO] - Instantiating callbacks...
[2024-05-30 16:16:22,563][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 16:16:22,566][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 16:16:22,567][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 16:16:22,568][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 16:16:22,568][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 16:16:22,570][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 16:16:22,571][train.py][INFO] - Instantiating loggers...
[2024-05-30 16:16:22,571][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 16:16:22,573][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 16:16:22,615][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/argued_someone/0.15/9/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.59it/s v_num: 0.000      
                                                              val/auc: 0.500    
                                                              val/f1: 0.375     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.300 val/mre:    
                                                              0.146 train/auc:  
                                                              0.815 train/f1:   
                                                              0.839             
                                                              train/precision:  
                                                              0.741             
                                                              train/recall:     
                                                              0.968 train/mre:  
                                                              0.137             
[2024-05-30 16:18:06,148][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/argued_someone/0.15/9>
[2024-05-30 16:18:06,149][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 16:18:06,152][HYDRA] 	#180 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=argued_someone data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=0
[2024-05-30 16:18:06,450][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 16:18:06,452][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 16:18:06,454][train.py][INFO] - Instantiating callbacks...
[2024-05-30 16:18:06,455][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 16:18:06,458][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 16:18:06,459][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 16:18:06,460][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 16:18:06,461][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 16:18:06,462][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 16:18:06,465][train.py][INFO] - Instantiating loggers...
[2024-05-30 16:18:06,466][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 16:18:06,468][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 16:18:06,555][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/argued_someone/0.2/0/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.57it/s v_num: 0.000      
                                                              val/auc: 0.350    
                                                              val/f1: 0.133     
                                                              val/precision:    
                                                              0.200 val/recall: 
                                                              0.100 val/mre:    
                                                              0.149 train/auc:  
                                                              0.903 train/f1:   
                                                              0.906             
                                                              train/precision:  
                                                              0.879             
                                                              train/recall:     
                                                              0.935 train/mre:  
                                                              0.143             
[2024-05-30 16:19:51,202][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/argued_someone/0.2/0>
[2024-05-30 16:19:51,203][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 16:19:51,205][HYDRA] 	#181 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=argued_someone data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=1
[rank: 0] Seed set to 1
[2024-05-30 16:19:51,505][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 16:19:51,507][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 16:19:51,509][train.py][INFO] - Instantiating callbacks...
[2024-05-30 16:19:51,509][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 16:19:51,513][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 16:19:51,514][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 16:19:51,514][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 16:19:51,515][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 16:19:51,517][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 16:19:51,525][train.py][INFO] - Instantiating loggers...
[2024-05-30 16:19:51,526][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 16:19:51,529][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 16:19:51,626][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/argued_someone/0.2/1/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.61it/s v_num: 0.000      
                                                              val/auc: 0.450    
                                                              val/f1: 0.476     
                                                              val/precision:    
                                                              0.455 val/recall: 
                                                              0.500 val/mre:    
                                                              0.147 train/auc:  
                                                              0.855 train/f1:   
                                                              0.866             
                                                              train/precision:  
                                                              0.806             
                                                              train/recall:     
                                                              0.935 train/mre:  
                                                              0.141             
[2024-05-30 16:21:35,293][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/argued_someone/0.2/1>
[2024-05-30 16:21:35,297][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 16:21:35,307][HYDRA] 	#182 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=argued_someone data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=2
[rank: 0] Seed set to 2
[2024-05-30 16:21:35,709][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 16:21:35,711][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 16:21:35,713][train.py][INFO] - Instantiating callbacks...
[2024-05-30 16:21:35,714][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 16:21:35,717][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 16:21:35,718][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 16:21:35,719][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 16:21:35,719][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 16:21:35,721][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 16:21:35,722][train.py][INFO] - Instantiating loggers...
[2024-05-30 16:21:35,722][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 16:21:35,725][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 16:21:35,818][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/argued_someone/0.2/2/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.61it/s v_num: 0.000      
                                                              val/auc: 0.650    
                                                              val/f1: 0.533     
                                                              val/precision:    
                                                              0.800 val/recall: 
                                                              0.400 val/mre:    
                                                              0.149 train/auc:  
                                                              0.823 train/f1:   
                                                              0.831             
                                                              train/precision:  
                                                              0.794             
                                                              train/recall:     
                                                              0.871 train/mre:  
                                                              0.143             
[2024-05-30 16:23:20,685][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/argued_someone/0.2/2>
[2024-05-30 16:23:20,685][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 16:23:20,689][HYDRA] 	#183 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=argued_someone data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=3
[rank: 0] Seed set to 3
[2024-05-30 16:23:20,975][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 16:23:20,977][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 16:23:20,979][train.py][INFO] - Instantiating callbacks...
[2024-05-30 16:23:20,980][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 16:23:20,983][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 16:23:20,984][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 16:23:20,985][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 16:23:20,985][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 16:23:20,987][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 16:23:20,988][train.py][INFO] - Instantiating loggers...
[2024-05-30 16:23:20,988][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 16:23:20,990][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 16:23:21,334][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/argued_someone/0.2/3/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.61it/s v_num: 0.000      
                                                              val/auc: 0.500    
                                                              val/f1: 0.375     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.300 val/mre:    
                                                              0.153 train/auc:  
                                                              0.895 train/f1:   
                                                              0.901             
                                                              train/precision:  
                                                              0.855             
                                                              train/recall:     
                                                              0.952 train/mre:  
                                                              0.143             
[2024-05-30 16:25:05,269][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/argued_someone/0.2/3>
[2024-05-30 16:25:05,270][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 16:25:05,273][HYDRA] 	#184 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=argued_someone data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=4
[rank: 0] Seed set to 4
[2024-05-30 16:25:05,572][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 16:25:05,574][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 16:25:05,576][train.py][INFO] - Instantiating callbacks...
[2024-05-30 16:25:05,577][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 16:25:05,580][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 16:25:05,581][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 16:25:05,582][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 16:25:05,583][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 16:25:05,584][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 16:25:05,586][train.py][INFO] - Instantiating loggers...
[2024-05-30 16:25:05,586][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 16:25:05,588][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 16:25:05,636][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/argued_someone/0.2/4/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.60it/s v_num: 0.000      
                                                              val/auc: 0.550    
                                                              val/f1: 0.471     
                                                              val/precision:    
                                                              0.571 val/recall: 
                                                              0.400 val/mre:    
                                                              0.157 train/auc:  
                                                              0.879 train/f1:   
                                                              0.889             
                                                              train/precision:  
                                                              0.822             
                                                              train/recall:     
                                                              0.968 train/mre:  
                                                              0.148             
[2024-05-30 16:26:48,508][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/argued_someone/0.2/4>
[2024-05-30 16:26:48,509][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 16:26:48,512][HYDRA] 	#185 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=argued_someone data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=5
[rank: 0] Seed set to 5
[2024-05-30 16:26:48,799][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 16:26:48,802][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 16:26:48,804][train.py][INFO] - Instantiating callbacks...
[2024-05-30 16:26:48,804][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 16:26:48,807][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 16:26:48,808][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 16:26:48,809][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 16:26:48,810][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 16:26:48,811][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 16:26:48,812][train.py][INFO] - Instantiating loggers...
[2024-05-30 16:26:48,813][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 16:26:48,815][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 16:26:48,858][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/argued_someone/0.2/5/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.58it/s v_num: 0.000      
                                                              val/auc: 0.500    
                                                              val/f1: 0.286     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.200 val/mre:    
                                                              0.159 train/auc:  
                                                              0.935 train/f1:   
                                                              0.935             
                                                              train/precision:  
                                                              0.935             
                                                              train/recall:     
                                                              0.935 train/mre:  
                                                              0.151             
[2024-05-30 16:28:32,001][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/argued_someone/0.2/5>
[2024-05-30 16:28:32,002][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 16:28:32,005][HYDRA] 	#186 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=argued_someone data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=6
[rank: 0] Seed set to 6
[2024-05-30 16:28:32,319][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 16:28:32,323][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 16:28:32,326][train.py][INFO] - Instantiating callbacks...
[2024-05-30 16:28:32,327][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 16:28:32,330][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 16:28:32,331][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 16:28:32,332][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 16:28:32,333][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 16:28:32,334][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 16:28:32,338][train.py][INFO] - Instantiating loggers...
[2024-05-30 16:28:32,338][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 16:28:32,340][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 16:28:32,457][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/argued_someone/0.2/6/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.59it/s v_num: 0.000      
                                                              val/auc: 0.550    
                                                              val/f1: 0.471     
                                                              val/precision:    
                                                              0.571 val/recall: 
                                                              0.400 val/mre:    
                                                              0.145 train/auc:  
                                                              0.839 train/f1:   
                                                              0.855             
                                                              train/precision:  
                                                              0.776             
                                                              train/recall:     
                                                              0.952 train/mre:  
                                                              0.137             
[2024-05-30 16:30:16,333][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/argued_someone/0.2/6>
[2024-05-30 16:30:16,334][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 16:30:16,337][HYDRA] 	#187 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=argued_someone data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=7
[rank: 0] Seed set to 7
[2024-05-30 16:30:16,671][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 16:30:16,676][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 16:30:16,680][train.py][INFO] - Instantiating callbacks...
[2024-05-30 16:30:16,681][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 16:30:16,684][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 16:30:16,685][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 16:30:16,685][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 16:30:16,686][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 16:30:16,688][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 16:30:16,691][train.py][INFO] - Instantiating loggers...
[2024-05-30 16:30:16,691][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 16:30:16,694][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 16:30:16,779][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/argued_someone/0.2/7/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.06it/s v_num: 0.000      
                                                              val/auc: 0.450    
                                                              val/f1: 0.421     
                                                              val/precision:    
                                                              0.444 val/recall: 
                                                              0.400 val/mre:    
                                                              0.152 train/auc:  
                                                              0.911 train/f1:   
                                                              0.919             
                                                              train/precision:  
                                                              0.849             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.143             
[2024-05-30 16:32:00,524][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/argued_someone/0.2/7>
[2024-05-30 16:32:00,525][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 16:32:00,527][HYDRA] 	#188 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=argued_someone data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=8
[rank: 0] Seed set to 8
[2024-05-30 16:32:00,830][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 16:32:00,832][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 16:32:00,834][train.py][INFO] - Instantiating callbacks...
[2024-05-30 16:32:00,834][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 16:32:00,838][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 16:32:00,839][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 16:32:00,839][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 16:32:00,840][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 16:32:00,842][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 16:32:00,843][train.py][INFO] - Instantiating loggers...
[2024-05-30 16:32:00,843][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 16:32:00,845][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 16:32:00,964][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/argued_someone/0.2/8/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.62it/s v_num: 0.000      
                                                              val/auc: 0.550    
                                                              val/f1: 0.526     
                                                              val/precision:    
                                                              0.556 val/recall: 
                                                              0.500 val/mre:    
                                                              0.156 train/auc:  
                                                              0.935 train/f1:   
                                                              0.937             
                                                              train/precision:  
                                                              0.922             
                                                              train/recall:     
                                                              0.952 train/mre:  
                                                              0.146             
[2024-05-30 16:33:44,410][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/argued_someone/0.2/8>
[2024-05-30 16:33:44,411][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 16:33:44,414][HYDRA] 	#189 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=argued_someone data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=9
[rank: 0] Seed set to 9
[2024-05-30 16:33:44,704][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 16:33:44,707][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 16:33:44,709][train.py][INFO] - Instantiating callbacks...
[2024-05-30 16:33:44,709][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 16:33:44,713][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 16:33:44,714][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 16:33:44,714][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 16:33:44,715][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 16:33:44,717][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 16:33:44,718][train.py][INFO] - Instantiating loggers...
[2024-05-30 16:33:44,718][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 16:33:44,720][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 16:33:44,763][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/argued_someone/0.2/9/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.60it/s v_num: 0.000      
                                                              val/auc: 0.500    
                                                              val/f1: 0.375     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.300 val/mre:    
                                                              0.151 train/auc:  
                                                              0.831 train/f1:   
                                                              0.853             
                                                              train/precision:  
                                                              0.753             
                                                              train/recall:     
                                                              0.984 train/mre:  
                                                              0.143             
[2024-05-30 16:35:30,047][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/argued_someone/0.2/9>
[2024-05-30 16:35:30,048][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 16:35:30,051][HYDRA] 	#190 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=argued_someone data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=0
[2024-05-30 16:35:30,334][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 16:35:30,336][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 16:35:30,338][train.py][INFO] - Instantiating callbacks...
[2024-05-30 16:35:30,339][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 16:35:30,342][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 16:35:30,343][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 16:35:30,343][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 16:35:30,344][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 16:35:30,346][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 16:35:30,347][train.py][INFO] - Instantiating loggers...
[2024-05-30 16:35:30,347][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 16:35:30,349][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 16:35:30,391][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/argued_someone/0.25/0/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.58it/s v_num: 0.000      
                                                              val/auc: 0.450    
                                                              val/f1: 0.353     
                                                              val/precision:    
                                                              0.429 val/recall: 
                                                              0.300 val/mre:    
                                                              0.155 train/auc:  
                                                              0.919 train/f1:   
                                                              0.924             
                                                              train/precision:  
                                                              0.871             
                                                              train/recall:     
                                                              0.984 train/mre:  
                                                              0.148             
[2024-05-30 16:37:15,650][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/argued_someone/0.25/0>
[2024-05-30 16:37:15,650][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 16:37:15,654][HYDRA] 	#191 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=argued_someone data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=1
[rank: 0] Seed set to 1
[2024-05-30 16:37:16,025][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 16:37:16,027][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 16:37:16,030][train.py][INFO] - Instantiating callbacks...
[2024-05-30 16:37:16,030][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 16:37:16,033][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 16:37:16,034][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 16:37:16,035][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 16:37:16,036][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 16:37:16,038][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 16:37:16,038][train.py][INFO] - Instantiating loggers...
[2024-05-30 16:37:16,039][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 16:37:16,041][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 16:37:16,135][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/argued_someone/0.25/1/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.60it/s v_num: 0.000      
                                                              val/auc: 0.500    
                                                              val/f1: 0.545     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.600 val/mre:    
                                                              0.151 train/auc:  
                                                              0.823 train/f1:   
                                                              0.841             
                                                              train/precision:  
                                                              0.763             
                                                              train/recall:     
                                                              0.935 train/mre:  
                                                              0.143             
[2024-05-30 16:39:00,807][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/argued_someone/0.25/1>
[2024-05-30 16:39:00,807][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 16:39:00,811][HYDRA] 	#192 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=argued_someone data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=2
[rank: 0] Seed set to 2
[2024-05-30 16:39:01,117][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 16:39:01,120][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 16:39:01,123][train.py][INFO] - Instantiating callbacks...
[2024-05-30 16:39:01,123][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 16:39:01,127][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 16:39:01,128][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 16:39:01,129][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 16:39:01,129][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 16:39:01,131][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 16:39:01,135][train.py][INFO] - Instantiating loggers...
[2024-05-30 16:39:01,135][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 16:39:01,137][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 16:39:01,223][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/argued_someone/0.25/2/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.59it/s v_num: 0.000      
                                                              val/auc: 0.500    
                                                              val/f1: 0.286     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.200 val/mre:    
                                                              0.154 train/auc:  
                                                              0.798 train/f1:   
                                                              0.815             
                                                              train/precision:  
                                                              0.753             
                                                              train/recall:     
                                                              0.887 train/mre:  
                                                              0.148             
[2024-05-30 16:40:45,325][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/argued_someone/0.25/2>
[2024-05-30 16:40:45,326][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 16:40:45,328][HYDRA] 	#193 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=argued_someone data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=3
[rank: 0] Seed set to 3
[2024-05-30 16:40:45,624][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 16:40:45,627][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 16:40:45,629][train.py][INFO] - Instantiating callbacks...
[2024-05-30 16:40:45,630][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 16:40:45,634][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 16:40:45,635][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 16:40:45,635][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 16:40:45,636][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 16:40:45,638][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 16:40:45,641][train.py][INFO] - Instantiating loggers...
[2024-05-30 16:40:45,641][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 16:40:45,643][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 16:40:45,732][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/argued_someone/0.25/3/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.63it/s v_num: 0.000      
                                                              val/auc: 0.450    
                                                              val/f1: 0.353     
                                                              val/precision:    
                                                              0.429 val/recall: 
                                                              0.300 val/mre:    
                                                              0.158 train/auc:  
                                                              0.919 train/f1:   
                                                              0.923             
                                                              train/precision:  
                                                              0.882             
                                                              train/recall:     
                                                              0.968 train/mre:  
                                                              0.148             
[2024-05-30 16:42:29,277][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/argued_someone/0.25/3>
[2024-05-30 16:42:29,278][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 16:42:29,281][HYDRA] 	#194 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=argued_someone data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=4
[rank: 0] Seed set to 4
[2024-05-30 16:42:29,566][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 16:42:29,569][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 16:42:29,571][train.py][INFO] - Instantiating callbacks...
[2024-05-30 16:42:29,571][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 16:42:29,575][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 16:42:29,575][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 16:42:29,576][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 16:42:29,577][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 16:42:29,579][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 16:42:29,579][train.py][INFO] - Instantiating loggers...
[2024-05-30 16:42:29,580][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 16:42:29,582][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 16:42:29,624][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/argued_someone/0.25/4/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.65it/s v_num: 0.000      
                                                              val/auc: 0.550    
                                                              val/f1: 0.471     
                                                              val/precision:    
                                                              0.571 val/recall: 
                                                              0.400 val/mre:    
                                                              0.156 train/auc:  
                                                              0.895 train/f1:   
                                                              0.902             
                                                              train/precision:  
                                                              0.845             
                                                              train/recall:     
                                                              0.968 train/mre:  
                                                              0.149             
[2024-05-30 16:44:13,077][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/argued_someone/0.25/4>
[2024-05-30 16:44:13,078][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 16:44:13,081][HYDRA] 	#195 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=argued_someone data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=5
[rank: 0] Seed set to 5
[2024-05-30 16:44:13,363][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 16:44:13,365][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 16:44:13,367][train.py][INFO] - Instantiating callbacks...
[2024-05-30 16:44:13,368][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 16:44:13,374][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 16:44:13,374][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 16:44:13,375][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 16:44:13,376][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 16:44:13,378][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 16:44:13,379][train.py][INFO] - Instantiating loggers...
[2024-05-30 16:44:13,379][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 16:44:13,381][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 16:44:13,424][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/argued_someone/0.25/5/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.61it/s v_num: 0.000      
                                                              val/auc: 0.600    
                                                              val/f1: 0.500     
                                                              val/precision:    
                                                              0.667 val/recall: 
                                                              0.400 val/mre:    
                                                              0.163 train/auc:  
                                                              0.952 train/f1:   
                                                              0.952             
                                                              train/precision:  
                                                              0.938             
                                                              train/recall:     
                                                              0.968 train/mre:  
                                                              0.156             
[2024-05-30 16:45:57,974][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/argued_someone/0.25/5>
[2024-05-30 16:45:57,975][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 16:45:57,978][HYDRA] 	#196 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=argued_someone data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=6
[rank: 0] Seed set to 6
[2024-05-30 16:45:58,277][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 16:45:58,279][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 16:45:58,281][train.py][INFO] - Instantiating callbacks...
[2024-05-30 16:45:58,282][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 16:45:58,285][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 16:45:58,286][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 16:45:58,287][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 16:45:58,288][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 16:45:58,289][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 16:45:58,290][train.py][INFO] - Instantiating loggers...
[2024-05-30 16:45:58,291][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 16:45:58,293][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 16:45:58,336][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/argued_someone/0.25/6/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.53it/s v_num: 0.000      
                                                              val/auc: 0.600    
                                                              val/f1: 0.556     
                                                              val/precision:    
                                                              0.625 val/recall: 
                                                              0.500 val/mre:    
                                                              0.148 train/auc:  
                                                              0.815 train/f1:   
                                                              0.835             
                                                              train/precision:  
                                                              0.753             
                                                              train/recall:     
                                                              0.935 train/mre:  
                                                              0.139             
[2024-05-30 16:47:42,625][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/argued_someone/0.25/6>
[2024-05-30 16:47:42,626][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 16:47:42,629][HYDRA] 	#197 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=argued_someone data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=7
[rank: 0] Seed set to 7
[2024-05-30 16:47:42,919][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 16:47:42,921][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 16:47:42,923][train.py][INFO] - Instantiating callbacks...
[2024-05-30 16:47:42,924][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 16:47:42,927][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 16:47:42,928][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 16:47:42,929][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 16:47:42,930][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 16:47:42,931][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 16:47:42,932][train.py][INFO] - Instantiating loggers...
[2024-05-30 16:47:42,933][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 16:47:42,935][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 16:47:42,978][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/argued_someone/0.25/7/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.59it/s v_num: 0.000      
                                                              val/auc: 0.550    
                                                              val/f1: 0.526     
                                                              val/precision:    
                                                              0.556 val/recall: 
                                                              0.500 val/mre:    
                                                              0.154 train/auc:  
                                                              0.903 train/f1:   
                                                              0.910             
                                                              train/precision:  
                                                              0.847             
                                                              train/recall:     
                                                              0.984 train/mre:  
                                                              0.145             
[2024-05-30 16:49:28,936][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/argued_someone/0.25/7>
[2024-05-30 16:49:28,936][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 16:49:28,940][HYDRA] 	#198 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=argued_someone data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=8
[rank: 0] Seed set to 8
[2024-05-30 16:49:29,239][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 16:49:29,242][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 16:49:29,244][train.py][INFO] - Instantiating callbacks...
[2024-05-30 16:49:29,244][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 16:49:29,247][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 16:49:29,248][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 16:49:29,249][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 16:49:29,250][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 16:49:29,251][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 16:49:29,255][train.py][INFO] - Instantiating loggers...
[2024-05-30 16:49:29,255][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 16:49:29,257][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 16:49:29,483][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/argued_someone/0.25/8/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.60it/s v_num: 0.000      
                                                              val/auc: 0.550    
                                                              val/f1: 0.526     
                                                              val/precision:    
                                                              0.556 val/recall: 
                                                              0.500 val/mre:    
                                                              0.162 train/auc:  
                                                              0.927 train/f1:   
                                                              0.929             
                                                              train/precision:  
                                                              0.908             
                                                              train/recall:     
                                                              0.952 train/mre:  
                                                              0.151             
[2024-05-30 16:51:14,582][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/argued_someone/0.25/8>
[2024-05-30 16:51:14,583][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 16:51:14,586][HYDRA] 	#199 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=argued_someone data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=9
[rank: 0] Seed set to 9
[2024-05-30 16:51:14,909][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 16:51:14,912][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 16:51:14,915][train.py][INFO] - Instantiating callbacks...
[2024-05-30 16:51:14,915][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 16:51:14,919][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 16:51:14,919][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 16:51:14,920][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 16:51:14,921][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 16:51:14,923][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 16:51:14,926][train.py][INFO] - Instantiating loggers...
[2024-05-30 16:51:14,927][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 16:51:14,929][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 16:51:15,039][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/argued_someone/0.25/9/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.64it/s v_num: 0.000      
                                                              val/auc: 0.550    
                                                              val/f1: 0.400     
                                                              val/precision:    
                                                              0.600 val/recall: 
                                                              0.300 val/mre:    
                                                              0.154 train/auc:  
                                                              0.839 train/f1:   
                                                              0.857             
                                                              train/precision:  
                                                              0.769             
                                                              train/recall:     
                                                              0.968 train/mre:  
                                                              0.145             
[2024-05-30 16:52:58,898][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/argued_someone/0.25/9>
[2024-05-30 16:52:58,899][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 16:52:58,903][HYDRA] 	#200 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=argued_someone data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=0
[2024-05-30 16:52:59,194][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 16:52:59,197][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 16:52:59,199][train.py][INFO] - Instantiating callbacks...
[2024-05-30 16:52:59,199][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 16:52:59,203][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 16:52:59,204][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 16:52:59,204][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 16:52:59,205][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 16:52:59,207][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 16:52:59,208][train.py][INFO] - Instantiating loggers...
[2024-05-30 16:52:59,208][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 16:52:59,212][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 16:52:59,258][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/argued_someone/0.3/0/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.58it/s v_num: 0.000      
                                                              val/auc: 0.400    
                                                              val/f1: 0.333     
                                                              val/precision:    
                                                              0.375 val/recall: 
                                                              0.300 val/mre:    
                                                              0.158 train/auc:  
                                                              0.903 train/f1:   
                                                              0.906             
                                                              train/precision:  
                                                              0.879             
                                                              train/recall:     
                                                              0.935 train/mre:  
                                                              0.151             
[2024-05-30 16:54:43,314][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/argued_someone/0.3/0>
[2024-05-30 16:54:43,316][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 16:54:43,322][HYDRA] 	#201 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=argued_someone data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=1
[rank: 0] Seed set to 1
[2024-05-30 16:54:43,622][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 16:54:43,624][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 16:54:43,626][train.py][INFO] - Instantiating callbacks...
[2024-05-30 16:54:43,627][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 16:54:43,630][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 16:54:43,631][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 16:54:43,632][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 16:54:43,632][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 16:54:43,634][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 16:54:43,635][train.py][INFO] - Instantiating loggers...
[2024-05-30 16:54:43,635][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 16:54:43,637][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 16:54:43,681][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/argued_someone/0.3/1/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.54it/s v_num: 0.000      
                                                              val/auc: 0.550    
                                                              val/f1: 0.571     
                                                              val/precision:    
                                                              0.545 val/recall: 
                                                              0.600 val/mre:    
                                                              0.158 train/auc:  
                                                              0.863 train/f1:   
                                                              0.872             
                                                              train/precision:  
                                                              0.817             
                                                              train/recall:     
                                                              0.935 train/mre:  
                                                              0.151             
[2024-05-30 16:56:27,658][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/argued_someone/0.3/1>
[2024-05-30 16:56:27,659][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 16:56:27,662][HYDRA] 	#202 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=argued_someone data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=2
[rank: 0] Seed set to 2
[2024-05-30 16:56:27,952][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 16:56:27,955][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 16:56:27,957][train.py][INFO] - Instantiating callbacks...
[2024-05-30 16:56:27,957][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 16:56:27,961][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 16:56:27,962][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 16:56:27,962][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 16:56:27,963][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 16:56:27,965][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 16:56:27,966][train.py][INFO] - Instantiating loggers...
[2024-05-30 16:56:27,966][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 16:56:27,968][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 16:56:28,011][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/argued_someone/0.3/2/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.61it/s v_num: 0.000      
                                                              val/auc: 0.500    
                                                              val/f1: 0.375     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.300 val/mre:    
                                                              0.157 train/auc:  
                                                              0.806 train/f1:   
                                                              0.812             
                                                              train/precision:  
                                                              0.788             
                                                              train/recall:     
                                                              0.839 train/mre:  
                                                              0.149             
[2024-05-30 16:58:12,100][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/argued_someone/0.3/2>
[2024-05-30 16:58:12,100][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 16:58:12,104][HYDRA] 	#203 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=argued_someone data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=3
[rank: 0] Seed set to 3
[2024-05-30 16:58:12,385][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 16:58:12,388][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 16:58:12,390][train.py][INFO] - Instantiating callbacks...
[2024-05-30 16:58:12,390][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 16:58:12,393][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 16:58:12,394][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 16:58:12,395][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 16:58:12,396][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 16:58:12,397][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 16:58:12,398][train.py][INFO] - Instantiating loggers...
[2024-05-30 16:58:12,399][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 16:58:12,401][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 16:58:12,443][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/argued_someone/0.3/3/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.61it/s v_num: 0.000      
                                                              val/auc: 0.550    
                                                              val/f1: 0.526     
                                                              val/precision:    
                                                              0.556 val/recall: 
                                                              0.500 val/mre:    
                                                              0.161 train/auc:  
                                                              0.919 train/f1:   
                                                              0.925             
                                                              train/precision:  
                                                              0.861             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.151             
[2024-05-30 16:59:56,791][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/argued_someone/0.3/3>
[2024-05-30 16:59:56,792][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 16:59:56,795][HYDRA] 	#204 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=argued_someone data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=4
[rank: 0] Seed set to 4
[2024-05-30 16:59:57,096][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 16:59:57,099][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 16:59:57,101][train.py][INFO] - Instantiating callbacks...
[2024-05-30 16:59:57,101][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 16:59:57,105][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 16:59:57,105][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 16:59:57,106][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 16:59:57,107][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 16:59:57,109][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 16:59:57,112][train.py][INFO] - Instantiating loggers...
[2024-05-30 16:59:57,112][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 16:59:57,114][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 16:59:57,202][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/argued_someone/0.3/4/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.59it/s v_num: 0.000      
                                                              val/auc: 0.500    
                                                              val/f1: 0.375     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.300 val/mre:    
                                                              0.159 train/auc:  
                                                              0.879 train/f1:   
                                                              0.889             
                                                              train/precision:  
                                                              0.822             
                                                              train/recall:     
                                                              0.968 train/mre:  
                                                              0.151             
[2024-05-30 17:01:41,600][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/argued_someone/0.3/4>
[2024-05-30 17:01:41,601][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 17:01:41,604][HYDRA] 	#205 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=argued_someone data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=5
[rank: 0] Seed set to 5
[2024-05-30 17:01:41,910][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 17:01:41,913][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 17:01:41,915][train.py][INFO] - Instantiating callbacks...
[2024-05-30 17:01:41,916][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 17:01:41,920][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 17:01:41,921][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 17:01:41,921][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 17:01:41,922][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 17:01:41,924][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 17:01:41,928][train.py][INFO] - Instantiating loggers...
[2024-05-30 17:01:41,928][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 17:01:41,930][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 17:01:42,030][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/argued_someone/0.3/5/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.59it/s v_num: 0.000      
                                                              val/auc: 0.500    
                                                              val/f1: 0.375     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.300 val/mre:    
                                                              0.165 train/auc:  
                                                              0.944 train/f1:   
                                                              0.945             
                                                              train/precision:  
                                                              0.923             
                                                              train/recall:     
                                                              0.968 train/mre:  
                                                              0.157             
[2024-05-30 17:03:26,010][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/argued_someone/0.3/5>
[2024-05-30 17:03:26,011][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 17:03:26,013][HYDRA] 	#206 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=argued_someone data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=6
[rank: 0] Seed set to 6
[2024-05-30 17:03:26,299][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 17:03:26,302][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 17:03:26,304][train.py][INFO] - Instantiating callbacks...
[2024-05-30 17:03:26,304][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 17:03:26,307][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 17:03:26,308][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 17:03:26,309][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 17:03:26,310][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 17:03:26,311][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 17:03:26,312][train.py][INFO] - Instantiating loggers...
[2024-05-30 17:03:26,313][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 17:03:26,315][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 17:03:26,358][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/argued_someone/0.3/6/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.64it/s v_num: 0.000      
                                                              val/auc: 0.650    
                                                              val/f1: 0.632     
                                                              val/precision:    
                                                              0.667 val/recall: 
                                                              0.600 val/mre:    
                                                              0.153 train/auc:  
                                                              0.831 train/f1:   
                                                              0.847             
                                                              train/precision:  
                                                              0.773             
                                                              train/recall:     
                                                              0.935 train/mre:  
                                                              0.143             
[2024-05-30 17:05:10,660][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/argued_someone/0.3/6>
[2024-05-30 17:05:10,660][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 17:05:10,664][HYDRA] 	#207 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=argued_someone data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=7
[rank: 0] Seed set to 7
[2024-05-30 17:05:10,948][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 17:05:10,950][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 17:05:10,953][train.py][INFO] - Instantiating callbacks...
[2024-05-30 17:05:10,953][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 17:05:10,956][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 17:05:10,957][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 17:05:10,958][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 17:05:10,958][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 17:05:10,960][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 17:05:10,961][train.py][INFO] - Instantiating loggers...
[2024-05-30 17:05:10,961][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 17:05:10,965][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 17:05:11,007][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/argued_someone/0.3/7/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.60it/s v_num: 0.000      
                                                              val/auc: 0.600    
                                                              val/f1: 0.636     
                                                              val/precision:    
                                                              0.583 val/recall: 
                                                              0.700 val/mre:    
                                                              0.157 train/auc:  
                                                              0.863 train/f1:   
                                                              0.874             
                                                              train/precision:  
                                                              0.808             
                                                              train/recall:     
                                                              0.952 train/mre:  
                                                              0.147             
[2024-05-30 17:06:54,610][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/argued_someone/0.3/7>
[2024-05-30 17:06:54,610][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 17:06:54,614][HYDRA] 	#208 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=argued_someone data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=8
[rank: 0] Seed set to 8
[2024-05-30 17:06:54,902][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 17:06:54,905][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 17:06:54,907][train.py][INFO] - Instantiating callbacks...
[2024-05-30 17:06:54,907][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 17:06:54,911][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 17:06:54,912][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 17:06:54,913][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 17:06:54,913][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 17:06:54,915][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 17:06:54,916][train.py][INFO] - Instantiating loggers...
[2024-05-30 17:06:54,916][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 17:06:54,918][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 17:06:54,961][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/argued_someone/0.3/8/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.58it/s v_num: 0.000      
                                                              val/auc: 0.550    
                                                              val/f1: 0.526     
                                                              val/precision:    
                                                              0.556 val/recall: 
                                                              0.500 val/mre:    
                                                              0.165 train/auc:  
                                                              0.903 train/f1:   
                                                              0.906             
                                                              train/precision:  
                                                              0.879             
                                                              train/recall:     
                                                              0.935 train/mre:  
                                                              0.155             
[2024-05-30 17:08:39,631][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/argued_someone/0.3/8>
[2024-05-30 17:08:39,632][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 17:08:39,637][HYDRA] 	#209 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=argued_someone data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=9
[rank: 0] Seed set to 9
[2024-05-30 17:08:39,929][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 17:08:39,932][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 17:08:39,934][train.py][INFO] - Instantiating callbacks...
[2024-05-30 17:08:39,934][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 17:08:39,938][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 17:08:39,938][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 17:08:39,939][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 17:08:39,940][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 17:08:39,942][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 17:08:39,943][train.py][INFO] - Instantiating loggers...
[2024-05-30 17:08:39,943][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 17:08:39,945][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 17:08:39,989][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/argued_someone/0.3/9/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.60it/s v_num: 0.000      
                                                              val/auc: 0.400    
                                                              val/f1: 0.250     
                                                              val/precision:    
                                                              0.333 val/recall: 
                                                              0.200 val/mre:    
                                                              0.162 train/auc:  
                                                              0.855 train/f1:   
                                                              0.868             
                                                              train/precision:  
                                                              0.797             
                                                              train/recall:     
                                                              0.952 train/mre:  
                                                              0.152             
[2024-05-30 17:10:23,465][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/argued_someone/0.3/9>
[2024-05-30 17:10:23,466][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 17:10:23,469][HYDRA] 	#210 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=controlling_temper data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=0
[2024-05-30 17:10:23,763][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 17:10:23,766][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 17:10:23,768][train.py][INFO] - Instantiating callbacks...
[2024-05-30 17:10:23,768][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 17:10:23,772][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 17:10:23,773][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 17:10:23,773][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 17:10:23,774][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 17:10:23,776][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 17:10:23,777][train.py][INFO] - Instantiating loggers...
[2024-05-30 17:10:23,777][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 17:10:23,781][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 17:10:23,824][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/controlling_temper/0.0/0/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.57it/s v_num: 0.000      
                                                              val/auc: 0.550    
                                                              val/f1: 0.308     
                                                              val/precision:    
                                                              0.667 val/recall: 
                                                              0.200 val/mre: nan
                                                              train/auc: 0.805  
                                                              train/f1: 0.785   
                                                              train/precision:  
                                                              0.875             
                                                              train/recall:     
                                                              0.712 train/mre:  
                                                              nan               
[2024-05-30 17:12:06,711][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/controlling_temper/0.0/0>
[2024-05-30 17:12:06,714][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 17:12:06,730][HYDRA] 	#211 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=controlling_temper data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=1
[rank: 0] Seed set to 1
[2024-05-30 17:12:07,189][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 17:12:07,191][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 17:12:07,193][train.py][INFO] - Instantiating callbacks...
[2024-05-30 17:12:07,194][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 17:12:07,197][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 17:12:07,198][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 17:12:07,199][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 17:12:07,200][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 17:12:07,201][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 17:12:07,204][train.py][INFO] - Instantiating loggers...
[2024-05-30 17:12:07,205][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 17:12:07,207][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 17:12:07,291][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/controlling_temper/0.0/1/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.61it/s v_num: 0.000      
                                                              val/auc: 0.600    
                                                              val/f1: 0.429     
                                                              val/precision:    
                                                              0.750 val/recall: 
                                                              0.300 val/mre: nan
                                                              train/auc: 0.847  
                                                              train/f1: 0.833   
                                                              train/precision:  
                                                              0.918             
                                                              train/recall:     
                                                              0.763 train/mre:  
                                                              nan               
[2024-05-30 17:13:50,165][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/controlling_temper/0.0/1>
[2024-05-30 17:13:50,166][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 17:13:50,169][HYDRA] 	#212 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=controlling_temper data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=2
[rank: 0] Seed set to 2
[2024-05-30 17:13:50,472][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 17:13:50,474][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 17:13:50,476][train.py][INFO] - Instantiating callbacks...
[2024-05-30 17:13:50,477][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 17:13:50,480][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 17:13:50,481][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 17:13:50,482][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 17:13:50,482][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 17:13:50,484][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 17:13:50,487][train.py][INFO] - Instantiating loggers...
[2024-05-30 17:13:50,488][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 17:13:50,490][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 17:13:50,576][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/controlling_temper/0.0/2/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.65it/s v_num: 0.000      
                                                              val/auc: 0.500    
                                                              val/f1: 0.444     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.400 val/mre: nan
                                                              train/auc: 0.881  
                                                              train/f1: 0.889   
                                                              train/precision:  
                                                              0.836             
                                                              train/recall:     
                                                              0.949 train/mre:  
                                                              nan               
[2024-05-30 17:15:33,329][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/controlling_temper/0.0/2>
[2024-05-30 17:15:33,329][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 17:15:33,333][HYDRA] 	#213 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=controlling_temper data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=3
[rank: 0] Seed set to 3
[2024-05-30 17:15:33,620][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 17:15:33,622][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 17:15:33,624][train.py][INFO] - Instantiating callbacks...
[2024-05-30 17:15:33,625][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 17:15:33,628][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 17:15:33,629][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 17:15:33,629][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 17:15:33,630][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 17:15:33,632][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 17:15:33,633][train.py][INFO] - Instantiating loggers...
[2024-05-30 17:15:33,633][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 17:15:33,635][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 17:15:33,678][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/controlling_temper/0.0/3/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.63it/s v_num: 0.000      
                                                              val/auc: 0.650    
                                                              val/f1: 0.462     
                                                              val/precision:    
                                                              1.000 val/recall: 
                                                              0.300 val/mre: nan
                                                              train/auc: 0.847  
                                                              train/f1: 0.839   
                                                              train/precision:  
                                                              0.887             
                                                              train/recall:     
                                                              0.797 train/mre:  
                                                              nan               
[2024-05-30 17:17:15,932][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/controlling_temper/0.0/3>
[2024-05-30 17:17:15,933][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 17:17:15,936][HYDRA] 	#214 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=controlling_temper data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=4
[rank: 0] Seed set to 4
[2024-05-30 17:17:16,220][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 17:17:16,223][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 17:17:16,225][train.py][INFO] - Instantiating callbacks...
[2024-05-30 17:17:16,225][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 17:17:16,228][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 17:17:16,229][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 17:17:16,230][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 17:17:16,231][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 17:17:16,232][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 17:17:16,233][train.py][INFO] - Instantiating loggers...
[2024-05-30 17:17:16,234][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 17:17:16,236][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 17:17:16,282][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/controlling_temper/0.0/4/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.61it/s v_num: 0.000      
                                                              val/auc: 0.600    
                                                              val/f1: 0.429     
                                                              val/precision:    
                                                              0.750 val/recall: 
                                                              0.300 val/mre: nan
                                                              train/auc: 0.856  
                                                              train/f1: 0.857   
                                                              train/precision:  
                                                              0.850             
                                                              train/recall:     
                                                              0.864 train/mre:  
                                                              nan               
[2024-05-30 17:18:58,425][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/controlling_temper/0.0/4>
[2024-05-30 17:18:58,426][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 17:18:58,429][HYDRA] 	#215 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=controlling_temper data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=5
[rank: 0] Seed set to 5
[2024-05-30 17:18:58,717][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 17:18:58,719][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 17:18:58,721][train.py][INFO] - Instantiating callbacks...
[2024-05-30 17:18:58,722][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 17:18:58,725][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 17:18:58,726][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 17:18:58,726][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 17:18:58,727][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 17:18:58,729][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 17:18:58,730][train.py][INFO] - Instantiating loggers...
[2024-05-30 17:18:58,730][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 17:18:58,733][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 17:18:58,776][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/controlling_temper/0.0/5/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.60it/s v_num: 0.000      
                                                              val/auc: 0.750    
                                                              val/f1: 0.667     
                                                              val/precision:    
                                                              1.000 val/recall: 
                                                              0.500 val/mre: nan
                                                              train/auc: 0.856  
                                                              train/f1: 0.862   
                                                              train/precision:  
                                                              0.828             
                                                              train/recall:     
                                                              0.898 train/mre:  
                                                              nan               
[2024-05-30 17:20:41,028][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/controlling_temper/0.0/5>
[2024-05-30 17:20:41,029][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 17:20:41,032][HYDRA] 	#216 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=controlling_temper data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=6
[rank: 0] Seed set to 6
[2024-05-30 17:20:41,320][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 17:20:41,322][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 17:20:41,324][train.py][INFO] - Instantiating callbacks...
[2024-05-30 17:20:41,325][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 17:20:41,328][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 17:20:41,329][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 17:20:41,330][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 17:20:41,331][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 17:20:41,332][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 17:20:41,333][train.py][INFO] - Instantiating loggers...
[2024-05-30 17:20:41,334][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 17:20:41,336][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 17:20:41,379][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/controlling_temper/0.0/6/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.62it/s v_num: 0.000      
                                                              val/auc: 0.650    
                                                              val/f1: 0.588     
                                                              val/precision:    
                                                              0.714 val/recall: 
                                                              0.500 val/mre: nan
                                                              train/auc: 0.890  
                                                              train/f1: 0.898   
                                                              train/precision:  
                                                              0.838             
                                                              train/recall:     
                                                              0.966 train/mre:  
                                                              nan               
[2024-05-30 17:22:24,449][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/controlling_temper/0.0/6>
[2024-05-30 17:22:24,450][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 17:22:24,453][HYDRA] 	#217 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=controlling_temper data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=7
[rank: 0] Seed set to 7
[2024-05-30 17:22:24,767][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 17:22:24,770][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 17:22:24,772][train.py][INFO] - Instantiating callbacks...
[2024-05-30 17:22:24,773][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 17:22:24,776][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 17:22:24,777][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 17:22:24,778][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 17:22:24,779][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 17:22:24,780][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 17:22:24,784][train.py][INFO] - Instantiating loggers...
[2024-05-30 17:22:24,784][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 17:22:24,786][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 17:22:24,874][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/controlling_temper/0.0/7/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.64it/s v_num: 0.000      
                                                              val/auc: 0.550    
                                                              val/f1: 0.308     
                                                              val/precision:    
                                                              0.667 val/recall: 
                                                              0.200 val/mre: nan
                                                              train/auc: 0.763  
                                                              train/f1: 0.770   
                                                              train/precision:  
                                                              0.746             
                                                              train/recall:     
                                                              0.797 train/mre:  
                                                              nan               
[2024-05-30 17:24:06,134][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/controlling_temper/0.0/7>
[2024-05-30 17:24:06,135][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 17:24:06,138][HYDRA] 	#218 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=controlling_temper data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=8
[rank: 0] Seed set to 8
[2024-05-30 17:24:06,441][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 17:24:06,443][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 17:24:06,445][train.py][INFO] - Instantiating callbacks...
[2024-05-30 17:24:06,446][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 17:24:06,449][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 17:24:06,450][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 17:24:06,451][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 17:24:06,451][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 17:24:06,453][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 17:24:06,456][train.py][INFO] - Instantiating loggers...
[2024-05-30 17:24:06,457][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 17:24:06,459][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 17:24:06,547][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/controlling_temper/0.0/8/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.53it/s v_num: 0.000      
                                                              val/auc: 0.600    
                                                              val/f1: 0.429     
                                                              val/precision:    
                                                              0.750 val/recall: 
                                                              0.300 val/mre: nan
                                                              train/auc: 0.915  
                                                              train/f1: 0.919   
                                                              train/precision:  
                                                              0.877             
                                                              train/recall:     
                                                              0.966 train/mre:  
                                                              nan               
[2024-05-30 17:25:49,327][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/controlling_temper/0.0/8>
[2024-05-30 17:25:49,327][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 17:25:49,331][HYDRA] 	#219 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=controlling_temper data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=9
[rank: 0] Seed set to 9
[2024-05-30 17:25:49,623][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 17:25:49,625][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 17:25:49,628][train.py][INFO] - Instantiating callbacks...
[2024-05-30 17:25:49,628][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 17:25:49,631][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 17:25:49,632][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 17:25:49,633][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 17:25:49,633][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 17:25:49,635][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 17:25:49,636][train.py][INFO] - Instantiating loggers...
[2024-05-30 17:25:49,636][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 17:25:49,639][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 17:25:49,680][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/controlling_temper/0.0/9/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.61it/s v_num: 0.000      
                                                              val/auc: 0.500    
                                                              val/f1: 0.375     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.300 val/mre: nan
                                                              train/auc: 0.856  
                                                              train/f1: 0.868   
                                                              train/precision:  
                                                              0.800             
                                                              train/recall:     
                                                              0.949 train/mre:  
                                                              nan               
[2024-05-30 17:27:32,340][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/controlling_temper/0.0/9>
[2024-05-30 17:27:32,341][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 17:27:32,344][HYDRA] 	#220 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=controlling_temper data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=0
[2024-05-30 17:27:32,661][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 17:27:32,663][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 17:27:32,665][train.py][INFO] - Instantiating callbacks...
[2024-05-30 17:27:32,666][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 17:27:32,669][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 17:27:32,670][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 17:27:32,671][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 17:27:32,671][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 17:27:32,673][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 17:27:32,674][train.py][INFO] - Instantiating loggers...
[2024-05-30 17:27:32,674][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 17:27:32,676][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 17:27:32,722][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/controlling_temper/0.05/0/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.60it/s v_num: 0.000      
                                                              val/auc: 0.500    
                                                              val/f1: 0.444     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.400 val/mre:    
                                                              0.086 train/auc:  
                                                              0.839 train/f1:   
                                                              0.838             
                                                              train/precision:  
                                                              0.845             
                                                              train/recall:     
                                                              0.831 train/mre:  
                                                              0.087             
[2024-05-30 17:29:16,973][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/controlling_temper/0.05/0>
[2024-05-30 17:29:16,974][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 17:29:16,978][HYDRA] 	#221 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=controlling_temper data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=1
[rank: 0] Seed set to 1
[2024-05-30 17:29:17,264][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 17:29:17,266][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 17:29:17,268][train.py][INFO] - Instantiating callbacks...
[2024-05-30 17:29:17,269][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 17:29:17,272][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 17:29:17,273][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 17:29:17,273][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 17:29:17,274][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 17:29:17,276][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 17:29:17,277][train.py][INFO] - Instantiating loggers...
[2024-05-30 17:29:17,278][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 17:29:17,280][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 17:29:17,323][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/controlling_temper/0.05/1/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.62it/s v_num: 0.000      
                                                              val/auc: 0.500    
                                                              val/f1: 0.286     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.200 val/mre:    
                                                              0.094 train/auc:  
                                                              0.915 train/f1:   
                                                              0.915             
                                                              train/precision:  
                                                              0.915             
                                                              train/recall:     
                                                              0.915 train/mre:  
                                                              0.092             
[2024-05-30 17:31:00,493][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/controlling_temper/0.05/1>
[2024-05-30 17:31:00,494][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 17:31:00,496][HYDRA] 	#222 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=controlling_temper data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=2
[rank: 0] Seed set to 2
[2024-05-30 17:31:00,790][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 17:31:00,792][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 17:31:00,794][train.py][INFO] - Instantiating callbacks...
[2024-05-30 17:31:00,795][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 17:31:00,798][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 17:31:00,799][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 17:31:00,800][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 17:31:00,800][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 17:31:00,802][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 17:31:00,803][train.py][INFO] - Instantiating loggers...
[2024-05-30 17:31:00,803][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 17:31:00,805][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 17:31:00,846][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/controlling_temper/0.05/2/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.62it/s v_num: 0.000      
                                                              val/auc: 0.600    
                                                              val/f1: 0.500     
                                                              val/precision:    
                                                              0.667 val/recall: 
                                                              0.400 val/mre:    
                                                              0.095 train/auc:  
                                                              0.847 train/f1:   
                                                              0.859             
                                                              train/precision:  
                                                              0.797             
                                                              train/recall:     
                                                              0.932 train/mre:  
                                                              0.093             
[2024-05-30 17:32:42,851][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/controlling_temper/0.05/2>
[2024-05-30 17:32:42,854][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 17:32:42,872][HYDRA] 	#223 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=controlling_temper data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=3
[rank: 0] Seed set to 3
[2024-05-30 17:32:43,375][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 17:32:43,378][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 17:32:43,381][train.py][INFO] - Instantiating callbacks...
[2024-05-30 17:32:43,382][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 17:32:43,386][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 17:32:43,387][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 17:32:43,388][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 17:32:43,388][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 17:32:43,390][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 17:32:43,391][train.py][INFO] - Instantiating loggers...
[2024-05-30 17:32:43,391][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 17:32:43,394][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 17:32:43,437][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/controlling_temper/0.05/3/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.63it/s v_num: 0.000      
                                                              val/auc: 0.600    
                                                              val/f1: 0.429     
                                                              val/precision:    
                                                              0.750 val/recall: 
                                                              0.300 val/mre:    
                                                              0.092 train/auc:  
                                                              0.831 train/f1:   
                                                              0.815             
                                                              train/precision:  
                                                              0.898             
                                                              train/recall:     
                                                              0.746 train/mre:  
                                                              0.092             
[2024-05-30 17:34:24,820][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/controlling_temper/0.05/3>
[2024-05-30 17:34:24,821][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 17:34:24,824][HYDRA] 	#224 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=controlling_temper data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=4
[rank: 0] Seed set to 4
[2024-05-30 17:34:25,119][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 17:34:25,121][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 17:34:25,123][train.py][INFO] - Instantiating callbacks...
[2024-05-30 17:34:25,124][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 17:34:25,131][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 17:34:25,132][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 17:34:25,133][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 17:34:25,134][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 17:34:25,135][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 17:34:25,138][train.py][INFO] - Instantiating loggers...
[2024-05-30 17:34:25,138][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 17:34:25,141][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 17:34:25,235][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/controlling_temper/0.05/4/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.55it/s v_num: 0.000      
                                                              val/auc: 0.500    
                                                              val/f1: 0.167     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.100 val/mre:    
                                                              0.095 train/auc:  
                                                              0.881 train/f1:   
                                                              0.873             
                                                              train/precision:  
                                                              0.941             
                                                              train/recall:     
                                                              0.814 train/mre:  
                                                              0.093             
[2024-05-30 17:36:07,796][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/controlling_temper/0.05/4>
[2024-05-30 17:36:07,799][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 17:36:07,810][HYDRA] 	#225 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=controlling_temper data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=5
[rank: 0] Seed set to 5
[2024-05-30 17:36:08,503][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 17:36:08,508][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 17:36:08,510][train.py][INFO] - Instantiating callbacks...
[2024-05-30 17:36:08,511][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 17:36:08,514][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 17:36:08,515][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 17:36:08,516][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 17:36:08,517][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 17:36:08,519][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 17:36:08,522][train.py][INFO] - Instantiating loggers...
[2024-05-30 17:36:08,522][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 17:36:08,524][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 17:36:08,666][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/controlling_temper/0.05/5/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.65it/s v_num: 0.000      
                                                              val/auc: 0.650    
                                                              val/f1: 0.533     
                                                              val/precision:    
                                                              0.800 val/recall: 
                                                              0.400 val/mre:    
                                                              0.095 train/auc:  
                                                              0.873 train/f1:   
                                                              0.872             
                                                              train/precision:  
                                                              0.879             
                                                              train/recall:     
                                                              0.864 train/mre:  
                                                              0.094             
[2024-05-30 17:37:52,058][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/controlling_temper/0.05/5>
[2024-05-30 17:37:52,059][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 17:37:52,062][HYDRA] 	#226 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=controlling_temper data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=6
[rank: 0] Seed set to 6
[2024-05-30 17:37:52,345][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 17:37:52,348][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 17:37:52,350][train.py][INFO] - Instantiating callbacks...
[2024-05-30 17:37:52,350][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 17:37:52,353][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 17:37:52,354][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 17:37:52,355][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 17:37:52,356][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 17:37:52,357][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 17:37:52,358][train.py][INFO] - Instantiating loggers...
[2024-05-30 17:37:52,359][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 17:37:52,361][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 17:37:52,403][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/controlling_temper/0.05/6/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.61it/s v_num: 0.000      
                                                              val/auc: 0.600    
                                                              val/f1: 0.556     
                                                              val/precision:    
                                                              0.625 val/recall: 
                                                              0.500 val/mre:    
                                                              0.090 train/auc:  
                                                              0.898 train/f1:   
                                                              0.902             
                                                              train/precision:  
                                                              0.873             
                                                              train/recall:     
                                                              0.932 train/mre:  
                                                              0.090             
[2024-05-30 17:39:34,789][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/controlling_temper/0.05/6>
[2024-05-30 17:39:34,790][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 17:39:34,793][HYDRA] 	#227 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=controlling_temper data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=7
[rank: 0] Seed set to 7
[2024-05-30 17:39:35,124][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 17:39:35,127][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 17:39:35,129][train.py][INFO] - Instantiating callbacks...
[2024-05-30 17:39:35,129][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 17:39:35,133][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 17:39:35,134][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 17:39:35,135][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 17:39:35,135][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 17:39:35,137][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 17:39:35,138][train.py][INFO] - Instantiating loggers...
[2024-05-30 17:39:35,138][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 17:39:35,141][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 17:39:35,192][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/controlling_temper/0.05/7/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.59it/s v_num: 0.000      
                                                              val/auc: 0.450    
                                                              val/f1: 0.000     
                                                              val/precision:    
                                                              0.000 val/recall: 
                                                              0.000 val/mre:    
                                                              0.084 train/auc:  
                                                              0.797 train/f1:   
                                                              0.789             
                                                              train/precision:  
                                                              0.818             
                                                              train/recall:     
                                                              0.763 train/mre:  
                                                              0.084             
[2024-05-30 17:41:18,151][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/controlling_temper/0.05/7>
[2024-05-30 17:41:18,152][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 17:41:18,155][HYDRA] 	#228 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=controlling_temper data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=8
[rank: 0] Seed set to 8
[2024-05-30 17:41:18,450][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 17:41:18,452][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 17:41:18,454][train.py][INFO] - Instantiating callbacks...
[2024-05-30 17:41:18,455][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 17:41:18,458][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 17:41:18,459][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 17:41:18,460][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 17:41:18,460][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 17:41:18,462][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 17:41:18,463][train.py][INFO] - Instantiating loggers...
[2024-05-30 17:41:18,463][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 17:41:18,465][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 17:41:18,507][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/controlling_temper/0.05/8/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.62it/s v_num: 0.000      
                                                              val/auc: 0.600    
                                                              val/f1: 0.500     
                                                              val/precision:    
                                                              0.667 val/recall: 
                                                              0.400 val/mre:    
                                                              0.094 train/auc:  
                                                              0.898 train/f1:   
                                                              0.900             
                                                              train/precision:  
                                                              0.885             
                                                              train/recall:     
                                                              0.915 train/mre:  
                                                              0.090             
[2024-05-30 17:42:59,887][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/controlling_temper/0.05/8>
[2024-05-30 17:42:59,889][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 17:42:59,893][HYDRA] 	#229 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=controlling_temper data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=9
[rank: 0] Seed set to 9
[2024-05-30 17:43:00,182][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 17:43:00,184][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 17:43:00,186][train.py][INFO] - Instantiating callbacks...
[2024-05-30 17:43:00,187][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 17:43:00,190][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 17:43:00,191][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 17:43:00,192][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 17:43:00,193][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 17:43:00,194][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 17:43:00,196][train.py][INFO] - Instantiating loggers...
[2024-05-30 17:43:00,196][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 17:43:00,198][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 17:43:00,243][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/controlling_temper/0.05/9/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.67it/s v_num: 0.000      
                                                              val/auc: 0.450    
                                                              val/f1: 0.421     
                                                              val/precision:    
                                                              0.444 val/recall: 
                                                              0.400 val/mre:    
                                                              0.089 train/auc:  
                                                              0.881 train/f1:   
                                                              0.885             
                                                              train/precision:  
                                                              0.857             
                                                              train/recall:     
                                                              0.915 train/mre:  
                                                              0.088             
[2024-05-30 17:44:41,908][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/controlling_temper/0.05/9>
[2024-05-30 17:44:41,908][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 17:44:41,912][HYDRA] 	#230 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=controlling_temper data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=0
[2024-05-30 17:44:42,521][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 17:44:42,523][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 17:44:42,525][train.py][INFO] - Instantiating callbacks...
[2024-05-30 17:44:42,526][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 17:44:42,529][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 17:44:42,530][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 17:44:42,531][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 17:44:42,531][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 17:44:42,533][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 17:44:42,534][train.py][INFO] - Instantiating loggers...
[2024-05-30 17:44:42,534][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 17:44:42,536][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 17:44:42,588][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/controlling_temper/0.1/0/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.62it/s v_num: 0.000      
                                                              val/auc: 0.550    
                                                              val/f1: 0.400     
                                                              val/precision:    
                                                              0.600 val/recall: 
                                                              0.300 val/mre:    
                                                              0.091 train/auc:  
                                                              0.847 train/f1:   
                                                              0.850             
                                                              train/precision:  
                                                              0.836             
                                                              train/recall:     
                                                              0.864 train/mre:  
                                                              0.090             
[2024-05-30 17:46:24,706][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/controlling_temper/0.1/0>
[2024-05-30 17:46:24,707][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 17:46:24,710][HYDRA] 	#231 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=controlling_temper data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=1
[rank: 0] Seed set to 1
[2024-05-30 17:46:25,012][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 17:46:25,015][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 17:46:25,017][train.py][INFO] - Instantiating callbacks...
[2024-05-30 17:46:25,017][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 17:46:25,021][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 17:46:25,021][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 17:46:25,022][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 17:46:25,023][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 17:46:25,025][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 17:46:25,028][train.py][INFO] - Instantiating loggers...
[2024-05-30 17:46:25,029][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 17:46:25,031][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 17:46:25,119][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/controlling_temper/0.1/1/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.64it/s v_num: 0.000      
                                                              val/auc: 0.450    
                                                              val/f1: 0.267     
                                                              val/precision:    
                                                              0.400 val/recall: 
                                                              0.200 val/mre:    
                                                              0.096 train/auc:  
                                                              0.907 train/f1:   
                                                              0.904             
                                                              train/precision:  
                                                              0.929             
                                                              train/recall:     
                                                              0.881 train/mre:  
                                                              0.095             
[2024-05-30 17:48:06,790][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/controlling_temper/0.1/1>
[2024-05-30 17:48:06,792][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 17:48:06,803][HYDRA] 	#232 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=controlling_temper data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=2
[rank: 0] Seed set to 2
[2024-05-30 17:48:07,111][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 17:48:07,114][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 17:48:07,116][train.py][INFO] - Instantiating callbacks...
[2024-05-30 17:48:07,116][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 17:48:07,120][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 17:48:07,121][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 17:48:07,121][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 17:48:07,122][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 17:48:07,124][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 17:48:07,127][train.py][INFO] - Instantiating loggers...
[2024-05-30 17:48:07,127][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 17:48:07,129][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 17:48:07,215][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/controlling_temper/0.1/2/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.68it/s v_num: 0.000      
                                                              val/auc: 0.700    
                                                              val/f1: 0.625     
                                                              val/precision:    
                                                              0.833 val/recall: 
                                                              0.500 val/mre:    
                                                              0.102 train/auc:  
                                                              0.907 train/f1:   
                                                              0.909             
                                                              train/precision:  
                                                              0.887             
                                                              train/recall:     
                                                              0.932 train/mre:  
                                                              0.100             
[2024-05-30 17:49:49,684][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/controlling_temper/0.1/2>
[2024-05-30 17:49:49,685][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 17:49:49,689][HYDRA] 	#233 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=controlling_temper data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=3
[rank: 0] Seed set to 3
[2024-05-30 17:49:49,978][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 17:49:49,981][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 17:49:49,983][train.py][INFO] - Instantiating callbacks...
[2024-05-30 17:49:49,983][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 17:49:49,987][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 17:49:49,988][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 17:49:49,988][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 17:49:49,989][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 17:49:49,991][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 17:49:49,992][train.py][INFO] - Instantiating loggers...
[2024-05-30 17:49:49,992][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 17:49:49,994][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 17:49:50,037][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/controlling_temper/0.1/3/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.64it/s v_num: 0.000      
                                                              val/auc: 0.550    
                                                              val/f1: 0.308     
                                                              val/precision:    
                                                              0.667 val/recall: 
                                                              0.200 val/mre:    
                                                              0.093 train/auc:  
                                                              0.763 train/f1:   
                                                              0.745             
                                                              train/precision:  
                                                              0.804             
                                                              train/recall:     
                                                              0.695 train/mre:  
                                                              0.094             
[2024-05-30 17:51:31,710][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/controlling_temper/0.1/3>
[2024-05-30 17:51:31,710][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 17:51:31,714][HYDRA] 	#234 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=controlling_temper data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=4
[rank: 0] Seed set to 4
[2024-05-30 17:51:32,006][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 17:51:32,009][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 17:51:32,011][train.py][INFO] - Instantiating callbacks...
[2024-05-30 17:51:32,011][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 17:51:32,015][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 17:51:32,015][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 17:51:32,016][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 17:51:32,017][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 17:51:32,019][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 17:51:32,020][train.py][INFO] - Instantiating loggers...
[2024-05-30 17:51:32,020][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 17:51:32,022][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 17:51:32,065][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/controlling_temper/0.1/4/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.64it/s v_num: 0.000      
                                                              val/auc: 0.550    
                                                              val/f1: 0.308     
                                                              val/precision:    
                                                              0.667 val/recall: 
                                                              0.200 val/mre:    
                                                              0.100 train/auc:  
                                                              0.890 train/f1:   
                                                              0.881             
                                                              train/precision:  
                                                              0.960             
                                                              train/recall:     
                                                              0.814 train/mre:  
                                                              0.099             
[2024-05-30 17:53:13,775][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/controlling_temper/0.1/4>
[2024-05-30 17:53:13,776][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 17:53:13,779][HYDRA] 	#235 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=controlling_temper data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=5
[rank: 0] Seed set to 5
[2024-05-30 17:53:14,062][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 17:53:14,065][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 17:53:14,067][train.py][INFO] - Instantiating callbacks...
[2024-05-30 17:53:14,067][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 17:53:14,071][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 17:53:14,072][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 17:53:14,072][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 17:53:14,073][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 17:53:14,075][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 17:53:14,076][train.py][INFO] - Instantiating loggers...
[2024-05-30 17:53:14,076][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 17:53:14,078][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 17:53:14,120][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/controlling_temper/0.1/5/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.63it/s v_num: 0.000      
                                                              val/auc: 0.500    
                                                              val/f1: 0.286     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.200 val/mre:    
                                                              0.096 train/auc:  
                                                              0.831 train/f1:   
                                                              0.815             
                                                              train/precision:  
                                                              0.898             
                                                              train/recall:     
                                                              0.746 train/mre:  
                                                              0.095             
[2024-05-30 17:54:56,613][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/controlling_temper/0.1/5>
[2024-05-30 17:54:56,613][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 17:54:56,617][HYDRA] 	#236 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=controlling_temper data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=6
[rank: 0] Seed set to 6
[2024-05-30 17:54:56,904][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 17:54:56,907][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 17:54:56,909][train.py][INFO] - Instantiating callbacks...
[2024-05-30 17:54:56,909][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 17:54:56,913][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 17:54:56,913][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 17:54:56,914][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 17:54:56,915][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 17:54:56,917][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 17:54:56,918][train.py][INFO] - Instantiating loggers...
[2024-05-30 17:54:56,918][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 17:54:56,920][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 17:54:56,963][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/controlling_temper/0.1/6/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.62it/s v_num: 0.000      
                                                              val/auc: 0.550    
                                                              val/f1: 0.471     
                                                              val/precision:    
                                                              0.571 val/recall: 
                                                              0.400 val/mre:    
                                                              0.096 train/auc:  
                                                              0.881 train/f1:   
                                                              0.883             
                                                              train/precision:  
                                                              0.869             
                                                              train/recall:     
                                                              0.898 train/mre:  
                                                              0.096             
[2024-05-30 17:56:39,020][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/controlling_temper/0.1/6>
[2024-05-30 17:56:39,020][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 17:56:39,024][HYDRA] 	#237 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=controlling_temper data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=7
[rank: 0] Seed set to 7
[2024-05-30 17:56:39,316][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 17:56:39,318][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 17:56:39,320][train.py][INFO] - Instantiating callbacks...
[2024-05-30 17:56:39,321][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 17:56:39,324][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 17:56:39,325][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 17:56:39,326][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 17:56:39,327][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 17:56:39,328][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 17:56:39,329][train.py][INFO] - Instantiating loggers...
[2024-05-30 17:56:39,329][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 17:56:39,332][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 17:56:39,374][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/controlling_temper/0.1/7/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.63it/s v_num: 0.000      
                                                              val/auc: 0.550    
                                                              val/f1: 0.182     
                                                              val/precision:    
                                                              1.000 val/recall: 
                                                              0.100 val/mre:    
                                                              0.091 train/auc:  
                                                              0.754 train/f1:   
                                                              0.729             
                                                              train/precision:  
                                                              0.812             
                                                              train/recall:     
                                                              0.661 train/mre:  
                                                              0.091             
[2024-05-30 17:58:21,261][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/controlling_temper/0.1/7>
[2024-05-30 17:58:21,262][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 17:58:21,265][HYDRA] 	#238 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=controlling_temper data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=8
[rank: 0] Seed set to 8
[2024-05-30 17:58:21,563][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 17:58:21,566][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 17:58:21,568][train.py][INFO] - Instantiating callbacks...
[2024-05-30 17:58:21,568][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 17:58:21,572][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 17:58:21,572][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 17:58:21,573][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 17:58:21,574][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 17:58:21,576][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 17:58:21,580][train.py][INFO] - Instantiating loggers...
[2024-05-30 17:58:21,580][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 17:58:21,582][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 17:58:21,668][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/controlling_temper/0.1/8/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.65it/s v_num: 0.000      
                                                              val/auc: 0.550    
                                                              val/f1: 0.400     
                                                              val/precision:    
                                                              0.600 val/recall: 
                                                              0.300 val/mre:    
                                                              0.096 train/auc:  
                                                              0.856 train/f1:   
                                                              0.855             
                                                              train/precision:  
                                                              0.862             
                                                              train/recall:     
                                                              0.847 train/mre:  
                                                              0.094             
[2024-05-30 18:00:03,980][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/controlling_temper/0.1/8>
[2024-05-30 18:00:03,981][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 18:00:03,984][HYDRA] 	#239 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=controlling_temper data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=9
[rank: 0] Seed set to 9
[2024-05-30 18:00:04,292][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 18:00:04,295][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 18:00:04,298][train.py][INFO] - Instantiating callbacks...
[2024-05-30 18:00:04,298][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 18:00:04,303][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 18:00:04,303][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 18:00:04,304][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 18:00:04,305][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 18:00:04,307][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 18:00:04,310][train.py][INFO] - Instantiating loggers...
[2024-05-30 18:00:04,310][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 18:00:04,312][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 18:00:04,401][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/controlling_temper/0.1/9/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.66it/s v_num: 0.000      
                                                              val/auc: 0.550    
                                                              val/f1: 0.526     
                                                              val/precision:    
                                                              0.556 val/recall: 
                                                              0.500 val/mre:    
                                                              0.094 train/auc:  
                                                              0.864 train/f1:   
                                                              0.862             
                                                              train/precision:  
                                                              0.877             
                                                              train/recall:     
                                                              0.847 train/mre:  
                                                              0.093             
[2024-05-30 18:01:45,673][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/controlling_temper/0.1/9>
[2024-05-30 18:01:45,674][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 18:01:45,677][HYDRA] 	#240 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=controlling_temper data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=0
[2024-05-30 18:01:45,969][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 18:01:45,972][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 18:01:45,974][train.py][INFO] - Instantiating callbacks...
[2024-05-30 18:01:45,974][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 18:01:45,978][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 18:01:45,978][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 18:01:45,979][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 18:01:45,980][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 18:01:45,982][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 18:01:45,984][train.py][INFO] - Instantiating loggers...
[2024-05-30 18:01:45,984][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 18:01:45,986][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 18:01:46,029][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/controlling_temper/0.15/0/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.04it/s v_num: 0.000      
                                                              val/auc: 0.500    
                                                              val/f1: 0.286     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.200 val/mre:    
                                                              0.093 train/auc:  
                                                              0.831 train/f1:   
                                                              0.836             
                                                              train/precision:  
                                                              0.810             
                                                              train/recall:     
                                                              0.864 train/mre:  
                                                              0.092             
[2024-05-30 18:03:28,789][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/controlling_temper/0.15/0>
[2024-05-30 18:03:28,790][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 18:03:28,793][HYDRA] 	#241 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=controlling_temper data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=1
[rank: 0] Seed set to 1
[2024-05-30 18:03:29,084][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 18:03:29,086][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 18:03:29,088][train.py][INFO] - Instantiating callbacks...
[2024-05-30 18:03:29,089][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 18:03:29,092][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 18:03:29,093][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 18:03:29,093][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 18:03:29,094][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 18:03:29,096][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 18:03:29,097][train.py][INFO] - Instantiating loggers...
[2024-05-30 18:03:29,097][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 18:03:29,099][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 18:03:29,142][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/controlling_temper/0.15/1/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.63it/s v_num: 0.000      
                                                              val/auc: 0.450    
                                                              val/f1: 0.353     
                                                              val/precision:    
                                                              0.429 val/recall: 
                                                              0.300 val/mre:    
                                                              0.099 train/auc:  
                                                              0.881 train/f1:   
                                                              0.877             
                                                              train/precision:  
                                                              0.909             
                                                              train/recall:     
                                                              0.847 train/mre:  
                                                              0.097             
[2024-05-30 18:05:11,724][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/controlling_temper/0.15/1>
[2024-05-30 18:05:11,726][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 18:05:11,730][HYDRA] 	#242 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=controlling_temper data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=2
[rank: 0] Seed set to 2
[2024-05-30 18:05:12,078][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 18:05:12,081][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 18:05:12,083][train.py][INFO] - Instantiating callbacks...
[2024-05-30 18:05:12,083][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 18:05:12,087][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 18:05:12,087][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 18:05:12,088][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 18:05:12,089][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 18:05:12,091][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 18:05:12,091][train.py][INFO] - Instantiating loggers...
[2024-05-30 18:05:12,092][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 18:05:12,094][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 18:05:12,212][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/controlling_temper/0.15/2/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.66it/s v_num: 0.000      
                                                              val/auc: 0.600    
                                                              val/f1: 0.556     
                                                              val/precision:    
                                                              0.625 val/recall: 
                                                              0.500 val/mre:    
                                                              0.102 train/auc:  
                                                              0.847 train/f1:   
                                                              0.855             
                                                              train/precision:  
                                                              0.815             
                                                              train/recall:     
                                                              0.898 train/mre:  
                                                              0.100             
[2024-05-30 18:06:54,121][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/controlling_temper/0.15/2>
[2024-05-30 18:06:54,122][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 18:06:54,125][HYDRA] 	#243 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=controlling_temper data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=3
[rank: 0] Seed set to 3
[2024-05-30 18:06:54,414][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 18:06:54,417][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 18:06:54,419][train.py][INFO] - Instantiating callbacks...
[2024-05-30 18:06:54,419][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 18:06:54,423][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 18:06:54,423][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 18:06:54,424][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 18:06:54,425][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 18:06:54,427][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 18:06:54,428][train.py][INFO] - Instantiating loggers...
[2024-05-30 18:06:54,428][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 18:06:54,430][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 18:06:54,476][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/controlling_temper/0.15/3/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.64it/s v_num: 0.000      
                                                              val/auc: 0.500    
                                                              val/f1: 0.286     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.200 val/mre:    
                                                              0.097 train/auc:  
                                                              0.763 train/f1:   
                                                              0.767             
                                                              train/precision:  
                                                              0.754             
                                                              train/recall:     
                                                              0.780 train/mre:  
                                                              0.097             
[2024-05-30 18:08:38,401][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/controlling_temper/0.15/3>
[2024-05-30 18:08:38,402][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 18:08:38,404][HYDRA] 	#244 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=controlling_temper data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=4
[rank: 0] Seed set to 4
[2024-05-30 18:08:38,688][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 18:08:38,691][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 18:08:38,693][train.py][INFO] - Instantiating callbacks...
[2024-05-30 18:08:38,693][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 18:08:38,696][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 18:08:38,697][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 18:08:38,698][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 18:08:38,699][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 18:08:38,700][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 18:08:38,702][train.py][INFO] - Instantiating loggers...
[2024-05-30 18:08:38,702][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 18:08:38,704][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 18:08:38,749][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/controlling_temper/0.15/4/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.63it/s v_num: 0.000      
                                                              val/auc: 0.550    
                                                              val/f1: 0.308     
                                                              val/precision:    
                                                              0.667 val/recall: 
                                                              0.200 val/mre:    
                                                              0.099 train/auc:  
                                                              0.839 train/f1:   
                                                              0.850             
                                                              train/precision:  
                                                              0.794             
                                                              train/recall:     
                                                              0.915 train/mre:  
                                                              0.099             
[2024-05-30 18:10:19,991][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/controlling_temper/0.15/4>
[2024-05-30 18:10:19,992][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 18:10:19,998][HYDRA] 	#245 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=controlling_temper data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=5
[rank: 0] Seed set to 5
[2024-05-30 18:10:20,357][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 18:10:20,360][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 18:10:20,362][train.py][INFO] - Instantiating callbacks...
[2024-05-30 18:10:20,362][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 18:10:20,366][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 18:10:20,367][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 18:10:20,367][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 18:10:20,368][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 18:10:20,370][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 18:10:20,371][train.py][INFO] - Instantiating loggers...
[2024-05-30 18:10:20,371][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 18:10:20,374][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 18:10:20,416][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/controlling_temper/0.15/5/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.62it/s v_num: 0.000      
                                                              val/auc: 0.650    
                                                              val/f1: 0.588     
                                                              val/precision:    
                                                              0.714 val/recall: 
                                                              0.500 val/mre:    
                                                              0.097 train/auc:  
                                                              0.814 train/f1:   
                                                              0.807             
                                                              train/precision:  
                                                              0.836             
                                                              train/recall:     
                                                              0.780 train/mre:  
                                                              0.097             
[2024-05-30 18:12:02,606][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/controlling_temper/0.15/5>
[2024-05-30 18:12:02,607][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 18:12:02,610][HYDRA] 	#246 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=controlling_temper data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=6
[rank: 0] Seed set to 6
[2024-05-30 18:12:02,914][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 18:12:02,917][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 18:12:02,919][train.py][INFO] - Instantiating callbacks...
[2024-05-30 18:12:02,920][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 18:12:02,924][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 18:12:02,925][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 18:12:02,925][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 18:12:02,926][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 18:12:02,928][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 18:12:02,929][train.py][INFO] - Instantiating loggers...
[2024-05-30 18:12:02,929][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 18:12:02,931][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 18:12:02,979][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/controlling_temper/0.15/6/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.62it/s v_num: 0.000      
                                                              val/auc: 0.600    
                                                              val/f1: 0.500     
                                                              val/precision:    
                                                              0.667 val/recall: 
                                                              0.400 val/mre:    
                                                              0.097 train/auc:  
                                                              0.864 train/f1:   
                                                              0.873             
                                                              train/precision:  
                                                              0.821             
                                                              train/recall:     
                                                              0.932 train/mre:  
                                                              0.097             
[2024-05-30 18:13:44,985][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/controlling_temper/0.15/6>
[2024-05-30 18:13:44,986][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 18:13:44,989][HYDRA] 	#247 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=controlling_temper data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=7
[rank: 0] Seed set to 7
[2024-05-30 18:13:45,293][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 18:13:45,296][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 18:13:45,299][train.py][INFO] - Instantiating callbacks...
[2024-05-30 18:13:45,299][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 18:13:45,303][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 18:13:45,304][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 18:13:45,305][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 18:13:45,306][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 18:13:45,308][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 18:13:45,311][train.py][INFO] - Instantiating loggers...
[2024-05-30 18:13:45,311][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 18:13:45,313][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 18:13:45,400][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/controlling_temper/0.15/7/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.67it/s v_num: 0.000      
                                                              val/auc: 0.550    
                                                              val/f1: 0.308     
                                                              val/precision:    
                                                              0.667 val/recall: 
                                                              0.200 val/mre:    
                                                              0.093 train/auc:  
                                                              0.797 train/f1:   
                                                              0.765             
                                                              train/precision:  
                                                              0.907             
                                                              train/recall:     
                                                              0.661 train/mre:  
                                                              0.093             
[2024-05-30 18:15:27,193][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/controlling_temper/0.15/7>
[2024-05-30 18:15:27,194][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 18:15:27,197][HYDRA] 	#248 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=controlling_temper data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=8
[rank: 0] Seed set to 8
[2024-05-30 18:15:27,504][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 18:15:27,507][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 18:15:27,509][train.py][INFO] - Instantiating callbacks...
[2024-05-30 18:15:27,509][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 18:15:27,513][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 18:15:27,513][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 18:15:27,514][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 18:15:27,515][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 18:15:27,517][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 18:15:27,520][train.py][INFO] - Instantiating loggers...
[2024-05-30 18:15:27,521][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 18:15:27,523][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 18:15:27,611][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/controlling_temper/0.15/8/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.62it/s v_num: 0.000      
                                                              val/auc: 0.650    
                                                              val/f1: 0.632     
                                                              val/precision:    
                                                              0.667 val/recall: 
                                                              0.600 val/mre:    
                                                              0.105 train/auc:  
                                                              0.898 train/f1:   
                                                              0.898             
                                                              train/precision:  
                                                              0.898             
                                                              train/recall:     
                                                              0.898 train/mre:  
                                                              0.103             
[2024-05-30 18:17:09,035][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/controlling_temper/0.15/8>
[2024-05-30 18:17:09,036][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 18:17:09,039][HYDRA] 	#249 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=controlling_temper data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=9
[rank: 0] Seed set to 9
[2024-05-30 18:17:09,340][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 18:17:09,342][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 18:17:09,344][train.py][INFO] - Instantiating callbacks...
[2024-05-30 18:17:09,345][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 18:17:09,348][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 18:17:09,349][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 18:17:09,350][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 18:17:09,350][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 18:17:09,352][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 18:17:09,355][train.py][INFO] - Instantiating loggers...
[2024-05-30 18:17:09,356][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 18:17:09,359][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 18:17:09,444][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/controlling_temper/0.15/9/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.69it/s v_num: 0.000      
                                                              val/auc: 0.500    
                                                              val/f1: 0.444     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.400 val/mre:    
                                                              0.097 train/auc:  
                                                              0.873 train/f1:   
                                                              0.870             
                                                              train/precision:  
                                                              0.893             
                                                              train/recall:     
                                                              0.847 train/mre:  
                                                              0.097             
[2024-05-30 18:18:52,145][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/controlling_temper/0.15/9>
[2024-05-30 18:18:52,146][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 18:18:52,149][HYDRA] 	#250 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=controlling_temper data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=0
[2024-05-30 18:18:52,436][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 18:18:52,438][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 18:18:52,440][train.py][INFO] - Instantiating callbacks...
[2024-05-30 18:18:52,441][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 18:18:52,444][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 18:18:52,445][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 18:18:52,447][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 18:18:52,448][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 18:18:52,449][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 18:18:52,450][train.py][INFO] - Instantiating loggers...
[2024-05-30 18:18:52,451][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 18:18:52,453][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 18:18:52,495][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/controlling_temper/0.2/0/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.66it/s v_num: 0.000      
                                                              val/auc: 0.500    
                                                              val/f1: 0.375     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.300 val/mre:    
                                                              0.098 train/auc:  
                                                              0.831 train/f1:   
                                                              0.844             
                                                              train/precision:  
                                                              0.783             
                                                              train/recall:     
                                                              0.915 train/mre:  
                                                              0.097             
[2024-05-30 18:20:36,011][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/controlling_temper/0.2/0>
[2024-05-30 18:20:36,012][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 18:20:36,016][HYDRA] 	#251 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=controlling_temper data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=1
[rank: 0] Seed set to 1
[2024-05-30 18:20:36,305][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 18:20:36,307][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 18:20:36,309][train.py][INFO] - Instantiating callbacks...
[2024-05-30 18:20:36,310][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 18:20:36,313][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 18:20:36,314][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 18:20:36,315][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 18:20:36,315][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 18:20:36,317][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 18:20:36,318][train.py][INFO] - Instantiating loggers...
[2024-05-30 18:20:36,318][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 18:20:36,321][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 18:20:36,362][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/controlling_temper/0.2/1/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.61it/s v_num: 0.000      
                                                              val/auc: 0.500    
                                                              val/f1: 0.286     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.200 val/mre:    
                                                              0.101 train/auc:  
                                                              0.864 train/f1:   
                                                              0.852             
                                                              train/precision:  
                                                              0.939             
                                                              train/recall:     
                                                              0.780 train/mre:  
                                                              0.101             
[2024-05-30 18:22:18,335][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/controlling_temper/0.2/1>
[2024-05-30 18:22:18,335][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 18:22:18,343][HYDRA] 	#252 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=controlling_temper data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=2
[rank: 0] Seed set to 2
[2024-05-30 18:22:18,644][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 18:22:18,646][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 18:22:18,648][train.py][INFO] - Instantiating callbacks...
[2024-05-30 18:22:18,648][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 18:22:18,652][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 18:22:18,653][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 18:22:18,653][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 18:22:18,654][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 18:22:18,656][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 18:22:18,657][train.py][INFO] - Instantiating loggers...
[2024-05-30 18:22:18,657][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 18:22:18,659][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 18:22:18,702][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/controlling_temper/0.2/2/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.58it/s v_num: 0.000      
                                                              val/auc: 0.650    
                                                              val/f1: 0.632     
                                                              val/precision:    
                                                              0.667 val/recall: 
                                                              0.600 val/mre:    
                                                              0.106 train/auc:  
                                                              0.890 train/f1:   
                                                              0.896             
                                                              train/precision:  
                                                              0.848             
                                                              train/recall:     
                                                              0.949 train/mre:  
                                                              0.105             
[2024-05-30 18:24:02,115][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/controlling_temper/0.2/2>
[2024-05-30 18:24:02,115][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 18:24:02,119][HYDRA] 	#253 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=controlling_temper data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=3
[rank: 0] Seed set to 3
[2024-05-30 18:24:02,418][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 18:24:02,422][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 18:24:02,424][train.py][INFO] - Instantiating callbacks...
[2024-05-30 18:24:02,425][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 18:24:02,429][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 18:24:02,430][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 18:24:02,431][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 18:24:02,432][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 18:24:02,433][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 18:24:02,435][train.py][INFO] - Instantiating loggers...
[2024-05-30 18:24:02,435][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 18:24:02,437][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 18:24:02,482][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/controlling_temper/0.2/3/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.65it/s v_num: 0.000      
                                                              val/auc: 0.450    
                                                              val/f1: 0.154     
                                                              val/precision:    
                                                              0.333 val/recall: 
                                                              0.100 val/mre:    
                                                              0.096 train/auc:  
                                                              0.763 train/f1:   
                                                              0.770             
                                                              train/precision:  
                                                              0.746             
                                                              train/recall:     
                                                              0.797 train/mre:  
                                                              0.097             
[2024-05-30 18:25:44,718][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/controlling_temper/0.2/3>
[2024-05-30 18:25:44,718][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 18:25:44,722][HYDRA] 	#254 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=controlling_temper data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=4
[rank: 0] Seed set to 4
[2024-05-30 18:25:45,017][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 18:25:45,019][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 18:25:45,021][train.py][INFO] - Instantiating callbacks...
[2024-05-30 18:25:45,022][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 18:25:45,026][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 18:25:45,026][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 18:25:45,027][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 18:25:45,028][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 18:25:45,030][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 18:25:45,030][train.py][INFO] - Instantiating loggers...
[2024-05-30 18:25:45,031][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 18:25:45,033][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 18:25:45,076][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/controlling_temper/0.2/4/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.66it/s v_num: 0.000      
                                                              val/auc: 0.550    
                                                              val/f1: 0.308     
                                                              val/precision:    
                                                              0.667 val/recall: 
                                                              0.200 val/mre:    
                                                              0.101 train/auc:  
                                                              0.898 train/f1:   
                                                              0.903             
                                                              train/precision:  
                                                              0.862             
                                                              train/recall:     
                                                              0.949 train/mre:  
                                                              0.101             
[2024-05-30 18:27:27,270][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/controlling_temper/0.2/4>
[2024-05-30 18:27:27,271][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 18:27:27,273][HYDRA] 	#255 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=controlling_temper data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=5
[rank: 0] Seed set to 5
[2024-05-30 18:27:27,555][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 18:27:27,557][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 18:27:27,559][train.py][INFO] - Instantiating callbacks...
[2024-05-30 18:27:27,559][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 18:27:27,563][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 18:27:27,564][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 18:27:27,564][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 18:27:27,565][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 18:27:27,567][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 18:27:27,568][train.py][INFO] - Instantiating loggers...
[2024-05-30 18:27:27,568][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 18:27:27,570][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 18:27:27,614][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/controlling_temper/0.2/5/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.67it/s v_num: 0.000      
                                                              val/auc: 0.600    
                                                              val/f1: 0.500     
                                                              val/precision:    
                                                              0.667 val/recall: 
                                                              0.400 val/mre:    
                                                              0.099 train/auc:  
                                                              0.805 train/f1:   
                                                              0.800             
                                                              train/precision:  
                                                              0.821             
                                                              train/recall:     
                                                              0.780 train/mre:  
                                                              0.100             
[2024-05-30 18:29:08,427][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/controlling_temper/0.2/5>
[2024-05-30 18:29:08,428][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 18:29:08,432][HYDRA] 	#256 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=controlling_temper data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=6
[rank: 0] Seed set to 6
[2024-05-30 18:29:08,729][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 18:29:08,732][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 18:29:08,734][train.py][INFO] - Instantiating callbacks...
[2024-05-30 18:29:08,734][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 18:29:08,738][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 18:29:08,738][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 18:29:08,739][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 18:29:08,740][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 18:29:08,742][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 18:29:08,745][train.py][INFO] - Instantiating loggers...
[2024-05-30 18:29:08,745][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 18:29:08,748][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 18:29:08,848][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/controlling_temper/0.2/6/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.64it/s v_num: 0.000      
                                                              val/auc: 0.550    
                                                              val/f1: 0.526     
                                                              val/precision:    
                                                              0.556 val/recall: 
                                                              0.500 val/mre:    
                                                              0.104 train/auc:  
                                                              0.941 train/f1:   
                                                              0.943             
                                                              train/precision:  
                                                              0.906             
                                                              train/recall:     
                                                              0.983 train/mre:  
                                                              0.103             
[2024-05-30 18:30:50,947][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/controlling_temper/0.2/6>
[2024-05-30 18:30:50,947][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 18:30:50,951][HYDRA] 	#257 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=controlling_temper data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=7
[rank: 0] Seed set to 7
[2024-05-30 18:30:51,247][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 18:30:51,250][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 18:30:51,253][train.py][INFO] - Instantiating callbacks...
[2024-05-30 18:30:51,253][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 18:30:51,257][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 18:30:51,257][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 18:30:51,258][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 18:30:51,259][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 18:30:51,261][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 18:30:51,264][train.py][INFO] - Instantiating loggers...
[2024-05-30 18:30:51,264][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 18:30:51,266][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 18:30:51,359][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/controlling_temper/0.2/7/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.65it/s v_num: 0.000      
                                                              val/auc: 0.500    
                                                              val/f1: 0.167     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.100 val/mre:    
                                                              0.094 train/auc:  
                                                              0.831 train/f1:   
                                                              0.828             
                                                              train/precision:  
                                                              0.842             
                                                              train/recall:     
                                                              0.814 train/mre:  
                                                              0.093             
[2024-05-30 18:32:34,060][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/controlling_temper/0.2/7>
[2024-05-30 18:32:34,061][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 18:32:34,063][HYDRA] 	#258 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=controlling_temper data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=8
[rank: 0] Seed set to 8
[2024-05-30 18:32:34,363][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 18:32:34,366][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 18:32:34,368][train.py][INFO] - Instantiating callbacks...
[2024-05-30 18:32:34,368][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 18:32:34,372][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 18:32:34,373][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 18:32:34,373][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 18:32:34,374][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 18:32:34,376][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 18:32:34,379][train.py][INFO] - Instantiating loggers...
[2024-05-30 18:32:34,379][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 18:32:34,381][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 18:32:34,467][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/controlling_temper/0.2/8/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.64it/s v_num: 0.000      
                                                              val/auc: 0.550    
                                                              val/f1: 0.471     
                                                              val/precision:    
                                                              0.571 val/recall: 
                                                              0.400 val/mre:    
                                                              0.104 train/auc:  
                                                              0.890 train/f1:   
                                                              0.891             
                                                              train/precision:  
                                                              0.883             
                                                              train/recall:     
                                                              0.898 train/mre:  
                                                              0.102             
[2024-05-30 18:34:16,346][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/controlling_temper/0.2/8>
[2024-05-30 18:34:16,346][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 18:34:16,349][HYDRA] 	#259 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=controlling_temper data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=9
[rank: 0] Seed set to 9
[2024-05-30 18:34:16,644][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 18:34:16,646][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 18:34:16,648][train.py][INFO] - Instantiating callbacks...
[2024-05-30 18:34:16,649][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 18:34:16,652][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 18:34:16,653][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 18:34:16,654][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 18:34:16,654][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 18:34:16,656][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 18:34:16,657][train.py][INFO] - Instantiating loggers...
[2024-05-30 18:34:16,657][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 18:34:16,660][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 18:34:16,702][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/controlling_temper/0.2/9/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.67it/s v_num: 0.000      
                                                              val/auc: 0.650    
                                                              val/f1: 0.632     
                                                              val/precision:    
                                                              0.667 val/recall: 
                                                              0.600 val/mre:    
                                                              0.101 train/auc:  
                                                              0.847 train/f1:   
                                                              0.850             
                                                              train/precision:  
                                                              0.836             
                                                              train/recall:     
                                                              0.864 train/mre:  
                                                              0.100             
[2024-05-30 18:35:58,611][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/controlling_temper/0.2/9>
[2024-05-30 18:35:58,612][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 18:35:58,615][HYDRA] 	#260 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=controlling_temper data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=0
[2024-05-30 18:35:58,903][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 18:35:58,905][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 18:35:58,907][train.py][INFO] - Instantiating callbacks...
[2024-05-30 18:35:58,908][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 18:35:58,911][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 18:35:58,912][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 18:35:58,912][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 18:35:58,913][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 18:35:58,915][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 18:35:58,916][train.py][INFO] - Instantiating loggers...
[2024-05-30 18:35:58,916][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 18:35:58,918][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 18:35:58,960][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/controlling_temper/0.25/0/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.58it/s v_num: 0.000      
                                                              val/auc: 0.600    
                                                              val/f1: 0.556     
                                                              val/precision:    
                                                              0.625 val/recall: 
                                                              0.500 val/mre:    
                                                              0.104 train/auc:  
                                                              0.831 train/f1:   
                                                              0.839             
                                                              train/precision:  
                                                              0.800             
                                                              train/recall:     
                                                              0.881 train/mre:  
                                                              0.103             
[2024-05-30 18:37:42,479][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/controlling_temper/0.25/0>
[2024-05-30 18:37:42,482][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 18:37:42,490][HYDRA] 	#261 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=controlling_temper data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=1
[rank: 0] Seed set to 1
[2024-05-30 18:37:42,834][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 18:37:42,837][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 18:37:42,839][train.py][INFO] - Instantiating callbacks...
[2024-05-30 18:37:42,839][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 18:37:42,843][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 18:37:42,844][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 18:37:42,845][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 18:37:42,845][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 18:37:42,847][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 18:37:42,848][train.py][INFO] - Instantiating loggers...
[2024-05-30 18:37:42,848][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 18:37:42,850][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 18:37:42,940][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/controlling_temper/0.25/1/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.64it/s v_num: 0.000      
                                                              val/auc: 0.550    
                                                              val/f1: 0.526     
                                                              val/precision:    
                                                              0.556 val/recall: 
                                                              0.500 val/mre:    
                                                              0.108 train/auc:  
                                                              0.881 train/f1:   
                                                              0.870             
                                                              train/precision:  
                                                              0.959             
                                                              train/recall:     
                                                              0.797 train/mre:  
                                                              0.107             
[2024-05-30 18:39:26,429][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/controlling_temper/0.25/1>
[2024-05-30 18:39:26,430][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 18:39:26,433][HYDRA] 	#262 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=controlling_temper data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=2
[rank: 0] Seed set to 2
[2024-05-30 18:39:26,720][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 18:39:26,722][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 18:39:26,725][train.py][INFO] - Instantiating callbacks...
[2024-05-30 18:39:26,725][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 18:39:26,728][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 18:39:26,729][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 18:39:26,730][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 18:39:26,731][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 18:39:26,732][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 18:39:26,733][train.py][INFO] - Instantiating loggers...
[2024-05-30 18:39:26,734][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 18:39:26,736][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 18:39:26,778][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/controlling_temper/0.25/2/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.64it/s v_num: 0.000      
                                                              val/auc: 0.550    
                                                              val/f1: 0.571     
                                                              val/precision:    
                                                              0.545 val/recall: 
                                                              0.600 val/mre:    
                                                              0.110 train/auc:  
                                                              0.881 train/f1:   
                                                              0.889             
                                                              train/precision:  
                                                              0.836             
                                                              train/recall:     
                                                              0.949 train/mre:  
                                                              0.109             
[2024-05-30 18:41:08,519][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/controlling_temper/0.25/2>
[2024-05-30 18:41:08,520][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 18:41:08,523][HYDRA] 	#263 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=controlling_temper data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=3
[rank: 0] Seed set to 3
[2024-05-30 18:41:08,806][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 18:41:08,808][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 18:41:08,811][train.py][INFO] - Instantiating callbacks...
[2024-05-30 18:41:08,811][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 18:41:08,814][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 18:41:08,815][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 18:41:08,816][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 18:41:08,817][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 18:41:08,818][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 18:41:08,820][train.py][INFO] - Instantiating loggers...
[2024-05-30 18:41:08,820][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 18:41:08,822][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 18:41:08,866][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/controlling_temper/0.25/3/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.62it/s v_num: 0.000      
                                                              val/auc: 0.450    
                                                              val/f1: 0.154     
                                                              val/precision:    
                                                              0.333 val/recall: 
                                                              0.100 val/mre:    
                                                              0.102 train/auc:  
                                                              0.856 train/f1:   
                                                              0.864             
                                                              train/precision:  
                                                              0.818             
                                                              train/recall:     
                                                              0.915 train/mre:  
                                                              0.102             
[2024-05-30 18:42:52,570][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/controlling_temper/0.25/3>
[2024-05-30 18:42:52,571][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 18:42:52,575][HYDRA] 	#264 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=controlling_temper data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=4
[rank: 0] Seed set to 4
[2024-05-30 18:42:52,863][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 18:42:52,866][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 18:42:52,868][train.py][INFO] - Instantiating callbacks...
[2024-05-30 18:42:52,868][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 18:42:52,872][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 18:42:52,872][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 18:42:52,873][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 18:42:52,874][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 18:42:52,876][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 18:42:52,877][train.py][INFO] - Instantiating loggers...
[2024-05-30 18:42:52,877][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 18:42:52,879][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 18:42:52,924][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/controlling_temper/0.25/4/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.66it/s v_num: 0.000      
                                                              val/auc: 0.500    
                                                              val/f1: 0.167     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.100 val/mre:    
                                                              0.104 train/auc:  
                                                              0.847 train/f1:   
                                                              0.855             
                                                              train/precision:  
                                                              0.815             
                                                              train/recall:     
                                                              0.898 train/mre:  
                                                              0.105             
[2024-05-30 18:44:34,678][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/controlling_temper/0.25/4>
[2024-05-30 18:44:34,679][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 18:44:34,682][HYDRA] 	#265 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=controlling_temper data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=5
[rank: 0] Seed set to 5
[2024-05-30 18:44:35,011][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 18:44:35,015][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 18:44:35,018][train.py][INFO] - Instantiating callbacks...
[2024-05-30 18:44:35,019][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 18:44:35,023][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 18:44:35,024][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 18:44:35,025][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 18:44:35,026][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 18:44:35,027][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 18:44:35,033][train.py][INFO] - Instantiating loggers...
[2024-05-30 18:44:35,033][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 18:44:35,036][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 18:44:35,121][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/controlling_temper/0.25/5/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.65it/s v_num: 0.000      
                                                              val/auc: 0.450    
                                                              val/f1: 0.353     
                                                              val/precision:    
                                                              0.429 val/recall: 
                                                              0.300 val/mre:    
                                                              0.096 train/auc:  
                                                              0.805 train/f1:   
                                                              0.822             
                                                              train/precision:  
                                                              0.757             
                                                              train/recall:     
                                                              0.898 train/mre:  
                                                              0.096             
[2024-05-30 18:46:16,234][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/controlling_temper/0.25/5>
[2024-05-30 18:46:16,235][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 18:46:16,243][HYDRA] 	#266 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=controlling_temper data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=6
[rank: 0] Seed set to 6
[2024-05-30 18:46:16,545][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 18:46:16,547][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 18:46:16,550][train.py][INFO] - Instantiating callbacks...
[2024-05-30 18:46:16,550][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 18:46:16,553][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 18:46:16,554][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 18:46:16,555][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 18:46:16,556][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 18:46:16,558][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 18:46:16,561][train.py][INFO] - Instantiating loggers...
[2024-05-30 18:46:16,561][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 18:46:16,563][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 18:46:16,650][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/controlling_temper/0.25/6/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.62it/s v_num: 0.000      
                                                              val/auc: 0.600    
                                                              val/f1: 0.600     
                                                              val/precision:    
                                                              0.600 val/recall: 
                                                              0.600 val/mre:    
                                                              0.108 train/auc:  
                                                              0.949 train/f1:   
                                                              0.947             
                                                              train/precision:  
                                                              0.982             
                                                              train/recall:     
                                                              0.915 train/mre:  
                                                              0.108             
[2024-05-30 18:47:59,628][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/controlling_temper/0.25/6>
[2024-05-30 18:47:59,628][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 18:47:59,632][HYDRA] 	#267 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=controlling_temper data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=7
[rank: 0] Seed set to 7
[2024-05-30 18:47:59,933][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 18:47:59,936][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 18:47:59,939][train.py][INFO] - Instantiating callbacks...
[2024-05-30 18:47:59,939][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 18:47:59,943][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 18:47:59,944][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 18:47:59,945][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 18:47:59,945][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 18:47:59,947][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 18:47:59,950][train.py][INFO] - Instantiating loggers...
[2024-05-30 18:47:59,951][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 18:47:59,953][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 18:48:00,051][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/controlling_temper/0.25/7/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.62it/s v_num: 0.000      
                                                              val/auc: 0.550    
                                                              val/f1: 0.182     
                                                              val/precision:    
                                                              1.000 val/recall: 
                                                              0.100 val/mre:    
                                                              0.096 train/auc:  
                                                              0.822 train/f1:   
                                                              0.800             
                                                              train/precision:  
                                                              0.913             
                                                              train/recall:     
                                                              0.712 train/mre:  
                                                              0.096             
[2024-05-30 18:49:41,595][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/controlling_temper/0.25/7>
[2024-05-30 18:49:41,595][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 18:49:41,599][HYDRA] 	#268 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=controlling_temper data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=8
[rank: 0] Seed set to 8
[2024-05-30 18:49:41,902][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 18:49:41,905][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 18:49:41,908][train.py][INFO] - Instantiating callbacks...
[2024-05-30 18:49:41,908][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 18:49:41,912][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 18:49:41,913][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 18:49:41,914][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 18:49:41,915][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 18:49:41,916][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 18:49:41,920][train.py][INFO] - Instantiating loggers...
[2024-05-30 18:49:41,920][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 18:49:41,922][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 18:49:42,028][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/controlling_temper/0.25/8/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.68it/s v_num: 0.000      
                                                              val/auc: 0.600    
                                                              val/f1: 0.429     
                                                              val/precision:    
                                                              0.750 val/recall: 
                                                              0.300 val/mre:    
                                                              0.109 train/auc:  
                                                              0.890 train/f1:   
                                                              0.887             
                                                              train/precision:  
                                                              0.911             
                                                              train/recall:     
                                                              0.864 train/mre:  
                                                              0.107             
[2024-05-30 18:51:24,328][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/controlling_temper/0.25/8>
[2024-05-30 18:51:24,330][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 18:51:24,333][HYDRA] 	#269 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=controlling_temper data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=9
[rank: 0] Seed set to 9
[2024-05-30 18:51:24,643][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 18:51:24,646][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 18:51:24,648][train.py][INFO] - Instantiating callbacks...
[2024-05-30 18:51:24,648][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 18:51:24,652][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 18:51:24,652][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 18:51:24,653][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 18:51:24,654][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 18:51:24,656][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 18:51:24,657][train.py][INFO] - Instantiating loggers...
[2024-05-30 18:51:24,657][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 18:51:24,659][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 18:51:24,702][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/controlling_temper/0.25/9/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.68it/s v_num: 0.000      
                                                              val/auc: 0.550    
                                                              val/f1: 0.526     
                                                              val/precision:    
                                                              0.556 val/recall: 
                                                              0.500 val/mre:    
                                                              0.105 train/auc:  
                                                              0.822 train/f1:   
                                                              0.824             
                                                              train/precision:  
                                                              0.817             
                                                              train/recall:     
                                                              0.831 train/mre:  
                                                              0.105             
[2024-05-30 18:53:07,108][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/controlling_temper/0.25/9>
[2024-05-30 18:53:07,109][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 18:53:07,112][HYDRA] 	#270 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=controlling_temper data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=0
[2024-05-30 18:53:07,398][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 18:53:07,400][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 18:53:07,402][train.py][INFO] - Instantiating callbacks...
[2024-05-30 18:53:07,403][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 18:53:07,406][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 18:53:07,407][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 18:53:07,408][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 18:53:07,408][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 18:53:07,410][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 18:53:07,411][train.py][INFO] - Instantiating loggers...
[2024-05-30 18:53:07,412][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 18:53:07,414][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 18:53:07,458][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/controlling_temper/0.3/0/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.61it/s v_num: 0.000      
                                                              val/auc: 0.500    
                                                              val/f1: 0.500     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.500 val/mre:    
                                                              0.110 train/auc:  
                                                              0.847 train/f1:   
                                                              0.862             
                                                              train/precision:  
                                                              0.789             
                                                              train/recall:     
                                                              0.949 train/mre:  
                                                              0.109             
[2024-05-30 18:54:50,649][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/controlling_temper/0.3/0>
[2024-05-30 18:54:50,650][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 18:54:50,653][HYDRA] 	#271 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=controlling_temper data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=1
[rank: 0] Seed set to 1
[2024-05-30 18:54:50,941][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 18:54:50,943][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 18:54:50,945][train.py][INFO] - Instantiating callbacks...
[2024-05-30 18:54:50,946][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 18:54:50,949][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 18:54:50,950][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 18:54:50,951][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 18:54:50,951][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 18:54:50,953][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 18:54:50,954][train.py][INFO] - Instantiating loggers...
[2024-05-30 18:54:50,954][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 18:54:50,958][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 18:54:51,000][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/controlling_temper/0.3/1/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.64it/s v_num: 0.000      
                                                              val/auc: 0.600    
                                                              val/f1: 0.556     
                                                              val/precision:    
                                                              0.625 val/recall: 
                                                              0.500 val/mre:    
                                                              0.112 train/auc:  
                                                              0.924 train/f1:   
                                                              0.922             
                                                              train/precision:  
                                                              0.946             
                                                              train/recall:     
                                                              0.898 train/mre:  
                                                              0.112             
[2024-05-30 18:56:33,909][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/controlling_temper/0.3/1>
[2024-05-30 18:56:33,910][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 18:56:33,913][HYDRA] 	#272 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=controlling_temper data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=2
[rank: 0] Seed set to 2
[2024-05-30 18:56:34,199][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 18:56:34,201][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 18:56:34,203][train.py][INFO] - Instantiating callbacks...
[2024-05-30 18:56:34,204][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 18:56:34,207][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 18:56:34,208][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 18:56:34,209][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 18:56:34,209][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 18:56:34,211][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 18:56:34,212][train.py][INFO] - Instantiating loggers...
[2024-05-30 18:56:34,212][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 18:56:34,215][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 18:56:34,257][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/controlling_temper/0.3/2/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.64it/s v_num: 0.000      
                                                              val/auc: 0.550    
                                                              val/f1: 0.571     
                                                              val/precision:    
                                                              0.545 val/recall: 
                                                              0.600 val/mre:    
                                                              0.112 train/auc:  
                                                              0.864 train/f1:   
                                                              0.873             
                                                              train/precision:  
                                                              0.821             
                                                              train/recall:     
                                                              0.932 train/mre:  
                                                              0.110             
[2024-05-30 18:58:17,767][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/controlling_temper/0.3/2>
[2024-05-30 18:58:17,768][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 18:58:17,771][HYDRA] 	#273 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=controlling_temper data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=3
[rank: 0] Seed set to 3
[2024-05-30 18:58:18,060][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 18:58:18,063][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 18:58:18,065][train.py][INFO] - Instantiating callbacks...
[2024-05-30 18:58:18,065][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 18:58:18,069][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 18:58:18,069][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 18:58:18,070][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 18:58:18,071][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 18:58:18,073][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 18:58:18,074][train.py][INFO] - Instantiating loggers...
[2024-05-30 18:58:18,074][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 18:58:18,076][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 18:58:18,120][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/controlling_temper/0.3/3/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.61it/s v_num: 0.000      
                                                              val/auc: 0.500    
                                                              val/f1: 0.286     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.200 val/mre:    
                                                              0.105 train/auc:  
                                                              0.839 train/f1:   
                                                              0.832             
                                                              train/precision:  
                                                              0.870             
                                                              train/recall:     
                                                              0.797 train/mre:  
                                                              0.107             
[2024-05-30 19:00:00,625][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/controlling_temper/0.3/3>
[2024-05-30 19:00:00,626][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 19:00:00,629][HYDRA] 	#274 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=controlling_temper data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=4
[rank: 0] Seed set to 4
[2024-05-30 19:00:00,918][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 19:00:00,921][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 19:00:00,923][train.py][INFO] - Instantiating callbacks...
[2024-05-30 19:00:00,923][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 19:00:00,927][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 19:00:00,927][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 19:00:00,928][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 19:00:00,929][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 19:00:00,931][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 19:00:00,932][train.py][INFO] - Instantiating loggers...
[2024-05-30 19:00:00,932][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 19:00:00,936][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 19:00:00,979][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/controlling_temper/0.3/4/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.63it/s v_num: 0.000      
                                                              val/auc: 0.500    
                                                              val/f1: 0.167     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.100 val/mre:    
                                                              0.109 train/auc:  
                                                              0.847 train/f1:   
                                                              0.845             
                                                              train/precision:  
                                                              0.860             
                                                              train/recall:     
                                                              0.831 train/mre:  
                                                              0.109             
[2024-05-30 19:01:43,348][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/controlling_temper/0.3/4>
[2024-05-30 19:01:43,348][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 19:01:43,353][HYDRA] 	#275 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=controlling_temper data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=5
[rank: 0] Seed set to 5
[2024-05-30 19:01:43,670][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 19:01:43,672][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 19:01:43,674][train.py][INFO] - Instantiating callbacks...
[2024-05-30 19:01:43,675][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 19:01:43,678][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 19:01:43,679][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 19:01:43,680][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 19:01:43,680][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 19:01:43,682][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 19:01:43,706][train.py][INFO] - Instantiating loggers...
[2024-05-30 19:01:43,707][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 19:01:43,712][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 19:01:43,838][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/controlling_temper/0.3/5/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.67it/s v_num: 0.000      
                                                              val/auc: 0.500    
                                                              val/f1: 0.444     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.400 val/mre:    
                                                              0.099 train/auc:  
                                                              0.771 train/f1:   
                                                              0.794             
                                                              train/precision:  
                                                              0.722             
                                                              train/recall:     
                                                              0.881 train/mre:  
                                                              0.099             
[2024-05-30 19:03:26,109][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/controlling_temper/0.3/5>
[2024-05-30 19:03:26,110][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 19:03:26,114][HYDRA] 	#276 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=controlling_temper data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=6
[rank: 0] Seed set to 6
[2024-05-30 19:03:26,417][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 19:03:26,419][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 19:03:26,421][train.py][INFO] - Instantiating callbacks...
[2024-05-30 19:03:26,422][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 19:03:26,425][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 19:03:26,426][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 19:03:26,427][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 19:03:26,428][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 19:03:26,430][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 19:03:26,432][train.py][INFO] - Instantiating loggers...
[2024-05-30 19:03:26,433][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 19:03:26,435][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 19:03:26,528][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/controlling_temper/0.3/6/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.66it/s v_num: 0.000      
                                                              val/auc: 0.550    
                                                              val/f1: 0.308     
                                                              val/precision:    
                                                              0.667 val/recall: 
                                                              0.200 val/mre:    
                                                              0.106 train/auc:  
                                                              0.873 train/f1:   
                                                              0.882             
                                                              train/precision:  
                                                              0.824             
                                                              train/recall:     
                                                              0.949 train/mre:  
                                                              0.106             
[2024-05-30 19:05:07,743][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/controlling_temper/0.3/6>
[2024-05-30 19:05:07,745][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 19:05:07,752][HYDRA] 	#277 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=controlling_temper data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=7
[rank: 0] Seed set to 7
[2024-05-30 19:05:08,085][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 19:05:08,087][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 19:05:08,089][train.py][INFO] - Instantiating callbacks...
[2024-05-30 19:05:08,090][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 19:05:08,093][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 19:05:08,094][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 19:05:08,095][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 19:05:08,096][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 19:05:08,097][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 19:05:08,101][train.py][INFO] - Instantiating loggers...
[2024-05-30 19:05:08,101][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 19:05:08,103][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 19:05:08,190][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/controlling_temper/0.3/7/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.66it/s v_num: 0.000      
                                                              val/auc: 0.600    
                                                              val/f1: 0.333     
                                                              val/precision:    
                                                              1.000 val/recall: 
                                                              0.200 val/mre:    
                                                              0.102 train/auc:  
                                                              0.847 train/f1:   
                                                              0.842             
                                                              train/precision:  
                                                              0.873             
                                                              train/recall:     
                                                              0.814 train/mre:  
                                                              0.101             
[2024-05-30 19:06:50,113][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/controlling_temper/0.3/7>
[2024-05-30 19:06:50,114][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 19:06:50,117][HYDRA] 	#278 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=controlling_temper data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=8
[rank: 0] Seed set to 8
[2024-05-30 19:06:50,399][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 19:06:50,402][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 19:06:50,404][train.py][INFO] - Instantiating callbacks...
[2024-05-30 19:06:50,404][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 19:06:50,408][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 19:06:50,408][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 19:06:50,409][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 19:06:50,410][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 19:06:50,412][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 19:06:50,413][train.py][INFO] - Instantiating loggers...
[2024-05-30 19:06:50,413][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 19:06:50,415][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 19:06:50,459][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/controlling_temper/0.3/8/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.66it/s v_num: 0.000      
                                                              val/auc: 0.550    
                                                              val/f1: 0.400     
                                                              val/precision:    
                                                              0.600 val/recall: 
                                                              0.300 val/mre:    
                                                              0.114 train/auc:  
                                                              0.915 train/f1:   
                                                              0.909             
                                                              train/precision:  
                                                              0.980             
                                                              train/recall:     
                                                              0.847 train/mre:  
                                                              0.113             
[2024-05-30 19:08:32,896][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/controlling_temper/0.3/8>
[2024-05-30 19:08:32,897][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 19:08:32,900][HYDRA] 	#279 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=controlling_temper data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=9
[rank: 0] Seed set to 9
[2024-05-30 19:08:33,187][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 19:08:33,189][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 19:08:33,191][train.py][INFO] - Instantiating callbacks...
[2024-05-30 19:08:33,192][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 19:08:33,195][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 19:08:33,196][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 19:08:33,196][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 19:08:33,197][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 19:08:33,199][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 19:08:33,200][train.py][INFO] - Instantiating loggers...
[2024-05-30 19:08:33,200][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 19:08:33,202][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 19:08:33,245][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/controlling_temper/0.3/9/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.64it/s v_num: 0.000      
                                                              val/auc: 0.550    
                                                              val/f1: 0.526     
                                                              val/precision:    
                                                              0.556 val/recall: 
                                                              0.500 val/mre:    
                                                              0.108 train/auc:  
                                                              0.864 train/f1:   
                                                              0.871             
                                                              train/precision:  
                                                              0.831             
                                                              train/recall:     
                                                              0.915 train/mre:  
                                                              0.107             
[2024-05-30 19:10:15,261][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/controlling_temper/0.3/9>
[2024-05-30 19:10:15,262][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 19:10:15,265][HYDRA] 	#280 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_depression data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=0
[2024-05-30 19:10:15,560][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 19:10:15,562][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 19:10:15,565][train.py][INFO] - Instantiating callbacks...
[2024-05-30 19:10:15,565][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 19:10:15,568][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 19:10:15,569][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 19:10:15,570][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 19:10:15,571][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 19:10:15,572][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 19:10:15,573][train.py][INFO] - Instantiating loggers...
[2024-05-30 19:10:15,574][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 19:10:15,576][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 19:10:15,619][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/diagnosed_depression/0.0/0/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.56it/s v_num: 0.000      
                                                              val/auc: 0.750    
                                                              val/f1: 0.667     
                                                              val/precision:    
                                                              0.750 val/recall: 
                                                              0.600 val/mre: nan
                                                              train/auc: 0.833  
                                                              train/f1: 0.851   
                                                              train/precision:  
                                                              0.769             
                                                              train/recall:     
                                                              0.952 train/mre:  
                                                              nan               
[2024-05-30 19:12:00,947][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/diagnosed_depression/0.0/0>
[2024-05-30 19:12:00,947][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 19:12:00,951][HYDRA] 	#281 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_depression data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=1
[rank: 0] Seed set to 1
[2024-05-30 19:12:01,263][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 19:12:01,267][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 19:12:01,269][train.py][INFO] - Instantiating callbacks...
[2024-05-30 19:12:01,270][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 19:12:01,273][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 19:12:01,274][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 19:12:01,275][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 19:12:01,276][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 19:12:01,277][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 19:12:01,278][train.py][INFO] - Instantiating loggers...
[2024-05-30 19:12:01,279][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 19:12:01,281][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 19:12:01,325][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/diagnosed_depression/0.0/1/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.51it/s v_num: 0.000      
                                                              val/auc: 0.550    
                                                              val/f1: 0.400     
                                                              val/precision:    
                                                              0.400 val/recall: 
                                                              0.400 val/mre: nan
                                                              train/auc: 0.825  
                                                              train/f1: 0.841   
                                                              train/precision:  
                                                              0.773             
                                                              train/recall:     
                                                              0.921 train/mre:  
                                                              nan               
[2024-05-30 19:13:47,021][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/diagnosed_depression/0.0/1>
[2024-05-30 19:13:47,022][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 19:13:47,025][HYDRA] 	#282 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_depression data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=2
[rank: 0] Seed set to 2
[2024-05-30 19:13:47,316][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 19:13:47,319][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 19:13:47,321][train.py][INFO] - Instantiating callbacks...
[2024-05-30 19:13:47,321][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 19:13:47,325][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 19:13:47,326][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 19:13:47,326][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 19:13:47,327][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 19:13:47,329][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 19:13:47,330][train.py][INFO] - Instantiating loggers...
[2024-05-30 19:13:47,330][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 19:13:47,332][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 19:13:47,374][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/diagnosed_depression/0.0/2/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.53it/s v_num: 0.000      
                                                              val/auc: 0.600    
                                                              val/f1: 0.444     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.400 val/mre: nan
                                                              train/auc: 0.841  
                                                              train/f1: 0.853   
                                                              train/precision:  
                                                              0.795             
                                                              train/recall:     
                                                              0.921 train/mre:  
                                                              nan               
[2024-05-30 19:15:32,338][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/diagnosed_depression/0.0/2>
[2024-05-30 19:15:32,339][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 19:15:32,344][HYDRA] 	#283 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_depression data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=3
[rank: 0] Seed set to 3
[2024-05-30 19:15:32,644][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 19:15:32,646][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 19:15:32,648][train.py][INFO] - Instantiating callbacks...
[2024-05-30 19:15:32,649][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 19:15:32,652][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 19:15:32,653][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 19:15:32,654][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 19:15:32,655][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 19:15:32,656][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 19:15:32,660][train.py][INFO] - Instantiating loggers...
[2024-05-30 19:15:32,660][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 19:15:32,662][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 19:15:32,754][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/diagnosed_depression/0.0/3/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.47it/s v_num: 0.000      
                                                              val/auc: 0.550    
                                                              val/f1: 0.286     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.200 val/mre: nan
                                                              train/auc: 0.683  
                                                              train/f1: 0.574   
                                                              train/precision:  
                                                              0.871             
                                                              train/recall:     
                                                              0.429 train/mre:  
                                                              nan               
[2024-05-30 19:17:17,340][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/diagnosed_depression/0.0/3>
[2024-05-30 19:17:17,341][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 19:17:17,344][HYDRA] 	#284 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_depression data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=4
[rank: 0] Seed set to 4
[2024-05-30 19:17:17,644][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 19:17:17,646][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 19:17:17,648][train.py][INFO] - Instantiating callbacks...
[2024-05-30 19:17:17,649][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 19:17:17,652][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 19:17:17,653][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 19:17:17,654][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 19:17:17,655][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 19:17:17,656][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 19:17:17,660][train.py][INFO] - Instantiating loggers...
[2024-05-30 19:17:17,661][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 19:17:17,663][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 19:17:17,748][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/diagnosed_depression/0.0/4/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.51it/s v_num: 0.000      
                                                              val/auc: 0.650    
                                                              val/f1: 0.500     
                                                              val/precision:    
                                                              0.667 val/recall: 
                                                              0.400 val/mre: nan
                                                              train/auc: 0.849  
                                                              train/f1: 0.850   
                                                              train/precision:  
                                                              0.844             
                                                              train/recall:     
                                                              0.857 train/mre:  
                                                              nan               
[2024-05-30 19:19:02,715][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/diagnosed_depression/0.0/4>
[2024-05-30 19:19:02,716][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 19:19:02,719][HYDRA] 	#285 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_depression data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=5
[rank: 0] Seed set to 5
[2024-05-30 19:19:03,036][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 19:19:03,038][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 19:19:03,040][train.py][INFO] - Instantiating callbacks...
[2024-05-30 19:19:03,041][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 19:19:03,044][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 19:19:03,045][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 19:19:03,046][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 19:19:03,047][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 19:19:03,048][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 19:19:03,052][train.py][INFO] - Instantiating loggers...
[2024-05-30 19:19:03,052][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 19:19:03,054][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 19:19:03,140][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/diagnosed_depression/0.0/5/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.53it/s v_num: 0.000      
                                                              val/auc: 0.450    
                                                              val/f1: 0.333     
                                                              val/precision:    
                                                              0.286 val/recall: 
                                                              0.400 val/mre: nan
                                                              train/auc: 0.794  
                                                              train/f1: 0.822   
                                                              train/precision:  
                                                              0.723             
                                                              train/recall:     
                                                              0.952 train/mre:  
                                                              nan               
[2024-05-30 19:20:46,778][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/diagnosed_depression/0.0/5>
[2024-05-30 19:20:46,779][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 19:20:46,782][HYDRA] 	#286 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_depression data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=6
[rank: 0] Seed set to 6
[2024-05-30 19:20:47,068][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 19:20:47,070][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 19:20:47,072][train.py][INFO] - Instantiating callbacks...
[2024-05-30 19:20:47,073][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 19:20:47,076][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 19:20:47,077][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 19:20:47,078][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 19:20:47,078][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 19:20:47,080][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 19:20:47,081][train.py][INFO] - Instantiating loggers...
[2024-05-30 19:20:47,081][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 19:20:47,083][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 19:20:47,125][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/diagnosed_depression/0.0/6/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.61it/s v_num: 0.000      
                                                              val/auc: 0.450    
                                                              val/f1: 0.222     
                                                              val/precision:    
                                                              0.250 val/recall: 
                                                              0.200 val/mre: nan
                                                              train/auc: 0.921  
                                                              train/f1: 0.925   
                                                              train/precision:  
                                                              0.873             
                                                              train/recall:     
                                                              0.984 train/mre:  
                                                              nan               
[2024-05-30 19:22:29,702][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/diagnosed_depression/0.0/6>
[2024-05-30 19:22:29,702][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 19:22:29,705][HYDRA] 	#287 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_depression data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=7
[rank: 0] Seed set to 7
[2024-05-30 19:22:29,990][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 19:22:29,993][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 19:22:29,995][train.py][INFO] - Instantiating callbacks...
[2024-05-30 19:22:29,995][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 19:22:29,999][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 19:22:29,999][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 19:22:30,000][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 19:22:30,001][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 19:22:30,003][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 19:22:30,003][train.py][INFO] - Instantiating loggers...
[2024-05-30 19:22:30,004][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 19:22:30,006][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 19:22:30,048][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/diagnosed_depression/0.0/7/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.53it/s v_num: 0.000      
                                                              val/auc: 0.650    
                                                              val/f1: 0.500     
                                                              val/precision:    
                                                              0.667 val/recall: 
                                                              0.400 val/mre: nan
                                                              train/auc: 0.833  
                                                              train/f1: 0.849   
                                                              train/precision:  
                                                              0.776             
                                                              train/recall:     
                                                              0.937 train/mre:  
                                                              nan               
[2024-05-30 19:24:14,138][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/diagnosed_depression/0.0/7>
[2024-05-30 19:24:14,139][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 19:24:14,142][HYDRA] 	#288 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_depression data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=8
[rank: 0] Seed set to 8
[2024-05-30 19:24:14,432][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 19:24:14,435][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 19:24:14,437][train.py][INFO] - Instantiating callbacks...
[2024-05-30 19:24:14,437][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 19:24:14,441][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 19:24:14,441][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 19:24:14,442][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 19:24:14,443][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 19:24:14,445][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 19:24:14,446][train.py][INFO] - Instantiating loggers...
[2024-05-30 19:24:14,446][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 19:24:14,448][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 19:24:14,490][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/diagnosed_depression/0.0/8/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.42it/s v_num: 0.000      
                                                              val/auc: 0.450    
                                                              val/f1: 0.222     
                                                              val/precision:    
                                                              0.250 val/recall: 
                                                              0.200 val/mre: nan
                                                              train/auc: 0.833  
                                                              train/f1: 0.837   
                                                              train/precision:  
                                                              0.818             
                                                              train/recall:     
                                                              0.857 train/mre:  
                                                              nan               
[2024-05-30 19:26:00,113][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/diagnosed_depression/0.0/8>
[2024-05-30 19:26:00,114][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 19:26:00,117][HYDRA] 	#289 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_depression data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=9
[rank: 0] Seed set to 9
[2024-05-30 19:26:00,462][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 19:26:00,464][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 19:26:00,466][train.py][INFO] - Instantiating callbacks...
[2024-05-30 19:26:00,467][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 19:26:00,470][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 19:26:00,471][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 19:26:00,472][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 19:26:00,472][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 19:26:00,474][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 19:26:00,475][train.py][INFO] - Instantiating loggers...
[2024-05-30 19:26:00,475][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 19:26:00,477][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 19:26:00,568][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/diagnosed_depression/0.0/9/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.57it/s v_num: 0.000      
                                                              val/auc: 0.550    
                                                              val/f1: 0.286     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.200 val/mre: nan
                                                              train/auc: 0.786  
                                                              train/f1: 0.765   
                                                              train/precision:  
                                                              0.846             
                                                              train/recall:     
                                                              0.698 train/mre:  
                                                              nan               
[2024-05-30 19:27:45,007][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/diagnosed_depression/0.0/9>
[2024-05-30 19:27:45,010][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 19:27:45,014][HYDRA] 	#290 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_depression data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=0
[2024-05-30 19:27:45,307][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 19:27:45,309][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 19:27:45,311][train.py][INFO] - Instantiating callbacks...
[2024-05-30 19:27:45,312][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 19:27:45,315][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 19:27:45,316][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 19:27:45,317][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 19:27:45,317][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 19:27:45,319][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 19:27:45,320][train.py][INFO] - Instantiating loggers...
[2024-05-30 19:27:45,320][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 19:27:45,324][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 19:27:45,369][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/diagnosed_depression/0.05/0/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.49it/s v_num: 0.000      
                                                              val/auc: 0.500    
                                                              val/f1: 0.364     
                                                              val/precision:    
                                                              0.333 val/recall: 
                                                              0.400 val/mre:    
                                                              0.069 train/auc:  
                                                              0.825 train/f1:   
                                                              0.817             
                                                              train/precision:  
                                                              0.860             
                                                              train/recall:     
                                                              0.778 train/mre:  
                                                              0.070             
[2024-05-30 19:29:29,979][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/diagnosed_depression/0.05/0>
[2024-05-30 19:29:29,980][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 19:29:29,983][HYDRA] 	#291 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_depression data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=1
[rank: 0] Seed set to 1
[2024-05-30 19:29:30,289][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 19:29:30,291][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 19:29:30,293][train.py][INFO] - Instantiating callbacks...
[2024-05-30 19:29:30,294][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 19:29:30,297][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 19:29:30,298][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 19:29:30,299][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 19:29:30,299][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 19:29:30,301][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 19:29:30,306][train.py][INFO] - Instantiating loggers...
[2024-05-30 19:29:30,307][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 19:29:30,309][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 19:29:30,400][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/diagnosed_depression/0.05/1/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.50it/s v_num: 0.000      
                                                              val/auc: 0.650    
                                                              val/f1: 0.545     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.600 val/mre:    
                                                              0.058 train/auc:  
                                                              0.794 train/f1:   
                                                              0.776             
                                                              train/precision:  
                                                              0.849             
                                                              train/recall:     
                                                              0.714 train/mre:  
                                                              0.060             
[2024-05-30 19:31:14,948][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/diagnosed_depression/0.05/1>
[2024-05-30 19:31:14,948][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 19:31:14,952][HYDRA] 	#292 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_depression data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=2
[rank: 0] Seed set to 2
[2024-05-30 19:31:15,256][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 19:31:15,258][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 19:31:15,260][train.py][INFO] - Instantiating callbacks...
[2024-05-30 19:31:15,261][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 19:31:15,264][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 19:31:15,265][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 19:31:15,266][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 19:31:15,266][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 19:31:15,268][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 19:31:15,272][train.py][INFO] - Instantiating loggers...
[2024-05-30 19:31:15,272][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 19:31:15,274][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 19:31:15,359][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/diagnosed_depression/0.05/2/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.50it/s v_num: 0.000      
                                                              val/auc: 0.500    
                                                              val/f1: 0.364     
                                                              val/precision:    
                                                              0.333 val/recall: 
                                                              0.400 val/mre:    
                                                              0.054 train/auc:  
                                                              0.897 train/f1:   
                                                              0.905             
                                                              train/precision:  
                                                              0.838             
                                                              train/recall:     
                                                              0.984 train/mre:  
                                                              0.059             
[2024-05-30 19:33:00,816][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/diagnosed_depression/0.05/2>
[2024-05-30 19:33:00,816][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 19:33:00,820][HYDRA] 	#293 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_depression data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=3
[rank: 0] Seed set to 3
[2024-05-30 19:33:01,141][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 19:33:01,145][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 19:33:01,149][train.py][INFO] - Instantiating callbacks...
[2024-05-30 19:33:01,149][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 19:33:01,154][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 19:33:01,154][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 19:33:01,155][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 19:33:01,156][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 19:33:01,158][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 19:33:01,161][train.py][INFO] - Instantiating loggers...
[2024-05-30 19:33:01,162][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 19:33:01,165][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 19:33:01,314][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/diagnosed_depression/0.05/3/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.48it/s v_num: 0.000      
                                                              val/auc: 0.550    
                                                              val/f1: 0.286     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.200 val/mre:    
                                                              0.048 train/auc:  
                                                              0.698 train/f1:   
                                                              0.596             
                                                              train/precision:  
                                                              0.903             
                                                              train/recall:     
                                                              0.444 train/mre:  
                                                              0.051             
[2024-05-30 19:34:47,336][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/diagnosed_depression/0.05/3>
[2024-05-30 19:34:47,336][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 19:34:47,340][HYDRA] 	#294 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_depression data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=4
[rank: 0] Seed set to 4
[2024-05-30 19:34:47,627][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 19:34:47,630][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 19:34:47,632][train.py][INFO] - Instantiating callbacks...
[2024-05-30 19:34:47,632][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 19:34:47,636][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 19:34:47,636][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 19:34:47,637][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 19:34:47,638][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 19:34:47,640][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 19:34:47,641][train.py][INFO] - Instantiating loggers...
[2024-05-30 19:34:47,641][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 19:34:47,643][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 19:34:47,686][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/diagnosed_depression/0.05/4/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.54it/s v_num: 0.000      
                                                              val/auc: 0.450    
                                                              val/f1: 0.400     
                                                              val/precision:    
                                                              0.300 val/recall: 
                                                              0.600 val/mre:    
                                                              0.058 train/auc:  
                                                              0.802 train/f1:   
                                                              0.783             
                                                              train/precision:  
                                                              0.865             
                                                              train/recall:     
                                                              0.714 train/mre:  
                                                              0.060             
[2024-05-30 19:36:31,299][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/diagnosed_depression/0.05/4>
[2024-05-30 19:36:31,300][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 19:36:31,304][HYDRA] 	#295 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_depression data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=5
[rank: 0] Seed set to 5
[2024-05-30 19:36:31,591][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 19:36:31,594][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 19:36:31,596][train.py][INFO] - Instantiating callbacks...
[2024-05-30 19:36:31,596][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 19:36:31,600][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 19:36:31,600][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 19:36:31,601][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 19:36:31,602][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 19:36:31,604][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 19:36:31,605][train.py][INFO] - Instantiating loggers...
[2024-05-30 19:36:31,605][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 19:36:31,607][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 19:36:31,649][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/diagnosed_depression/0.05/5/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.58it/s v_num: 0.000      
                                                              val/auc: 0.350    
                                                              val/f1: 0.182     
                                                              val/precision:    
                                                              0.167 val/recall: 
                                                              0.200 val/mre:    
                                                              0.056 train/auc:  
                                                              0.921 train/f1:   
                                                              0.919             
                                                              train/precision:  
                                                              0.934             
                                                              train/recall:     
                                                              0.905 train/mre:  
                                                              0.061             
[2024-05-30 19:38:14,786][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/diagnosed_depression/0.05/5>
[2024-05-30 19:38:14,787][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 19:38:14,790][HYDRA] 	#296 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_depression data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=6
[rank: 0] Seed set to 6
[2024-05-30 19:38:15,078][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 19:38:15,080][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 19:38:15,082][train.py][INFO] - Instantiating callbacks...
[2024-05-30 19:38:15,083][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 19:38:15,086][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 19:38:15,087][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 19:38:15,088][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 19:38:15,089][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 19:38:15,090][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 19:38:15,091][train.py][INFO] - Instantiating loggers...
[2024-05-30 19:38:15,091][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 19:38:15,094][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 19:38:15,135][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/diagnosed_depression/0.05/6/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.52it/s v_num: 0.000      
                                                              val/auc: 0.400    
                                                              val/f1: 0.200     
                                                              val/precision:    
                                                              0.200 val/recall: 
                                                              0.200 val/mre:    
                                                              0.060 train/auc:  
                                                              0.889 train/f1:   
                                                              0.892             
                                                              train/precision:  
                                                              0.866             
                                                              train/recall:     
                                                              0.921 train/mre:  
                                                              0.066             
[2024-05-30 19:39:59,113][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/diagnosed_depression/0.05/6>
[2024-05-30 19:39:59,114][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 19:39:59,118][HYDRA] 	#297 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_depression data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=7
[rank: 0] Seed set to 7
[2024-05-30 19:39:59,423][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 19:39:59,426][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 19:39:59,429][train.py][INFO] - Instantiating callbacks...
[2024-05-30 19:39:59,430][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 19:39:59,434][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 19:39:59,435][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 19:39:59,436][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 19:39:59,437][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 19:39:59,439][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 19:39:59,439][train.py][INFO] - Instantiating loggers...
[2024-05-30 19:39:59,440][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 19:39:59,442][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 19:39:59,485][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/diagnosed_depression/0.05/7/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.54it/s v_num: 0.000      
                                                              val/auc: 0.500    
                                                              val/f1: 0.429     
                                                              val/precision:    
                                                              0.333 val/recall: 
                                                              0.600 val/mre:    
                                                              0.058 train/auc:  
                                                              0.825 train/f1:   
                                                              0.831             
                                                              train/precision:  
                                                              0.806             
                                                              train/recall:     
                                                              0.857 train/mre:  
                                                              0.057             
[2024-05-30 19:41:43,049][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/diagnosed_depression/0.05/7>
[2024-05-30 19:41:43,049][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 19:41:43,053][HYDRA] 	#298 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_depression data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=8
[rank: 0] Seed set to 8
[2024-05-30 19:41:43,339][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 19:41:43,342][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 19:41:43,344][train.py][INFO] - Instantiating callbacks...
[2024-05-30 19:41:43,344][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 19:41:43,347][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 19:41:43,348][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 19:41:43,349][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 19:41:43,350][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 19:41:43,352][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 19:41:43,352][train.py][INFO] - Instantiating loggers...
[2024-05-30 19:41:43,353][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 19:41:43,355][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 19:41:43,401][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/diagnosed_depression/0.05/8/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.52it/s v_num: 0.000      
                                                              val/auc: 0.550    
                                                              val/f1: 0.286     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.200 val/mre:    
                                                              0.065 train/auc:  
                                                              0.849 train/f1:   
                                                              0.843             
                                                              train/precision:  
                                                              0.879             
                                                              train/recall:     
                                                              0.810 train/mre:  
                                                              0.065             
[2024-05-30 19:43:27,439][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/diagnosed_depression/0.05/8>
[2024-05-30 19:43:27,440][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 19:43:27,443][HYDRA] 	#299 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_depression data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=9
[rank: 0] Seed set to 9
[2024-05-30 19:43:27,744][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 19:43:27,747][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 19:43:27,749][train.py][INFO] - Instantiating callbacks...
[2024-05-30 19:43:27,749][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 19:43:27,752][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 19:43:27,753][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 19:43:27,754][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 19:43:27,755][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 19:43:27,756][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 19:43:27,759][train.py][INFO] - Instantiating loggers...
[2024-05-30 19:43:27,760][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 19:43:27,762][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 19:43:27,845][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/diagnosed_depression/0.05/9/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.60it/s v_num: 0.000      
                                                              val/auc: 0.600    
                                                              val/f1: 0.500     
                                                              val/precision:    
                                                              0.429 val/recall: 
                                                              0.600 val/mre:    
                                                              0.058 train/auc:  
                                                              0.873 train/f1:   
                                                              0.869             
                                                              train/precision:  
                                                              0.898             
                                                              train/recall:     
                                                              0.841 train/mre:  
                                                              0.064             
[2024-05-30 19:45:12,199][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/diagnosed_depression/0.05/9>
[2024-05-30 19:45:12,200][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 19:45:12,203][HYDRA] 	#300 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_depression data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=0
[2024-05-30 19:45:12,503][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 19:45:12,505][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 19:45:12,507][train.py][INFO] - Instantiating callbacks...
[2024-05-30 19:45:12,508][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 19:45:12,511][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 19:45:12,512][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 19:45:12,513][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 19:45:12,514][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 19:45:12,515][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 19:45:12,519][train.py][INFO] - Instantiating loggers...
[2024-05-30 19:45:12,519][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 19:45:12,521][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 19:45:12,612][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/diagnosed_depression/0.1/0/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.51it/s v_num: 0.000      
                                                              val/auc: 0.600    
                                                              val/f1: 0.444     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.400 val/mre:    
                                                              0.068 train/auc:  
                                                              0.810 train/f1:   
                                                              0.803             
                                                              train/precision:  
                                                              0.831             
                                                              train/recall:     
                                                              0.778 train/mre:  
                                                              0.069             
[2024-05-30 19:46:56,544][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/diagnosed_depression/0.1/0>
[2024-05-30 19:46:56,545][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 19:46:56,549][HYDRA] 	#301 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_depression data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=1
[rank: 0] Seed set to 1
[2024-05-30 19:46:56,846][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 19:46:56,848][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 19:46:56,850][train.py][INFO] - Instantiating callbacks...
[2024-05-30 19:46:56,851][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 19:46:56,854][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 19:46:56,855][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 19:46:56,856][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 19:46:56,857][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 19:46:56,859][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 19:46:56,859][train.py][INFO] - Instantiating loggers...
[2024-05-30 19:46:56,860][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 19:46:56,862][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 19:46:56,907][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/diagnosed_depression/0.1/1/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.57it/s v_num: 0.000      
                                                              val/auc: 0.450    
                                                              val/f1: 0.333     
                                                              val/precision:    
                                                              0.286 val/recall: 
                                                              0.400 val/mre:    
                                                              0.070 train/auc:  
                                                              0.802 train/f1:   
                                                              0.786             
                                                              train/precision:  
                                                              0.852             
                                                              train/recall:     
                                                              0.730 train/mre:  
                                                              0.073             
[2024-05-30 19:48:41,570][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/diagnosed_depression/0.1/1>
[2024-05-30 19:48:41,570][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 19:48:41,573][HYDRA] 	#302 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_depression data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=2
[rank: 0] Seed set to 2
[2024-05-30 19:48:41,859][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 19:48:41,861][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 19:48:41,863][train.py][INFO] - Instantiating callbacks...
[2024-05-30 19:48:41,864][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 19:48:41,867][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 19:48:41,868][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 19:48:41,868][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 19:48:41,869][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 19:48:41,871][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 19:48:41,872][train.py][INFO] - Instantiating loggers...
[2024-05-30 19:48:41,872][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 19:48:41,874][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 19:48:41,915][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/diagnosed_depression/0.1/2/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.50it/s v_num: 0.000      
                                                              val/auc: 0.700    
                                                              val/f1: 0.600     
                                                              val/precision:    
                                                              0.600 val/recall: 
                                                              0.600 val/mre:    
                                                              0.056 train/auc:  
                                                              0.921 train/f1:   
                                                              0.922             
                                                              train/precision:  
                                                              0.908             
                                                              train/recall:     
                                                              0.937 train/mre:  
                                                              0.061             
[2024-05-30 19:50:27,927][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/diagnosed_depression/0.1/2>
[2024-05-30 19:50:27,927][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 19:50:27,931][HYDRA] 	#303 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_depression data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=3
[rank: 0] Seed set to 3
[2024-05-30 19:50:28,222][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 19:50:28,224][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 19:50:28,226][train.py][INFO] - Instantiating callbacks...
[2024-05-30 19:50:28,227][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 19:50:28,230][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 19:50:28,231][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 19:50:28,232][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 19:50:28,233][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 19:50:28,234][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 19:50:28,235][train.py][INFO] - Instantiating loggers...
[2024-05-30 19:50:28,236][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 19:50:28,238][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 19:50:28,281][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/diagnosed_depression/0.1/3/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.53it/s v_num: 0.000      
                                                              val/auc: 0.550    
                                                              val/f1: 0.286     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.200 val/mre:    
                                                              0.052 train/auc:  
                                                              0.722 train/f1:   
                                                              0.615             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              0.444 train/mre:  
                                                              0.054             
[2024-05-30 19:52:13,092][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/diagnosed_depression/0.1/3>
[2024-05-30 19:52:13,093][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 19:52:13,096][HYDRA] 	#304 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_depression data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=4
[rank: 0] Seed set to 4
[2024-05-30 19:52:13,379][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 19:52:13,382][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 19:52:13,384][train.py][INFO] - Instantiating callbacks...
[2024-05-30 19:52:13,384][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 19:52:13,388][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 19:52:13,388][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 19:52:13,389][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 19:52:13,390][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 19:52:13,392][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 19:52:13,393][train.py][INFO] - Instantiating loggers...
[2024-05-30 19:52:13,393][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 19:52:13,395][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 19:52:13,437][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/diagnosed_depression/0.1/4/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.55it/s v_num: 0.000      
                                                              val/auc: 0.500    
                                                              val/f1: 0.364     
                                                              val/precision:    
                                                              0.333 val/recall: 
                                                              0.400 val/mre:    
                                                              0.061 train/auc:  
                                                              0.817 train/f1:   
                                                              0.807             
                                                              train/precision:  
                                                              0.857             
                                                              train/recall:     
                                                              0.762 train/mre:  
                                                              0.064             
[2024-05-30 19:53:57,050][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/diagnosed_depression/0.1/4>
[2024-05-30 19:53:57,052][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 19:53:57,070][HYDRA] 	#305 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_depression data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=5
[rank: 0] Seed set to 5
[2024-05-30 19:53:57,474][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 19:53:57,477][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 19:53:57,480][train.py][INFO] - Instantiating callbacks...
[2024-05-30 19:53:57,480][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 19:53:57,484][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 19:53:57,485][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 19:53:57,486][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 19:53:57,486][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 19:53:57,488][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 19:53:57,491][train.py][INFO] - Instantiating loggers...
[2024-05-30 19:53:57,492][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 19:53:57,494][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 19:53:57,582][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/diagnosed_depression/0.1/5/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.51it/s v_num: 0.000      
                                                              val/auc: 0.450    
                                                              val/f1: 0.333     
                                                              val/precision:    
                                                              0.286 val/recall: 
                                                              0.400 val/mre:    
                                                              0.059 train/auc:  
                                                              0.897 train/f1:   
                                                              0.899             
                                                              train/precision:  
                                                              0.879             
                                                              train/recall:     
                                                              0.921 train/mre:  
                                                              0.065             
[2024-05-30 19:55:41,250][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/diagnosed_depression/0.1/5>
[2024-05-30 19:55:41,251][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 19:55:41,254][HYDRA] 	#306 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_depression data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=6
[rank: 0] Seed set to 6
[2024-05-30 19:55:41,559][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 19:55:41,561][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 19:55:41,564][train.py][INFO] - Instantiating callbacks...
[2024-05-30 19:55:41,564][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 19:55:41,567][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 19:55:41,568][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 19:55:41,569][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 19:55:41,570][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 19:55:41,571][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 19:55:41,575][train.py][INFO] - Instantiating loggers...
[2024-05-30 19:55:41,575][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 19:55:41,577][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 19:55:41,662][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/diagnosed_depression/0.1/6/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.53it/s v_num: 0.000      
                                                              val/auc: 0.400    
                                                              val/f1: 0.200     
                                                              val/precision:    
                                                              0.200 val/recall: 
                                                              0.200 val/mre:    
                                                              0.065 train/auc:  
                                                              0.921 train/f1:   
                                                              0.918             
                                                              train/precision:  
                                                              0.949             
                                                              train/recall:     
                                                              0.889 train/mre:  
                                                              0.070             
[2024-05-30 19:57:25,180][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/diagnosed_depression/0.1/6>
[2024-05-30 19:57:25,180][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 19:57:25,184][HYDRA] 	#307 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_depression data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=7
[rank: 0] Seed set to 7
[2024-05-30 19:57:25,492][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 19:57:25,494][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 19:57:25,496][train.py][INFO] - Instantiating callbacks...
[2024-05-30 19:57:25,497][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 19:57:25,500][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 19:57:25,501][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 19:57:25,501][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 19:57:25,502][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 19:57:25,504][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 19:57:25,508][train.py][INFO] - Instantiating loggers...
[2024-05-30 19:57:25,508][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 19:57:25,511][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 19:57:25,594][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/diagnosed_depression/0.1/7/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.57it/s v_num: 0.000      
                                                              val/auc: 0.300    
                                                              val/f1: 0.167     
                                                              val/precision:    
                                                              0.143 val/recall: 
                                                              0.200 val/mre:    
                                                              0.059 train/auc:  
                                                              0.786 train/f1:   
                                                              0.780             
                                                              train/precision:  
                                                              0.800             
                                                              train/recall:     
                                                              0.762 train/mre:  
                                                              0.060             
[2024-05-30 19:59:09,037][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/diagnosed_depression/0.1/7>
[2024-05-30 19:59:09,037][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 19:59:09,041][HYDRA] 	#308 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_depression data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=8
[rank: 0] Seed set to 8
[2024-05-30 19:59:09,328][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 19:59:09,330][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 19:59:09,332][train.py][INFO] - Instantiating callbacks...
[2024-05-30 19:59:09,333][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 19:59:09,336][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 19:59:09,337][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 19:59:09,337][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 19:59:09,338][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 19:59:09,340][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 19:59:09,341][train.py][INFO] - Instantiating loggers...
[2024-05-30 19:59:09,341][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 19:59:09,343][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 19:59:09,385][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/diagnosed_depression/0.1/8/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.54it/s v_num: 0.000      
                                                              val/auc: 0.550    
                                                              val/f1: 0.286     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.200 val/mre:    
                                                              0.061 train/auc:  
                                                              0.794 train/f1:   
                                                              0.800             
                                                              train/precision:  
                                                              0.776             
                                                              train/recall:     
                                                              0.825 train/mre:  
                                                              0.064             
[2024-05-30 20:00:53,978][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/diagnosed_depression/0.1/8>
[2024-05-30 20:00:53,979][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 20:00:53,982][HYDRA] 	#309 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_depression data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=9
[rank: 0] Seed set to 9
[2024-05-30 20:00:54,273][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 20:00:54,275][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 20:00:54,278][train.py][INFO] - Instantiating callbacks...
[2024-05-30 20:00:54,278][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 20:00:54,281][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 20:00:54,282][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 20:00:54,283][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 20:00:54,284][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 20:00:54,286][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 20:00:54,286][train.py][INFO] - Instantiating loggers...
[2024-05-30 20:00:54,287][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 20:00:54,289][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 20:00:54,332][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/diagnosed_depression/0.1/9/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.53it/s v_num: 0.000      
                                                              val/auc: 0.700    
                                                              val/f1: 0.600     
                                                              val/precision:    
                                                              0.600 val/recall: 
                                                              0.600 val/mre:    
                                                              0.060 train/auc:  
                                                              0.889 train/f1:   
                                                              0.889             
                                                              train/precision:  
                                                              0.889             
                                                              train/recall:     
                                                              0.889 train/mre:  
                                                              0.065             
[2024-05-30 20:02:37,348][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/diagnosed_depression/0.1/9>
[2024-05-30 20:02:37,349][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 20:02:37,352][HYDRA] 	#310 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_depression data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=0
[2024-05-30 20:02:37,641][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 20:02:37,644][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 20:02:37,646][train.py][INFO] - Instantiating callbacks...
[2024-05-30 20:02:37,646][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 20:02:37,650][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 20:02:37,650][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 20:02:37,651][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 20:02:37,652][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 20:02:37,654][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 20:02:37,655][train.py][INFO] - Instantiating loggers...
[2024-05-30 20:02:37,655][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 20:02:37,657][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 20:02:37,700][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/diagnosed_depression/0.15/0/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.49it/s v_num: 0.000      
                                                              val/auc: 0.700    
                                                              val/f1: 0.600     
                                                              val/precision:    
                                                              0.600 val/recall: 
                                                              0.600 val/mre:    
                                                              0.063 train/auc:  
                                                              0.810 train/f1:   
                                                              0.810             
                                                              train/precision:  
                                                              0.810             
                                                              train/recall:     
                                                              0.810 train/mre:  
                                                              0.067             
[2024-05-30 20:04:21,347][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/diagnosed_depression/0.15/0>
[2024-05-30 20:04:21,348][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 20:04:21,351][HYDRA] 	#311 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_depression data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=1
[rank: 0] Seed set to 1
[2024-05-30 20:04:21,646][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 20:04:21,648][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 20:04:21,650][train.py][INFO] - Instantiating callbacks...
[2024-05-30 20:04:21,651][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 20:04:21,654][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 20:04:21,655][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 20:04:21,656][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 20:04:21,656][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 20:04:21,658][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 20:04:21,659][train.py][INFO] - Instantiating loggers...
[2024-05-30 20:04:21,659][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 20:04:21,662][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 20:04:21,705][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/diagnosed_depression/0.15/1/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.55it/s v_num: 0.000      
                                                              val/auc: 0.500    
                                                              val/f1: 0.364     
                                                              val/precision:    
                                                              0.333 val/recall: 
                                                              0.400 val/mre:    
                                                              0.070 train/auc:  
                                                              0.770 train/f1:   
                                                              0.782             
                                                              train/precision:  
                                                              0.743             
                                                              train/recall:     
                                                              0.825 train/mre:  
                                                              0.068             
[2024-05-30 20:06:08,059][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/diagnosed_depression/0.15/1>
[2024-05-30 20:06:08,060][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 20:06:08,063][HYDRA] 	#312 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_depression data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=2
[rank: 0] Seed set to 2
[2024-05-30 20:06:08,366][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 20:06:08,369][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 20:06:08,371][train.py][INFO] - Instantiating callbacks...
[2024-05-30 20:06:08,372][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 20:06:08,376][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 20:06:08,376][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 20:06:08,377][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 20:06:08,378][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 20:06:08,380][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 20:06:08,383][train.py][INFO] - Instantiating loggers...
[2024-05-30 20:06:08,383][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 20:06:08,385][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 20:06:08,483][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/diagnosed_depression/0.15/2/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.54it/s v_num: 0.000      
                                                              val/auc: 0.800    
                                                              val/f1: 0.727     
                                                              val/precision:    
                                                              0.667 val/recall: 
                                                              0.800 val/mre:    
                                                              0.059 train/auc:  
                                                              0.881 train/f1:   
                                                              0.885             
                                                              train/precision:  
                                                              0.853             
                                                              train/recall:     
                                                              0.921 train/mre:  
                                                              0.063             
[2024-05-30 20:07:53,351][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/diagnosed_depression/0.15/2>
[2024-05-30 20:07:53,352][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 20:07:53,358][HYDRA] 	#313 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_depression data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=3
[rank: 0] Seed set to 3
[2024-05-30 20:07:53,682][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 20:07:53,685][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 20:07:53,687][train.py][INFO] - Instantiating callbacks...
[2024-05-30 20:07:53,687][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 20:07:53,690][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 20:07:53,691][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 20:07:53,692][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 20:07:53,693][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 20:07:53,694][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 20:07:53,698][train.py][INFO] - Instantiating loggers...
[2024-05-30 20:07:53,699][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 20:07:53,701][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 20:07:53,787][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/diagnosed_depression/0.15/3/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.52it/s v_num: 0.000      
                                                              val/auc: 0.550    
                                                              val/f1: 0.286     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.200 val/mre:    
                                                              0.051 train/auc:  
                                                              0.690 train/f1:   
                                                              0.571             
                                                              train/precision:  
                                                              0.929             
                                                              train/recall:     
                                                              0.413 train/mre:  
                                                              0.055             
[2024-05-30 20:09:38,393][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/diagnosed_depression/0.15/3>
[2024-05-30 20:09:38,395][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 20:09:38,399][HYDRA] 	#314 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_depression data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=4
[rank: 0] Seed set to 4
[2024-05-30 20:09:38,750][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 20:09:38,754][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 20:09:38,756][train.py][INFO] - Instantiating callbacks...
[2024-05-30 20:09:38,756][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 20:09:38,760][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 20:09:38,760][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 20:09:38,761][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 20:09:38,762][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 20:09:38,764][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 20:09:38,765][train.py][INFO] - Instantiating loggers...
[2024-05-30 20:09:38,765][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 20:09:38,767][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 20:09:38,821][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/diagnosed_depression/0.15/4/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.53it/s v_num: 0.000      
                                                              val/auc: 0.550    
                                                              val/f1: 0.400     
                                                              val/precision:    
                                                              0.400 val/recall: 
                                                              0.400 val/mre:    
                                                              0.063 train/auc:  
                                                              0.770 train/f1:   
                                                              0.734             
                                                              train/precision:  
                                                              0.870             
                                                              train/recall:     
                                                              0.635 train/mre:  
                                                              0.067             
[2024-05-30 20:11:23,548][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/diagnosed_depression/0.15/4>
[2024-05-30 20:11:23,549][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 20:11:23,552][HYDRA] 	#315 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_depression data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=5
[rank: 0] Seed set to 5
[2024-05-30 20:11:23,849][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 20:11:23,852][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 20:11:23,854][train.py][INFO] - Instantiating callbacks...
[2024-05-30 20:11:23,854][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 20:11:23,858][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 20:11:23,859][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 20:11:23,859][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 20:11:23,860][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 20:11:23,862][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 20:11:23,863][train.py][INFO] - Instantiating loggers...
[2024-05-30 20:11:23,863][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 20:11:23,865][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 20:11:23,908][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/diagnosed_depression/0.15/5/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.52it/s v_num: 0.000      
                                                              val/auc: 0.400    
                                                              val/f1: 0.308     
                                                              val/precision:    
                                                              0.250 val/recall: 
                                                              0.400 val/mre:    
                                                              0.064 train/auc:  
                                                              0.897 train/f1:   
                                                              0.893             
                                                              train/precision:  
                                                              0.931             
                                                              train/recall:     
                                                              0.857 train/mre:  
                                                              0.069             
[2024-05-30 20:13:07,351][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/diagnosed_depression/0.15/5>
[2024-05-30 20:13:07,352][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 20:13:07,355][HYDRA] 	#316 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_depression data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=6
[rank: 0] Seed set to 6
[2024-05-30 20:13:07,641][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 20:13:07,643][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 20:13:07,645][train.py][INFO] - Instantiating callbacks...
[2024-05-30 20:13:07,646][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 20:13:07,649][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 20:13:07,650][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 20:13:07,651][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 20:13:07,651][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 20:13:07,653][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 20:13:07,655][train.py][INFO] - Instantiating loggers...
[2024-05-30 20:13:07,655][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 20:13:07,657][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 20:13:07,700][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/diagnosed_depression/0.15/6/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.55it/s v_num: 0.000      
                                                              val/auc: 0.400    
                                                              val/f1: 0.200     
                                                              val/precision:    
                                                              0.200 val/recall: 
                                                              0.200 val/mre:    
                                                              0.067 train/auc:  
                                                              0.929 train/f1:   
                                                              0.928             
                                                              train/precision:  
                                                              0.935             
                                                              train/recall:     
                                                              0.921 train/mre:  
                                                              0.072             
[2024-05-30 20:14:51,083][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/diagnosed_depression/0.15/6>
[2024-05-30 20:14:51,084][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 20:14:51,088][HYDRA] 	#317 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_depression data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=7
[rank: 0] Seed set to 7
[2024-05-30 20:14:51,377][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 20:14:51,380][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 20:14:51,382][train.py][INFO] - Instantiating callbacks...
[2024-05-30 20:14:51,382][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 20:14:51,386][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 20:14:51,386][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 20:14:51,387][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 20:14:51,388][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 20:14:51,390][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 20:14:51,391][train.py][INFO] - Instantiating loggers...
[2024-05-30 20:14:51,391][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 20:14:51,393][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 20:14:51,438][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/diagnosed_depression/0.15/7/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.55it/s v_num: 0.000      
                                                              val/auc: 0.550    
                                                              val/f1: 0.286     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.200 val/mre:    
                                                              0.056 train/auc:  
                                                              0.675 train/f1:   
                                                              0.539             
                                                              train/precision:  
                                                              0.923             
                                                              train/recall:     
                                                              0.381 train/mre:  
                                                              0.058             
[2024-05-30 20:16:35,310][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/diagnosed_depression/0.15/7>
[2024-05-30 20:16:35,311][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 20:16:35,314][HYDRA] 	#318 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_depression data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=8
[rank: 0] Seed set to 8
[2024-05-30 20:16:35,617][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 20:16:35,619][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 20:16:35,621][train.py][INFO] - Instantiating callbacks...
[2024-05-30 20:16:35,622][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 20:16:35,625][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 20:16:35,626][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 20:16:35,627][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 20:16:35,628][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 20:16:35,629][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 20:16:35,633][train.py][INFO] - Instantiating loggers...
[2024-05-30 20:16:35,633][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 20:16:35,635][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 20:16:35,722][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/diagnosed_depression/0.15/8/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.54it/s v_num: 0.000      
                                                              val/auc: 0.550    
                                                              val/f1: 0.286     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.200 val/mre:    
                                                              0.065 train/auc:  
                                                              0.873 train/f1:   
                                                              0.875             
                                                              train/precision:  
                                                              0.862             
                                                              train/recall:     
                                                              0.889 train/mre:  
                                                              0.068             
[2024-05-30 20:18:20,056][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/diagnosed_depression/0.15/8>
[2024-05-30 20:18:20,056][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 20:18:20,060][HYDRA] 	#319 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_depression data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=9
[rank: 0] Seed set to 9
[2024-05-30 20:18:20,363][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 20:18:20,365][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 20:18:20,367][train.py][INFO] - Instantiating callbacks...
[2024-05-30 20:18:20,368][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 20:18:20,371][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 20:18:20,372][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 20:18:20,372][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 20:18:20,373][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 20:18:20,375][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 20:18:20,379][train.py][INFO] - Instantiating loggers...
[2024-05-30 20:18:20,379][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 20:18:20,383][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 20:18:20,479][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/diagnosed_depression/0.15/9/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.62it/s v_num: 0.000      
                                                              val/auc: 0.800    
                                                              val/f1: 0.727     
                                                              val/precision:    
                                                              0.667 val/recall: 
                                                              0.800 val/mre:    
                                                              0.062 train/auc:  
                                                              0.810 train/f1:   
                                                              0.800             
                                                              train/precision:  
                                                              0.842             
                                                              train/recall:     
                                                              0.762 train/mre:  
                                                              0.065             
[2024-05-30 20:20:03,562][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/diagnosed_depression/0.15/9>
[2024-05-30 20:20:03,563][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 20:20:03,566][HYDRA] 	#320 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_depression data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=0
[2024-05-30 20:20:03,867][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 20:20:03,870][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 20:20:03,872][train.py][INFO] - Instantiating callbacks...
[2024-05-30 20:20:03,872][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 20:20:03,876][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 20:20:03,876][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 20:20:03,877][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 20:20:03,878][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 20:20:03,880][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 20:20:03,881][train.py][INFO] - Instantiating loggers...
[2024-05-30 20:20:03,881][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 20:20:03,883][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 20:20:03,930][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/diagnosed_depression/0.2/0/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.51it/s v_num: 0.000      
                                                              val/auc: 0.600    
                                                              val/f1: 0.500     
                                                              val/precision:    
                                                              0.429 val/recall: 
                                                              0.600 val/mre:    
                                                              0.070 train/auc:  
                                                              0.921 train/f1:   
                                                              0.921             
                                                              train/precision:  
                                                              0.921             
                                                              train/recall:     
                                                              0.921 train/mre:  
                                                              0.073             
[2024-05-30 20:21:49,216][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/diagnosed_depression/0.2/0>
[2024-05-30 20:21:49,216][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 20:21:49,221][HYDRA] 	#321 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_depression data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=1
[rank: 0] Seed set to 1
[2024-05-30 20:21:49,539][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 20:21:49,542][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 20:21:49,544][train.py][INFO] - Instantiating callbacks...
[2024-05-30 20:21:49,544][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 20:21:49,547][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 20:21:49,548][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 20:21:49,549][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 20:21:49,550][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 20:21:49,552][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 20:21:49,552][train.py][INFO] - Instantiating loggers...
[2024-05-30 20:21:49,553][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 20:21:49,555][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 20:21:49,599][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/diagnosed_depression/0.2/1/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.54it/s v_num: 0.000      
                                                              val/auc: 0.750    
                                                              val/f1: 0.667     
                                                              val/precision:    
                                                              0.750 val/recall: 
                                                              0.600 val/mre:    
                                                              0.074 train/auc:  
                                                              0.849 train/f1:   
                                                              0.848             
                                                              train/precision:  
                                                              0.855             
                                                              train/recall:     
                                                              0.841 train/mre:  
                                                              0.078             
[2024-05-30 20:23:34,187][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/diagnosed_depression/0.2/1>
[2024-05-30 20:23:34,188][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 20:23:34,191][HYDRA] 	#322 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_depression data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=2
[rank: 0] Seed set to 2
[2024-05-30 20:23:34,479][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 20:23:34,481][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 20:23:34,483][train.py][INFO] - Instantiating callbacks...
[2024-05-30 20:23:34,484][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 20:23:34,487][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 20:23:34,488][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 20:23:34,489][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 20:23:34,489][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 20:23:34,491][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 20:23:34,492][train.py][INFO] - Instantiating loggers...
[2024-05-30 20:23:34,492][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 20:23:34,496][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 20:23:34,538][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/diagnosed_depression/0.2/2/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.54it/s v_num: 0.000      
                                                              val/auc: 0.750    
                                                              val/f1: 0.667     
                                                              val/precision:    
                                                              0.571 val/recall: 
                                                              0.800 val/mre:    
                                                              0.064 train/auc:  
                                                              0.897 train/f1:   
                                                              0.901             
                                                              train/precision:  
                                                              0.868             
                                                              train/recall:     
                                                              0.937 train/mre:  
                                                              0.068             
[2024-05-30 20:25:19,512][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/diagnosed_depression/0.2/2>
[2024-05-30 20:25:19,513][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 20:25:19,516][HYDRA] 	#323 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_depression data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=3
[rank: 0] Seed set to 3
[2024-05-30 20:25:19,800][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 20:25:19,802][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 20:25:19,804][train.py][INFO] - Instantiating callbacks...
[2024-05-30 20:25:19,805][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 20:25:19,808][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 20:25:19,809][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 20:25:19,810][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 20:25:19,810][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 20:25:19,812][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 20:25:19,813][train.py][INFO] - Instantiating loggers...
[2024-05-30 20:25:19,813][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 20:25:19,817][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 20:25:19,859][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/diagnosed_depression/0.2/3/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.46it/s v_num: 0.000      
                                                              val/auc: 0.550    
                                                              val/f1: 0.286     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.200 val/mre:    
                                                              0.056 train/auc:  
                                                              0.698 train/f1:   
                                                              0.604             
                                                              train/precision:  
                                                              0.879             
                                                              train/recall:     
                                                              0.460 train/mre:  
                                                              0.061             
[2024-05-30 20:27:04,828][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/diagnosed_depression/0.2/3>
[2024-05-30 20:27:04,829][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 20:27:04,832][HYDRA] 	#324 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_depression data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=4
[rank: 0] Seed set to 4
[2024-05-30 20:27:05,131][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 20:27:05,134][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 20:27:05,136][train.py][INFO] - Instantiating callbacks...
[2024-05-30 20:27:05,136][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 20:27:05,139][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 20:27:05,140][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 20:27:05,141][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 20:27:05,142][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 20:27:05,144][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 20:27:05,170][train.py][INFO] - Instantiating loggers...
[2024-05-30 20:27:05,171][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 20:27:05,176][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 20:27:05,288][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/diagnosed_depression/0.2/4/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.53it/s v_num: 0.000      
                                                              val/auc: 0.700    
                                                              val/f1: 0.600     
                                                              val/precision:    
                                                              0.600 val/recall: 
                                                              0.600 val/mre:    
                                                              0.070 train/auc:  
                                                              0.746 train/f1:   
                                                              0.704             
                                                              train/precision:  
                                                              0.844             
                                                              train/recall:     
                                                              0.603 train/mre:  
                                                              0.072             
[2024-05-30 20:28:48,790][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/diagnosed_depression/0.2/4>
[2024-05-30 20:28:48,790][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 20:28:48,794][HYDRA] 	#325 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_depression data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=5
[rank: 0] Seed set to 5
[2024-05-30 20:28:49,109][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 20:28:49,111][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 20:28:49,113][train.py][INFO] - Instantiating callbacks...
[2024-05-30 20:28:49,114][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 20:28:49,117][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 20:28:49,118][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 20:28:49,119][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 20:28:49,119][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 20:28:49,121][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 20:28:49,124][train.py][INFO] - Instantiating loggers...
[2024-05-30 20:28:49,125][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 20:28:49,127][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 20:28:49,222][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/diagnosed_depression/0.2/5/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.48it/s v_num: 0.000      
                                                              val/auc: 0.350    
                                                              val/f1: 0.182     
                                                              val/precision:    
                                                              0.167 val/recall: 
                                                              0.200 val/mre:    
                                                              0.067 train/auc:  
                                                              0.897 train/f1:   
                                                              0.894             
                                                              train/precision:  
                                                              0.917             
                                                              train/recall:     
                                                              0.873 train/mre:  
                                                              0.073             
[2024-05-30 20:30:32,822][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/diagnosed_depression/0.2/5>
[2024-05-30 20:30:32,823][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 20:30:32,826][HYDRA] 	#326 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_depression data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=6
[rank: 0] Seed set to 6
[2024-05-30 20:30:33,113][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 20:30:33,116][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 20:30:33,118][train.py][INFO] - Instantiating callbacks...
[2024-05-30 20:30:33,118][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 20:30:33,121][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 20:30:33,122][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 20:30:33,123][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 20:30:33,124][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 20:30:33,125][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 20:30:33,126][train.py][INFO] - Instantiating loggers...
[2024-05-30 20:30:33,127][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 20:30:33,129][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 20:30:33,170][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/diagnosed_depression/0.2/6/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.58it/s v_num: 0.000      
                                                              val/auc: 0.650    
                                                              val/f1: 0.571     
                                                              val/precision:    
                                                              0.444 val/recall: 
                                                              0.800 val/mre:    
                                                              0.071 train/auc:  
                                                              0.960 train/f1:   
                                                              0.960             
                                                              train/precision:  
                                                              0.968             
                                                              train/recall:     
                                                              0.952 train/mre:  
                                                              0.076             
[2024-05-30 20:32:16,539][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/diagnosed_depression/0.2/6>
[2024-05-30 20:32:16,539][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 20:32:16,543][HYDRA] 	#327 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_depression data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=7
[rank: 0] Seed set to 7
[2024-05-30 20:32:16,826][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 20:32:16,828][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 20:32:16,830][train.py][INFO] - Instantiating callbacks...
[2024-05-30 20:32:16,831][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 20:32:16,834][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 20:32:16,835][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 20:32:16,836][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 20:32:16,836][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 20:32:16,838][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 20:32:16,839][train.py][INFO] - Instantiating loggers...
[2024-05-30 20:32:16,839][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 20:32:16,842][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 20:32:16,883][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/diagnosed_depression/0.2/7/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.54it/s v_num: 0.000      
                                                              val/auc: 0.550    
                                                              val/f1: 0.286     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.200 val/mre:    
                                                              0.057 train/auc:  
                                                              0.675 train/f1:   
                                                              0.539             
                                                              train/precision:  
                                                              0.923             
                                                              train/recall:     
                                                              0.381 train/mre:  
                                                              0.058             
[2024-05-30 20:34:01,313][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/diagnosed_depression/0.2/7>
[2024-05-30 20:34:01,313][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 20:34:01,317][HYDRA] 	#328 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_depression data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=8
[rank: 0] Seed set to 8
[2024-05-30 20:34:01,623][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 20:34:01,626][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 20:34:01,628][train.py][INFO] - Instantiating callbacks...
[2024-05-30 20:34:01,628][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 20:34:01,632][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 20:34:01,633][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 20:34:01,633][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 20:34:01,634][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 20:34:01,636][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 20:34:01,637][train.py][INFO] - Instantiating loggers...
[2024-05-30 20:34:01,637][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 20:34:01,639][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 20:34:01,684][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/diagnosed_depression/0.2/8/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.56it/s v_num: 0.000      
                                                              val/auc: 0.550    
                                                              val/f1: 0.286     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.200 val/mre:    
                                                              0.064 train/auc:  
                                                              0.849 train/f1:   
                                                              0.850             
                                                              train/precision:  
                                                              0.844             
                                                              train/recall:     
                                                              0.857 train/mre:  
                                                              0.068             
[2024-05-30 20:35:45,012][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/diagnosed_depression/0.2/8>
[2024-05-30 20:35:45,013][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 20:35:45,016][HYDRA] 	#329 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_depression data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=9
[rank: 0] Seed set to 9
[2024-05-30 20:35:45,308][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 20:35:45,310][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 20:35:45,312][train.py][INFO] - Instantiating callbacks...
[2024-05-30 20:35:45,313][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 20:35:45,316][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 20:35:45,317][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 20:35:45,318][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 20:35:45,318][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 20:35:45,320][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 20:35:45,321][train.py][INFO] - Instantiating loggers...
[2024-05-30 20:35:45,321][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 20:35:45,324][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 20:35:45,366][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/diagnosed_depression/0.2/9/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.57it/s v_num: 0.000      
                                                              val/auc: 0.550    
                                                              val/f1: 0.400     
                                                              val/precision:    
                                                              0.400 val/recall: 
                                                              0.400 val/mre:    
                                                              0.067 train/auc:  
                                                              0.817 train/f1:   
                                                              0.800             
                                                              train/precision:  
                                                              0.885             
                                                              train/recall:     
                                                              0.730 train/mre:  
                                                              0.071             
[2024-05-30 20:37:28,274][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/diagnosed_depression/0.2/9>
[2024-05-30 20:37:28,275][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 20:37:28,279][HYDRA] 	#330 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_depression data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=0
[2024-05-30 20:37:28,578][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 20:37:28,580][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 20:37:28,582][train.py][INFO] - Instantiating callbacks...
[2024-05-30 20:37:28,583][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 20:37:28,586][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 20:37:28,587][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 20:37:28,587][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 20:37:28,588][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 20:37:28,590][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 20:37:28,594][train.py][INFO] - Instantiating loggers...
[2024-05-30 20:37:28,594][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 20:37:28,596][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 20:37:28,685][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/diagnosed_depression/0.25/0/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.54it/s v_num: 0.000      
                                                              val/auc: 0.550    
                                                              val/f1: 0.462     
                                                              val/precision:    
                                                              0.375 val/recall: 
                                                              0.600 val/mre:    
                                                              0.071 train/auc:  
                                                              0.929 train/f1:   
                                                              0.926             
                                                              train/precision:  
                                                              0.966             
                                                              train/recall:     
                                                              0.889 train/mre:  
                                                              0.075             
[2024-05-30 20:39:12,929][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/diagnosed_depression/0.25/0>
[2024-05-30 20:39:12,930][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 20:39:12,933][HYDRA] 	#331 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_depression data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=1
[rank: 0] Seed set to 1
[2024-05-30 20:39:13,235][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 20:39:13,238][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 20:39:13,240][train.py][INFO] - Instantiating callbacks...
[2024-05-30 20:39:13,240][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 20:39:13,243][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 20:39:13,244][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 20:39:13,245][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 20:39:13,246][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 20:39:13,247][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 20:39:13,252][train.py][INFO] - Instantiating loggers...
[2024-05-30 20:39:13,252][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 20:39:13,254][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 20:39:13,340][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/diagnosed_depression/0.25/1/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.55it/s v_num: 0.000      
                                                              val/auc: 0.450    
                                                              val/f1: 0.222     
                                                              val/precision:    
                                                              0.250 val/recall: 
                                                              0.200 val/mre:    
                                                              0.069 train/auc:  
                                                              0.722 train/f1:   
                                                              0.679             
                                                              train/precision:  
                                                              0.804             
                                                              train/recall:     
                                                              0.587 train/mre:  
                                                              0.073             
[2024-05-30 20:40:57,821][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/diagnosed_depression/0.25/1>
[2024-05-30 20:40:57,822][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 20:40:57,825][HYDRA] 	#332 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_depression data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=2
[rank: 0] Seed set to 2
[2024-05-30 20:40:58,111][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 20:40:58,114][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 20:40:58,116][train.py][INFO] - Instantiating callbacks...
[2024-05-30 20:40:58,116][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 20:40:58,119][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 20:40:58,120][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 20:40:58,121][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 20:40:58,122][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 20:40:58,124][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 20:40:58,124][train.py][INFO] - Instantiating loggers...
[2024-05-30 20:40:58,125][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 20:40:58,127][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 20:40:58,169][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/diagnosed_depression/0.25/2/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.54it/s v_num: 0.000      
                                                              val/auc: 0.650    
                                                              val/f1: 0.545     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.600 val/mre:    
                                                              0.067 train/auc:  
                                                              0.881 train/f1:   
                                                              0.882             
                                                              train/precision:  
                                                              0.875             
                                                              train/recall:     
                                                              0.889 train/mre:  
                                                              0.072             
[2024-05-30 20:42:44,817][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/diagnosed_depression/0.25/2>
[2024-05-30 20:42:44,818][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 20:42:44,821][HYDRA] 	#333 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_depression data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=3
[rank: 0] Seed set to 3
[2024-05-30 20:42:45,107][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 20:42:45,109][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 20:42:45,111][train.py][INFO] - Instantiating callbacks...
[2024-05-30 20:42:45,112][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 20:42:45,115][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 20:42:45,116][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 20:42:45,117][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 20:42:45,117][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 20:42:45,119][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 20:42:45,120][train.py][INFO] - Instantiating loggers...
[2024-05-30 20:42:45,121][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 20:42:45,123][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 20:42:45,166][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/diagnosed_depression/0.25/3/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.45it/s v_num: 0.000      
                                                              val/auc: 0.550    
                                                              val/f1: 0.400     
                                                              val/precision:    
                                                              0.400 val/recall: 
                                                              0.400 val/mre:    
                                                              0.062 train/auc:  
                                                              0.770 train/f1:   
                                                              0.724             
                                                              train/precision:  
                                                              0.905             
                                                              train/recall:     
                                                              0.603 train/mre:  
                                                              0.067             
[2024-05-30 20:44:30,917][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/diagnosed_depression/0.25/3>
[2024-05-30 20:44:30,918][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 20:44:30,922][HYDRA] 	#334 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_depression data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=4
[rank: 0] Seed set to 4
[2024-05-30 20:44:31,211][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 20:44:31,214][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 20:44:31,216][train.py][INFO] - Instantiating callbacks...
[2024-05-30 20:44:31,216][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 20:44:31,220][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 20:44:31,221][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 20:44:31,221][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 20:44:31,222][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 20:44:31,224][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 20:44:31,225][train.py][INFO] - Instantiating loggers...
[2024-05-30 20:44:31,225][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 20:44:31,227][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 20:44:31,270][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/diagnosed_depression/0.25/4/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.52it/s v_num: 0.000      
                                                              val/auc: 0.700    
                                                              val/f1: 0.600     
                                                              val/precision:    
                                                              0.600 val/recall: 
                                                              0.600 val/mre:    
                                                              0.070 train/auc:  
                                                              0.722 train/f1:   
                                                              0.646             
                                                              train/precision:  
                                                              0.889             
                                                              train/recall:     
                                                              0.508 train/mre:  
                                                              0.073             
[2024-05-30 20:46:16,220][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/diagnosed_depression/0.25/4>
[2024-05-30 20:46:16,220][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 20:46:16,224][HYDRA] 	#335 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_depression data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=5
[rank: 0] Seed set to 5
[2024-05-30 20:46:16,537][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 20:46:16,539][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 20:46:16,541][train.py][INFO] - Instantiating callbacks...
[2024-05-30 20:46:16,542][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 20:46:16,549][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 20:46:16,550][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 20:46:16,551][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 20:46:16,551][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 20:46:16,553][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 20:46:16,554][train.py][INFO] - Instantiating loggers...
[2024-05-30 20:46:16,554][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 20:46:16,557][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 20:46:16,632][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/diagnosed_depression/0.25/5/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.60it/s v_num: 0.000      
                                                              val/auc: 0.400    
                                                              val/f1: 0.308     
                                                              val/precision:    
                                                              0.250 val/recall: 
                                                              0.400 val/mre:    
                                                              0.067 train/auc:  
                                                              0.889 train/f1:   
                                                              0.889             
                                                              train/precision:  
                                                              0.889             
                                                              train/recall:     
                                                              0.889 train/mre:  
                                                              0.073             
[2024-05-30 20:48:00,362][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/diagnosed_depression/0.25/5>
[2024-05-30 20:48:00,362][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 20:48:00,367][HYDRA] 	#336 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_depression data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=6
[rank: 0] Seed set to 6
[2024-05-30 20:48:00,670][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 20:48:00,673][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 20:48:00,676][train.py][INFO] - Instantiating callbacks...
[2024-05-30 20:48:00,676][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 20:48:00,680][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 20:48:00,681][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 20:48:00,682][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 20:48:00,683][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 20:48:00,684][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 20:48:00,688][train.py][INFO] - Instantiating loggers...
[2024-05-30 20:48:00,688][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 20:48:00,690][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 20:48:00,776][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/diagnosed_depression/0.25/6/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.51it/s v_num: 0.000      
                                                              val/auc: 0.600    
                                                              val/f1: 0.533     
                                                              val/precision:    
                                                              0.400 val/recall: 
                                                              0.800 val/mre:    
                                                              0.072 train/auc:  
                                                              0.937 train/f1:   
                                                              0.938             
                                                              train/precision:  
                                                              0.910             
                                                              train/recall:     
                                                              0.968 train/mre:  
                                                              0.077             
[2024-05-30 20:49:45,556][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/diagnosed_depression/0.25/6>
[2024-05-30 20:49:45,557][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 20:49:45,560][HYDRA] 	#337 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_depression data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=7
[rank: 0] Seed set to 7
[2024-05-30 20:49:45,912][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 20:49:45,916][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 20:49:45,918][train.py][INFO] - Instantiating callbacks...
[2024-05-30 20:49:45,919][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 20:49:45,926][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 20:49:45,927][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 20:49:45,928][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 20:49:45,928][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 20:49:45,930][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 20:49:45,936][train.py][INFO] - Instantiating loggers...
[2024-05-30 20:49:45,936][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 20:49:45,938][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 20:49:46,162][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/diagnosed_depression/0.25/7/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.60it/s v_num: 0.000      
                                                              val/auc: 0.550    
                                                              val/f1: 0.286     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.200 val/mre:    
                                                              0.060 train/auc:  
                                                              0.675 train/f1:   
                                                              0.529             
                                                              train/precision:  
                                                              0.958             
                                                              train/recall:     
                                                              0.365 train/mre:  
                                                              0.062             
[2024-05-30 20:51:29,855][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/diagnosed_depression/0.25/7>
[2024-05-30 20:51:29,856][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 20:51:29,860][HYDRA] 	#338 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_depression data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=8
[rank: 0] Seed set to 8
[2024-05-30 20:51:30,145][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 20:51:30,147][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 20:51:30,149][train.py][INFO] - Instantiating callbacks...
[2024-05-30 20:51:30,150][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 20:51:30,153][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 20:51:30,154][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 20:51:30,155][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 20:51:30,155][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 20:51:30,157][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 20:51:30,158][train.py][INFO] - Instantiating loggers...
[2024-05-30 20:51:30,158][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 20:51:30,161][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 20:51:30,204][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/diagnosed_depression/0.25/8/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.60it/s v_num: 0.000      
                                                              val/auc: 0.550    
                                                              val/f1: 0.286     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.200 val/mre:    
                                                              0.068 train/auc:  
                                                              0.841 train/f1:   
                                                              0.825             
                                                              train/precision:  
                                                              0.922             
                                                              train/recall:     
                                                              0.746 train/mre:  
                                                              0.072             
[2024-05-30 20:53:12,813][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/diagnosed_depression/0.25/8>
[2024-05-30 20:53:12,814][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 20:53:12,817][HYDRA] 	#339 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_depression data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=9
[rank: 0] Seed set to 9
[2024-05-30 20:53:13,103][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 20:53:13,106][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 20:53:13,108][train.py][INFO] - Instantiating callbacks...
[2024-05-30 20:53:13,108][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 20:53:13,111][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 20:53:13,112][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 20:53:13,113][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 20:53:13,114][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 20:53:13,115][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 20:53:13,116][train.py][INFO] - Instantiating loggers...
[2024-05-30 20:53:13,117][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 20:53:13,119][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 20:53:13,160][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/diagnosed_depression/0.25/9/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.58it/s v_num: 0.000      
                                                              val/auc: 0.600    
                                                              val/f1: 0.444     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.400 val/mre:    
                                                              0.069 train/auc:  
                                                              0.897 train/f1:   
                                                              0.894             
                                                              train/precision:  
                                                              0.917             
                                                              train/recall:     
                                                              0.873 train/mre:  
                                                              0.073             
[2024-05-30 20:54:57,244][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/diagnosed_depression/0.25/9>
[2024-05-30 20:54:57,245][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 20:54:57,249][HYDRA] 	#340 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_depression data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=0
[2024-05-30 20:54:57,931][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 20:54:57,933][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 20:54:57,935][train.py][INFO] - Instantiating callbacks...
[2024-05-30 20:54:57,936][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 20:54:57,939][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 20:54:57,940][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 20:54:57,941][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 20:54:57,941][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 20:54:57,943][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 20:54:57,944][train.py][INFO] - Instantiating loggers...
[2024-05-30 20:54:57,944][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 20:54:57,947][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 20:54:57,989][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/diagnosed_depression/0.3/0/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.54it/s v_num: 0.000      
                                                              val/auc: 0.550    
                                                              val/f1: 0.462     
                                                              val/precision:    
                                                              0.375 val/recall: 
                                                              0.600 val/mre:    
                                                              0.078 train/auc:  
                                                              0.913 train/f1:   
                                                              0.912             
                                                              train/precision:  
                                                              0.919             
                                                              train/recall:     
                                                              0.905 train/mre:  
                                                              0.081             
[2024-05-30 20:56:41,712][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/diagnosed_depression/0.3/0>
[2024-05-30 20:56:41,713][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 20:56:41,716][HYDRA] 	#341 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_depression data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=1
[rank: 0] Seed set to 1
[2024-05-30 20:56:42,031][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 20:56:42,034][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 20:56:42,036][train.py][INFO] - Instantiating callbacks...
[2024-05-30 20:56:42,036][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 20:56:42,040][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 20:56:42,041][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 20:56:42,042][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 20:56:42,042][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 20:56:42,044][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 20:56:42,045][train.py][INFO] - Instantiating loggers...
[2024-05-30 20:56:42,045][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 20:56:42,047][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 20:56:42,091][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/diagnosed_depression/0.3/1/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.58it/s v_num: 0.000      
                                                              val/auc: 0.500    
                                                              val/f1: 0.250     
                                                              val/precision:    
                                                              0.333 val/recall: 
                                                              0.200 val/mre:    
                                                              0.074 train/auc:  
                                                              0.794 train/f1:   
                                                              0.780             
                                                              train/precision:  
                                                              0.836             
                                                              train/recall:     
                                                              0.730 train/mre:  
                                                              0.078             
[2024-05-30 20:58:25,957][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/diagnosed_depression/0.3/1>
[2024-05-30 20:58:25,958][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 20:58:25,962][HYDRA] 	#342 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_depression data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=2
[rank: 0] Seed set to 2
[2024-05-30 20:58:26,264][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 20:58:26,267][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 20:58:26,269][train.py][INFO] - Instantiating callbacks...
[2024-05-30 20:58:26,269][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 20:58:26,273][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 20:58:26,273][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 20:58:26,274][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 20:58:26,275][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 20:58:26,277][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 20:58:26,280][train.py][INFO] - Instantiating loggers...
[2024-05-30 20:58:26,280][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 20:58:26,282][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 20:58:26,372][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/diagnosed_depression/0.3/2/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.56it/s v_num: 0.000      
                                                              val/auc: 0.600    
                                                              val/f1: 0.444     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.400 val/mre:    
                                                              0.068 train/auc:  
                                                              0.833 train/f1:   
                                                              0.817             
                                                              train/precision:  
                                                              0.904             
                                                              train/recall:     
                                                              0.746 train/mre:  
                                                              0.072             
[2024-05-30 21:00:11,009][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/diagnosed_depression/0.3/2>
[2024-05-30 21:00:11,010][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 21:00:11,013][HYDRA] 	#343 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_depression data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=3
[rank: 0] Seed set to 3
[2024-05-30 21:00:11,315][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 21:00:11,318][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 21:00:11,320][train.py][INFO] - Instantiating callbacks...
[2024-05-30 21:00:11,320][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 21:00:11,324][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 21:00:11,324][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 21:00:11,325][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 21:00:11,326][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 21:00:11,328][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 21:00:11,331][train.py][INFO] - Instantiating loggers...
[2024-05-30 21:00:11,331][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 21:00:11,333][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 21:00:11,421][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/diagnosed_depression/0.3/3/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.53it/s v_num: 0.000      
                                                              val/auc: 0.400    
                                                              val/f1: 0.200     
                                                              val/precision:    
                                                              0.200 val/recall: 
                                                              0.200 val/mre:    
                                                              0.065 train/auc:  
                                                              0.778 train/f1:   
                                                              0.731             
                                                              train/precision:  
                                                              0.927             
                                                              train/recall:     
                                                              0.603 train/mre:  
                                                              0.070             
[2024-05-30 21:01:56,922][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/diagnosed_depression/0.3/3>
[2024-05-30 21:01:56,923][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 21:01:56,933][HYDRA] 	#344 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_depression data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=4
[rank: 0] Seed set to 4
[2024-05-30 21:01:57,242][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 21:01:57,244][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 21:01:57,246][train.py][INFO] - Instantiating callbacks...
[2024-05-30 21:01:57,246][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 21:01:57,250][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 21:01:57,251][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 21:01:57,251][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 21:01:57,252][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 21:01:57,254][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 21:01:57,255][train.py][INFO] - Instantiating loggers...
[2024-05-30 21:01:57,255][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 21:01:57,257][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 21:01:57,319][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/diagnosed_depression/0.3/4/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.55it/s v_num: 0.000      
                                                              val/auc: 0.650    
                                                              val/f1: 0.500     
                                                              val/precision:    
                                                              0.667 val/recall: 
                                                              0.400 val/mre:    
                                                              0.069 train/auc:  
                                                              0.722 train/f1:   
                                                              0.632             
                                                              train/precision:  
                                                              0.938             
                                                              train/recall:     
                                                              0.476 train/mre:  
                                                              0.073             
[2024-05-30 21:03:41,364][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/diagnosed_depression/0.3/4>
[2024-05-30 21:03:41,365][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 21:03:41,368][HYDRA] 	#345 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_depression data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=5
[rank: 0] Seed set to 5
[2024-05-30 21:03:41,664][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 21:03:41,667][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 21:03:41,669][train.py][INFO] - Instantiating callbacks...
[2024-05-30 21:03:41,670][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 21:03:41,673][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 21:03:41,674][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 21:03:41,675][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 21:03:41,675][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 21:03:41,677][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 21:03:41,678][train.py][INFO] - Instantiating loggers...
[2024-05-30 21:03:41,678][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 21:03:41,681][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 21:03:41,723][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/diagnosed_depression/0.3/5/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.57it/s v_num: 0.000      
                                                              val/auc: 0.450    
                                                              val/f1: 0.222     
                                                              val/precision:    
                                                              0.250 val/recall: 
                                                              0.200 val/mre:    
                                                              0.067 train/auc:  
                                                              0.889 train/f1:   
                                                              0.889             
                                                              train/precision:  
                                                              0.889             
                                                              train/recall:     
                                                              0.889 train/mre:  
                                                              0.072             
[2024-05-30 21:05:24,425][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/diagnosed_depression/0.3/5>
[2024-05-30 21:05:24,426][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 21:05:24,430][HYDRA] 	#346 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_depression data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=6
[rank: 0] Seed set to 6
[2024-05-30 21:05:24,842][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 21:05:24,846][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 21:05:24,848][train.py][INFO] - Instantiating callbacks...
[2024-05-30 21:05:24,849][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 21:05:24,853][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 21:05:24,854][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 21:05:24,854][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 21:05:24,855][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 21:05:24,857][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 21:05:24,858][train.py][INFO] - Instantiating loggers...
[2024-05-30 21:05:24,858][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 21:05:24,860][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 21:05:24,971][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/diagnosed_depression/0.3/6/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.53it/s v_num: 0.000      
                                                              val/auc: 0.450    
                                                              val/f1: 0.333     
                                                              val/precision:    
                                                              0.286 val/recall: 
                                                              0.400 val/mre:    
                                                              0.081 train/auc:  
                                                              0.929 train/f1:   
                                                              0.929             
                                                              train/precision:  
                                                              0.922             
                                                              train/recall:     
                                                              0.937 train/mre:  
                                                              0.086             
[2024-05-30 21:07:10,022][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/diagnosed_depression/0.3/6>
[2024-05-30 21:07:10,023][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 21:07:10,026][HYDRA] 	#347 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_depression data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=7
[rank: 0] Seed set to 7
[2024-05-30 21:07:10,317][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 21:07:10,319][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 21:07:10,321][train.py][INFO] - Instantiating callbacks...
[2024-05-30 21:07:10,321][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 21:07:10,325][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 21:07:10,326][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 21:07:10,327][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 21:07:10,327][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 21:07:10,329][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 21:07:10,330][train.py][INFO] - Instantiating loggers...
[2024-05-30 21:07:10,331][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 21:07:10,333][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 21:07:10,376][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/diagnosed_depression/0.3/7/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.57it/s v_num: 0.000      
                                                              val/auc: 0.550    
                                                              val/f1: 0.286     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.200 val/mre:    
                                                              0.065 train/auc:  
                                                              0.714 train/f1:   
                                                              0.609             
                                                              train/precision:  
                                                              0.966             
                                                              train/recall:     
                                                              0.444 train/mre:  
                                                              0.067             
[2024-05-30 21:08:53,583][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/diagnosed_depression/0.3/7>
[2024-05-30 21:08:53,584][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 21:08:53,589][HYDRA] 	#348 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_depression data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=8
[rank: 0] Seed set to 8
[2024-05-30 21:08:53,939][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 21:08:53,942][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 21:08:53,944][train.py][INFO] - Instantiating callbacks...
[2024-05-30 21:08:53,945][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 21:08:53,949][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 21:08:53,950][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 21:08:53,951][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 21:08:53,951][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 21:08:53,953][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 21:08:53,957][train.py][INFO] - Instantiating loggers...
[2024-05-30 21:08:53,957][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 21:08:53,960][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 21:08:54,061][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/diagnosed_depression/0.3/8/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.61it/s v_num: 0.000      
                                                              val/auc: 0.400    
                                                              val/f1: 0.308     
                                                              val/precision:    
                                                              0.250 val/recall: 
                                                              0.400 val/mre:    
                                                              0.074 train/auc:  
                                                              0.881 train/f1:   
                                                              0.880             
                                                              train/precision:  
                                                              0.887             
                                                              train/recall:     
                                                              0.873 train/mre:  
                                                              0.079             
[2024-05-30 21:10:37,023][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/diagnosed_depression/0.3/8>
[2024-05-30 21:10:37,024][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 21:10:37,027][HYDRA] 	#349 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_depression data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=9
[rank: 0] Seed set to 9
[2024-05-30 21:10:37,331][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 21:10:37,334][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 21:10:37,337][train.py][INFO] - Instantiating callbacks...
[2024-05-30 21:10:37,337][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 21:10:37,341][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 21:10:37,342][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 21:10:37,342][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 21:10:37,343][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 21:10:37,345][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 21:10:37,348][train.py][INFO] - Instantiating loggers...
[2024-05-30 21:10:37,348][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 21:10:37,351][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 21:10:37,432][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/diagnosed_depression/0.3/9/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.55it/s v_num: 0.000      
                                                              val/auc: 0.700    
                                                              val/f1: 0.600     
                                                              val/precision:    
                                                              0.600 val/recall: 
                                                              0.600 val/mre:    
                                                              0.074 train/auc:  
                                                              0.889 train/f1:   
                                                              0.877             
                                                              train/precision:  
                                                              0.980             
                                                              train/recall:     
                                                              0.794 train/mre:  
                                                              0.078             
[2024-05-30 21:12:21,686][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/diagnosed_depression/0.3/9>
[2024-05-30 21:12:21,687][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 21:12:21,689][HYDRA] 	#350 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_p_t_s_d data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=0
[2024-05-30 21:12:21,978][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 21:12:21,981][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 21:12:21,983][train.py][INFO] - Instantiating callbacks...
[2024-05-30 21:12:21,983][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 21:12:21,986][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 21:12:21,987][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 21:12:21,988][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 21:12:21,988][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 21:12:21,990][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 21:12:21,991][train.py][INFO] - Instantiating loggers...
[2024-05-30 21:12:21,991][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 21:12:21,993][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 21:12:22,036][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/diagnosed_p_t_s_d/0.0/0/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.71it/s v_num: 0.000      
                                                              val/auc: 0.563    
                                                              val/f1: 0.533     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.571 val/mre: nan
                                                              train/auc: 0.684  
                                                              train/f1: 0.617   
                                                              train/precision:  
                                                              0.784             
                                                              train/recall:     
                                                              0.509 train/mre:  
                                                              nan               
[2024-05-30 21:14:01,322][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/diagnosed_p_t_s_d/0.0/0>
[2024-05-30 21:14:01,322][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 21:14:01,326][HYDRA] 	#351 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_p_t_s_d data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=1
[rank: 0] Seed set to 1
[2024-05-30 21:14:01,613][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 21:14:01,615][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 21:14:01,617][train.py][INFO] - Instantiating callbacks...
[2024-05-30 21:14:01,618][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 21:14:01,621][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 21:14:01,622][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 21:14:01,622][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 21:14:01,623][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 21:14:01,625][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 21:14:01,626][train.py][INFO] - Instantiating loggers...
[2024-05-30 21:14:01,626][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 21:14:01,628][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 21:14:01,672][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/diagnosed_p_t_s_d/0.0/1/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.61it/s v_num: 0.000      
                                                              val/auc: 0.659    
                                                              val/f1: 0.545     
                                                              val/precision:    
                                                              0.750 val/recall: 
                                                              0.429 val/mre: nan
                                                              train/auc: 0.860  
                                                              train/f1: 0.852   
                                                              train/precision:  
                                                              0.902             
                                                              train/recall:     
                                                              0.807 train/mre:  
                                                              nan               
[2024-05-30 21:15:42,890][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/diagnosed_p_t_s_d/0.0/1>
[2024-05-30 21:15:42,891][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 21:15:42,894][HYDRA] 	#352 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_p_t_s_d data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=2
[rank: 0] Seed set to 2
[2024-05-30 21:15:43,184][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 21:15:43,186][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 21:15:43,188][train.py][INFO] - Instantiating callbacks...
[2024-05-30 21:15:43,189][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 21:15:43,192][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 21:15:43,193][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 21:15:43,194][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 21:15:43,195][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 21:15:43,196][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 21:15:43,197][train.py][INFO] - Instantiating loggers...
[2024-05-30 21:15:43,198][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 21:15:43,200][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 21:15:43,242][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/diagnosed_p_t_s_d/0.0/2/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.60it/s v_num: 0.000      
                                                              val/auc: 0.563    
                                                              val/f1: 0.533     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.571 val/mre: nan
                                                              train/auc: 0.851  
                                                              train/f1: 0.838   
                                                              train/precision:  
                                                              0.917             
                                                              train/recall:     
                                                              0.772 train/mre:  
                                                              nan               
[2024-05-30 21:17:24,252][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/diagnosed_p_t_s_d/0.0/2>
[2024-05-30 21:17:24,253][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 21:17:24,256][HYDRA] 	#353 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_p_t_s_d data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=3
[rank: 0] Seed set to 3
[2024-05-30 21:17:24,542][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 21:17:24,544][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 21:17:24,546][train.py][INFO] - Instantiating callbacks...
[2024-05-30 21:17:24,547][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 21:17:24,550][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 21:17:24,551][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 21:17:24,551][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 21:17:24,552][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 21:17:24,554][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 21:17:24,555][train.py][INFO] - Instantiating loggers...
[2024-05-30 21:17:24,555][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 21:17:24,557][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 21:17:24,601][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/diagnosed_p_t_s_d/0.0/3/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.68it/s v_num: 0.000      
                                                              val/auc: 0.381    
                                                              val/f1: 0.375     
                                                              val/precision:    
                                                              0.333 val/recall: 
                                                              0.429 val/mre: nan
                                                              train/auc: 0.684  
                                                              train/f1: 0.727   
                                                              train/precision:  
                                                              0.640             
                                                              train/recall:     
                                                              0.842 train/mre:  
                                                              nan               
[2024-05-30 21:19:05,629][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/diagnosed_p_t_s_d/0.0/3>
[2024-05-30 21:19:05,630][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 21:19:05,656][HYDRA] 	#354 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_p_t_s_d data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=4
[rank: 0] Seed set to 4
[2024-05-30 21:19:05,949][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 21:19:05,952][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 21:19:05,954][train.py][INFO] - Instantiating callbacks...
[2024-05-30 21:19:05,954][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 21:19:05,957][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 21:19:05,958][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 21:19:05,959][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 21:19:05,960][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 21:19:05,961][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 21:19:05,962][train.py][INFO] - Instantiating loggers...
[2024-05-30 21:19:05,963][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 21:19:05,965][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 21:19:06,008][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/diagnosed_p_t_s_d/0.0/4/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.64it/s v_num: 0.000      
                                                              val/auc: 0.452    
                                                              val/f1: 0.471     
                                                              val/precision:    
                                                              0.400 val/recall: 
                                                              0.571 val/mre: nan
                                                              train/auc: 0.693  
                                                              train/f1: 0.752   
                                                              train/precision:  
                                                              0.631             
                                                              train/recall:     
                                                              0.930 train/mre:  
                                                              nan               
[2024-05-30 21:20:46,519][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/diagnosed_p_t_s_d/0.0/4>
[2024-05-30 21:20:46,519][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 21:20:46,523][HYDRA] 	#355 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_p_t_s_d data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=5
[rank: 0] Seed set to 5
[2024-05-30 21:20:46,823][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 21:20:46,826][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 21:20:46,828][train.py][INFO] - Instantiating callbacks...
[2024-05-30 21:20:46,828][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 21:20:46,831][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 21:20:46,832][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 21:20:46,833][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 21:20:46,834][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 21:20:46,835][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 21:20:46,839][train.py][INFO] - Instantiating loggers...
[2024-05-30 21:20:46,839][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 21:20:46,842][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 21:20:46,927][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/diagnosed_p_t_s_d/0.0/5/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.66it/s v_num: 0.000      
                                                              val/auc: 0.675    
                                                              val/f1: 0.615     
                                                              val/precision:    
                                                              0.667 val/recall: 
                                                              0.571 val/mre: nan
                                                              train/auc: 0.781  
                                                              train/f1: 0.793   
                                                              train/precision:  
                                                              0.750             
                                                              train/recall:     
                                                              0.842 train/mre:  
                                                              nan               
[2024-05-30 21:22:26,954][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/diagnosed_p_t_s_d/0.0/5>
[2024-05-30 21:22:26,957][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 21:22:26,975][HYDRA] 	#356 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_p_t_s_d data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=6
[rank: 0] Seed set to 6
[2024-05-30 21:22:27,638][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 21:22:27,643][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 21:22:27,648][train.py][INFO] - Instantiating callbacks...
[2024-05-30 21:22:27,649][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 21:22:27,654][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 21:22:27,654][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 21:22:27,655][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 21:22:27,656][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 21:22:27,658][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 21:22:27,661][train.py][INFO] - Instantiating loggers...
[2024-05-30 21:22:27,661][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 21:22:27,664][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 21:22:27,835][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/diagnosed_p_t_s_d/0.0/6/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.67it/s v_num: 0.000      
                                                              val/auc: 0.579    
                                                              val/f1: 0.588     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.714 val/mre: nan
                                                              train/auc: 0.684  
                                                              train/f1: 0.739   
                                                              train/precision:  
                                                              0.630             
                                                              train/recall:     
                                                              0.895 train/mre:  
                                                              nan               
[2024-05-30 21:24:08,386][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/diagnosed_p_t_s_d/0.0/6>
[2024-05-30 21:24:08,387][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 21:24:08,391][HYDRA] 	#357 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_p_t_s_d data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=7
[rank: 0] Seed set to 7
[2024-05-30 21:24:08,700][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 21:24:08,703][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 21:24:08,705][train.py][INFO] - Instantiating callbacks...
[2024-05-30 21:24:08,705][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 21:24:08,709][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 21:24:08,710][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 21:24:08,711][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 21:24:08,711][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 21:24:08,713][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 21:24:08,717][train.py][INFO] - Instantiating loggers...
[2024-05-30 21:24:08,717][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 21:24:08,719][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 21:24:08,814][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/diagnosed_p_t_s_d/0.0/7/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.64it/s v_num: 0.000      
                                                              val/auc: 0.548    
                                                              val/f1: 0.462     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.429 val/mre: nan
                                                              train/auc: 0.746  
                                                              train/f1: 0.788   
                                                              train/precision:  
                                                              0.675             
                                                              train/recall:     
                                                              0.947 train/mre:  
                                                              nan               
[2024-05-30 21:25:49,310][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/diagnosed_p_t_s_d/0.0/7>
[2024-05-30 21:25:49,311][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 21:25:49,314][HYDRA] 	#358 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_p_t_s_d data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=8
[rank: 0] Seed set to 8
[2024-05-30 21:25:49,598][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 21:25:49,600][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 21:25:49,602][train.py][INFO] - Instantiating callbacks...
[2024-05-30 21:25:49,603][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 21:25:49,606][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 21:25:49,607][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 21:25:49,608][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 21:25:49,608][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 21:25:49,610][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 21:25:49,611][train.py][INFO] - Instantiating loggers...
[2024-05-30 21:25:49,611][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 21:25:49,614][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 21:25:49,655][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/diagnosed_p_t_s_d/0.0/8/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.62it/s v_num: 0.000      
                                                              val/auc: 0.516    
                                                              val/f1: 0.222     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.143 val/mre: nan
                                                              train/auc: 0.851  
                                                              train/f1: 0.857   
                                                              train/precision:  
                                                              0.823             
                                                              train/recall:     
                                                              0.895 train/mre:  
                                                              nan               
[2024-05-30 21:27:31,487][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/diagnosed_p_t_s_d/0.0/8>
[2024-05-30 21:27:31,489][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 21:27:31,498][HYDRA] 	#359 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_p_t_s_d data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=9
[rank: 0] Seed set to 9
[2024-05-30 21:27:31,812][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 21:27:31,815][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 21:27:31,817][train.py][INFO] - Instantiating callbacks...
[2024-05-30 21:27:31,817][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 21:27:31,821][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 21:27:31,822][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 21:27:31,822][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 21:27:31,823][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 21:27:31,825][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 21:27:31,826][train.py][INFO] - Instantiating loggers...
[2024-05-30 21:27:31,826][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 21:27:31,828][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 21:27:31,871][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/diagnosed_p_t_s_d/0.0/9/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.63it/s v_num: 0.000      
                                                              val/auc: 0.619    
                                                              val/f1: 0.571     
                                                              val/precision:    
                                                              0.571 val/recall: 
                                                              0.571 val/mre: nan
                                                              train/auc: 0.632  
                                                              train/f1: 0.667   
                                                              train/precision:  
                                                              0.609             
                                                              train/recall:     
                                                              0.737 train/mre:  
                                                              nan               
[2024-05-30 21:29:12,559][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/diagnosed_p_t_s_d/0.0/9>
[2024-05-30 21:29:12,560][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 21:29:12,563][HYDRA] 	#360 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_p_t_s_d data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=0
[2024-05-30 21:29:12,855][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 21:29:12,858][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 21:29:12,860][train.py][INFO] - Instantiating callbacks...
[2024-05-30 21:29:12,860][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 21:29:12,863][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 21:29:12,864][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 21:29:12,865][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 21:29:12,866][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 21:29:12,868][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 21:29:12,868][train.py][INFO] - Instantiating loggers...
[2024-05-30 21:29:12,869][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 21:29:12,871][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 21:29:12,920][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/diagnosed_p_t_s_d/0.05/0/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.61it/s v_num: 0.000      
                                                              val/auc: 0.619    
                                                              val/f1: 0.571     
                                                              val/precision:    
                                                              0.571 val/recall: 
                                                              0.571 val/mre:    
                                                              0.042 train/auc:  
                                                              0.658 train/f1:   
                                                              0.723             
                                                              train/precision:  
                                                              0.607             
                                                              train/recall:     
                                                              0.895 train/mre:  
                                                              0.042             
[2024-05-30 21:30:53,927][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/diagnosed_p_t_s_d/0.05/0>
[2024-05-30 21:30:53,928][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 21:30:53,931][HYDRA] 	#361 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_p_t_s_d data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=1
[rank: 0] Seed set to 1
[2024-05-30 21:30:54,226][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 21:30:54,229][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 21:30:54,231][train.py][INFO] - Instantiating callbacks...
[2024-05-30 21:30:54,232][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 21:30:54,236][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 21:30:54,237][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 21:30:54,238][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 21:30:54,238][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 21:30:54,240][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 21:30:54,241][train.py][INFO] - Instantiating loggers...
[2024-05-30 21:30:54,242][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 21:30:54,244][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 21:30:54,293][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/diagnosed_p_t_s_d/0.05/1/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.56it/s v_num: 0.000      
                                                              val/auc: 0.460    
                                                              val/f1: 0.200     
                                                              val/precision:    
                                                              0.333 val/recall: 
                                                              0.143 val/mre:    
                                                              0.049 train/auc:  
                                                              0.877 train/f1:   
                                                              0.875             
                                                              train/precision:  
                                                              0.891             
                                                              train/recall:     
                                                              0.860 train/mre:  
                                                              0.048             
[2024-05-30 21:32:36,955][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/diagnosed_p_t_s_d/0.05/1>
[2024-05-30 21:32:36,956][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 21:32:36,959][HYDRA] 	#362 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_p_t_s_d data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=2
[rank: 0] Seed set to 2
[2024-05-30 21:32:37,260][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 21:32:37,262][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 21:32:37,264][train.py][INFO] - Instantiating callbacks...
[2024-05-30 21:32:37,265][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 21:32:37,268][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 21:32:37,269][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 21:32:37,270][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 21:32:37,271][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 21:32:37,272][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 21:32:37,273][train.py][INFO] - Instantiating loggers...
[2024-05-30 21:32:37,274][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 21:32:37,276][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 21:32:37,327][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/diagnosed_p_t_s_d/0.05/2/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.63it/s v_num: 0.000      
                                                              val/auc: 0.452    
                                                              val/f1: 0.471     
                                                              val/precision:    
                                                              0.400 val/recall: 
                                                              0.571 val/mre:    
                                                              0.045 train/auc:  
                                                              0.807 train/f1:   
                                                              0.796             
                                                              train/precision:  
                                                              0.843             
                                                              train/recall:     
                                                              0.754 train/mre:  
                                                              0.046             
[2024-05-30 21:34:18,168][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/diagnosed_p_t_s_d/0.05/2>
[2024-05-30 21:34:18,169][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 21:34:18,172][HYDRA] 	#363 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_p_t_s_d data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=3
[rank: 0] Seed set to 3
[2024-05-30 21:34:18,472][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 21:34:18,475][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 21:34:18,477][train.py][INFO] - Instantiating callbacks...
[2024-05-30 21:34:18,477][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 21:34:18,481][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 21:34:18,481][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 21:34:18,482][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 21:34:18,483][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 21:34:18,485][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 21:34:18,488][train.py][INFO] - Instantiating loggers...
[2024-05-30 21:34:18,488][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 21:34:18,490][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 21:34:18,579][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/diagnosed_p_t_s_d/0.05/3/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.61it/s v_num: 0.000      
                                                              val/auc: 0.746    
                                                              val/f1: 0.714     
                                                              val/precision:    
                                                              0.714 val/recall: 
                                                              0.714 val/mre:    
                                                              0.037 train/auc:  
                                                              0.754 train/f1:   
                                                              0.781             
                                                              train/precision:  
                                                              0.704             
                                                              train/recall:     
                                                              0.877 train/mre:  
                                                              0.038             
[2024-05-30 21:35:59,868][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/diagnosed_p_t_s_d/0.05/3>
[2024-05-30 21:35:59,869][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 21:35:59,872][HYDRA] 	#364 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_p_t_s_d data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=4
[rank: 0] Seed set to 4
[2024-05-30 21:36:00,266][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 21:36:00,269][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 21:36:00,271][train.py][INFO] - Instantiating callbacks...
[2024-05-30 21:36:00,271][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 21:36:00,275][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 21:36:00,276][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 21:36:00,276][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 21:36:00,277][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 21:36:00,279][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 21:36:00,283][train.py][INFO] - Instantiating loggers...
[2024-05-30 21:36:00,283][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 21:36:00,285][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 21:36:00,436][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/diagnosed_p_t_s_d/0.05/4/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.68it/s v_num: 0.000      
                                                              val/auc: 0.619    
                                                              val/f1: 0.571     
                                                              val/precision:    
                                                              0.571 val/recall: 
                                                              0.571 val/mre:    
                                                              0.043 train/auc:  
                                                              0.675 train/f1:   
                                                              0.689             
                                                              train/precision:  
                                                              0.661             
                                                              train/recall:     
                                                              0.719 train/mre:  
                                                              0.040             
[2024-05-30 21:37:41,241][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/diagnosed_p_t_s_d/0.05/4>
[2024-05-30 21:37:41,242][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 21:37:41,245][HYDRA] 	#365 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_p_t_s_d data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=5
[rank: 0] Seed set to 5
[2024-05-30 21:37:41,580][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 21:37:41,582][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 21:37:41,584][train.py][INFO] - Instantiating callbacks...
[2024-05-30 21:37:41,585][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 21:37:41,588][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 21:37:41,589][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 21:37:41,590][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 21:37:41,590][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 21:37:41,592][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 21:37:41,596][train.py][INFO] - Instantiating loggers...
[2024-05-30 21:37:41,596][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 21:37:41,598][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 21:37:41,685][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/diagnosed_p_t_s_d/0.05/5/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.66it/s v_num: 0.000      
                                                              val/auc: 0.508    
                                                              val/f1: 0.500     
                                                              val/precision:    
                                                              0.444 val/recall: 
                                                              0.571 val/mre:    
                                                              0.044 train/auc:  
                                                              0.684 train/f1:   
                                                              0.739             
                                                              train/precision:  
                                                              0.630             
                                                              train/recall:     
                                                              0.895 train/mre:  
                                                              0.042             
[2024-05-30 21:39:21,872][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/diagnosed_p_t_s_d/0.05/5>
[2024-05-30 21:39:21,873][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 21:39:21,877][HYDRA] 	#366 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_p_t_s_d data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=6
[rank: 0] Seed set to 6
[2024-05-30 21:39:22,192][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 21:39:22,194][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 21:39:22,196][train.py][INFO] - Instantiating callbacks...
[2024-05-30 21:39:22,197][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 21:39:22,201][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 21:39:22,202][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 21:39:22,203][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 21:39:22,204][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 21:39:22,205][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 21:39:22,209][train.py][INFO] - Instantiating loggers...
[2024-05-30 21:39:22,209][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 21:39:22,211][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 21:39:22,315][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/diagnosed_p_t_s_d/0.05/6/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.68it/s v_num: 0.000      
                                                              val/auc: 0.563    
                                                              val/f1: 0.533     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.571 val/mre:    
                                                              0.055 train/auc:  
                                                              0.877 train/f1:   
                                                              0.881             
                                                              train/precision:  
                                                              0.852             
                                                              train/recall:     
                                                              0.912 train/mre:  
                                                              0.055             
[2024-05-30 21:41:02,939][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/diagnosed_p_t_s_d/0.05/6>
[2024-05-30 21:41:02,940][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 21:41:02,943][HYDRA] 	#367 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_p_t_s_d data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=7
[rank: 0] Seed set to 7
[2024-05-30 21:41:03,231][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 21:41:03,234][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 21:41:03,236][train.py][INFO] - Instantiating callbacks...
[2024-05-30 21:41:03,236][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 21:41:03,239][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 21:41:03,240][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 21:41:03,241][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 21:41:03,242][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 21:41:03,243][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 21:41:03,244][train.py][INFO] - Instantiating loggers...
[2024-05-30 21:41:03,245][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 21:41:03,247][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 21:41:03,289][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/diagnosed_p_t_s_d/0.05/7/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.59it/s v_num: 0.000      
                                                              val/auc: 0.571    
                                                              val/f1: 0.250     
                                                              val/precision:    
                                                              1.000 val/recall: 
                                                              0.143 val/mre:    
                                                              0.059 train/auc:  
                                                              0.789 train/f1:   
                                                              0.806             
                                                              train/precision:  
                                                              0.746             
                                                              train/recall:     
                                                              0.877 train/mre:  
                                                              0.053             
[2024-05-30 21:42:43,353][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/diagnosed_p_t_s_d/0.05/7>
[2024-05-30 21:42:43,354][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 21:42:43,358][HYDRA] 	#368 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_p_t_s_d data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=8
[rank: 0] Seed set to 8
[2024-05-30 21:42:43,681][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 21:42:43,684][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 21:42:43,686][train.py][INFO] - Instantiating callbacks...
[2024-05-30 21:42:43,686][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 21:42:43,693][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 21:42:43,694][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 21:42:43,695][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 21:42:43,695][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 21:42:43,697][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 21:42:43,698][train.py][INFO] - Instantiating loggers...
[2024-05-30 21:42:43,698][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 21:42:43,701][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 21:42:43,764][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/diagnosed_p_t_s_d/0.05/8/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.62it/s v_num: 0.000      
                                                              val/auc: 0.714    
                                                              val/f1: 0.600     
                                                              val/precision:    
                                                              1.000 val/recall: 
                                                              0.429 val/mre:    
                                                              0.050 train/auc:  
                                                              0.868 train/f1:   
                                                              0.870             
                                                              train/precision:  
                                                              0.862             
                                                              train/recall:     
                                                              0.877 train/mre:  
                                                              0.053             
[2024-05-30 21:44:26,417][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/diagnosed_p_t_s_d/0.05/8>
[2024-05-30 21:44:26,417][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 21:44:26,421][HYDRA] 	#369 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_p_t_s_d data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=9
[rank: 0] Seed set to 9
[2024-05-30 21:44:26,704][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 21:44:26,706][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 21:44:26,708][train.py][INFO] - Instantiating callbacks...
[2024-05-30 21:44:26,709][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 21:44:26,712][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 21:44:26,713][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 21:44:26,713][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 21:44:26,714][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 21:44:26,716][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 21:44:26,717][train.py][INFO] - Instantiating loggers...
[2024-05-30 21:44:26,717][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 21:44:26,719][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 21:44:26,761][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/diagnosed_p_t_s_d/0.05/9/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.62it/s v_num: 0.000      
                                                              val/auc: 0.619    
                                                              val/f1: 0.571     
                                                              val/precision:    
                                                              0.571 val/recall: 
                                                              0.571 val/mre:    
                                                              0.042 train/auc:  
                                                              0.702 train/f1:   
                                                              0.761             
                                                              train/precision:  
                                                              0.635             
                                                              train/recall:     
                                                              0.947 train/mre:  
                                                              0.042             
[2024-05-30 21:46:07,372][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/diagnosed_p_t_s_d/0.05/9>
[2024-05-30 21:46:07,373][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 21:46:07,377][HYDRA] 	#370 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_p_t_s_d data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=0
[2024-05-30 21:46:07,662][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 21:46:07,664][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 21:46:07,667][train.py][INFO] - Instantiating callbacks...
[2024-05-30 21:46:07,667][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 21:46:07,671][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 21:46:07,671][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 21:46:07,672][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 21:46:07,673][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 21:46:07,675][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 21:46:07,675][train.py][INFO] - Instantiating loggers...
[2024-05-30 21:46:07,676][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 21:46:07,678][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 21:46:07,720][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/diagnosed_p_t_s_d/0.1/0/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.64it/s v_num: 0.000      
                                                              val/auc: 0.603    
                                                              val/f1: 0.500     
                                                              val/precision:    
                                                              0.600 val/recall: 
                                                              0.429 val/mre:    
                                                              0.044 train/auc:  
                                                              0.684 train/f1:   
                                                              0.746             
                                                              train/precision:  
                                                              0.624             
                                                              train/recall:     
                                                              0.930 train/mre:  
                                                              0.045             
[2024-05-30 21:47:48,298][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/diagnosed_p_t_s_d/0.1/0>
[2024-05-30 21:47:48,299][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 21:47:48,302][HYDRA] 	#371 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_p_t_s_d data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=1
[rank: 0] Seed set to 1
[2024-05-30 21:47:48,587][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 21:47:48,590][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 21:47:48,592][train.py][INFO] - Instantiating callbacks...
[2024-05-30 21:47:48,592][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 21:47:48,595][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 21:47:48,596][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 21:47:48,597][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 21:47:48,598][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 21:47:48,599][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 21:47:48,600][train.py][INFO] - Instantiating loggers...
[2024-05-30 21:47:48,601][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 21:47:48,603][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 21:47:48,645][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/diagnosed_p_t_s_d/0.1/1/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.56it/s v_num: 0.000      
                                                              val/auc: 0.619    
                                                              val/f1: 0.571     
                                                              val/precision:    
                                                              0.571 val/recall: 
                                                              0.571 val/mre:    
                                                              0.047 train/auc:  
                                                              0.781 train/f1:   
                                                              0.771             
                                                              train/precision:  
                                                              0.808             
                                                              train/recall:     
                                                              0.737 train/mre:  
                                                              0.047             
[2024-05-30 21:49:31,707][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/diagnosed_p_t_s_d/0.1/1>
[2024-05-30 21:49:31,708][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 21:49:31,711][HYDRA] 	#372 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_p_t_s_d data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=2
[rank: 0] Seed set to 2
[2024-05-30 21:49:32,004][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 21:49:32,006][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 21:49:32,008][train.py][INFO] - Instantiating callbacks...
[2024-05-30 21:49:32,009][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 21:49:32,012][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 21:49:32,013][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 21:49:32,014][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 21:49:32,014][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 21:49:32,016][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 21:49:32,017][train.py][INFO] - Instantiating loggers...
[2024-05-30 21:49:32,018][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 21:49:32,020][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 21:49:32,064][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/diagnosed_p_t_s_d/0.1/2/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.64it/s v_num: 0.000      
                                                              val/auc: 0.563    
                                                              val/f1: 0.533     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.571 val/mre:    
                                                              0.053 train/auc:  
                                                              0.833 train/f1:   
                                                              0.822             
                                                              train/precision:  
                                                              0.880             
                                                              train/recall:     
                                                              0.772 train/mre:  
                                                              0.054             
[2024-05-30 21:51:12,514][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/diagnosed_p_t_s_d/0.1/2>
[2024-05-30 21:51:12,515][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 21:51:12,518][HYDRA] 	#373 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_p_t_s_d data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=3
[rank: 0] Seed set to 3
[2024-05-30 21:51:12,805][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 21:51:12,808][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 21:51:12,810][train.py][INFO] - Instantiating callbacks...
[2024-05-30 21:51:12,810][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 21:51:12,814][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 21:51:12,814][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 21:51:12,815][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 21:51:12,816][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 21:51:12,818][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 21:51:12,819][train.py][INFO] - Instantiating loggers...
[2024-05-30 21:51:12,819][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 21:51:12,821][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 21:51:12,864][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/diagnosed_p_t_s_d/0.1/3/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.67it/s v_num: 0.000      
                                                              val/auc: 0.690    
                                                              val/f1: 0.667     
                                                              val/precision:    
                                                              0.625 val/recall: 
                                                              0.714 val/mre:    
                                                              0.042 train/auc:  
                                                              0.754 train/f1:   
                                                              0.788             
                                                              train/precision:  
                                                              0.693             
                                                              train/recall:     
                                                              0.912 train/mre:  
                                                              0.042             
[2024-05-30 21:52:53,831][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/diagnosed_p_t_s_d/0.1/3>
[2024-05-30 21:52:53,832][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 21:52:53,835][HYDRA] 	#374 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_p_t_s_d data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=4
[rank: 0] Seed set to 4
[2024-05-30 21:52:54,131][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 21:52:54,134][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 21:52:54,137][train.py][INFO] - Instantiating callbacks...
[2024-05-30 21:52:54,137][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 21:52:54,141][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 21:52:54,141][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 21:52:54,142][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 21:52:54,143][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 21:52:54,145][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 21:52:54,146][train.py][INFO] - Instantiating loggers...
[2024-05-30 21:52:54,146][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 21:52:54,149][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 21:52:54,193][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/diagnosed_p_t_s_d/0.1/4/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.61it/s v_num: 0.000      
                                                              val/auc: 0.476    
                                                              val/f1: 0.333     
                                                              val/precision:    
                                                              0.400 val/recall: 
                                                              0.286 val/mre:    
                                                              0.046 train/auc:  
                                                              0.798 train/f1:   
                                                              0.813             
                                                              train/precision:  
                                                              0.758             
                                                              train/recall:     
                                                              0.877 train/mre:  
                                                              0.045             
[2024-05-30 21:54:34,339][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/diagnosed_p_t_s_d/0.1/4>
[2024-05-30 21:54:34,340][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 21:54:34,343][HYDRA] 	#375 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_p_t_s_d data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=5
[rank: 0] Seed set to 5
[2024-05-30 21:54:34,644][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 21:54:34,646][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 21:54:34,648][train.py][INFO] - Instantiating callbacks...
[2024-05-30 21:54:34,649][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 21:54:34,652][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 21:54:34,653][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 21:54:34,654][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 21:54:34,654][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 21:54:34,656][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 21:54:34,659][train.py][INFO] - Instantiating loggers...
[2024-05-30 21:54:34,660][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 21:54:34,662][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 21:54:34,749][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/diagnosed_p_t_s_d/0.1/5/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.62it/s v_num: 0.000      
                                                              val/auc: 0.635    
                                                              val/f1: 0.625     
                                                              val/precision:    
                                                              0.556 val/recall: 
                                                              0.714 val/mre:    
                                                              0.041 train/auc:  
                                                              0.667 train/f1:   
                                                              0.698             
                                                              train/precision:  
                                                              0.638             
                                                              train/recall:     
                                                              0.772 train/mre:  
                                                              0.040             
[2024-05-30 21:56:15,284][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/diagnosed_p_t_s_d/0.1/5>
[2024-05-30 21:56:15,285][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 21:56:15,288][HYDRA] 	#376 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_p_t_s_d data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=6
[rank: 0] Seed set to 6
[2024-05-30 21:56:15,590][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 21:56:15,593][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 21:56:15,595][train.py][INFO] - Instantiating callbacks...
[2024-05-30 21:56:15,596][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 21:56:15,600][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 21:56:15,600][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 21:56:15,601][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 21:56:15,602][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 21:56:15,604][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 21:56:15,607][train.py][INFO] - Instantiating loggers...
[2024-05-30 21:56:15,607][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 21:56:15,609][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 21:56:15,707][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/diagnosed_p_t_s_d/0.1/6/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.67it/s v_num: 0.000      
                                                              val/auc: 0.579    
                                                              val/f1: 0.588     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.714 val/mre:    
                                                              0.055 train/auc:  
                                                              0.895 train/f1:   
                                                              0.889             
                                                              train/precision:  
                                                              0.941             
                                                              train/recall:     
                                                              0.842 train/mre:  
                                                              0.056             
[2024-05-30 21:57:55,352][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/diagnosed_p_t_s_d/0.1/6>
[2024-05-30 21:57:55,352][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 21:57:55,355][HYDRA] 	#377 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_p_t_s_d data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=7
[rank: 0] Seed set to 7
[2024-05-30 21:57:55,666][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 21:57:55,670][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 21:57:55,673][train.py][INFO] - Instantiating callbacks...
[2024-05-30 21:57:55,674][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 21:57:55,677][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 21:57:55,678][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 21:57:55,679][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 21:57:55,680][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 21:57:55,681][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 21:57:55,709][train.py][INFO] - Instantiating loggers...
[2024-05-30 21:57:55,710][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 21:57:55,713][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 21:57:55,824][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/diagnosed_p_t_s_d/0.1/7/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.69it/s v_num: 0.000      
                                                              val/auc: 0.516    
                                                              val/f1: 0.222     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.143 val/mre:    
                                                              0.069 train/auc:  
                                                              0.825 train/f1:   
                                                              0.839             
                                                              train/precision:  
                                                              0.776             
                                                              train/recall:     
                                                              0.912 train/mre:  
                                                              0.061             
[2024-05-30 21:59:35,424][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/diagnosed_p_t_s_d/0.1/7>
[2024-05-30 21:59:35,425][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 21:59:35,429][HYDRA] 	#378 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_p_t_s_d data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=8
[rank: 0] Seed set to 8
[2024-05-30 21:59:35,728][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 21:59:35,730][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 21:59:35,732][train.py][INFO] - Instantiating callbacks...
[2024-05-30 21:59:35,733][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 21:59:35,736][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 21:59:35,737][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 21:59:35,738][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 21:59:35,738][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 21:59:35,740][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 21:59:35,744][train.py][INFO] - Instantiating loggers...
[2024-05-30 21:59:35,744][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 21:59:35,746][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 21:59:35,836][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/diagnosed_p_t_s_d/0.1/8/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.65it/s v_num: 0.000      
                                                              val/auc: 0.563    
                                                              val/f1: 0.533     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.571 val/mre:    
                                                              0.049 train/auc:  
                                                              0.798 train/f1:   
                                                              0.793             
                                                              train/precision:  
                                                              0.815             
                                                              train/recall:     
                                                              0.772 train/mre:  
                                                              0.050             
[2024-05-30 22:01:16,625][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/diagnosed_p_t_s_d/0.1/8>
[2024-05-30 22:01:16,626][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 22:01:16,629][HYDRA] 	#379 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_p_t_s_d data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=9
[rank: 0] Seed set to 9
[2024-05-30 22:01:16,933][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 22:01:16,935][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 22:01:16,937][train.py][INFO] - Instantiating callbacks...
[2024-05-30 22:01:16,938][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 22:01:16,941][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 22:01:16,942][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 22:01:16,943][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 22:01:16,943][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 22:01:16,945][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 22:01:16,951][train.py][INFO] - Instantiating loggers...
[2024-05-30 22:01:16,952][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 22:01:16,954][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 22:01:17,040][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/diagnosed_p_t_s_d/0.1/9/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.69it/s v_num: 0.000      
                                                              val/auc: 0.603    
                                                              val/f1: 0.500     
                                                              val/precision:    
                                                              0.600 val/recall: 
                                                              0.429 val/mre:    
                                                              0.043 train/auc:  
                                                              0.728 train/f1:   
                                                              0.739             
                                                              train/precision:  
                                                              0.710             
                                                              train/recall:     
                                                              0.772 train/mre:  
                                                              0.043             
[2024-05-30 22:02:57,343][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/diagnosed_p_t_s_d/0.1/9>
[2024-05-30 22:02:57,344][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 22:02:57,347][HYDRA] 	#380 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_p_t_s_d data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=0
[2024-05-30 22:02:57,635][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 22:02:57,637][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 22:02:57,639][train.py][INFO] - Instantiating callbacks...
[2024-05-30 22:02:57,640][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 22:02:57,643][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 22:02:57,644][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 22:02:57,644][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 22:02:57,645][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 22:02:57,647][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 22:02:57,648][train.py][INFO] - Instantiating loggers...
[2024-05-30 22:02:57,648][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 22:02:57,651][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 22:02:57,693][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/diagnosed_p_t_s_d/0.15/0/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.65it/s v_num: 0.000      
                                                              val/auc: 0.405    
                                                              val/f1: 0.182     
                                                              val/precision:    
                                                              0.250 val/recall: 
                                                              0.143 val/mre:    
                                                              0.049 train/auc:  
                                                              0.825 train/f1:   
                                                              0.833             
                                                              train/precision:  
                                                              0.794             
                                                              train/recall:     
                                                              0.877 train/mre:  
                                                              0.049             
[2024-05-30 22:04:40,685][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/diagnosed_p_t_s_d/0.15/0>
[2024-05-30 22:04:40,686][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 22:04:40,689][HYDRA] 	#381 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_p_t_s_d data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=1
[rank: 0] Seed set to 1
[2024-05-30 22:04:40,982][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 22:04:40,985][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 22:04:40,987][train.py][INFO] - Instantiating callbacks...
[2024-05-30 22:04:40,988][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 22:04:40,992][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 22:04:40,993][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 22:04:40,993][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 22:04:40,994][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 22:04:40,996][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 22:04:40,997][train.py][INFO] - Instantiating loggers...
[2024-05-30 22:04:40,998][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 22:04:41,000][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 22:04:41,043][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/diagnosed_p_t_s_d/0.15/1/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.57it/s v_num: 0.000      
                                                              val/auc: 0.516    
                                                              val/f1: 0.222     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.143 val/mre:    
                                                              0.050 train/auc:  
                                                              0.763 train/f1:   
                                                              0.710             
                                                              train/precision:  
                                                              0.917             
                                                              train/recall:     
                                                              0.579 train/mre:  
                                                              0.050             
[2024-05-30 22:06:24,338][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/diagnosed_p_t_s_d/0.15/1>
[2024-05-30 22:06:24,339][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 22:06:24,343][HYDRA] 	#382 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_p_t_s_d data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=2
[rank: 0] Seed set to 2
[2024-05-30 22:06:24,647][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 22:06:24,650][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 22:06:24,652][train.py][INFO] - Instantiating callbacks...
[2024-05-30 22:06:24,652][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 22:06:24,656][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 22:06:24,656][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 22:06:24,657][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 22:06:24,658][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 22:06:24,660][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 22:06:24,661][train.py][INFO] - Instantiating loggers...
[2024-05-30 22:06:24,661][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 22:06:24,663][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 22:06:24,707][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/diagnosed_p_t_s_d/0.15/2/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.67it/s v_num: 0.000      
                                                              val/auc: 0.563    
                                                              val/f1: 0.533     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.571 val/mre:    
                                                              0.048 train/auc:  
                                                              0.667 train/f1:   
                                                              0.721             
                                                              train/precision:  
                                                              0.620             
                                                              train/recall:     
                                                              0.860 train/mre:  
                                                              0.049             
[2024-05-30 22:08:06,035][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/diagnosed_p_t_s_d/0.15/2>
[2024-05-30 22:08:06,035][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 22:08:06,038][HYDRA] 	#383 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_p_t_s_d data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=3
[rank: 0] Seed set to 3
[2024-05-30 22:08:06,330][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 22:08:06,332][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 22:08:06,334][train.py][INFO] - Instantiating callbacks...
[2024-05-30 22:08:06,335][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 22:08:06,338][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 22:08:06,339][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 22:08:06,340][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 22:08:06,340][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 22:08:06,342][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 22:08:06,343][train.py][INFO] - Instantiating loggers...
[2024-05-30 22:08:06,343][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 22:08:06,345][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 22:08:06,387][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/diagnosed_p_t_s_d/0.15/3/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.64it/s v_num: 0.000      
                                                              val/auc: 0.508    
                                                              val/f1: 0.500     
                                                              val/precision:    
                                                              0.444 val/recall: 
                                                              0.571 val/mre:    
                                                              0.041 train/auc:  
                                                              0.649 train/f1:   
                                                              0.714             
                                                              train/precision:  
                                                              0.602             
                                                              train/recall:     
                                                              0.877 train/mre:  
                                                              0.042             
[2024-05-30 22:09:47,771][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/diagnosed_p_t_s_d/0.15/3>
[2024-05-30 22:09:47,772][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 22:09:47,775][HYDRA] 	#384 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_p_t_s_d data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=4
[rank: 0] Seed set to 4
[2024-05-30 22:09:48,156][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 22:09:48,160][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 22:09:48,162][train.py][INFO] - Instantiating callbacks...
[2024-05-30 22:09:48,163][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 22:09:48,166][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 22:09:48,167][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 22:09:48,168][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 22:09:48,169][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 22:09:48,171][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 22:09:48,172][train.py][INFO] - Instantiating loggers...
[2024-05-30 22:09:48,172][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 22:09:48,174][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 22:09:48,239][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/diagnosed_p_t_s_d/0.15/4/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.68it/s v_num: 0.000      
                                                              val/auc: 0.548    
                                                              val/f1: 0.462     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.429 val/mre:    
                                                              0.048 train/auc:  
                                                              0.754 train/f1:   
                                                              0.754             
                                                              train/precision:  
                                                              0.754             
                                                              train/recall:     
                                                              0.754 train/mre:  
                                                              0.048             
[2024-05-30 22:11:28,525][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/diagnosed_p_t_s_d/0.15/4>
[2024-05-30 22:11:28,526][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 22:11:28,529][HYDRA] 	#385 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_p_t_s_d data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=5
[rank: 0] Seed set to 5
[2024-05-30 22:11:28,817][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 22:11:28,819][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 22:11:28,821][train.py][INFO] - Instantiating callbacks...
[2024-05-30 22:11:28,822][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 22:11:28,825][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 22:11:28,826][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 22:11:28,826][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 22:11:28,827][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 22:11:28,829][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 22:11:28,830][train.py][INFO] - Instantiating loggers...
[2024-05-30 22:11:28,830][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 22:11:28,833][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 22:11:28,876][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/diagnosed_p_t_s_d/0.15/5/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.61it/s v_num: 0.000      
                                                              val/auc: 0.452    
                                                              val/f1: 0.471     
                                                              val/precision:    
                                                              0.400 val/recall: 
                                                              0.571 val/mre:    
                                                              0.046 train/auc:  
                                                              0.754 train/f1:   
                                                              0.778             
                                                              train/precision:  
                                                              0.710             
                                                              train/recall:     
                                                              0.860 train/mre:  
                                                              0.046             
[2024-05-30 22:13:09,334][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/diagnosed_p_t_s_d/0.15/5>
[2024-05-30 22:13:09,334][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 22:13:09,337][HYDRA] 	#386 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_p_t_s_d data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=6
[rank: 0] Seed set to 6
[2024-05-30 22:13:09,626][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 22:13:09,628][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 22:13:09,630][train.py][INFO] - Instantiating callbacks...
[2024-05-30 22:13:09,630][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 22:13:09,634][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 22:13:09,635][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 22:13:09,635][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 22:13:09,636][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 22:13:09,638][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 22:13:09,639][train.py][INFO] - Instantiating loggers...
[2024-05-30 22:13:09,639][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 22:13:09,642][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 22:13:09,687][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/diagnosed_p_t_s_d/0.15/6/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.60it/s v_num: 0.000      
                                                              val/auc: 0.524    
                                                              val/f1: 0.556     
                                                              val/precision:    
                                                              0.455 val/recall: 
                                                              0.714 val/mre:    
                                                              0.060 train/auc:  
                                                              0.886 train/f1:   
                                                              0.896             
                                                              train/precision:  
                                                              0.824             
                                                              train/recall:     
                                                              0.982 train/mre:  
                                                              0.062             
[2024-05-30 22:14:50,162][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/diagnosed_p_t_s_d/0.15/6>
[2024-05-30 22:14:50,163][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 22:14:50,166][HYDRA] 	#387 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_p_t_s_d data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=7
[rank: 0] Seed set to 7
[2024-05-30 22:14:51,093][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 22:14:51,098][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 22:14:51,100][train.py][INFO] - Instantiating callbacks...
[2024-05-30 22:14:51,101][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 22:14:51,104][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 22:14:51,105][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 22:14:51,106][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 22:14:51,107][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 22:14:51,108][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 22:14:51,111][train.py][INFO] - Instantiating loggers...
[2024-05-30 22:14:51,112][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 22:14:51,114][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 22:14:51,201][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/diagnosed_p_t_s_d/0.15/7/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.72it/s v_num: 0.000      
                                                              val/auc: 0.516    
                                                              val/f1: 0.222     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.143 val/mre:    
                                                              0.062 train/auc:  
                                                              0.772 train/f1:   
                                                              0.794             
                                                              train/precision:  
                                                              0.725             
                                                              train/recall:     
                                                              0.877 train/mre:  
                                                              0.058             
[2024-05-30 22:16:30,517][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/diagnosed_p_t_s_d/0.15/7>
[2024-05-30 22:16:30,518][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 22:16:30,521][HYDRA] 	#388 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_p_t_s_d data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=8
[rank: 0] Seed set to 8
[2024-05-30 22:16:30,825][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 22:16:30,828][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 22:16:30,830][train.py][INFO] - Instantiating callbacks...
[2024-05-30 22:16:30,830][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 22:16:30,834][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 22:16:30,834][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 22:16:30,835][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 22:16:30,836][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 22:16:30,838][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 22:16:30,841][train.py][INFO] - Instantiating loggers...
[2024-05-30 22:16:30,841][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 22:16:30,843][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 22:16:30,941][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/diagnosed_p_t_s_d/0.15/8/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.63it/s v_num: 0.000      
                                                              val/auc: 0.437    
                                                              val/f1: 0.400     
                                                              val/precision:    
                                                              0.375 val/recall: 
                                                              0.429 val/mre:    
                                                              0.048 train/auc:  
                                                              0.772 train/f1:   
                                                              0.783             
                                                              train/precision:  
                                                              0.746             
                                                              train/recall:     
                                                              0.825 train/mre:  
                                                              0.049             
[2024-05-30 22:18:11,808][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/diagnosed_p_t_s_d/0.15/8>
[2024-05-30 22:18:11,809][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 22:18:11,813][HYDRA] 	#389 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_p_t_s_d data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=9
[rank: 0] Seed set to 9
[2024-05-30 22:18:12,120][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 22:18:12,123][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 22:18:12,125][train.py][INFO] - Instantiating callbacks...
[2024-05-30 22:18:12,125][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 22:18:12,129][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 22:18:12,129][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 22:18:12,130][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 22:18:12,131][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 22:18:12,133][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 22:18:12,136][train.py][INFO] - Instantiating loggers...
[2024-05-30 22:18:12,136][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 22:18:12,139][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 22:18:12,228][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/diagnosed_p_t_s_d/0.15/9/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.63it/s v_num: 0.000      
                                                              val/auc: 0.587    
                                                              val/f1: 0.400     
                                                              val/precision:    
                                                              0.667 val/recall: 
                                                              0.286 val/mre:    
                                                              0.048 train/auc:  
                                                              0.772 train/f1:   
                                                              0.764             
                                                              train/precision:  
                                                              0.792             
                                                              train/recall:     
                                                              0.737 train/mre:  
                                                              0.048             
[2024-05-30 22:19:52,484][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/diagnosed_p_t_s_d/0.15/9>
[2024-05-30 22:19:52,485][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 22:19:52,489][HYDRA] 	#390 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_p_t_s_d data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=0
[2024-05-30 22:19:52,790][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 22:19:52,793][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 22:19:52,795][train.py][INFO] - Instantiating callbacks...
[2024-05-30 22:19:52,795][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 22:19:52,799][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 22:19:52,799][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 22:19:52,800][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 22:19:52,801][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 22:19:52,803][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 22:19:52,806][train.py][INFO] - Instantiating loggers...
[2024-05-30 22:19:52,806][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 22:19:52,808][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 22:19:52,902][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/diagnosed_p_t_s_d/0.2/0/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.69it/s v_num: 0.000      
                                                              val/auc: 0.421    
                                                              val/f1: 0.308     
                                                              val/precision:    
                                                              0.333 val/recall: 
                                                              0.286 val/mre:    
                                                              0.053 train/auc:  
                                                              0.781 train/f1:   
                                                              0.809             
                                                              train/precision:  
                                                              0.716             
                                                              train/recall:     
                                                              0.930 train/mre:  
                                                              0.053             
[2024-05-30 22:21:33,669][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/diagnosed_p_t_s_d/0.2/0>
[2024-05-30 22:21:33,669][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 22:21:33,673][HYDRA] 	#391 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_p_t_s_d data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=1
[rank: 0] Seed set to 1
[2024-05-30 22:21:33,959][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 22:21:33,961][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 22:21:33,963][train.py][INFO] - Instantiating callbacks...
[2024-05-30 22:21:33,964][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 22:21:33,967][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 22:21:33,968][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 22:21:33,969][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 22:21:33,969][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 22:21:33,971][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 22:21:33,972][train.py][INFO] - Instantiating loggers...
[2024-05-30 22:21:33,972][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 22:21:33,974][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 22:21:34,016][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/diagnosed_p_t_s_d/0.2/1/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.58it/s v_num: 0.000      
                                                              val/auc: 0.643    
                                                              val/f1: 0.444     
                                                              val/precision:    
                                                              1.000 val/recall: 
                                                              0.286 val/mre:    
                                                              0.051 train/auc:  
                                                              0.816 train/f1:   
                                                              0.804             
                                                              train/precision:  
                                                              0.860             
                                                              train/recall:     
                                                              0.754 train/mre:  
                                                              0.052             
[2024-05-30 22:23:16,869][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/diagnosed_p_t_s_d/0.2/1>
[2024-05-30 22:23:16,870][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 22:23:16,874][HYDRA] 	#392 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_p_t_s_d data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=2
[rank: 0] Seed set to 2
[2024-05-30 22:23:17,159][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 22:23:17,161][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 22:23:17,163][train.py][INFO] - Instantiating callbacks...
[2024-05-30 22:23:17,163][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 22:23:17,167][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 22:23:17,167][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 22:23:17,168][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 22:23:17,169][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 22:23:17,171][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 22:23:17,172][train.py][INFO] - Instantiating loggers...
[2024-05-30 22:23:17,172][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 22:23:17,174][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 22:23:17,215][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/diagnosed_p_t_s_d/0.2/2/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.69it/s v_num: 0.000      
                                                              val/auc: 0.619    
                                                              val/f1: 0.571     
                                                              val/precision:    
                                                              0.571 val/recall: 
                                                              0.571 val/mre:    
                                                              0.051 train/auc:  
                                                              0.667 train/f1:   
                                                              0.725             
                                                              train/precision:  
                                                              0.617             
                                                              train/recall:     
                                                              0.877 train/mre:  
                                                              0.052             
[2024-05-30 22:24:58,398][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/diagnosed_p_t_s_d/0.2/2>
[2024-05-30 22:24:58,399][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 22:24:58,402][HYDRA] 	#393 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_p_t_s_d data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=3
[rank: 0] Seed set to 3
[2024-05-30 22:24:58,704][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 22:24:58,707][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 22:24:58,709][train.py][INFO] - Instantiating callbacks...
[2024-05-30 22:24:58,709][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 22:24:58,713][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 22:24:58,714][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 22:24:58,714][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 22:24:58,715][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 22:24:58,717][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 22:24:58,718][train.py][INFO] - Instantiating loggers...
[2024-05-30 22:24:58,718][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 22:24:58,720][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 22:24:58,763][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/diagnosed_p_t_s_d/0.2/3/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.68it/s v_num: 0.000      
                                                              val/auc: 0.619    
                                                              val/f1: 0.571     
                                                              val/precision:    
                                                              0.571 val/recall: 
                                                              0.571 val/mre:    
                                                              0.047 train/auc:  
                                                              0.667 train/f1:   
                                                              0.708             
                                                              train/precision:  
                                                              0.630             
                                                              train/recall:     
                                                              0.807 train/mre:  
                                                              0.048             
[2024-05-30 22:26:39,248][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/diagnosed_p_t_s_d/0.2/3>
[2024-05-30 22:26:39,248][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 22:26:39,251][HYDRA] 	#394 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_p_t_s_d data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=4
[rank: 0] Seed set to 4
[2024-05-30 22:26:39,535][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 22:26:39,538][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 22:26:39,540][train.py][INFO] - Instantiating callbacks...
[2024-05-30 22:26:39,540][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 22:26:39,544][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 22:26:39,544][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 22:26:39,545][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 22:26:39,546][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 22:26:39,548][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 22:26:39,548][train.py][INFO] - Instantiating loggers...
[2024-05-30 22:26:39,549][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 22:26:39,551][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 22:26:39,593][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/diagnosed_p_t_s_d/0.2/4/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.68it/s v_num: 0.000      
                                                              val/auc: 0.563    
                                                              val/f1: 0.533     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.571 val/mre:    
                                                              0.054 train/auc:  
                                                              0.798 train/f1:   
                                                              0.822             
                                                              train/precision:  
                                                              0.736             
                                                              train/recall:     
                                                              0.930 train/mre:  
                                                              0.055             
[2024-05-30 22:28:19,431][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/diagnosed_p_t_s_d/0.2/4>
[2024-05-30 22:28:19,432][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 22:28:19,435][HYDRA] 	#395 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_p_t_s_d data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=5
[rank: 0] Seed set to 5
[2024-05-30 22:28:19,723][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 22:28:19,725][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 22:28:19,728][train.py][INFO] - Instantiating callbacks...
[2024-05-30 22:28:19,728][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 22:28:19,731][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 22:28:19,732][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 22:28:19,733][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 22:28:19,734][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 22:28:19,735][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 22:28:19,736][train.py][INFO] - Instantiating loggers...
[2024-05-30 22:28:19,737][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 22:28:19,739][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 22:28:19,780][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/diagnosed_p_t_s_d/0.2/5/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.63it/s v_num: 0.000      
                                                              val/auc: 0.381    
                                                              val/f1: 0.375     
                                                              val/precision:    
                                                              0.333 val/recall: 
                                                              0.429 val/mre:    
                                                              0.050 train/auc:  
                                                              0.746 train/f1:   
                                                              0.764             
                                                              train/precision:  
                                                              0.712             
                                                              train/recall:     
                                                              0.825 train/mre:  
                                                              0.049             
[2024-05-30 22:29:59,999][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/diagnosed_p_t_s_d/0.2/5>
[2024-05-30 22:30:00,001][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 22:30:00,006][HYDRA] 	#396 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_p_t_s_d data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=6
[rank: 0] Seed set to 6
[2024-05-30 22:30:00,303][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 22:30:00,305][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 22:30:00,307][train.py][INFO] - Instantiating callbacks...
[2024-05-30 22:30:00,308][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 22:30:00,311][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 22:30:00,312][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 22:30:00,313][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 22:30:00,314][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 22:30:00,315][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 22:30:00,316][train.py][INFO] - Instantiating loggers...
[2024-05-30 22:30:00,317][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 22:30:00,319][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 22:30:00,361][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/diagnosed_p_t_s_d/0.2/6/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.63it/s v_num: 0.000      
                                                              val/auc: 0.437    
                                                              val/f1: 0.400     
                                                              val/precision:    
                                                              0.375 val/recall: 
                                                              0.429 val/mre:    
                                                              0.057 train/auc:  
                                                              0.921 train/f1:   
                                                              0.927             
                                                              train/precision:  
                                                              0.864             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.059             
[2024-05-30 22:31:41,758][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/diagnosed_p_t_s_d/0.2/6>
[2024-05-30 22:31:41,759][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 22:31:41,763][HYDRA] 	#397 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_p_t_s_d data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=7
[rank: 0] Seed set to 7
[2024-05-30 22:31:42,048][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 22:31:42,050][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 22:31:42,052][train.py][INFO] - Instantiating callbacks...
[2024-05-30 22:31:42,052][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 22:31:42,056][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 22:31:42,057][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 22:31:42,057][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 22:31:42,058][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 22:31:42,060][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 22:31:42,061][train.py][INFO] - Instantiating loggers...
[2024-05-30 22:31:42,062][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 22:31:42,064][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 22:31:42,107][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/diagnosed_p_t_s_d/0.2/7/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.70it/s v_num: 0.000      
                                                              val/auc: 0.532    
                                                              val/f1: 0.364     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.286 val/mre:    
                                                              0.064 train/auc:  
                                                              0.825 train/f1:   
                                                              0.839             
                                                              train/precision:  
                                                              0.776             
                                                              train/recall:     
                                                              0.912 train/mre:  
                                                              0.062             
[2024-05-30 22:33:22,376][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/diagnosed_p_t_s_d/0.2/7>
[2024-05-30 22:33:22,377][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 22:33:22,380][HYDRA] 	#398 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_p_t_s_d data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=8
[rank: 0] Seed set to 8
[2024-05-30 22:33:22,690][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 22:33:22,693][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 22:33:22,695][train.py][INFO] - Instantiating callbacks...
[2024-05-30 22:33:22,695][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 22:33:22,699][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 22:33:22,700][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 22:33:22,700][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 22:33:22,701][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 22:33:22,703][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 22:33:22,704][train.py][INFO] - Instantiating loggers...
[2024-05-30 22:33:22,704][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 22:33:22,707][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 22:33:22,750][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/diagnosed_p_t_s_d/0.2/8/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.63it/s v_num: 0.000      
                                                              val/auc: 0.452    
                                                              val/f1: 0.471     
                                                              val/precision:    
                                                              0.400 val/recall: 
                                                              0.571 val/mre:    
                                                              0.052 train/auc:  
                                                              0.746 train/f1:   
                                                              0.768             
                                                              train/precision:  
                                                              0.706             
                                                              train/recall:     
                                                              0.842 train/mre:  
                                                              0.052             
[2024-05-30 22:35:03,263][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/diagnosed_p_t_s_d/0.2/8>
[2024-05-30 22:35:03,264][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 22:35:03,267][HYDRA] 	#399 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_p_t_s_d data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=9
[rank: 0] Seed set to 9
[2024-05-30 22:35:03,556][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 22:35:03,559][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 22:35:03,561][train.py][INFO] - Instantiating callbacks...
[2024-05-30 22:35:03,561][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 22:35:03,565][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 22:35:03,566][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 22:35:03,566][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 22:35:03,567][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 22:35:03,569][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 22:35:03,570][train.py][INFO] - Instantiating loggers...
[2024-05-30 22:35:03,570][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 22:35:03,572][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 22:35:03,617][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/diagnosed_p_t_s_d/0.2/9/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.66it/s v_num: 0.000      
                                                              val/auc: 0.508    
                                                              val/f1: 0.500     
                                                              val/precision:    
                                                              0.444 val/recall: 
                                                              0.571 val/mre:    
                                                              0.049 train/auc:  
                                                              0.772 train/f1:   
                                                              0.790             
                                                              train/precision:  
                                                              0.731             
                                                              train/recall:     
                                                              0.860 train/mre:  
                                                              0.049             
[2024-05-30 22:36:44,996][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/diagnosed_p_t_s_d/0.2/9>
[2024-05-30 22:36:44,996][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 22:36:44,999][HYDRA] 	#400 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_p_t_s_d data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=0
[2024-05-30 22:36:45,298][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 22:36:45,301][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 22:36:45,303][train.py][INFO] - Instantiating callbacks...
[2024-05-30 22:36:45,303][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 22:36:45,306][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 22:36:45,307][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 22:36:45,308][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 22:36:45,309][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 22:36:45,310][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 22:36:45,314][train.py][INFO] - Instantiating loggers...
[2024-05-30 22:36:45,314][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 22:36:45,316][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 22:36:45,406][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/diagnosed_p_t_s_d/0.25/0/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.67it/s v_num: 0.000      
                                                              val/auc: 0.421    
                                                              val/f1: 0.308     
                                                              val/precision:    
                                                              0.333 val/recall: 
                                                              0.286 val/mre:    
                                                              0.058 train/auc:  
                                                              0.816 train/f1:   
                                                              0.829             
                                                              train/precision:  
                                                              0.773             
                                                              train/recall:     
                                                              0.895 train/mre:  
                                                              0.058             
[2024-05-30 22:38:26,101][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/diagnosed_p_t_s_d/0.25/0>
[2024-05-30 22:38:26,102][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 22:38:26,105][HYDRA] 	#401 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_p_t_s_d data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=1
[rank: 0] Seed set to 1
[2024-05-30 22:38:26,412][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 22:38:26,414][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 22:38:26,416][train.py][INFO] - Instantiating callbacks...
[2024-05-30 22:38:26,416][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 22:38:26,420][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 22:38:26,421][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 22:38:26,421][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 22:38:26,422][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 22:38:26,424][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 22:38:26,428][train.py][INFO] - Instantiating loggers...
[2024-05-30 22:38:26,428][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 22:38:26,430][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 22:38:26,516][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/diagnosed_p_t_s_d/0.25/1/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.56it/s v_num: 0.000      
                                                              val/auc: 0.690    
                                                              val/f1: 0.667     
                                                              val/precision:    
                                                              0.625 val/recall: 
                                                              0.714 val/mre:    
                                                              0.055 train/auc:  
                                                              0.798 train/f1:   
                                                              0.813             
                                                              train/precision:  
                                                              0.758             
                                                              train/recall:     
                                                              0.877 train/mre:  
                                                              0.056             
[2024-05-30 22:40:09,683][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/diagnosed_p_t_s_d/0.25/1>
[2024-05-30 22:40:09,684][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 22:40:09,687][HYDRA] 	#402 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_p_t_s_d data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=2
[rank: 0] Seed set to 2
[2024-05-30 22:40:09,988][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 22:40:09,990][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 22:40:09,992][train.py][INFO] - Instantiating callbacks...
[2024-05-30 22:40:09,993][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 22:40:09,996][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 22:40:09,997][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 22:40:09,998][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 22:40:09,998][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 22:40:10,000][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 22:40:10,003][train.py][INFO] - Instantiating loggers...
[2024-05-30 22:40:10,004][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 22:40:10,006][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 22:40:10,091][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/diagnosed_p_t_s_d/0.25/2/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.67it/s v_num: 0.000      
                                                              val/auc: 0.619    
                                                              val/f1: 0.571     
                                                              val/precision:    
                                                              0.571 val/recall: 
                                                              0.571 val/mre:    
                                                              0.060 train/auc:  
                                                              0.693 train/f1:   
                                                              0.632             
                                                              train/precision:  
                                                              0.789             
                                                              train/recall:     
                                                              0.526 train/mre:  
                                                              0.061             
[2024-05-30 22:41:51,959][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/diagnosed_p_t_s_d/0.25/2>
[2024-05-30 22:41:51,960][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 22:41:51,963][HYDRA] 	#403 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_p_t_s_d data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=3
[rank: 0] Seed set to 3
[2024-05-30 22:41:52,245][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 22:41:52,248][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 22:41:52,250][train.py][INFO] - Instantiating callbacks...
[2024-05-30 22:41:52,250][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 22:41:52,253][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 22:41:52,254][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 22:41:52,255][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 22:41:52,256][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 22:41:52,257][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 22:41:52,258][train.py][INFO] - Instantiating loggers...
[2024-05-30 22:41:52,259][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 22:41:52,261][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 22:41:52,302][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/diagnosed_p_t_s_d/0.25/3/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.59it/s v_num: 0.000      
                                                              val/auc: 0.563    
                                                              val/f1: 0.533     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.571 val/mre:    
                                                              0.055 train/auc:  
                                                              0.693 train/f1:   
                                                              0.715             
                                                              train/precision:  
                                                              0.667             
                                                              train/recall:     
                                                              0.772 train/mre:  
                                                              0.055             
[2024-05-30 22:43:32,625][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/diagnosed_p_t_s_d/0.25/3>
[2024-05-30 22:43:32,626][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 22:43:32,630][HYDRA] 	#404 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_p_t_s_d data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=4
[rank: 0] Seed set to 4
[2024-05-30 22:43:32,939][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 22:43:32,942][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 22:43:32,944][train.py][INFO] - Instantiating callbacks...
[2024-05-30 22:43:32,944][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 22:43:32,948][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 22:43:32,948][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 22:43:32,949][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 22:43:32,950][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 22:43:32,952][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 22:43:32,953][train.py][INFO] - Instantiating loggers...
[2024-05-30 22:43:32,953][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 22:43:32,955][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 22:43:33,046][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/diagnosed_p_t_s_d/0.25/4/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.66it/s v_num: 0.000      
                                                              val/auc: 0.508    
                                                              val/f1: 0.500     
                                                              val/precision:    
                                                              0.444 val/recall: 
                                                              0.571 val/mre:    
                                                              0.057 train/auc:  
                                                              0.754 train/f1:   
                                                              0.791             
                                                              train/precision:  
                                                              0.688             
                                                              train/recall:     
                                                              0.930 train/mre:  
                                                              0.057             
[2024-05-30 22:45:16,687][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/diagnosed_p_t_s_d/0.25/4>
[2024-05-30 22:45:16,688][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 22:45:16,691][HYDRA] 	#405 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_p_t_s_d data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=5
[rank: 0] Seed set to 5
[2024-05-30 22:45:16,982][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 22:45:16,985][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 22:45:16,987][train.py][INFO] - Instantiating callbacks...
[2024-05-30 22:45:16,987][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 22:45:16,990][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 22:45:16,991][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 22:45:16,992][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 22:45:16,993][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 22:45:16,995][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 22:45:16,996][train.py][INFO] - Instantiating loggers...
[2024-05-30 22:45:16,996][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 22:45:16,998][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 22:45:17,042][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/diagnosed_p_t_s_d/0.25/5/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.62it/s v_num: 0.000      
                                                              val/auc: 0.381    
                                                              val/f1: 0.375     
                                                              val/precision:    
                                                              0.333 val/recall: 
                                                              0.429 val/mre:    
                                                              0.054 train/auc:  
                                                              0.772 train/f1:   
                                                              0.800             
                                                              train/precision:  
                                                              0.712             
                                                              train/recall:     
                                                              0.912 train/mre:  
                                                              0.055             
[2024-05-30 22:46:57,687][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/diagnosed_p_t_s_d/0.25/5>
[2024-05-30 22:46:57,688][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 22:46:57,691][HYDRA] 	#406 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_p_t_s_d data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=6
[rank: 0] Seed set to 6
[2024-05-30 22:46:57,978][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 22:46:57,981][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 22:46:57,983][train.py][INFO] - Instantiating callbacks...
[2024-05-30 22:46:57,983][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 22:46:57,986][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 22:46:57,987][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 22:46:57,988][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 22:46:57,989][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 22:46:57,990][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 22:46:57,991][train.py][INFO] - Instantiating loggers...
[2024-05-30 22:46:57,992][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 22:46:57,994][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 22:46:58,036][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/diagnosed_p_t_s_d/0.25/6/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.70it/s v_num: 0.000      
                                                              val/auc: 0.524    
                                                              val/f1: 0.556     
                                                              val/precision:    
                                                              0.455 val/recall: 
                                                              0.714 val/mre:    
                                                              0.061 train/auc:  
                                                              0.947 train/f1:   
                                                              0.947             
                                                              train/precision:  
                                                              0.947             
                                                              train/recall:     
                                                              0.947 train/mre:  
                                                              0.062             
[2024-05-30 22:48:37,763][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/diagnosed_p_t_s_d/0.25/6>
[2024-05-30 22:48:37,764][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 22:48:37,767][HYDRA] 	#407 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_p_t_s_d data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=7
[rank: 0] Seed set to 7
[2024-05-30 22:48:38,053][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 22:48:38,055][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 22:48:38,057][train.py][INFO] - Instantiating callbacks...
[2024-05-30 22:48:38,058][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 22:48:38,061][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 22:48:38,062][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 22:48:38,063][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 22:48:38,063][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 22:48:38,065][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 22:48:38,066][train.py][INFO] - Instantiating loggers...
[2024-05-30 22:48:38,066][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 22:48:38,069][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 22:48:38,111][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/diagnosed_p_t_s_d/0.25/7/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.67it/s v_num: 0.000      
                                                              val/auc: 0.746    
                                                              val/f1: 0.714     
                                                              val/precision:    
                                                              0.714 val/recall: 
                                                              0.714 val/mre:    
                                                              0.061 train/auc:  
                                                              0.763 train/f1:   
                                                              0.784             
                                                              train/precision:  
                                                              0.721             
                                                              train/recall:     
                                                              0.860 train/mre:  
                                                              0.059             
[2024-05-30 22:50:17,467][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/diagnosed_p_t_s_d/0.25/7>
[2024-05-30 22:50:17,468][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 22:50:17,471][HYDRA] 	#408 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_p_t_s_d data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=8
[rank: 0] Seed set to 8
[2024-05-30 22:50:18,193][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 22:50:18,195][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 22:50:18,197][train.py][INFO] - Instantiating callbacks...
[2024-05-30 22:50:18,198][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 22:50:18,201][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 22:50:18,202][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 22:50:18,203][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 22:50:18,204][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 22:50:18,205][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 22:50:18,206][train.py][INFO] - Instantiating loggers...
[2024-05-30 22:50:18,206][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 22:50:18,209][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 22:50:18,251][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/diagnosed_p_t_s_d/0.25/8/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.66it/s v_num: 0.000      
                                                              val/auc: 0.690    
                                                              val/f1: 0.667     
                                                              val/precision:    
                                                              0.625 val/recall: 
                                                              0.714 val/mre:    
                                                              0.060 train/auc:  
                                                              0.860 train/f1:   
                                                              0.849             
                                                              train/precision:  
                                                              0.918             
                                                              train/recall:     
                                                              0.789 train/mre:  
                                                              0.060             
[2024-05-30 22:51:58,860][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/diagnosed_p_t_s_d/0.25/8>
[2024-05-30 22:51:58,861][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 22:51:58,864][HYDRA] 	#409 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_p_t_s_d data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=9
[rank: 0] Seed set to 9
[2024-05-30 22:51:59,156][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 22:51:59,159][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 22:51:59,162][train.py][INFO] - Instantiating callbacks...
[2024-05-30 22:51:59,162][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 22:51:59,166][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 22:51:59,167][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 22:51:59,167][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 22:51:59,168][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 22:51:59,170][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 22:51:59,172][train.py][INFO] - Instantiating loggers...
[2024-05-30 22:51:59,172][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 22:51:59,174][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 22:51:59,218][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/diagnosed_p_t_s_d/0.25/9/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.68it/s v_num: 0.000      
                                                              val/auc: 0.492    
                                                              val/f1: 0.429     
                                                              val/precision:    
                                                              0.429 val/recall: 
                                                              0.429 val/mre:    
                                                              0.054 train/auc:  
                                                              0.737 train/f1:   
                                                              0.762             
                                                              train/precision:  
                                                              0.696             
                                                              train/recall:     
                                                              0.842 train/mre:  
                                                              0.054             
[2024-05-30 22:53:39,053][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/diagnosed_p_t_s_d/0.25/9>
[2024-05-30 22:53:39,054][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 22:53:39,057][HYDRA] 	#410 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_p_t_s_d data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=0
[2024-05-30 22:53:39,345][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 22:53:39,348][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 22:53:39,350][train.py][INFO] - Instantiating callbacks...
[2024-05-30 22:53:39,350][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 22:53:39,354][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 22:53:39,355][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 22:53:39,355][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 22:53:39,356][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 22:53:39,358][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 22:53:39,360][train.py][INFO] - Instantiating loggers...
[2024-05-30 22:53:39,361][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 22:53:39,363][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 22:53:39,420][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/diagnosed_p_t_s_d/0.3/0/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.72it/s v_num: 0.000      
                                                              val/auc: 0.619    
                                                              val/f1: 0.571     
                                                              val/precision:    
                                                              0.571 val/recall: 
                                                              0.571 val/mre:    
                                                              0.061 train/auc:  
                                                              0.789 train/f1:   
                                                              0.821             
                                                              train/precision:  
                                                              0.714             
                                                              train/recall:     
                                                              0.965 train/mre:  
                                                              0.061             
[2024-05-30 22:55:19,420][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/diagnosed_p_t_s_d/0.3/0>
[2024-05-30 22:55:19,421][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 22:55:19,424][HYDRA] 	#411 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_p_t_s_d data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=1
[rank: 0] Seed set to 1
[2024-05-30 22:55:19,718][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 22:55:19,721][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 22:55:19,724][train.py][INFO] - Instantiating callbacks...
[2024-05-30 22:55:19,724][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 22:55:19,728][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 22:55:19,729][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 22:55:19,730][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 22:55:19,731][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 22:55:19,732][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 22:55:19,738][train.py][INFO] - Instantiating loggers...
[2024-05-30 22:55:19,738][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 22:55:19,740][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 22:55:19,829][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/diagnosed_p_t_s_d/0.3/1/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.56it/s v_num: 0.000      
                                                              val/auc: 0.587    
                                                              val/f1: 0.400     
                                                              val/precision:    
                                                              0.667 val/recall: 
                                                              0.286 val/mre:    
                                                              0.060 train/auc:  
                                                              0.904 train/f1:   
                                                              0.906             
                                                              train/precision:  
                                                              0.883             
                                                              train/recall:     
                                                              0.930 train/mre:  
                                                              0.061             
[2024-05-30 22:57:02,655][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/diagnosed_p_t_s_d/0.3/1>
[2024-05-30 22:57:02,655][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 22:57:02,658][HYDRA] 	#412 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_p_t_s_d data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=2
[rank: 0] Seed set to 2
[2024-05-30 22:57:02,961][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 22:57:02,963][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 22:57:02,966][train.py][INFO] - Instantiating callbacks...
[2024-05-30 22:57:02,966][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 22:57:02,969][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 22:57:02,970][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 22:57:02,971][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 22:57:02,972][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 22:57:02,974][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 22:57:02,977][train.py][INFO] - Instantiating loggers...
[2024-05-30 22:57:02,977][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 22:57:02,979][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 22:57:03,071][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/diagnosed_p_t_s_d/0.3/2/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.64it/s v_num: 0.000      
                                                              val/auc: 0.675    
                                                              val/f1: 0.615     
                                                              val/precision:    
                                                              0.667 val/recall: 
                                                              0.571 val/mre:    
                                                              0.066 train/auc:  
                                                              0.693 train/f1:   
                                                              0.598             
                                                              train/precision:  
                                                              0.867             
                                                              train/recall:     
                                                              0.456 train/mre:  
                                                              0.067             
[2024-05-30 22:58:45,062][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/diagnosed_p_t_s_d/0.3/2>
[2024-05-30 22:58:45,063][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 22:58:45,066][HYDRA] 	#413 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_p_t_s_d data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=3
[rank: 0] Seed set to 3
[2024-05-30 22:58:45,372][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 22:58:45,374][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 22:58:45,376][train.py][INFO] - Instantiating callbacks...
[2024-05-30 22:58:45,377][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 22:58:45,380][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 22:58:45,381][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 22:58:45,382][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 22:58:45,383][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 22:58:45,384][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 22:58:45,388][train.py][INFO] - Instantiating loggers...
[2024-05-30 22:58:45,388][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 22:58:45,390][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 22:58:45,480][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/diagnosed_p_t_s_d/0.3/3/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.70it/s v_num: 0.000      
                                                              val/auc: 0.508    
                                                              val/f1: 0.500     
                                                              val/precision:    
                                                              0.444 val/recall: 
                                                              0.571 val/mre:    
                                                              0.061 train/auc:  
                                                              0.719 train/f1:   
                                                              0.733             
                                                              train/precision:  
                                                              0.698             
                                                              train/recall:     
                                                              0.772 train/mre:  
                                                              0.060             
[2024-05-30 23:00:25,517][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/diagnosed_p_t_s_d/0.3/3>
[2024-05-30 23:00:25,518][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 23:00:25,521][HYDRA] 	#414 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_p_t_s_d data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=4
[rank: 0] Seed set to 4
[2024-05-30 23:00:25,822][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 23:00:25,824][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 23:00:25,826][train.py][INFO] - Instantiating callbacks...
[2024-05-30 23:00:25,827][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 23:00:25,830][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 23:00:25,831][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 23:00:25,832][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 23:00:25,832][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 23:00:25,834][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 23:00:25,837][train.py][INFO] - Instantiating loggers...
[2024-05-30 23:00:25,838][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 23:00:25,840][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 23:00:25,925][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/diagnosed_p_t_s_d/0.3/4/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.68it/s v_num: 0.000      
                                                              val/auc: 0.548    
                                                              val/f1: 0.462     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.429 val/mre:    
                                                              0.064 train/auc:  
                                                              0.781 train/f1:   
                                                              0.800             
                                                              train/precision:  
                                                              0.735             
                                                              train/recall:     
                                                              0.877 train/mre:  
                                                              0.065             
[2024-05-30 23:02:05,559][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/diagnosed_p_t_s_d/0.3/4>
[2024-05-30 23:02:05,560][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 23:02:05,563][HYDRA] 	#415 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_p_t_s_d data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=5
[rank: 0] Seed set to 5
[2024-05-30 23:02:05,860][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 23:02:05,862][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 23:02:05,864][train.py][INFO] - Instantiating callbacks...
[2024-05-30 23:02:05,865][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 23:02:05,868][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 23:02:05,869][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 23:02:05,870][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 23:02:05,870][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 23:02:05,872][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 23:02:05,878][train.py][INFO] - Instantiating loggers...
[2024-05-30 23:02:05,878][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 23:02:05,881][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 23:02:05,968][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/diagnosed_p_t_s_d/0.3/5/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.64it/s v_num: 0.000      
                                                              val/auc: 0.437    
                                                              val/f1: 0.400     
                                                              val/precision:    
                                                              0.375 val/recall: 
                                                              0.429 val/mre:    
                                                              0.060 train/auc:  
                                                              0.763 train/f1:   
                                                              0.791             
                                                              train/precision:  
                                                              0.708             
                                                              train/recall:     
                                                              0.895 train/mre:  
                                                              0.060             
[2024-05-30 23:03:47,070][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/diagnosed_p_t_s_d/0.3/5>
[2024-05-30 23:03:47,070][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 23:03:47,074][HYDRA] 	#416 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_p_t_s_d data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=6
[rank: 0] Seed set to 6
[2024-05-30 23:03:47,361][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 23:03:47,364][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 23:03:47,366][train.py][INFO] - Instantiating callbacks...
[2024-05-30 23:03:47,366][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 23:03:47,369][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 23:03:47,370][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 23:03:47,371][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 23:03:47,372][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 23:03:47,374][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 23:03:47,374][train.py][INFO] - Instantiating loggers...
[2024-05-30 23:03:47,375][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 23:03:47,377][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 23:03:47,420][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/diagnosed_p_t_s_d/0.3/6/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.62it/s v_num: 0.000      
                                                              val/auc: 0.635    
                                                              val/f1: 0.625     
                                                              val/precision:    
                                                              0.556 val/recall: 
                                                              0.714 val/mre:    
                                                              0.067 train/auc:  
                                                              0.868 train/f1:   
                                                              0.872             
                                                              train/precision:  
                                                              0.850             
                                                              train/recall:     
                                                              0.895 train/mre:  
                                                              0.068             
[2024-05-30 23:05:28,642][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/diagnosed_p_t_s_d/0.3/6>
[2024-05-30 23:05:28,643][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 23:05:28,646][HYDRA] 	#417 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_p_t_s_d data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=7
[rank: 0] Seed set to 7
[2024-05-30 23:05:28,941][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 23:05:28,944][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 23:05:28,946][train.py][INFO] - Instantiating callbacks...
[2024-05-30 23:05:28,946][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 23:05:28,950][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 23:05:28,951][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 23:05:28,951][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 23:05:28,952][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 23:05:28,954][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 23:05:28,955][train.py][INFO] - Instantiating loggers...
[2024-05-30 23:05:28,955][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 23:05:28,957][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 23:05:29,002][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/diagnosed_p_t_s_d/0.3/7/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.63it/s v_num: 0.000      
                                                              val/auc: 0.675    
                                                              val/f1: 0.615     
                                                              val/precision:    
                                                              0.667 val/recall: 
                                                              0.571 val/mre:    
                                                              0.063 train/auc:  
                                                              0.693 train/f1:   
                                                              0.733             
                                                              train/precision:  
                                                              0.649             
                                                              train/recall:     
                                                              0.842 train/mre:  
                                                              0.061             
[2024-05-30 23:07:08,114][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/diagnosed_p_t_s_d/0.3/7>
[2024-05-30 23:07:08,115][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 23:07:08,118][HYDRA] 	#418 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_p_t_s_d data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=8
[rank: 0] Seed set to 8
[2024-05-30 23:07:08,408][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 23:07:08,410][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 23:07:08,412][train.py][INFO] - Instantiating callbacks...
[2024-05-30 23:07:08,413][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 23:07:08,416][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 23:07:08,417][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 23:07:08,418][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 23:07:08,418][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 23:07:08,420][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 23:07:08,421][train.py][INFO] - Instantiating loggers...
[2024-05-30 23:07:08,421][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 23:07:08,424][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 23:07:08,467][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/diagnosed_p_t_s_d/0.3/8/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.69it/s v_num: 0.000      
                                                              val/auc: 0.619    
                                                              val/f1: 0.571     
                                                              val/precision:    
                                                              0.571 val/recall: 
                                                              0.571 val/mre:    
                                                              0.068 train/auc:  
                                                              0.851 train/f1:   
                                                              0.841             
                                                              train/precision:  
                                                              0.900             
                                                              train/recall:     
                                                              0.789 train/mre:  
                                                              0.068             
[2024-05-30 23:08:49,091][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/diagnosed_p_t_s_d/0.3/8>
[2024-05-30 23:08:49,091][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 23:08:49,095][HYDRA] 	#419 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_p_t_s_d data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=9
[rank: 0] Seed set to 9
[2024-05-30 23:08:49,385][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 23:08:49,387][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 23:08:49,389][train.py][INFO] - Instantiating callbacks...
[2024-05-30 23:08:49,390][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 23:08:49,393][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 23:08:49,394][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 23:08:49,395][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 23:08:49,396][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 23:08:49,397][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 23:08:49,398][train.py][INFO] - Instantiating loggers...
[2024-05-30 23:08:49,399][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 23:08:49,401][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 23:08:49,444][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/diagnosed_p_t_s_d/0.3/9/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.68it/s v_num: 0.000      
                                                              val/auc: 0.452    
                                                              val/f1: 0.471     
                                                              val/precision:    
                                                              0.400 val/recall: 
                                                              0.571 val/mre:    
                                                              0.061 train/auc:  
                                                              0.693 train/f1:   
                                                              0.737             
                                                              train/precision:  
                                                              0.645             
                                                              train/recall:     
                                                              0.860 train/mre:  
                                                              0.061             
[2024-05-30 23:10:30,107][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/diagnosed_p_t_s_d/0.3/9>
[2024-05-30 23:10:30,108][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 23:10:30,111][HYDRA] 	#420 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=doing_today data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=0
[2024-05-30 23:10:30,403][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 23:10:30,405][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 23:10:30,407][train.py][INFO] - Instantiating callbacks...
[2024-05-30 23:10:30,408][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 23:10:30,411][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 23:10:30,412][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 23:10:30,413][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 23:10:30,413][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 23:10:30,415][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 23:10:30,417][train.py][INFO] - Instantiating loggers...
[2024-05-30 23:10:30,417][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 23:10:30,421][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 23:10:30,463][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/doing_today/0.0/0/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.56it/s v_num: 0.000      
                                                              val/auc: 0.550    
                                                              val/f1: 0.308     
                                                              val/precision:    
                                                              0.667 val/recall: 
                                                              0.200 val/mre: nan
                                                              train/auc: 0.789  
                                                              train/f1: 0.761   
                                                              train/precision:  
                                                              0.878             
                                                              train/recall:     
                                                              0.672 train/mre:  
                                                              nan               
[2024-05-30 23:12:15,467][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/doing_today/0.0/0>
[2024-05-30 23:12:15,468][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 23:12:15,471][HYDRA] 	#421 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=doing_today data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=1
[rank: 0] Seed set to 1
[2024-05-30 23:12:15,757][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 23:12:15,760][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 23:12:15,762][train.py][INFO] - Instantiating callbacks...
[2024-05-30 23:12:15,762][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 23:12:15,766][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 23:12:15,766][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 23:12:15,767][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 23:12:15,768][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 23:12:15,770][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 23:12:15,771][train.py][INFO] - Instantiating loggers...
[2024-05-30 23:12:15,771][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 23:12:15,773][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 23:12:15,816][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/doing_today/0.0/1/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.50it/s v_num: 0.000      
                                                              val/auc: 0.500    
                                                              val/f1: 0.167     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.100 val/mre: nan
                                                              train/auc: 0.812  
                                                              train/f1: 0.793   
                                                              train/precision:  
                                                              0.885             
                                                              train/recall:     
                                                              0.719 train/mre:  
                                                              nan               
[2024-05-30 23:14:02,042][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/doing_today/0.0/1>
[2024-05-30 23:14:02,043][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 23:14:02,047][HYDRA] 	#422 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=doing_today data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=2
[rank: 0] Seed set to 2
[2024-05-30 23:14:02,346][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 23:14:02,349][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 23:14:02,351][train.py][INFO] - Instantiating callbacks...
[2024-05-30 23:14:02,351][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 23:14:02,355][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 23:14:02,355][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 23:14:02,356][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 23:14:02,357][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 23:14:02,359][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 23:14:02,368][train.py][INFO] - Instantiating loggers...
[2024-05-30 23:14:02,368][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 23:14:02,373][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 23:14:02,464][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/doing_today/0.0/2/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.49it/s v_num: 0.000      
                                                              val/auc: 0.500    
                                                              val/f1: 0.375     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.300 val/mre: nan
                                                              train/auc: 0.852  
                                                              train/f1: 0.859   
                                                              train/precision:  
                                                              0.817             
                                                              train/recall:     
                                                              0.906 train/mre:  
                                                              nan               
[2024-05-30 23:15:49,682][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/doing_today/0.0/2>
[2024-05-30 23:15:49,683][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 23:15:49,686][HYDRA] 	#423 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=doing_today data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=3
[rank: 0] Seed set to 3
[2024-05-30 23:15:49,988][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 23:15:49,991][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 23:15:49,993][train.py][INFO] - Instantiating callbacks...
[2024-05-30 23:15:49,993][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 23:15:49,996][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 23:15:49,997][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 23:15:49,998][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 23:15:49,999][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 23:15:50,000][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 23:15:50,004][train.py][INFO] - Instantiating loggers...
[2024-05-30 23:15:50,004][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 23:15:50,006][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 23:15:50,095][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/doing_today/0.0/3/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.47it/s v_num: 0.000      
                                                              val/auc: 0.500    
                                                              val/f1: 0.444     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.400 val/mre: nan
                                                              train/auc: 0.797  
                                                              train/f1: 0.809   
                                                              train/precision:  
                                                              0.764             
                                                              train/recall:     
                                                              0.859 train/mre:  
                                                              nan               
[2024-05-30 23:17:36,015][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/doing_today/0.0/3>
[2024-05-30 23:17:36,015][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 23:17:36,019][HYDRA] 	#424 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=doing_today data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=4
[rank: 0] Seed set to 4
[2024-05-30 23:17:36,309][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 23:17:36,311][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 23:17:36,313][train.py][INFO] - Instantiating callbacks...
[2024-05-30 23:17:36,314][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 23:17:36,317][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 23:17:36,318][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 23:17:36,318][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 23:17:36,319][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 23:17:36,321][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 23:17:36,322][train.py][INFO] - Instantiating loggers...
[2024-05-30 23:17:36,322][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 23:17:36,324][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 23:17:36,366][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/doing_today/0.0/4/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.48it/s v_num: 0.000      
                                                              val/auc: 0.500    
                                                              val/f1: 0.286     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.200 val/mre: nan
                                                              train/auc: 0.805  
                                                              train/f1: 0.803   
                                                              train/precision:  
                                                              0.810             
                                                              train/recall:     
                                                              0.797 train/mre:  
                                                              nan               
[2024-05-30 23:19:22,108][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/doing_today/0.0/4>
[2024-05-30 23:19:22,109][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 23:19:22,112][HYDRA] 	#425 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=doing_today data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=5
[rank: 0] Seed set to 5
[2024-05-30 23:19:22,395][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 23:19:22,398][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 23:19:22,400][train.py][INFO] - Instantiating callbacks...
[2024-05-30 23:19:22,400][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 23:19:22,403][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 23:19:22,404][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 23:19:22,405][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 23:19:22,406][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 23:19:22,407][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 23:19:22,408][train.py][INFO] - Instantiating loggers...
[2024-05-30 23:19:22,409][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 23:19:22,411][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 23:19:22,452][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/doing_today/0.0/5/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.51it/s v_num: 0.000      
                                                              val/auc: 0.550    
                                                              val/f1: 0.400     
                                                              val/precision:    
                                                              0.600 val/recall: 
                                                              0.300 val/mre: nan
                                                              train/auc: 0.820  
                                                              train/f1: 0.835   
                                                              train/precision:  
                                                              0.773             
                                                              train/recall:     
                                                              0.906 train/mre:  
                                                              nan               
[2024-05-30 23:21:07,030][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/doing_today/0.0/5>
[2024-05-30 23:21:07,031][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 23:21:07,034][HYDRA] 	#426 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=doing_today data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=6
[rank: 0] Seed set to 6
[2024-05-30 23:21:07,319][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 23:21:07,322][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 23:21:07,324][train.py][INFO] - Instantiating callbacks...
[2024-05-30 23:21:07,324][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 23:21:07,328][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 23:21:07,328][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 23:21:07,329][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 23:21:07,330][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 23:21:07,332][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 23:21:07,333][train.py][INFO] - Instantiating loggers...
[2024-05-30 23:21:07,333][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 23:21:07,335][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 23:21:07,378][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/doing_today/0.0/6/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.52it/s v_num: 0.000      
                                                              val/auc: 0.600    
                                                              val/f1: 0.429     
                                                              val/precision:    
                                                              0.750 val/recall: 
                                                              0.300 val/mre: nan
                                                              train/auc: 0.859  
                                                              train/f1: 0.852   
                                                              train/precision:  
                                                              0.897             
                                                              train/recall:     
                                                              0.812 train/mre:  
                                                              nan               
[2024-05-30 23:22:53,338][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/doing_today/0.0/6>
[2024-05-30 23:22:53,340][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 23:22:53,346][HYDRA] 	#427 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=doing_today data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=7
[rank: 0] Seed set to 7
[2024-05-30 23:22:53,692][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 23:22:53,695][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 23:22:53,697][train.py][INFO] - Instantiating callbacks...
[2024-05-30 23:22:53,697][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 23:22:53,700][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 23:22:53,701][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 23:22:53,702][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 23:22:53,703][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 23:22:53,705][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 23:22:53,705][train.py][INFO] - Instantiating loggers...
[2024-05-30 23:22:53,706][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 23:22:53,708][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 23:22:53,920][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/doing_today/0.0/7/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.53it/s v_num: 0.000      
                                                              val/auc: 0.600    
                                                              val/f1: 0.429     
                                                              val/precision:    
                                                              0.750 val/recall: 
                                                              0.300 val/mre: nan
                                                              train/auc: 0.844  
                                                              train/f1: 0.848   
                                                              train/precision:  
                                                              0.824             
                                                              train/recall:     
                                                              0.875 train/mre:  
                                                              nan               
[2024-05-30 23:24:38,824][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/doing_today/0.0/7>
[2024-05-30 23:24:38,825][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 23:24:38,828][HYDRA] 	#428 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=doing_today data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=8
[rank: 0] Seed set to 8
[2024-05-30 23:24:39,140][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 23:24:39,143][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 23:24:39,145][train.py][INFO] - Instantiating callbacks...
[2024-05-30 23:24:39,145][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 23:24:39,149][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 23:24:39,149][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 23:24:39,150][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 23:24:39,151][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 23:24:39,153][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 23:24:39,156][train.py][INFO] - Instantiating loggers...
[2024-05-30 23:24:39,156][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 23:24:39,158][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 23:24:39,250][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/doing_today/0.0/8/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.55it/s v_num: 0.000      
                                                              val/auc: 0.550    
                                                              val/f1: 0.308     
                                                              val/precision:    
                                                              0.667 val/recall: 
                                                              0.200 val/mre: nan
                                                              train/auc: 0.852  
                                                              train/f1: 0.840   
                                                              train/precision:  
                                                              0.909             
                                                              train/recall:     
                                                              0.781 train/mre:  
                                                              nan               
[2024-05-30 23:26:23,836][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/doing_today/0.0/8>
[2024-05-30 23:26:23,837][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 23:26:23,840][HYDRA] 	#429 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=doing_today data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=9
[rank: 0] Seed set to 9
[2024-05-30 23:26:24,139][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 23:26:24,142][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 23:26:24,144][train.py][INFO] - Instantiating callbacks...
[2024-05-30 23:26:24,145][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 23:26:24,149][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 23:26:24,150][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 23:26:24,150][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 23:26:24,151][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 23:26:24,153][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 23:26:24,156][train.py][INFO] - Instantiating loggers...
[2024-05-30 23:26:24,156][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 23:26:24,159][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 23:26:24,242][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/doing_today/0.0/9/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.55it/s v_num: 0.000      
                                                              val/auc: 0.600    
                                                              val/f1: 0.333     
                                                              val/precision:    
                                                              1.000 val/recall: 
                                                              0.200 val/mre: nan
                                                              train/auc: 0.828  
                                                              train/f1: 0.833   
                                                              train/precision:  
                                                              0.809             
                                                              train/recall:     
                                                              0.859 train/mre:  
                                                              nan               
[2024-05-30 23:28:09,944][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/doing_today/0.0/9>
[2024-05-30 23:28:09,945][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 23:28:09,948][HYDRA] 	#430 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=doing_today data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=0
[2024-05-30 23:28:10,250][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 23:28:10,253][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 23:28:10,255][train.py][INFO] - Instantiating callbacks...
[2024-05-30 23:28:10,255][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 23:28:10,258][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 23:28:10,259][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 23:28:10,260][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 23:28:10,261][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 23:28:10,263][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 23:28:10,266][train.py][INFO] - Instantiating loggers...
[2024-05-30 23:28:10,266][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 23:28:10,268][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 23:28:10,370][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/doing_today/0.05/0/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.55it/s v_num: 0.000      
                                                              val/auc: 0.650    
                                                              val/f1: 0.588     
                                                              val/precision:    
                                                              0.714 val/recall: 
                                                              0.500 val/mre:    
                                                              0.068 train/auc:  
                                                              0.820 train/f1:   
                                                              0.824             
                                                              train/precision:  
                                                              0.806             
                                                              train/recall:     
                                                              0.844 train/mre:  
                                                              0.059             
[2024-05-30 23:29:56,942][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/doing_today/0.05/0>
[2024-05-30 23:29:56,943][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 23:29:56,946][HYDRA] 	#431 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=doing_today data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=1
[rank: 0] Seed set to 1
[2024-05-30 23:29:57,246][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 23:29:57,249][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 23:29:57,251][train.py][INFO] - Instantiating callbacks...
[2024-05-30 23:29:57,252][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 23:29:57,256][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 23:29:57,257][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 23:29:57,257][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 23:29:57,258][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 23:29:57,260][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 23:29:57,263][train.py][INFO] - Instantiating loggers...
[2024-05-30 23:29:57,264][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 23:29:57,266][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 23:29:57,350][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/doing_today/0.05/1/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.53it/s v_num: 0.000      
                                                              val/auc: 0.650    
                                                              val/f1: 0.588     
                                                              val/precision:    
                                                              0.714 val/recall: 
                                                              0.500 val/mre:    
                                                              0.071 train/auc:  
                                                              0.852 train/f1:   
                                                              0.832             
                                                              train/precision:  
                                                              0.959             
                                                              train/recall:     
                                                              0.734 train/mre:  
                                                              0.065             
[2024-05-30 23:31:43,567][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/doing_today/0.05/1>
[2024-05-30 23:31:43,568][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 23:31:43,571][HYDRA] 	#432 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=doing_today data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=2
[rank: 0] Seed set to 2
[2024-05-30 23:31:43,871][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 23:31:43,874][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 23:31:43,876][train.py][INFO] - Instantiating callbacks...
[2024-05-30 23:31:43,877][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 23:31:43,881][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 23:31:43,882][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 23:31:43,883][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 23:31:43,883][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 23:31:43,885][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 23:31:43,889][train.py][INFO] - Instantiating loggers...
[2024-05-30 23:31:43,889][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 23:31:43,891][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 23:31:43,981][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/doing_today/0.05/2/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.48it/s v_num: 0.000      
                                                              val/auc: 0.550    
                                                              val/f1: 0.400     
                                                              val/precision:    
                                                              0.600 val/recall: 
                                                              0.300 val/mre:    
                                                              0.072 train/auc:  
                                                              0.930 train/f1:   
                                                              0.931             
                                                              train/precision:  
                                                              0.910             
                                                              train/recall:     
                                                              0.953 train/mre:  
                                                              0.063             
[2024-05-30 23:33:30,977][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/doing_today/0.05/2>
[2024-05-30 23:33:30,978][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 23:33:30,982][HYDRA] 	#433 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=doing_today data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=3
[rank: 0] Seed set to 3
[2024-05-30 23:33:31,306][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 23:33:31,309][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 23:33:31,311][train.py][INFO] - Instantiating callbacks...
[2024-05-30 23:33:31,311][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 23:33:31,314][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 23:33:31,315][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 23:33:31,316][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 23:33:31,317][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 23:33:31,318][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 23:33:31,322][train.py][INFO] - Instantiating loggers...
[2024-05-30 23:33:31,322][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 23:33:31,324][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 23:33:31,416][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/doing_today/0.05/3/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.51it/s v_num: 0.000      
                                                              val/auc: 0.500    
                                                              val/f1: 0.375     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.300 val/mre:    
                                                              0.070 train/auc:  
                                                              0.859 train/f1:   
                                                              0.870             
                                                              train/precision:  
                                                              0.811             
                                                              train/recall:     
                                                              0.938 train/mre:  
                                                              0.063             
[2024-05-30 23:35:16,452][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/doing_today/0.05/3>
[2024-05-30 23:35:16,452][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 23:35:16,456][HYDRA] 	#434 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=doing_today data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=4
[rank: 0] Seed set to 4
[2024-05-30 23:35:16,750][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 23:35:16,753][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 23:35:16,755][train.py][INFO] - Instantiating callbacks...
[2024-05-30 23:35:16,756][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 23:35:16,760][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 23:35:16,760][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 23:35:16,761][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 23:35:16,762][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 23:35:16,764][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 23:35:16,768][train.py][INFO] - Instantiating loggers...
[2024-05-30 23:35:16,768][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 23:35:16,770][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 23:35:16,861][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/doing_today/0.05/4/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.49it/s v_num: 0.000      
                                                              val/auc: 0.450    
                                                              val/f1: 0.353     
                                                              val/precision:    
                                                              0.429 val/recall: 
                                                              0.300 val/mre:    
                                                              0.070 train/auc:  
                                                              0.922 train/f1:   
                                                              0.921             
                                                              train/precision:  
                                                              0.935             
                                                              train/recall:     
                                                              0.906 train/mre:  
                                                              0.062             
[2024-05-30 23:37:02,162][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/doing_today/0.05/4>
[2024-05-30 23:37:02,162][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 23:37:02,166][HYDRA] 	#435 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=doing_today data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=5
[rank: 0] Seed set to 5
[2024-05-30 23:37:02,470][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 23:37:02,473][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 23:37:02,476][train.py][INFO] - Instantiating callbacks...
[2024-05-30 23:37:02,476][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 23:37:02,480][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 23:37:02,481][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 23:37:02,482][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 23:37:02,482][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 23:37:02,484][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 23:37:02,487][train.py][INFO] - Instantiating loggers...
[2024-05-30 23:37:02,488][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 23:37:02,490][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 23:37:02,580][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/doing_today/0.05/5/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.55it/s v_num: 0.000      
                                                              val/auc: 0.350    
                                                              val/f1: 0.235     
                                                              val/precision:    
                                                              0.286 val/recall: 
                                                              0.200 val/mre:    
                                                              0.082 train/auc:  
                                                              0.922 train/f1:   
                                                              0.921             
                                                              train/precision:  
                                                              0.935             
                                                              train/recall:     
                                                              0.906 train/mre:  
                                                              0.074             
[2024-05-30 23:38:46,324][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/doing_today/0.05/5>
[2024-05-30 23:38:46,325][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 23:38:46,328][HYDRA] 	#436 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=doing_today data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=6
[rank: 0] Seed set to 6
[2024-05-30 23:38:46,632][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 23:38:46,634][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 23:38:46,636][train.py][INFO] - Instantiating callbacks...
[2024-05-30 23:38:46,637][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 23:38:46,640][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 23:38:46,641][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 23:38:46,642][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 23:38:46,642][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 23:38:46,644][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 23:38:46,647][train.py][INFO] - Instantiating loggers...
[2024-05-30 23:38:46,648][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 23:38:46,650][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 23:38:46,755][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/doing_today/0.05/6/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.52it/s v_num: 0.000      
                                                              val/auc: 0.550    
                                                              val/f1: 0.526     
                                                              val/precision:    
                                                              0.556 val/recall: 
                                                              0.500 val/mre:    
                                                              0.072 train/auc:  
                                                              0.953 train/f1:   
                                                              0.952             
                                                              train/precision:  
                                                              0.983             
                                                              train/recall:     
                                                              0.922 train/mre:  
                                                              0.063             
[2024-05-30 23:40:32,816][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/doing_today/0.05/6>
[2024-05-30 23:40:32,817][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 23:40:32,821][HYDRA] 	#437 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=doing_today data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=7
[rank: 0] Seed set to 7
[2024-05-30 23:40:33,155][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 23:40:33,158][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 23:40:33,160][train.py][INFO] - Instantiating callbacks...
[2024-05-30 23:40:33,160][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 23:40:33,164][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 23:40:33,165][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 23:40:33,165][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 23:40:33,166][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 23:40:33,168][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 23:40:33,173][train.py][INFO] - Instantiating loggers...
[2024-05-30 23:40:33,174][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 23:40:33,176][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 23:40:33,294][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/doing_today/0.05/7/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.54it/s v_num: 0.000      
                                                              val/auc: 0.600    
                                                              val/f1: 0.333     
                                                              val/precision:    
                                                              1.000 val/recall: 
                                                              0.200 val/mre:    
                                                              0.076 train/auc:  
                                                              0.805 train/f1:   
                                                              0.793             
                                                              train/precision:  
                                                              0.842             
                                                              train/recall:     
                                                              0.750 train/mre:  
                                                              0.069             
[2024-05-30 23:42:16,788][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/doing_today/0.05/7>
[2024-05-30 23:42:16,789][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 23:42:16,792][HYDRA] 	#438 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=doing_today data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=8
[rank: 0] Seed set to 8
[2024-05-30 23:42:17,087][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 23:42:17,090][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 23:42:17,092][train.py][INFO] - Instantiating callbacks...
[2024-05-30 23:42:17,092][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 23:42:17,095][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 23:42:17,096][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 23:42:17,097][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 23:42:17,098][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 23:42:17,099][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 23:42:17,103][train.py][INFO] - Instantiating loggers...
[2024-05-30 23:42:17,103][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 23:42:17,105][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 23:42:17,188][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/doing_today/0.05/8/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.57it/s v_num: 0.000      
                                                              val/auc: 0.550    
                                                              val/f1: 0.400     
                                                              val/precision:    
                                                              0.600 val/recall: 
                                                              0.300 val/mre:    
                                                              0.078 train/auc:  
                                                              0.922 train/f1:   
                                                              0.918             
                                                              train/precision:  
                                                              0.966             
                                                              train/recall:     
                                                              0.875 train/mre:  
                                                              0.068             
[2024-05-30 23:44:01,979][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/doing_today/0.05/8>
[2024-05-30 23:44:01,980][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 23:44:01,983][HYDRA] 	#439 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=doing_today data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=9
[rank: 0] Seed set to 9
[2024-05-30 23:44:02,292][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 23:44:02,295][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 23:44:02,297][train.py][INFO] - Instantiating callbacks...
[2024-05-30 23:44:02,297][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 23:44:02,301][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 23:44:02,301][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 23:44:02,302][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 23:44:02,303][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 23:44:02,305][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 23:44:02,308][train.py][INFO] - Instantiating loggers...
[2024-05-30 23:44:02,308][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 23:44:02,310][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 23:44:02,400][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/doing_today/0.05/9/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.52it/s v_num: 0.000      
                                                              val/auc: 0.600    
                                                              val/f1: 0.429     
                                                              val/precision:    
                                                              0.750 val/recall: 
                                                              0.300 val/mre:    
                                                              0.076 train/auc:  
                                                              0.914 train/f1:   
                                                              0.911             
                                                              train/precision:  
                                                              0.949             
                                                              train/recall:     
                                                              0.875 train/mre:  
                                                              0.067             
[2024-05-30 23:45:48,174][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/doing_today/0.05/9>
[2024-05-30 23:45:48,175][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 23:45:48,178][HYDRA] 	#440 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=doing_today data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=0
[2024-05-30 23:45:48,474][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 23:45:48,477][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 23:45:48,479][train.py][INFO] - Instantiating callbacks...
[2024-05-30 23:45:48,479][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 23:45:48,483][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 23:45:48,483][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 23:45:48,484][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 23:45:48,485][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 23:45:48,487][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 23:45:48,490][train.py][INFO] - Instantiating loggers...
[2024-05-30 23:45:48,490][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 23:45:48,492][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 23:45:48,582][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/doing_today/0.1/0/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.53it/s v_num: 0.000      
                                                              val/auc: 0.600    
                                                              val/f1: 0.429     
                                                              val/precision:    
                                                              0.750 val/recall: 
                                                              0.300 val/mre:    
                                                              0.072 train/auc:  
                                                              0.852 train/f1:   
                                                              0.857             
                                                              train/precision:  
                                                              0.826             
                                                              train/recall:     
                                                              0.891 train/mre:  
                                                              0.063             
[2024-05-30 23:47:35,205][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/doing_today/0.1/0>
[2024-05-30 23:47:35,206][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 23:47:35,209][HYDRA] 	#441 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=doing_today data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=1
[rank: 0] Seed set to 1
[2024-05-30 23:47:35,506][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 23:47:35,509][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 23:47:35,511][train.py][INFO] - Instantiating callbacks...
[2024-05-30 23:47:35,512][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 23:47:35,516][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 23:47:35,517][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 23:47:35,517][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 23:47:35,518][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 23:47:35,520][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 23:47:35,523][train.py][INFO] - Instantiating loggers...
[2024-05-30 23:47:35,523][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 23:47:35,525][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 23:47:35,613][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/doing_today/0.1/1/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.55it/s v_num: 0.000      
                                                              val/auc: 0.600    
                                                              val/f1: 0.600     
                                                              val/precision:    
                                                              0.600 val/recall: 
                                                              0.600 val/mre:    
                                                              0.074 train/auc:  
                                                              0.852 train/f1:   
                                                              0.848             
                                                              train/precision:  
                                                              0.869             
                                                              train/recall:     
                                                              0.828 train/mre:  
                                                              0.066             
[2024-05-30 23:49:21,557][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/doing_today/0.1/1>
[2024-05-30 23:49:21,558][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 23:49:21,561][HYDRA] 	#442 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=doing_today data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=2
[rank: 0] Seed set to 2
[2024-05-30 23:49:21,870][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 23:49:21,873][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 23:49:21,875][train.py][INFO] - Instantiating callbacks...
[2024-05-30 23:49:21,875][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 23:49:21,879][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 23:49:21,879][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 23:49:21,880][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 23:49:21,881][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 23:49:21,883][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 23:49:21,886][train.py][INFO] - Instantiating loggers...
[2024-05-30 23:49:21,886][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 23:49:21,888][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 23:49:21,975][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/doing_today/0.1/2/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.50it/s v_num: 0.000      
                                                              val/auc: 0.400    
                                                              val/f1: 0.400     
                                                              val/precision:    
                                                              0.400 val/recall: 
                                                              0.400 val/mre:    
                                                              0.076 train/auc:  
                                                              0.906 train/f1:   
                                                              0.906             
                                                              train/precision:  
                                                              0.906             
                                                              train/recall:     
                                                              0.906 train/mre:  
                                                              0.068             
[2024-05-30 23:51:08,411][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/doing_today/0.1/2>
[2024-05-30 23:51:08,411][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 23:51:08,418][HYDRA] 	#443 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=doing_today data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=3
[rank: 0] Seed set to 3
[2024-05-30 23:51:08,735][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 23:51:08,737][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 23:51:08,739][train.py][INFO] - Instantiating callbacks...
[2024-05-30 23:51:08,740][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 23:51:08,743][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 23:51:08,744][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 23:51:08,744][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 23:51:08,745][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 23:51:08,747][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 23:51:08,751][train.py][INFO] - Instantiating loggers...
[2024-05-30 23:51:08,752][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 23:51:08,754][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 23:51:08,876][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/doing_today/0.1/3/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.52it/s v_num: 0.000      
                                                              val/auc: 0.500    
                                                              val/f1: 0.375     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.300 val/mre:    
                                                              0.079 train/auc:  
                                                              0.844 train/f1:   
                                                              0.848             
                                                              train/precision:  
                                                              0.824             
                                                              train/recall:     
                                                              0.875 train/mre:  
                                                              0.071             
[2024-05-30 23:52:54,334][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/doing_today/0.1/3>
[2024-05-30 23:52:54,334][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 23:52:54,338][HYDRA] 	#444 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=doing_today data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=4
[rank: 0] Seed set to 4
[2024-05-30 23:52:54,636][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 23:52:54,638][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 23:52:54,640][train.py][INFO] - Instantiating callbacks...
[2024-05-30 23:52:54,641][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 23:52:54,644][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 23:52:54,645][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 23:52:54,645][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 23:52:54,646][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 23:52:54,648][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 23:52:54,652][train.py][INFO] - Instantiating loggers...
[2024-05-30 23:52:54,652][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 23:52:54,654][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 23:52:54,746][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/doing_today/0.1/4/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.48it/s v_num: 0.000      
                                                              val/auc: 0.400    
                                                              val/f1: 0.333     
                                                              val/precision:    
                                                              0.375 val/recall: 
                                                              0.300 val/mre:    
                                                              0.073 train/auc:  
                                                              0.883 train/f1:   
                                                              0.885             
                                                              train/precision:  
                                                              0.866             
                                                              train/recall:     
                                                              0.906 train/mre:  
                                                              0.064             
[2024-05-30 23:54:40,152][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/doing_today/0.1/4>
[2024-05-30 23:54:40,153][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 23:54:40,156][HYDRA] 	#445 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=doing_today data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=5
[rank: 0] Seed set to 5
[2024-05-30 23:54:40,456][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 23:54:40,458][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 23:54:40,460][train.py][INFO] - Instantiating callbacks...
[2024-05-30 23:54:40,461][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 23:54:40,464][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 23:54:40,465][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 23:54:40,466][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 23:54:40,466][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 23:54:40,468][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 23:54:40,471][train.py][INFO] - Instantiating loggers...
[2024-05-30 23:54:40,471][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 23:54:40,473][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 23:54:40,562][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/doing_today/0.1/5/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.57it/s v_num: 0.000      
                                                              val/auc: 0.350    
                                                              val/f1: 0.316     
                                                              val/precision:    
                                                              0.333 val/recall: 
                                                              0.300 val/mre:    
                                                              0.081 train/auc:  
                                                              0.875 train/f1:   
                                                              0.873             
                                                              train/precision:  
                                                              0.887             
                                                              train/recall:     
                                                              0.859 train/mre:  
                                                              0.073             
[2024-05-30 23:56:24,647][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/doing_today/0.1/5>
[2024-05-30 23:56:24,647][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 23:56:24,650][HYDRA] 	#446 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=doing_today data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=6
[rank: 0] Seed set to 6
[2024-05-30 23:56:24,949][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 23:56:24,952][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 23:56:24,954][train.py][INFO] - Instantiating callbacks...
[2024-05-30 23:56:24,955][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 23:56:24,959][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 23:56:24,960][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 23:56:24,960][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 23:56:24,961][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 23:56:24,963][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 23:56:24,966][train.py][INFO] - Instantiating loggers...
[2024-05-30 23:56:24,966][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 23:56:24,969][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 23:56:25,053][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/doing_today/0.1/6/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.55it/s v_num: 0.000      
                                                              val/auc: 0.650    
                                                              val/f1: 0.632     
                                                              val/precision:    
                                                              0.667 val/recall: 
                                                              0.600 val/mre:    
                                                              0.076 train/auc:  
                                                              0.953 train/f1:   
                                                              0.952             
                                                              train/precision:  
                                                              0.983             
                                                              train/recall:     
                                                              0.922 train/mre:  
                                                              0.069             
[2024-05-30 23:58:10,676][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/doing_today/0.1/6>
[2024-05-30 23:58:10,677][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 23:58:10,681][HYDRA] 	#447 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=doing_today data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=7
[rank: 0] Seed set to 7
[2024-05-30 23:58:10,982][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 23:58:10,985][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 23:58:10,988][train.py][INFO] - Instantiating callbacks...
[2024-05-30 23:58:10,988][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 23:58:10,992][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 23:58:10,993][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 23:58:10,994][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 23:58:10,995][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 23:58:10,996][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 23:58:10,999][train.py][INFO] - Instantiating loggers...
[2024-05-30 23:58:11,000][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 23:58:11,002][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 23:58:11,102][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/doing_today/0.1/7/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.53it/s v_num: 0.000      
                                                              val/auc: 0.550    
                                                              val/f1: 0.400     
                                                              val/precision:    
                                                              0.600 val/recall: 
                                                              0.300 val/mre:    
                                                              0.082 train/auc:  
                                                              0.875 train/f1:   
                                                              0.867             
                                                              train/precision:  
                                                              0.929             
                                                              train/recall:     
                                                              0.812 train/mre:  
                                                              0.075             
[2024-05-30 23:59:55,143][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/doing_today/0.1/7>
[2024-05-30 23:59:55,144][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 23:59:55,147][HYDRA] 	#448 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=doing_today data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=8
[rank: 0] Seed set to 8
[2024-05-30 23:59:55,446][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 23:59:55,449][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-30 23:59:55,452][train.py][INFO] - Instantiating callbacks...
[2024-05-30 23:59:55,452][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 23:59:55,456][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 23:59:55,457][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 23:59:55,458][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 23:59:55,458][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 23:59:55,460][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 23:59:55,463][train.py][INFO] - Instantiating loggers...
[2024-05-30 23:59:55,464][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 23:59:55,466][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 23:59:55,582][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/doing_today/0.1/8/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.56it/s v_num: 0.000      
                                                              val/auc: 0.550    
                                                              val/f1: 0.471     
                                                              val/precision:    
                                                              0.571 val/recall: 
                                                              0.400 val/mre:    
                                                              0.083 train/auc:  
                                                              0.938 train/f1:   
                                                              0.938             
                                                              train/precision:  
                                                              0.938             
                                                              train/recall:     
                                                              0.938 train/mre:  
                                                              0.075             
[2024-05-31 00:01:44,090][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/doing_today/0.1/8>
[2024-05-31 00:01:44,090][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-31 00:01:44,097][HYDRA] 	#449 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=doing_today data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=9
[rank: 0] Seed set to 9
[2024-05-31 00:01:44,771][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-31 00:01:44,775][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-31 00:01:44,778][train.py][INFO] - Instantiating callbacks...
[2024-05-31 00:01:44,779][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-31 00:01:44,784][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-31 00:01:44,784][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-31 00:01:44,785][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-31 00:01:44,786][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-31 00:01:44,788][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-31 00:01:44,805][train.py][INFO] - Instantiating loggers...
[2024-05-31 00:01:44,806][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-31 00:01:44,810][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-31 00:01:45,293][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/doing_today/0.1/9/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.59it/s v_num: 0.000      
                                                              val/auc: 0.450    
                                                              val/f1: 0.267     
                                                              val/precision:    
                                                              0.400 val/recall: 
                                                              0.200 val/mre:    
                                                              0.075 train/auc:  
                                                              0.797 train/f1:   
                                                              0.806             
                                                              train/precision:  
                                                              0.771             
                                                              train/recall:     
                                                              0.844 train/mre:  
                                                              0.070             
[2024-05-31 00:03:40,862][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/doing_today/0.1/9>
[2024-05-31 00:03:40,865][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-31 00:03:40,888][HYDRA] 	#450 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=doing_today data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=0
[2024-05-31 00:03:41,519][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-31 00:03:41,522][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-31 00:03:41,524][train.py][INFO] - Instantiating callbacks...
[2024-05-31 00:03:41,524][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-31 00:03:41,529][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-31 00:03:41,530][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-31 00:03:41,531][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-31 00:03:41,532][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-31 00:03:41,534][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-31 00:03:41,560][train.py][INFO] - Instantiating loggers...
[2024-05-31 00:03:41,560][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-31 00:03:41,565][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-31 00:03:42,168][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/doing_today/0.15/0/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.58it/s v_num: 0.000      
                                                              val/auc: 0.600    
                                                              val/f1: 0.429     
                                                              val/precision:    
                                                              0.750 val/recall: 
                                                              0.300 val/mre:    
                                                              0.079 train/auc:  
                                                              0.914 train/f1:   
                                                              0.916             
                                                              train/precision:  
                                                              0.896             
                                                              train/recall:     
                                                              0.938 train/mre:  
                                                              0.069             
[2024-05-31 00:05:35,018][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/doing_today/0.15/0>
[2024-05-31 00:05:35,019][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-31 00:05:35,033][HYDRA] 	#451 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=doing_today data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=1
[rank: 0] Seed set to 1
[2024-05-31 00:05:35,487][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-31 00:05:35,492][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-31 00:05:35,496][train.py][INFO] - Instantiating callbacks...
[2024-05-31 00:05:35,497][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-31 00:05:35,501][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-31 00:05:35,501][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-31 00:05:35,502][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-31 00:05:35,503][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-31 00:05:35,505][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-31 00:05:35,526][train.py][INFO] - Instantiating loggers...
[2024-05-31 00:05:35,526][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-31 00:05:35,531][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-31 00:05:35,873][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/doing_today/0.15/1/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.58it/s v_num: 0.000      
                                                              val/auc: 0.650    
                                                              val/f1: 0.588     
                                                              val/precision:    
                                                              0.714 val/recall: 
                                                              0.500 val/mre:    
                                                              0.079 train/auc:  
                                                              0.891 train/f1:   
                                                              0.879             
                                                              train/precision:  
                                                              0.981             
                                                              train/recall:     
                                                              0.797 train/mre:  
                                                              0.071             
[2024-05-31 00:07:24,893][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/doing_today/0.15/1>
[2024-05-31 00:07:24,894][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-31 00:07:24,899][HYDRA] 	#452 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=doing_today data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=2
[rank: 0] Seed set to 2
[2024-05-31 00:07:25,250][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-31 00:07:25,254][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-31 00:07:25,257][train.py][INFO] - Instantiating callbacks...
[2024-05-31 00:07:25,258][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-31 00:07:25,261][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-31 00:07:25,262][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-31 00:07:25,263][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-31 00:07:25,264][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-31 00:07:25,265][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-31 00:07:25,270][train.py][INFO] - Instantiating loggers...
[2024-05-31 00:07:25,270][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-31 00:07:25,272][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-31 00:07:25,405][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/doing_today/0.15/2/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.51it/s v_num: 0.000      
                                                              val/auc: 0.600    
                                                              val/f1: 0.556     
                                                              val/precision:    
                                                              0.625 val/recall: 
                                                              0.500 val/mre:    
                                                              0.078 train/auc:  
                                                              0.898 train/f1:   
                                                              0.896             
                                                              train/precision:  
                                                              0.918             
                                                              train/recall:     
                                                              0.875 train/mre:  
                                                              0.070             
[2024-05-31 00:09:13,622][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/doing_today/0.15/2>
[2024-05-31 00:09:13,623][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-31 00:09:13,627][HYDRA] 	#453 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=doing_today data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=3
[rank: 0] Seed set to 3
[2024-05-31 00:09:14,091][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-31 00:09:14,096][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-31 00:09:14,100][train.py][INFO] - Instantiating callbacks...
[2024-05-31 00:09:14,101][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-31 00:09:14,111][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-31 00:09:14,112][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-31 00:09:14,113][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-31 00:09:14,114][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-31 00:09:14,117][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-31 00:09:14,123][train.py][INFO] - Instantiating loggers...
[2024-05-31 00:09:14,123][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-31 00:09:14,125][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-31 00:09:14,485][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/doing_today/0.15/3/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.59it/s v_num: 0.000      
                                                              val/auc: 0.550    
                                                              val/f1: 0.526     
                                                              val/precision:    
                                                              0.556 val/recall: 
                                                              0.500 val/mre:    
                                                              0.078 train/auc:  
                                                              0.898 train/f1:   
                                                              0.905             
                                                              train/precision:  
                                                              0.849             
                                                              train/recall:     
                                                              0.969 train/mre:  
                                                              0.071             
[2024-05-31 00:11:01,556][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/doing_today/0.15/3>
[2024-05-31 00:11:01,557][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-31 00:11:01,562][HYDRA] 	#454 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=doing_today data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=4
[rank: 0] Seed set to 4
[2024-05-31 00:11:01,936][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-31 00:11:01,940][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-31 00:11:01,943][train.py][INFO] - Instantiating callbacks...
[2024-05-31 00:11:01,944][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-31 00:11:01,949][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-31 00:11:01,950][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-31 00:11:01,950][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-31 00:11:01,951][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-31 00:11:01,953][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-31 00:11:01,966][train.py][INFO] - Instantiating loggers...
[2024-05-31 00:11:01,966][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-31 00:11:01,970][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-31 00:11:02,165][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/doing_today/0.15/4/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.57it/s v_num: 0.000      
                                                              val/auc: 0.350    
                                                              val/f1: 0.316     
                                                              val/precision:    
                                                              0.333 val/recall: 
                                                              0.300 val/mre:    
                                                              0.078 train/auc:  
                                                              0.930 train/f1:   
                                                              0.930             
                                                              train/precision:  
                                                              0.923             
                                                              train/recall:     
                                                              0.938 train/mre:  
                                                              0.069             
[2024-05-31 00:12:47,421][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/doing_today/0.15/4>
[2024-05-31 00:12:47,422][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-31 00:12:47,425][HYDRA] 	#455 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=doing_today data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=5
[rank: 0] Seed set to 5
[2024-05-31 00:12:47,722][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-31 00:12:47,724][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-31 00:12:47,726][train.py][INFO] - Instantiating callbacks...
[2024-05-31 00:12:47,727][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-31 00:12:47,730][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-31 00:12:47,731][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-31 00:12:47,732][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-31 00:12:47,732][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-31 00:12:47,734][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-31 00:12:47,745][train.py][INFO] - Instantiating loggers...
[2024-05-31 00:12:47,746][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-31 00:12:47,750][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-31 00:12:47,835][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/doing_today/0.15/5/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.56it/s v_num: 0.000      
                                                              val/auc: 0.500    
                                                              val/f1: 0.500     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.500 val/mre:    
                                                              0.090 train/auc:  
                                                              0.938 train/f1:   
                                                              0.937             
                                                              train/precision:  
                                                              0.952             
                                                              train/recall:     
                                                              0.922 train/mre:  
                                                              0.081             
[2024-05-31 00:14:32,750][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/doing_today/0.15/5>
[2024-05-31 00:14:32,751][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-31 00:14:32,753][HYDRA] 	#456 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=doing_today data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=6
[rank: 0] Seed set to 6
[2024-05-31 00:14:33,053][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-31 00:14:33,055][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-31 00:14:33,058][train.py][INFO] - Instantiating callbacks...
[2024-05-31 00:14:33,058][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-31 00:14:33,061][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-31 00:14:33,062][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-31 00:14:33,063][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-31 00:14:33,064][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-31 00:14:33,065][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-31 00:14:33,068][train.py][INFO] - Instantiating loggers...
[2024-05-31 00:14:33,069][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-31 00:14:33,071][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-31 00:14:33,159][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/doing_today/0.15/6/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.58it/s v_num: 0.000      
                                                              val/auc: 0.650    
                                                              val/f1: 0.533     
                                                              val/precision:    
                                                              0.800 val/recall: 
                                                              0.400 val/mre:    
                                                              0.079 train/auc:  
                                                              0.930 train/f1:   
                                                              0.930             
                                                              train/precision:  
                                                              0.923             
                                                              train/recall:     
                                                              0.938 train/mre:  
                                                              0.070             
[2024-05-31 00:16:19,497][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/doing_today/0.15/6>
[2024-05-31 00:16:19,497][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-31 00:16:19,501][HYDRA] 	#457 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=doing_today data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=7
[rank: 0] Seed set to 7
[2024-05-31 00:16:19,826][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-31 00:16:19,829][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-31 00:16:19,832][train.py][INFO] - Instantiating callbacks...
[2024-05-31 00:16:19,832][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-31 00:16:19,836][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-31 00:16:19,837][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-31 00:16:19,838][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-31 00:16:19,838][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-31 00:16:19,840][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-31 00:16:19,843][train.py][INFO] - Instantiating loggers...
[2024-05-31 00:16:19,843][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-31 00:16:19,846][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-31 00:16:19,950][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/doing_today/0.15/7/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.52it/s v_num: 0.000      
                                                              val/auc: 0.600    
                                                              val/f1: 0.429     
                                                              val/precision:    
                                                              0.750 val/recall: 
                                                              0.300 val/mre:    
                                                              0.086 train/auc:  
                                                              0.875 train/f1:   
                                                              0.860             
                                                              train/precision:  
                                                              0.980             
                                                              train/recall:     
                                                              0.766 train/mre:  
                                                              0.077             
[2024-05-31 00:18:04,582][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/doing_today/0.15/7>
[2024-05-31 00:18:04,583][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-31 00:18:04,586][HYDRA] 	#458 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=doing_today data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=8
[rank: 0] Seed set to 8
[2024-05-31 00:18:04,887][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-31 00:18:04,890][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-31 00:18:04,892][train.py][INFO] - Instantiating callbacks...
[2024-05-31 00:18:04,892][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-31 00:18:04,895][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-31 00:18:04,896][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-31 00:18:04,897][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-31 00:18:04,898][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-31 00:18:04,900][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-31 00:18:04,913][train.py][INFO] - Instantiating loggers...
[2024-05-31 00:18:04,913][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-31 00:18:04,918][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-31 00:18:05,006][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/doing_today/0.15/8/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.56it/s v_num: 0.000      
                                                              val/auc: 0.550    
                                                              val/f1: 0.400     
                                                              val/precision:    
                                                              0.600 val/recall: 
                                                              0.300 val/mre:    
                                                              0.084 train/auc:  
                                                              0.859 train/f1:   
                                                              0.859             
                                                              train/precision:  
                                                              0.859             
                                                              train/recall:     
                                                              0.859 train/mre:  
                                                              0.074             
[2024-05-31 00:19:49,083][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/doing_today/0.15/8>
[2024-05-31 00:19:49,084][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-31 00:19:49,087][HYDRA] 	#459 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=doing_today data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=9
[rank: 0] Seed set to 9
[2024-05-31 00:19:49,385][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-31 00:19:49,388][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-31 00:19:49,390][train.py][INFO] - Instantiating callbacks...
[2024-05-31 00:19:49,391][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-31 00:19:49,394][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-31 00:19:49,395][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-31 00:19:49,396][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-31 00:19:49,397][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-31 00:19:49,398][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-31 00:19:49,402][train.py][INFO] - Instantiating loggers...
[2024-05-31 00:19:49,402][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-31 00:19:49,404][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-31 00:19:49,492][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/doing_today/0.15/9/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.58it/s v_num: 0.000      
                                                              val/auc: 0.650    
                                                              val/f1: 0.588     
                                                              val/precision:    
                                                              0.714 val/recall: 
                                                              0.500 val/mre:    
                                                              0.077 train/auc:  
                                                              0.875 train/f1:   
                                                              0.881             
                                                              train/precision:  
                                                              0.843             
                                                              train/recall:     
                                                              0.922 train/mre:  
                                                              0.070             
[2024-05-31 00:21:34,742][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/doing_today/0.15/9>
[2024-05-31 00:21:34,742][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-31 00:21:34,746][HYDRA] 	#460 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=doing_today data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=0
[2024-05-31 00:21:35,043][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-31 00:21:35,045][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-31 00:21:35,047][train.py][INFO] - Instantiating callbacks...
[2024-05-31 00:21:35,048][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-31 00:21:35,051][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-31 00:21:35,052][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-31 00:21:35,052][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-31 00:21:35,053][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-31 00:21:35,055][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-31 00:21:35,059][train.py][INFO] - Instantiating loggers...
[2024-05-31 00:21:35,059][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-31 00:21:35,061][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-31 00:21:35,148][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/doing_today/0.2/0/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.54it/s v_num: 0.000      
                                                              val/auc: 0.600    
                                                              val/f1: 0.600     
                                                              val/precision:    
                                                              0.600 val/recall: 
                                                              0.600 val/mre:    
                                                              0.079 train/auc:  
                                                              0.906 train/f1:   
                                                              0.909             
                                                              train/precision:  
                                                              0.882             
                                                              train/recall:     
                                                              0.938 train/mre:  
                                                              0.069             
[2024-05-31 00:23:20,320][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/doing_today/0.2/0>
[2024-05-31 00:23:20,321][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-31 00:23:20,324][HYDRA] 	#461 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=doing_today data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=1
[rank: 0] Seed set to 1
[2024-05-31 00:23:20,625][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-31 00:23:20,627][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-31 00:23:20,629][train.py][INFO] - Instantiating callbacks...
[2024-05-31 00:23:20,629][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-31 00:23:20,633][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-31 00:23:20,633][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-31 00:23:20,634][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-31 00:23:20,635][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-31 00:23:20,637][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-31 00:23:20,640][train.py][INFO] - Instantiating loggers...
[2024-05-31 00:23:20,640][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-31 00:23:20,643][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-31 00:23:20,726][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/doing_today/0.2/1/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.51it/s v_num: 0.000      
                                                              val/auc: 0.600    
                                                              val/f1: 0.429     
                                                              val/precision:    
                                                              0.750 val/recall: 
                                                              0.300 val/mre:    
                                                              0.080 train/auc:  
                                                              0.930 train/f1:   
                                                              0.928             
                                                              train/precision:  
                                                              0.951             
                                                              train/recall:     
                                                              0.906 train/mre:  
                                                              0.072             
[2024-05-31 00:25:07,234][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/doing_today/0.2/1>
[2024-05-31 00:25:07,234][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-31 00:25:07,238][HYDRA] 	#462 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=doing_today data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=2
[rank: 0] Seed set to 2
[2024-05-31 00:25:07,535][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-31 00:25:07,538][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-31 00:25:07,540][train.py][INFO] - Instantiating callbacks...
[2024-05-31 00:25:07,540][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-31 00:25:07,544][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-31 00:25:07,544][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-31 00:25:07,545][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-31 00:25:07,546][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-31 00:25:07,548][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-31 00:25:07,565][train.py][INFO] - Instantiating loggers...
[2024-05-31 00:25:07,566][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-31 00:25:07,570][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-31 00:25:07,668][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/doing_today/0.2/2/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.54it/s v_num: 0.000      
                                                              val/auc: 0.400    
                                                              val/f1: 0.400     
                                                              val/precision:    
                                                              0.400 val/recall: 
                                                              0.400 val/mre:    
                                                              0.086 train/auc:  
                                                              0.867 train/f1:   
                                                              0.864             
                                                              train/precision:  
                                                              0.885             
                                                              train/recall:     
                                                              0.844 train/mre:  
                                                              0.078             
[2024-05-31 00:26:54,452][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/doing_today/0.2/2>
[2024-05-31 00:26:54,452][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-31 00:26:54,455][HYDRA] 	#463 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=doing_today data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=3
[rank: 0] Seed set to 3
[2024-05-31 00:26:54,760][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-31 00:26:54,762][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-31 00:26:54,764][train.py][INFO] - Instantiating callbacks...
[2024-05-31 00:26:54,764][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-31 00:26:54,768][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-31 00:26:54,768][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-31 00:26:54,769][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-31 00:26:54,770][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-31 00:26:54,772][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-31 00:26:54,775][train.py][INFO] - Instantiating loggers...
[2024-05-31 00:26:54,775][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-31 00:26:54,778][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-31 00:26:54,866][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/doing_today/0.2/3/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.51it/s v_num: 0.000      
                                                              val/auc: 0.500    
                                                              val/f1: 0.375     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.300 val/mre:    
                                                              0.082 train/auc:  
                                                              0.914 train/f1:   
                                                              0.917             
                                                              train/precision:  
                                                              0.884             
                                                              train/recall:     
                                                              0.953 train/mre:  
                                                              0.073             
[2024-05-31 00:28:39,889][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/doing_today/0.2/3>
[2024-05-31 00:28:39,889][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-31 00:28:39,893][HYDRA] 	#464 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=doing_today data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=4
[rank: 0] Seed set to 4
[2024-05-31 00:28:40,195][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-31 00:28:40,198][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-31 00:28:40,200][train.py][INFO] - Instantiating callbacks...
[2024-05-31 00:28:40,200][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-31 00:28:40,203][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-31 00:28:40,204][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-31 00:28:40,205][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-31 00:28:40,206][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-31 00:28:40,207][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-31 00:28:40,210][train.py][INFO] - Instantiating loggers...
[2024-05-31 00:28:40,211][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-31 00:28:40,213][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-31 00:28:40,298][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/doing_today/0.2/4/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.54it/s v_num: 0.000      
                                                              val/auc: 0.400    
                                                              val/f1: 0.333     
                                                              val/precision:    
                                                              0.375 val/recall: 
                                                              0.300 val/mre:    
                                                              0.082 train/auc:  
                                                              0.898 train/f1:   
                                                              0.898             
                                                              train/precision:  
                                                              0.905             
                                                              train/recall:     
                                                              0.891 train/mre:  
                                                              0.073             
[2024-05-31 00:30:24,663][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/doing_today/0.2/4>
[2024-05-31 00:30:24,664][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-31 00:30:24,668][HYDRA] 	#465 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=doing_today data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=5
[rank: 0] Seed set to 5
[2024-05-31 00:30:24,993][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-31 00:30:24,996][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-31 00:30:24,999][train.py][INFO] - Instantiating callbacks...
[2024-05-31 00:30:24,999][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-31 00:30:25,003][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-31 00:30:25,004][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-31 00:30:25,005][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-31 00:30:25,006][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-31 00:30:25,007][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-31 00:30:25,010][train.py][INFO] - Instantiating loggers...
[2024-05-31 00:30:25,011][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-31 00:30:25,013][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-31 00:30:25,099][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/doing_today/0.2/5/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.55it/s v_num: 0.000      
                                                              val/auc: 0.600    
                                                              val/f1: 0.500     
                                                              val/precision:    
                                                              0.667 val/recall: 
                                                              0.400 val/mre:    
                                                              0.090 train/auc:  
                                                              0.922 train/f1:   
                                                              0.919             
                                                              train/precision:  
                                                              0.950             
                                                              train/recall:     
                                                              0.891 train/mre:  
                                                              0.083             
[2024-05-31 00:32:09,095][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/doing_today/0.2/5>
[2024-05-31 00:32:09,095][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-31 00:32:09,099][HYDRA] 	#466 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=doing_today data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=6
[rank: 0] Seed set to 6
[2024-05-31 00:32:09,395][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-31 00:32:09,397][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-31 00:32:09,399][train.py][INFO] - Instantiating callbacks...
[2024-05-31 00:32:09,400][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-31 00:32:09,403][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-31 00:32:09,404][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-31 00:32:09,405][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-31 00:32:09,405][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-31 00:32:09,407][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-31 00:32:09,410][train.py][INFO] - Instantiating loggers...
[2024-05-31 00:32:09,410][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-31 00:32:09,413][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-31 00:32:09,496][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/doing_today/0.2/6/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.52it/s v_num: 0.000      
                                                              val/auc: 0.650    
                                                              val/f1: 0.632     
                                                              val/precision:    
                                                              0.667 val/recall: 
                                                              0.600 val/mre:    
                                                              0.084 train/auc:  
                                                              0.953 train/f1:   
                                                              0.955             
                                                              train/precision:  
                                                              0.926             
                                                              train/recall:     
                                                              0.984 train/mre:  
                                                              0.075             
[2024-05-31 00:33:55,325][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/doing_today/0.2/6>
[2024-05-31 00:33:55,326][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-31 00:33:55,329][HYDRA] 	#467 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=doing_today data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=7
[rank: 0] Seed set to 7
[2024-05-31 00:33:55,622][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-31 00:33:55,625][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-31 00:33:55,627][train.py][INFO] - Instantiating callbacks...
[2024-05-31 00:33:55,627][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-31 00:33:55,631][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-31 00:33:55,631][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-31 00:33:55,632][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-31 00:33:55,633][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-31 00:33:55,635][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-31 00:33:55,638][train.py][INFO] - Instantiating loggers...
[2024-05-31 00:33:55,638][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-31 00:33:55,640][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-31 00:33:55,724][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/doing_today/0.2/7/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.55it/s v_num: 0.000      
                                                              val/auc: 0.600    
                                                              val/f1: 0.429     
                                                              val/precision:    
                                                              0.750 val/recall: 
                                                              0.300 val/mre:    
                                                              0.089 train/auc:  
                                                              0.852 train/f1:   
                                                              0.835             
                                                              train/precision:  
                                                              0.941             
                                                              train/recall:     
                                                              0.750 train/mre:  
                                                              0.079             
[2024-05-31 00:35:40,870][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/doing_today/0.2/7>
[2024-05-31 00:35:40,871][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-31 00:35:40,875][HYDRA] 	#468 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=doing_today data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=8
[rank: 0] Seed set to 8
[2024-05-31 00:35:41,176][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-31 00:35:41,179][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-31 00:35:41,181][train.py][INFO] - Instantiating callbacks...
[2024-05-31 00:35:41,181][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-31 00:35:41,185][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-31 00:35:41,185][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-31 00:35:41,186][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-31 00:35:41,187][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-31 00:35:41,189][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-31 00:35:41,192][train.py][INFO] - Instantiating loggers...
[2024-05-31 00:35:41,192][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-31 00:35:41,194][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-31 00:35:41,282][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/doing_today/0.2/8/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.57it/s v_num: 0.000      
                                                              val/auc: 0.400    
                                                              val/f1: 0.333     
                                                              val/precision:    
                                                              0.375 val/recall: 
                                                              0.300 val/mre:    
                                                              0.084 train/auc:  
                                                              0.883 train/f1:   
                                                              0.887             
                                                              train/precision:  
                                                              0.855             
                                                              train/recall:     
                                                              0.922 train/mre:  
                                                              0.076             
[2024-05-31 00:37:25,452][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/doing_today/0.2/8>
[2024-05-31 00:37:25,453][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-31 00:37:25,458][HYDRA] 	#469 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=doing_today data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=9
[rank: 0] Seed set to 9
[2024-05-31 00:37:25,769][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-31 00:37:25,772][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-31 00:37:25,774][train.py][INFO] - Instantiating callbacks...
[2024-05-31 00:37:25,774][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-31 00:37:25,778][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-31 00:37:25,778][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-31 00:37:25,779][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-31 00:37:25,780][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-31 00:37:25,782][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-31 00:37:25,785][train.py][INFO] - Instantiating loggers...
[2024-05-31 00:37:25,785][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-31 00:37:25,787][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-31 00:37:25,872][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/doing_today/0.2/9/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.59it/s v_num: 0.000      
                                                              val/auc: 0.550    
                                                              val/f1: 0.400     
                                                              val/precision:    
                                                              0.600 val/recall: 
                                                              0.300 val/mre:    
                                                              0.084 train/auc:  
                                                              0.875 train/f1:   
                                                              0.882             
                                                              train/precision:  
                                                              0.833             
                                                              train/recall:     
                                                              0.938 train/mre:  
                                                              0.077             
[2024-05-31 00:39:10,863][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/doing_today/0.2/9>
[2024-05-31 00:39:10,863][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-31 00:39:10,867][HYDRA] 	#470 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=doing_today data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=0
[2024-05-31 00:39:11,162][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-31 00:39:11,165][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-31 00:39:11,167][train.py][INFO] - Instantiating callbacks...
[2024-05-31 00:39:11,167][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-31 00:39:11,170][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-31 00:39:11,171][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-31 00:39:11,172][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-31 00:39:11,173][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-31 00:39:11,175][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-31 00:39:11,179][train.py][INFO] - Instantiating loggers...
[2024-05-31 00:39:11,179][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-31 00:39:11,181][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-31 00:39:11,271][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/doing_today/0.25/0/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.54it/s v_num: 0.000      
                                                              val/auc: 0.700    
                                                              val/f1: 0.667     
                                                              val/precision:    
                                                              0.750 val/recall: 
                                                              0.600 val/mre:    
                                                              0.084 train/auc:  
                                                              0.898 train/f1:   
                                                              0.902             
                                                              train/precision:  
                                                              0.870             
                                                              train/recall:     
                                                              0.938 train/mre:  
                                                              0.073             
[2024-05-31 00:40:57,467][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/doing_today/0.25/0>
[2024-05-31 00:40:57,467][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-31 00:40:57,470][HYDRA] 	#471 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=doing_today data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=1
[rank: 0] Seed set to 1
[2024-05-31 00:40:57,776][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-31 00:40:57,779][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-31 00:40:57,781][train.py][INFO] - Instantiating callbacks...
[2024-05-31 00:40:57,782][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-31 00:40:57,785][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-31 00:40:57,786][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-31 00:40:57,787][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-31 00:40:57,788][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-31 00:40:57,789][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-31 00:40:57,793][train.py][INFO] - Instantiating loggers...
[2024-05-31 00:40:57,793][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-31 00:40:57,795][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-31 00:40:57,900][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/doing_today/0.25/1/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.54it/s v_num: 0.000      
                                                              val/auc: 0.750    
                                                              val/f1: 0.737     
                                                              val/precision:    
                                                              0.778 val/recall: 
                                                              0.700 val/mre:    
                                                              0.083 train/auc:  
                                                              0.906 train/f1:   
                                                              0.906             
                                                              train/precision:  
                                                              0.906             
                                                              train/recall:     
                                                              0.906 train/mre:  
                                                              0.074             
[2024-05-31 00:42:45,706][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/doing_today/0.25/1>
[2024-05-31 00:42:45,707][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-31 00:42:45,710][HYDRA] 	#472 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=doing_today data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=2
[rank: 0] Seed set to 2
[2024-05-31 00:42:46,009][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-31 00:42:46,011][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-31 00:42:46,013][train.py][INFO] - Instantiating callbacks...
[2024-05-31 00:42:46,014][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-31 00:42:46,017][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-31 00:42:46,018][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-31 00:42:46,018][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-31 00:42:46,019][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-31 00:42:46,021][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-31 00:42:46,024][train.py][INFO] - Instantiating loggers...
[2024-05-31 00:42:46,025][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-31 00:42:46,027][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-31 00:42:46,111][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/doing_today/0.25/2/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.57it/s v_num: 0.000      
                                                              val/auc: 0.450    
                                                              val/f1: 0.421     
                                                              val/precision:    
                                                              0.444 val/recall: 
                                                              0.400 val/mre:    
                                                              0.090 train/auc:  
                                                              0.867 train/f1:   
                                                              0.862             
                                                              train/precision:  
                                                              0.898             
                                                              train/recall:     
                                                              0.828 train/mre:  
                                                              0.081             
[2024-05-31 00:44:32,213][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/doing_today/0.25/2>
[2024-05-31 00:44:32,214][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-31 00:44:32,217][HYDRA] 	#473 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=doing_today data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=3
[rank: 0] Seed set to 3
[2024-05-31 00:44:32,517][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-31 00:44:32,520][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-31 00:44:32,523][train.py][INFO] - Instantiating callbacks...
[2024-05-31 00:44:32,523][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-31 00:44:32,527][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-31 00:44:32,528][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-31 00:44:32,529][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-31 00:44:32,529][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-31 00:44:32,531][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-31 00:44:32,558][train.py][INFO] - Instantiating loggers...
[2024-05-31 00:44:32,559][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-31 00:44:32,563][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-31 00:44:32,655][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/doing_today/0.25/3/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.56it/s v_num: 0.000      
                                                              val/auc: 0.550    
                                                              val/f1: 0.471     
                                                              val/precision:    
                                                              0.571 val/recall: 
                                                              0.400 val/mre:    
                                                              0.084 train/auc:  
                                                              0.914 train/f1:   
                                                              0.915             
                                                              train/precision:  
                                                              0.908             
                                                              train/recall:     
                                                              0.922 train/mre:  
                                                              0.075             
[2024-05-31 00:46:17,128][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/doing_today/0.25/3>
[2024-05-31 00:46:17,129][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-31 00:46:17,132][HYDRA] 	#474 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=doing_today data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=4
[rank: 0] Seed set to 4
[2024-05-31 00:46:17,436][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-31 00:46:17,438][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-31 00:46:17,440][train.py][INFO] - Instantiating callbacks...
[2024-05-31 00:46:17,441][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-31 00:46:17,444][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-31 00:46:17,445][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-31 00:46:17,446][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-31 00:46:17,447][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-31 00:46:17,448][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-31 00:46:17,452][train.py][INFO] - Instantiating loggers...
[2024-05-31 00:46:17,452][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-31 00:46:17,454][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-31 00:46:17,553][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/doing_today/0.25/4/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.58it/s v_num: 0.000      
                                                              val/auc: 0.500    
                                                              val/f1: 0.444     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.400 val/mre:    
                                                              0.086 train/auc:  
                                                              0.898 train/f1:   
                                                              0.896             
                                                              train/precision:  
                                                              0.918             
                                                              train/recall:     
                                                              0.875 train/mre:  
                                                              0.078             
[2024-05-31 00:48:03,998][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/doing_today/0.25/4>
[2024-05-31 00:48:04,000][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-31 00:48:04,018][HYDRA] 	#475 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=doing_today data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=5
[rank: 0] Seed set to 5
[2024-05-31 00:48:04,512][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-31 00:48:04,514][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-31 00:48:04,516][train.py][INFO] - Instantiating callbacks...
[2024-05-31 00:48:04,517][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-31 00:48:04,520][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-31 00:48:04,521][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-31 00:48:04,521][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-31 00:48:04,522][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-31 00:48:04,524][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-31 00:48:04,528][train.py][INFO] - Instantiating loggers...
[2024-05-31 00:48:04,528][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-31 00:48:04,530][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-31 00:48:04,616][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/doing_today/0.25/5/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.54it/s v_num: 0.000      
                                                              val/auc: 0.400    
                                                              val/f1: 0.333     
                                                              val/precision:    
                                                              0.375 val/recall: 
                                                              0.300 val/mre:    
                                                              0.088 train/auc:  
                                                              0.898 train/f1:   
                                                              0.898             
                                                              train/precision:  
                                                              0.905             
                                                              train/recall:     
                                                              0.891 train/mre:  
                                                              0.082             
[2024-05-31 00:49:49,491][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/doing_today/0.25/5>
[2024-05-31 00:49:49,492][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-31 00:49:49,496][HYDRA] 	#476 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=doing_today data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=6
[rank: 0] Seed set to 6
[2024-05-31 00:49:49,796][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-31 00:49:49,798][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-31 00:49:49,800][train.py][INFO] - Instantiating callbacks...
[2024-05-31 00:49:49,801][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-31 00:49:49,804][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-31 00:49:49,805][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-31 00:49:49,806][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-31 00:49:49,807][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-31 00:49:49,808][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-31 00:49:49,812][train.py][INFO] - Instantiating loggers...
[2024-05-31 00:49:49,812][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-31 00:49:49,815][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-31 00:49:49,900][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/doing_today/0.25/6/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.54it/s v_num: 0.000      
                                                              val/auc: 0.650    
                                                              val/f1: 0.588     
                                                              val/precision:    
                                                              0.714 val/recall: 
                                                              0.500 val/mre:    
                                                              0.093 train/auc:  
                                                              0.945 train/f1:   
                                                              0.947             
                                                              train/precision:  
                                                              0.925             
                                                              train/recall:     
                                                              0.969 train/mre:  
                                                              0.082             
[2024-05-31 00:51:36,982][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/doing_today/0.25/6>
[2024-05-31 00:51:36,983][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-31 00:51:36,986][HYDRA] 	#477 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=doing_today data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=7
[rank: 0] Seed set to 7
[2024-05-31 00:51:37,285][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-31 00:51:37,288][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-31 00:51:37,291][train.py][INFO] - Instantiating callbacks...
[2024-05-31 00:51:37,291][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-31 00:51:37,295][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-31 00:51:37,296][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-31 00:51:37,297][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-31 00:51:37,297][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-31 00:51:37,299][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-31 00:51:37,302][train.py][INFO] - Instantiating loggers...
[2024-05-31 00:51:37,302][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-31 00:51:37,305][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-31 00:51:37,388][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/doing_today/0.25/7/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.56it/s v_num: 0.000      
                                                              val/auc: 0.500    
                                                              val/f1: 0.375     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.300 val/mre:    
                                                              0.094 train/auc:  
                                                              0.875 train/f1:   
                                                              0.873             
                                                              train/precision:  
                                                              0.887             
                                                              train/recall:     
                                                              0.859 train/mre:  
                                                              0.085             
[2024-05-31 00:53:21,646][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/doing_today/0.25/7>
[2024-05-31 00:53:21,646][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-31 00:53:21,650][HYDRA] 	#478 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=doing_today data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=8
[rank: 0] Seed set to 8
[2024-05-31 00:53:21,947][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-31 00:53:21,949][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-31 00:53:21,951][train.py][INFO] - Instantiating callbacks...
[2024-05-31 00:53:21,952][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-31 00:53:21,955][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-31 00:53:21,956][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-31 00:53:21,957][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-31 00:53:21,957][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-31 00:53:21,959][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-31 00:53:21,982][train.py][INFO] - Instantiating loggers...
[2024-05-31 00:53:21,983][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-31 00:53:21,987][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-31 00:53:22,098][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/doing_today/0.25/8/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.55it/s v_num: 0.000      
                                                              val/auc: 0.450    
                                                              val/f1: 0.353     
                                                              val/precision:    
                                                              0.429 val/recall: 
                                                              0.300 val/mre:    
                                                              0.087 train/auc:  
                                                              0.867 train/f1:   
                                                              0.860             
                                                              train/precision:  
                                                              0.912             
                                                              train/recall:     
                                                              0.812 train/mre:  
                                                              0.079             
[2024-05-31 00:55:06,850][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/doing_today/0.25/8>
[2024-05-31 00:55:06,851][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-31 00:55:06,854][HYDRA] 	#479 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=doing_today data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=9
[rank: 0] Seed set to 9
[2024-05-31 00:55:07,337][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-31 00:55:07,341][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-31 00:55:07,343][train.py][INFO] - Instantiating callbacks...
[2024-05-31 00:55:07,343][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-31 00:55:07,347][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-31 00:55:07,348][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-31 00:55:07,348][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-31 00:55:07,349][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-31 00:55:07,351][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-31 00:55:07,354][train.py][INFO] - Instantiating loggers...
[2024-05-31 00:55:07,354][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-31 00:55:07,356][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-31 00:55:07,499][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/doing_today/0.25/9/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.54it/s v_num: 0.000      
                                                              val/auc: 0.600    
                                                              val/f1: 0.556     
                                                              val/precision:    
                                                              0.625 val/recall: 
                                                              0.500 val/mre:    
                                                              0.086 train/auc:  
                                                              0.852 train/f1:   
                                                              0.855             
                                                              train/precision:  
                                                              0.836             
                                                              train/recall:     
                                                              0.875 train/mre:  
                                                              0.077             
[2024-05-31 00:56:52,998][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/doing_today/0.25/9>
[2024-05-31 00:56:52,999][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-31 00:56:53,003][HYDRA] 	#480 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=doing_today data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=0
[2024-05-31 00:56:53,300][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-31 00:56:53,302][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-31 00:56:53,304][train.py][INFO] - Instantiating callbacks...
[2024-05-31 00:56:53,305][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-31 00:56:53,308][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-31 00:56:53,309][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-31 00:56:53,310][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-31 00:56:53,310][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-31 00:56:53,312][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-31 00:56:53,315][train.py][INFO] - Instantiating loggers...
[2024-05-31 00:56:53,315][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-31 00:56:53,319][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-31 00:56:53,407][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/doing_today/0.3/0/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.53it/s v_num: 0.000      
                                                              val/auc: 0.700    
                                                              val/f1: 0.700     
                                                              val/precision:    
                                                              0.700 val/recall: 
                                                              0.700 val/mre:    
                                                              0.086 train/auc:  
                                                              0.867 train/f1:   
                                                              0.870             
                                                              train/precision:  
                                                              0.851             
                                                              train/recall:     
                                                              0.891 train/mre:  
                                                              0.076             
[2024-05-31 00:58:39,453][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/doing_today/0.3/0>
[2024-05-31 00:58:39,454][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-31 00:58:39,457][HYDRA] 	#481 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=doing_today data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=1
[rank: 0] Seed set to 1
[2024-05-31 00:58:39,756][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-31 00:58:39,758][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-31 00:58:39,760][train.py][INFO] - Instantiating callbacks...
[2024-05-31 00:58:39,760][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-31 00:58:39,764][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-31 00:58:39,765][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-31 00:58:39,765][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-31 00:58:39,766][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-31 00:58:39,768][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-31 00:58:39,771][train.py][INFO] - Instantiating loggers...
[2024-05-31 00:58:39,771][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-31 00:58:39,773][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-31 00:58:39,859][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/doing_today/0.3/1/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.54it/s v_num: 0.000      
                                                              val/auc: 0.650    
                                                              val/f1: 0.588     
                                                              val/precision:    
                                                              0.714 val/recall: 
                                                              0.500 val/mre:    
                                                              0.090 train/auc:  
                                                              0.891 train/f1:   
                                                              0.887             
                                                              train/precision:  
                                                              0.917             
                                                              train/recall:     
                                                              0.859 train/mre:  
                                                              0.080             
[2024-05-31 01:00:26,679][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/doing_today/0.3/1>
[2024-05-31 01:00:26,680][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-31 01:00:26,683][HYDRA] 	#482 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=doing_today data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=2
[rank: 0] Seed set to 2
[2024-05-31 01:00:27,000][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-31 01:00:27,003][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-31 01:00:27,005][train.py][INFO] - Instantiating callbacks...
[2024-05-31 01:00:27,005][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-31 01:00:27,008][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-31 01:00:27,009][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-31 01:00:27,010][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-31 01:00:27,011][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-31 01:00:27,012][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-31 01:00:27,015][train.py][INFO] - Instantiating loggers...
[2024-05-31 01:00:27,016][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-31 01:00:27,018][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-31 01:00:27,106][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/doing_today/0.3/2/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.54it/s v_num: 0.000      
                                                              val/auc: 0.450    
                                                              val/f1: 0.353     
                                                              val/precision:    
                                                              0.429 val/recall: 
                                                              0.300 val/mre:    
                                                              0.088 train/auc:  
                                                              0.836 train/f1:   
                                                              0.832             
                                                              train/precision:  
                                                              0.852             
                                                              train/recall:     
                                                              0.812 train/mre:  
                                                              0.080             
[2024-05-31 01:02:13,661][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/doing_today/0.3/2>
[2024-05-31 01:02:13,661][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-31 01:02:13,664][HYDRA] 	#483 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=doing_today data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=3
[rank: 0] Seed set to 3
[2024-05-31 01:02:13,967][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-31 01:02:13,970][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-31 01:02:13,972][train.py][INFO] - Instantiating callbacks...
[2024-05-31 01:02:13,972][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-31 01:02:13,975][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-31 01:02:13,976][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-31 01:02:13,977][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-31 01:02:13,978][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-31 01:02:13,980][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-31 01:02:13,983][train.py][INFO] - Instantiating loggers...
[2024-05-31 01:02:13,983][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-31 01:02:13,985][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-31 01:02:14,073][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/doing_today/0.3/3/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.50it/s v_num: 0.000      
                                                              val/auc: 0.550    
                                                              val/f1: 0.400     
                                                              val/precision:    
                                                              0.600 val/recall: 
                                                              0.300 val/mre:    
                                                              0.090 train/auc:  
                                                              0.945 train/f1:   
                                                              0.946             
                                                              train/precision:  
                                                              0.938             
                                                              train/recall:     
                                                              0.953 train/mre:  
                                                              0.080             
[2024-05-31 01:03:59,442][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/doing_today/0.3/3>
[2024-05-31 01:03:59,442][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-31 01:03:59,446][HYDRA] 	#484 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=doing_today data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=4
[rank: 0] Seed set to 4
[2024-05-31 01:03:59,746][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-31 01:03:59,749][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-31 01:03:59,752][train.py][INFO] - Instantiating callbacks...
[2024-05-31 01:03:59,752][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-31 01:03:59,756][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-31 01:03:59,757][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-31 01:03:59,758][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-31 01:03:59,758][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-31 01:03:59,760][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-31 01:03:59,764][train.py][INFO] - Instantiating loggers...
[2024-05-31 01:03:59,764][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-31 01:03:59,766][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-31 01:03:59,855][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/doing_today/0.3/4/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.55it/s v_num: 0.000      
                                                              val/auc: 0.550    
                                                              val/f1: 0.471     
                                                              val/precision:    
                                                              0.571 val/recall: 
                                                              0.400 val/mre:    
                                                              0.092 train/auc:  
                                                              0.844 train/f1:   
                                                              0.853             
                                                              train/precision:  
                                                              0.806             
                                                              train/recall:     
                                                              0.906 train/mre:  
                                                              0.083             
[2024-05-31 01:05:44,290][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/doing_today/0.3/4>
[2024-05-31 01:05:44,290][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-31 01:05:44,294][HYDRA] 	#485 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=doing_today data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=5
[rank: 0] Seed set to 5
[2024-05-31 01:05:44,597][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-31 01:05:44,600][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-31 01:05:44,602][train.py][INFO] - Instantiating callbacks...
[2024-05-31 01:05:44,603][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-31 01:05:44,607][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-31 01:05:44,607][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-31 01:05:44,608][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-31 01:05:44,609][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-31 01:05:44,611][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-31 01:05:44,614][train.py][INFO] - Instantiating loggers...
[2024-05-31 01:05:44,614][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-31 01:05:44,616][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-31 01:05:44,703][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/doing_today/0.3/5/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.58it/s v_num: 0.000      
                                                              val/auc: 0.550    
                                                              val/f1: 0.526     
                                                              val/precision:    
                                                              0.556 val/recall: 
                                                              0.500 val/mre:    
                                                              0.092 train/auc:  
                                                              0.891 train/f1:   
                                                              0.897             
                                                              train/precision:  
                                                              0.847             
                                                              train/recall:     
                                                              0.953 train/mre:  
                                                              0.084             
[2024-05-31 01:07:29,001][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/doing_today/0.3/5>
[2024-05-31 01:07:29,002][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-31 01:07:29,005][HYDRA] 	#486 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=doing_today data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=6
[rank: 0] Seed set to 6
[2024-05-31 01:07:29,308][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-31 01:07:29,310][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-31 01:07:29,312][train.py][INFO] - Instantiating callbacks...
[2024-05-31 01:07:29,313][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-31 01:07:29,316][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-31 01:07:29,317][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-31 01:07:29,318][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-31 01:07:29,318][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-31 01:07:29,320][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-31 01:07:29,323][train.py][INFO] - Instantiating loggers...
[2024-05-31 01:07:29,324][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-31 01:07:29,326][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-31 01:07:29,415][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/doing_today/0.3/6/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.59it/s v_num: 0.000      
                                                              val/auc: 0.650    
                                                              val/f1: 0.588     
                                                              val/precision:    
                                                              0.714 val/recall: 
                                                              0.500 val/mre:    
                                                              0.097 train/auc:  
                                                              0.969 train/f1:   
                                                              0.969             
                                                              train/precision:  
                                                              0.969             
                                                              train/recall:     
                                                              0.969 train/mre:  
                                                              0.088             
[2024-05-31 01:09:14,686][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/doing_today/0.3/6>
[2024-05-31 01:09:14,687][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-31 01:09:14,690][HYDRA] 	#487 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=doing_today data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=7
[rank: 0] Seed set to 7
[2024-05-31 01:09:14,990][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-31 01:09:14,993][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-31 01:09:14,995][train.py][INFO] - Instantiating callbacks...
[2024-05-31 01:09:14,996][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-31 01:09:15,000][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-31 01:09:15,001][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-31 01:09:15,002][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-31 01:09:15,002][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-31 01:09:15,004][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-31 01:09:15,008][train.py][INFO] - Instantiating loggers...
[2024-05-31 01:09:15,009][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-31 01:09:15,011][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-31 01:09:15,101][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/doing_today/0.3/7/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.56it/s v_num: 0.000      
                                                              val/auc: 0.600    
                                                              val/f1: 0.556     
                                                              val/precision:    
                                                              0.625 val/recall: 
                                                              0.500 val/mre:    
                                                              0.091 train/auc:  
                                                              0.859 train/f1:   
                                                              0.857             
                                                              train/precision:  
                                                              0.871             
                                                              train/recall:     
                                                              0.844 train/mre:  
                                                              0.084             
[2024-05-31 01:10:59,725][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/doing_today/0.3/7>
[2024-05-31 01:10:59,727][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-31 01:10:59,733][HYDRA] 	#488 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=doing_today data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=8
[rank: 0] Seed set to 8
[2024-05-31 01:11:00,145][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-31 01:11:00,150][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-31 01:11:00,154][train.py][INFO] - Instantiating callbacks...
[2024-05-31 01:11:00,154][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-31 01:11:00,159][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-31 01:11:00,160][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-31 01:11:00,161][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-31 01:11:00,161][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-31 01:11:00,163][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-31 01:11:00,166][train.py][INFO] - Instantiating loggers...
[2024-05-31 01:11:00,167][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-31 01:11:00,169][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-31 01:11:00,531][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/doing_today/0.3/8/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.55it/s v_num: 0.000      
                                                              val/auc: 0.650    
                                                              val/f1: 0.588     
                                                              val/precision:    
                                                              0.714 val/recall: 
                                                              0.500 val/mre:    
                                                              0.090 train/auc:  
                                                              0.867 train/f1:   
                                                              0.866             
                                                              train/precision:  
                                                              0.873             
                                                              train/recall:     
                                                              0.859 train/mre:  
                                                              0.080             
[2024-05-31 01:12:45,211][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/doing_today/0.3/8>
[2024-05-31 01:12:45,212][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-31 01:12:45,215][HYDRA] 	#489 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=doing_today data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=usgan model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=9
[rank: 0] Seed set to 9
[2024-05-31 01:12:45,520][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-31 01:12:45,523][train.py][INFO] - Instantiating model <src.methods.usgan.lightningmodule.USGANLightningModule>
[2024-05-31 01:12:45,525][train.py][INFO] - Instantiating callbacks...
[2024-05-31 01:12:45,526][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-31 01:12:45,529][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-31 01:12:45,530][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-31 01:12:45,531][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-31 01:12:45,532][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-31 01:12:45,533][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-31 01:12:45,536][train.py][INFO] - Instantiating loggers...
[2024-05-31 01:12:45,537][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-31 01:12:45,539][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-31 01:12:45,627][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/doing_today/0.3/9/csv_artifacts/
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                    ┃ Type               ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ generator                               │ MultiTaskBRITS     │  327 K │
│ 1  │ generator.model                         │ MyBackboneBRITS    │  323 K │
│ 2  │ generator.model.rits_f                  │ MyBackboneRITS     │  161 K │
│ 3  │ generator.model.rits_f.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 4  │ generator.model.rits_f.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 5  │ generator.model.rits_f.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 6  │ generator.model.rits_f.hist_reg         │ Linear             │  5.3 K │
│ 7  │ generator.model.rits_f.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 8  │ generator.model.rits_f.combining_weight │ Linear             │ 52.6 K │
│ 9  │ generator.model.rits_b                  │ MyBackboneRITS     │  161 K │
│ 10 │ generator.model.rits_b.rnn_cell         │ LSTMCell           │ 45.8 K │
│ 11 │ generator.model.rits_b.temp_decay_h     │ TemporalDecay      │  5.2 K │
│ 12 │ generator.model.rits_b.temp_decay_x     │ TemporalDecay      │ 26.4 K │
│ 13 │ generator.model.rits_b.hist_reg         │ Linear             │  5.3 K │
│ 14 │ generator.model.rits_b.feat_reg         │ FeatureRegression  │ 26.4 K │
│ 15 │ generator.model.rits_b.combining_weight │ Linear             │ 52.6 K │
│ 16 │ generator.out                           │ Linear             │  1.9 K │
│ 17 │ generator.W_s1                          │ Linear             │    990 │
│ 18 │ generator.W_s2                          │ Linear             │    930 │
│ 19 │ discriminator                           │ UsganDiscriminator │ 79.3 K │
│ 20 │ discriminator.biRNN                     │ GRU                │ 68.7 K │
│ 21 │ discriminator.dropout                   │ Dropout            │      0 │
│ 22 │ discriminator.read_out                  │ Linear             │ 10.5 K │
└────┴─────────────────────────────────────────┴────────────────────┴────────┘
Trainable params: 406 K                                                         
Non-trainable params: 0                                                         
Total params: 406 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.57it/s v_num: 0.000      
                                                              val/auc: 0.650    
                                                              val/f1: 0.533     
                                                              val/precision:    
                                                              0.800 val/recall: 
                                                              0.400 val/mre:    
                                                              0.089 train/auc:  
                                                              0.758 train/f1:   
                                                              0.786             
                                                              train/precision:  
                                                              0.704             
                                                              train/recall:     
                                                              0.891 train/mre:  
                                                              0.082             
[2024-05-31 01:14:30,649][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-11-05-07/doing_today/0.3/9>
[2024-05-31 01:14:30,649][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
