[2024-06-06 14:30:50,588][HYDRA] Launching 70 jobs locally
[2024-06-06 14:30:50,588][HYDRA] 	#0 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=advice_yourself data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=gru callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False logger=csv test=False seed=0
[2024-06-06 14:30:50,792][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-06 14:30:52,100][train.py][INFO] - Instantiating model <src.methods.gru.lightningmodule.GRULightningModule>
[2024-06-06 14:30:52,104][train.py][INFO] - Instantiating callbacks...
[2024-06-06 14:30:52,104][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-06 14:30:52,129][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-06 14:30:52,129][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-06 14:30:52,133][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-06 14:30:52,133][train.py][INFO] - Instantiating loggers...
[2024-06-06 14:30:52,133][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-06 14:30:52,135][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
/home/khickey/test_impute/env/lib/python3.12/site-packages/torch/cuda/__init__.py:628: UserWarning: Can't initialize NVML
  warnings.warn("Can't initialize NVML")
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-06 14:30:52,538][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-06-14-06-41/advice_yourself/0.0/0/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃   ┃ Name          ┃ Type             ┃ Params ┃
┡━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0 │ loss          │ CrossEntropyLoss │      0 │
│ 1 │ model         │ GRU_base         │ 68.9 K │
│ 2 │ model.RNNCell │ GRU              │ 68.7 K │
│ 3 │ model.predict │ Linear           │    130 │
│ 4 │ model.act     │ Sigmoid          │      0 │
└───┴───────────────┴──────────────────┴────────┘
Trainable params: 68.9 K                                                        
Non-trainable params: 0                                                         
Total params: 68.9 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 37.31it/s v_num: 0.000      
                                                              val/auc: 0.567    
                                                              val/f1: 0.429     
                                                              val/precision:    
                                                              0.600 val/recall: 
                                                              0.333 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000             
[2024-06-06 14:31:10,992][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-06-14-06-41/advice_yourself/0.0/0>
[2024-06-06 14:31:10,993][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-06 14:31:10,996][HYDRA] 	#1 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=advice_yourself data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=gru callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False logger=csv test=False seed=1
[rank: 0] Seed set to 1
[2024-06-06 14:31:11,171][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-06 14:31:11,173][train.py][INFO] - Instantiating model <src.methods.gru.lightningmodule.GRULightningModule>
[2024-06-06 14:31:11,174][train.py][INFO] - Instantiating callbacks...
[2024-06-06 14:31:11,174][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-06 14:31:11,176][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-06 14:31:11,177][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-06 14:31:11,177][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-06 14:31:11,178][train.py][INFO] - Instantiating loggers...
[2024-06-06 14:31:11,178][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-06 14:31:11,180][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-06 14:31:11,211][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-06-14-06-41/advice_yourself/0.0/1/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃   ┃ Name          ┃ Type             ┃ Params ┃
┡━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0 │ loss          │ CrossEntropyLoss │      0 │
│ 1 │ model         │ GRU_base         │ 68.9 K │
│ 2 │ model.RNNCell │ GRU              │ 68.7 K │
│ 3 │ model.predict │ Linear           │    130 │
│ 4 │ model.act     │ Sigmoid          │      0 │
└───┴───────────────┴──────────────────┴────────┘
Trainable params: 68.9 K                                                        
Non-trainable params: 0                                                         
Total params: 68.9 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 37.78it/s v_num: 0.000      
                                                              val/auc: 0.406    
                                                              val/f1: 0.154     
                                                              val/precision:    
                                                              0.250 val/recall: 
                                                              0.111 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000             
[2024-06-06 14:31:28,101][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-06-14-06-41/advice_yourself/0.0/1>
[2024-06-06 14:31:28,101][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-06 14:31:28,105][HYDRA] 	#2 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=advice_yourself data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=gru callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False logger=csv test=False seed=2
[rank: 0] Seed set to 2
[2024-06-06 14:31:28,280][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-06 14:31:28,282][train.py][INFO] - Instantiating model <src.methods.gru.lightningmodule.GRULightningModule>
[2024-06-06 14:31:28,283][train.py][INFO] - Instantiating callbacks...
[2024-06-06 14:31:28,284][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-06 14:31:28,286][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-06 14:31:28,286][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-06 14:31:28,287][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-06 14:31:28,288][train.py][INFO] - Instantiating loggers...
[2024-06-06 14:31:28,288][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-06 14:31:28,289][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-06 14:31:28,319][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-06-14-06-41/advice_yourself/0.0/2/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃   ┃ Name          ┃ Type             ┃ Params ┃
┡━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0 │ loss          │ CrossEntropyLoss │      0 │
│ 1 │ model         │ GRU_base         │ 68.9 K │
│ 2 │ model.RNNCell │ GRU              │ 68.7 K │
│ 3 │ model.predict │ Linear           │    130 │
│ 4 │ model.act     │ Sigmoid          │      0 │
└───┴───────────────┴──────────────────┴────────┘
Trainable params: 68.9 K                                                        
Non-trainable params: 0                                                         
Total params: 68.9 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 35.78it/s v_num: 0.000      
                                                              val/auc: 0.572    
                                                              val/f1: 0.500     
                                                              val/precision:    
                                                              0.571 val/recall: 
                                                              0.444 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000             
[2024-06-06 14:31:44,891][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-06-14-06-41/advice_yourself/0.0/2>
[2024-06-06 14:31:44,892][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-06 14:31:44,894][HYDRA] 	#3 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=advice_yourself data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=gru callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False logger=csv test=False seed=3
[rank: 0] Seed set to 3
[2024-06-06 14:31:45,072][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-06 14:31:45,073][train.py][INFO] - Instantiating model <src.methods.gru.lightningmodule.GRULightningModule>
[2024-06-06 14:31:45,074][train.py][INFO] - Instantiating callbacks...
[2024-06-06 14:31:45,074][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-06 14:31:45,077][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-06 14:31:45,077][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-06 14:31:45,078][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-06 14:31:45,080][train.py][INFO] - Instantiating loggers...
[2024-06-06 14:31:45,080][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-06 14:31:45,081][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-06 14:31:45,113][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-06-14-06-41/advice_yourself/0.0/3/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃   ┃ Name          ┃ Type             ┃ Params ┃
┡━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0 │ loss          │ CrossEntropyLoss │      0 │
│ 1 │ model         │ GRU_base         │ 68.9 K │
│ 2 │ model.RNNCell │ GRU              │ 68.7 K │
│ 3 │ model.predict │ Linear           │    130 │
│ 4 │ model.act     │ Sigmoid          │      0 │
└───┴───────────────┴──────────────────┴────────┘
Trainable params: 68.9 K                                                        
Non-trainable params: 0                                                         
Total params: 68.9 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 41.86it/s v_num: 0.000      
                                                              val/auc: 0.517    
                                                              val/f1: 0.400     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.333 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000             
[2024-06-06 14:32:00,939][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-06-14-06-41/advice_yourself/0.0/3>
[2024-06-06 14:32:00,940][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-06 14:32:00,942][HYDRA] 	#4 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=advice_yourself data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=gru callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False logger=csv test=False seed=4
[rank: 0] Seed set to 4
[2024-06-06 14:32:01,111][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-06 14:32:01,113][train.py][INFO] - Instantiating model <src.methods.gru.lightningmodule.GRULightningModule>
[2024-06-06 14:32:01,114][train.py][INFO] - Instantiating callbacks...
[2024-06-06 14:32:01,114][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-06 14:32:01,116][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-06 14:32:01,117][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-06 14:32:01,117][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-06 14:32:01,118][train.py][INFO] - Instantiating loggers...
[2024-06-06 14:32:01,118][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-06 14:32:01,120][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-06 14:32:01,186][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-06-14-06-41/advice_yourself/0.0/4/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃   ┃ Name          ┃ Type             ┃ Params ┃
┡━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0 │ loss          │ CrossEntropyLoss │      0 │
│ 1 │ model         │ GRU_base         │ 68.9 K │
│ 2 │ model.RNNCell │ GRU              │ 68.7 K │
│ 3 │ model.predict │ Linear           │    130 │
│ 4 │ model.act     │ Sigmoid          │      0 │
└───┴───────────────┴──────────────────┴────────┘
Trainable params: 68.9 K                                                        
Non-trainable params: 0                                                         
Total params: 68.9 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 44.15it/s v_num: 0.000      
                                                              val/auc: 0.411    
                                                              val/f1: 0.267     
                                                              val/precision:    
                                                              0.333 val/recall: 
                                                              0.222 train/auc:  
                                                              0.976 train/f1:   
                                                              0.976             
                                                              train/precision:  
                                                              0.968             
                                                              train/recall:     
                                                              0.984             
[2024-06-06 14:32:16,903][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-06-14-06-41/advice_yourself/0.0/4>
[2024-06-06 14:32:16,904][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-06 14:32:16,906][HYDRA] 	#5 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=advice_yourself data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=gru callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False logger=csv test=False seed=5
[rank: 0] Seed set to 5
[2024-06-06 14:32:17,077][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-06 14:32:17,079][train.py][INFO] - Instantiating model <src.methods.gru.lightningmodule.GRULightningModule>
[2024-06-06 14:32:17,080][train.py][INFO] - Instantiating callbacks...
[2024-06-06 14:32:17,080][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-06 14:32:17,082][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-06 14:32:17,083][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-06 14:32:17,083][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-06 14:32:17,084][train.py][INFO] - Instantiating loggers...
[2024-06-06 14:32:17,084][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-06 14:32:17,085][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-06 14:32:17,115][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-06-14-06-41/advice_yourself/0.0/5/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃   ┃ Name          ┃ Type             ┃ Params ┃
┡━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0 │ loss          │ CrossEntropyLoss │      0 │
│ 1 │ model         │ GRU_base         │ 68.9 K │
│ 2 │ model.RNNCell │ GRU              │ 68.7 K │
│ 3 │ model.predict │ Linear           │    130 │
│ 4 │ model.act     │ Sigmoid          │      0 │
└───┴───────────────┴──────────────────┴────────┘
Trainable params: 68.9 K                                                        
Non-trainable params: 0                                                         
Total params: 68.9 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 31.92it/s v_num: 0.000      
                                                              val/auc: 0.572    
                                                              val/f1: 0.500     
                                                              val/precision:    
                                                              0.571 val/recall: 
                                                              0.444 train/auc:  
                                                              0.935 train/f1:   
                                                              0.937             
                                                              train/precision:  
                                                              0.922             
                                                              train/recall:     
                                                              0.952             
[2024-06-06 14:32:33,600][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-06-14-06-41/advice_yourself/0.0/5>
[2024-06-06 14:32:33,600][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-06 14:32:33,603][HYDRA] 	#6 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=advice_yourself data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=gru callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False logger=csv test=False seed=6
[rank: 0] Seed set to 6
[2024-06-06 14:32:33,773][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-06 14:32:33,775][train.py][INFO] - Instantiating model <src.methods.gru.lightningmodule.GRULightningModule>
[2024-06-06 14:32:33,775][train.py][INFO] - Instantiating callbacks...
[2024-06-06 14:32:33,776][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-06 14:32:33,778][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-06 14:32:33,778][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-06 14:32:33,779][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-06 14:32:33,779][train.py][INFO] - Instantiating loggers...
[2024-06-06 14:32:33,779][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-06 14:32:33,781][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-06 14:32:33,809][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-06-14-06-41/advice_yourself/0.0/6/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃   ┃ Name          ┃ Type             ┃ Params ┃
┡━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0 │ loss          │ CrossEntropyLoss │      0 │
│ 1 │ model         │ GRU_base         │ 68.9 K │
│ 2 │ model.RNNCell │ GRU              │ 68.7 K │
│ 3 │ model.predict │ Linear           │    130 │
│ 4 │ model.act     │ Sigmoid          │      0 │
└───┴───────────────┴──────────────────┴────────┘
Trainable params: 68.9 K                                                        
Non-trainable params: 0                                                         
Total params: 68.9 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 40.31it/s v_num: 0.000      
                                                              val/auc: 0.461    
                                                              val/f1: 0.286     
                                                              val/precision:    
                                                              0.400 val/recall: 
                                                              0.222 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000             
[2024-06-06 14:32:50,809][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-06-14-06-41/advice_yourself/0.0/6>
[2024-06-06 14:32:50,809][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-06 14:32:50,812][HYDRA] 	#7 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=advice_yourself data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=gru callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False logger=csv test=False seed=7
[rank: 0] Seed set to 7
[2024-06-06 14:32:50,987][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-06 14:32:50,989][train.py][INFO] - Instantiating model <src.methods.gru.lightningmodule.GRULightningModule>
[2024-06-06 14:32:50,990][train.py][INFO] - Instantiating callbacks...
[2024-06-06 14:32:50,990][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-06 14:32:50,992][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-06 14:32:50,993][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-06 14:32:50,993][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-06 14:32:50,995][train.py][INFO] - Instantiating loggers...
[2024-06-06 14:32:50,995][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-06 14:32:50,997][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-06 14:32:51,028][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-06-14-06-41/advice_yourself/0.0/7/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃   ┃ Name          ┃ Type             ┃ Params ┃
┡━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0 │ loss          │ CrossEntropyLoss │      0 │
│ 1 │ model         │ GRU_base         │ 68.9 K │
│ 2 │ model.RNNCell │ GRU              │ 68.7 K │
│ 3 │ model.predict │ Linear           │    130 │
│ 4 │ model.act     │ Sigmoid          │      0 │
└───┴───────────────┴──────────────────┴────────┘
Trainable params: 68.9 K                                                        
Non-trainable params: 0                                                         
Total params: 68.9 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 37.03it/s v_num: 0.000      
                                                              val/auc: 0.517    
                                                              val/f1: 0.400     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.333 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000             
[2024-06-06 14:33:07,583][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-06-14-06-41/advice_yourself/0.0/7>
[2024-06-06 14:33:07,583][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-06 14:33:07,585][HYDRA] 	#8 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=advice_yourself data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=gru callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False logger=csv test=False seed=8
[rank: 0] Seed set to 8
[2024-06-06 14:33:07,762][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-06 14:33:07,764][train.py][INFO] - Instantiating model <src.methods.gru.lightningmodule.GRULightningModule>
[2024-06-06 14:33:07,765][train.py][INFO] - Instantiating callbacks...
[2024-06-06 14:33:07,766][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-06 14:33:07,768][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-06 14:33:07,768][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-06 14:33:07,769][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-06 14:33:07,770][train.py][INFO] - Instantiating loggers...
[2024-06-06 14:33:07,770][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-06 14:33:07,771][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-06 14:33:07,834][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-06-14-06-41/advice_yourself/0.0/8/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃   ┃ Name          ┃ Type             ┃ Params ┃
┡━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0 │ loss          │ CrossEntropyLoss │      0 │
│ 1 │ model         │ GRU_base         │ 68.9 K │
│ 2 │ model.RNNCell │ GRU              │ 68.7 K │
│ 3 │ model.predict │ Linear           │    130 │
│ 4 │ model.act     │ Sigmoid          │      0 │
└───┴───────────────┴──────────────────┴────────┘
Trainable params: 68.9 K                                                        
Non-trainable params: 0                                                         
Total params: 68.9 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 38.32it/s v_num: 0.000      
                                                              val/auc: 0.461    
                                                              val/f1: 0.286     
                                                              val/precision:    
                                                              0.400 val/recall: 
                                                              0.222 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000             
[2024-06-06 14:33:24,575][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-06-14-06-41/advice_yourself/0.0/8>
[2024-06-06 14:33:24,576][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-06 14:33:24,579][HYDRA] 	#9 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=advice_yourself data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=gru callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False logger=csv test=False seed=9
[rank: 0] Seed set to 9
[2024-06-06 14:33:24,753][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-06 14:33:24,754][train.py][INFO] - Instantiating model <src.methods.gru.lightningmodule.GRULightningModule>
[2024-06-06 14:33:24,755][train.py][INFO] - Instantiating callbacks...
[2024-06-06 14:33:24,756][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-06 14:33:24,758][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-06 14:33:24,758][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-06 14:33:24,759][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-06 14:33:24,759][train.py][INFO] - Instantiating loggers...
[2024-06-06 14:33:24,759][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-06 14:33:24,761][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-06 14:33:24,791][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-06-14-06-41/advice_yourself/0.0/9/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃   ┃ Name          ┃ Type             ┃ Params ┃
┡━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0 │ loss          │ CrossEntropyLoss │      0 │
│ 1 │ model         │ GRU_base         │ 68.9 K │
│ 2 │ model.RNNCell │ GRU              │ 68.7 K │
│ 3 │ model.predict │ Linear           │    130 │
│ 4 │ model.act     │ Sigmoid          │      0 │
└───┴───────────────┴──────────────────┴────────┘
Trainable params: 68.9 K                                                        
Non-trainable params: 0                                                         
Total params: 68.9 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 40.12it/s v_num: 0.000      
                                                              val/auc: 0.511    
                                                              val/f1: 0.308     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.222 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000             
[2024-06-06 14:33:41,254][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-06-14-06-41/advice_yourself/0.0/9>
[2024-06-06 14:33:41,255][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-06 14:33:41,257][HYDRA] 	#10 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=anything_regret data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=gru callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False logger=csv test=False seed=0
[2024-06-06 14:33:41,432][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-06 14:33:41,433][train.py][INFO] - Instantiating model <src.methods.gru.lightningmodule.GRULightningModule>
[2024-06-06 14:33:41,434][train.py][INFO] - Instantiating callbacks...
[2024-06-06 14:33:41,434][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-06 14:33:41,436][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-06 14:33:41,437][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-06 14:33:41,437][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-06 14:33:41,438][train.py][INFO] - Instantiating loggers...
[2024-06-06 14:33:41,438][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-06 14:33:41,440][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-06 14:33:41,468][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-06-14-06-41/anything_regret/0.0/0/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃   ┃ Name          ┃ Type             ┃ Params ┃
┡━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0 │ loss          │ CrossEntropyLoss │      0 │
│ 1 │ model         │ GRU_base         │ 68.9 K │
│ 2 │ model.RNNCell │ GRU              │ 68.7 K │
│ 3 │ model.predict │ Linear           │    130 │
│ 4 │ model.act     │ Sigmoid          │      0 │
└───┴───────────────┴──────────────────┴────────┘
Trainable params: 68.9 K                                                        
Non-trainable params: 0                                                         
Total params: 68.9 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 39.24it/s v_num: 0.000      
                                                              val/auc: 0.444    
                                                              val/f1: 0.545     
                                                              val/precision:    
                                                              0.462 val/recall: 
                                                              0.667 train/auc:  
                                                              0.661 train/f1:   
                                                              0.747             
                                                              train/precision:  
                                                              0.596             
                                                              train/recall:     
                                                              1.000             
[2024-06-06 14:33:56,869][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-06-14-06-41/anything_regret/0.0/0>
[2024-06-06 14:33:56,870][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-06 14:33:56,873][HYDRA] 	#11 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=anything_regret data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=gru callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False logger=csv test=False seed=1
[rank: 0] Seed set to 1
[2024-06-06 14:33:57,052][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-06 14:33:57,054][train.py][INFO] - Instantiating model <src.methods.gru.lightningmodule.GRULightningModule>
[2024-06-06 14:33:57,054][train.py][INFO] - Instantiating callbacks...
[2024-06-06 14:33:57,055][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-06 14:33:57,057][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-06 14:33:57,057][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-06 14:33:57,058][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-06 14:33:57,060][train.py][INFO] - Instantiating loggers...
[2024-06-06 14:33:57,060][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-06 14:33:57,062][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-06 14:33:57,096][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-06-14-06-41/anything_regret/0.0/1/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃   ┃ Name          ┃ Type             ┃ Params ┃
┡━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0 │ loss          │ CrossEntropyLoss │      0 │
│ 1 │ model         │ GRU_base         │ 68.9 K │
│ 2 │ model.RNNCell │ GRU              │ 68.7 K │
│ 3 │ model.predict │ Linear           │    130 │
│ 4 │ model.act     │ Sigmoid          │      0 │
└───┴───────────────┴──────────────────┴────────┘
Trainable params: 68.9 K                                                        
Non-trainable params: 0                                                         
Total params: 68.9 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 42.76it/s v_num: 0.000      
                                                              val/auc: 0.444    
                                                              val/f1: 0.500     
                                                              val/precision:    
                                                              0.455 val/recall: 
                                                              0.556 train/auc:  
                                                              0.759 train/f1:   
                                                              0.806             
                                                              train/precision:  
                                                              0.675             
                                                              train/recall:     
                                                              1.000             
[2024-06-06 14:34:12,372][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-06-14-06-41/anything_regret/0.0/1>
[2024-06-06 14:34:12,372][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-06 14:34:12,375][HYDRA] 	#12 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=anything_regret data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=gru callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False logger=csv test=False seed=2
[rank: 0] Seed set to 2
[2024-06-06 14:34:12,545][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-06 14:34:12,547][train.py][INFO] - Instantiating model <src.methods.gru.lightningmodule.GRULightningModule>
[2024-06-06 14:34:12,548][train.py][INFO] - Instantiating callbacks...
[2024-06-06 14:34:12,548][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-06 14:34:12,550][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-06 14:34:12,551][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-06 14:34:12,551][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-06 14:34:12,552][train.py][INFO] - Instantiating loggers...
[2024-06-06 14:34:12,552][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-06 14:34:12,554][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-06 14:34:12,620][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-06-14-06-41/anything_regret/0.0/2/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃   ┃ Name          ┃ Type             ┃ Params ┃
┡━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0 │ loss          │ CrossEntropyLoss │      0 │
│ 1 │ model         │ GRU_base         │ 68.9 K │
│ 2 │ model.RNNCell │ GRU              │ 68.7 K │
│ 3 │ model.predict │ Linear           │    130 │
│ 4 │ model.act     │ Sigmoid          │      0 │
└───┴───────────────┴──────────────────┴────────┘
Trainable params: 68.9 K                                                        
Non-trainable params: 0                                                         
Total params: 68.9 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 41.02it/s v_num: 0.000      
                                                              val/auc: 0.611    
                                                              val/f1: 0.667     
                                                              val/precision:    
                                                              0.583 val/recall: 
                                                              0.778 train/auc:  
                                                              0.821 train/f1:   
                                                              0.833             
                                                              train/precision:  
                                                              0.781             
                                                              train/recall:     
                                                              0.893             
[2024-06-06 14:34:27,883][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-06-14-06-41/anything_regret/0.0/2>
[2024-06-06 14:34:27,883][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-06 14:34:27,885][HYDRA] 	#13 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=anything_regret data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=gru callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False logger=csv test=False seed=3
[rank: 0] Seed set to 3
[2024-06-06 14:34:28,059][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-06 14:34:28,061][train.py][INFO] - Instantiating model <src.methods.gru.lightningmodule.GRULightningModule>
[2024-06-06 14:34:28,062][train.py][INFO] - Instantiating callbacks...
[2024-06-06 14:34:28,062][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-06 14:34:28,064][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-06 14:34:28,064][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-06 14:34:28,065][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-06 14:34:28,065][train.py][INFO] - Instantiating loggers...
[2024-06-06 14:34:28,066][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-06 14:34:28,067][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-06 14:34:28,098][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-06-14-06-41/anything_regret/0.0/3/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃   ┃ Name          ┃ Type             ┃ Params ┃
┡━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0 │ loss          │ CrossEntropyLoss │      0 │
│ 1 │ model         │ GRU_base         │ 68.9 K │
│ 2 │ model.RNNCell │ GRU              │ 68.7 K │
│ 3 │ model.predict │ Linear           │    130 │
│ 4 │ model.act     │ Sigmoid          │      0 │
└───┴───────────────┴──────────────────┴────────┘
Trainable params: 68.9 K                                                        
Non-trainable params: 0                                                         
Total params: 68.9 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 41.73it/s v_num: 0.000      
                                                              val/auc: 0.389    
                                                              val/f1: 0.267     
                                                              val/precision:    
                                                              0.333 val/recall: 
                                                              0.222 train/auc:  
                                                              0.902 train/f1:   
                                                              0.908             
                                                              train/precision:  
                                                              0.857             
                                                              train/recall:     
                                                              0.964             
[2024-06-06 14:34:43,473][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-06-14-06-41/anything_regret/0.0/3>
[2024-06-06 14:34:43,474][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-06 14:34:43,476][HYDRA] 	#14 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=anything_regret data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=gru callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False logger=csv test=False seed=4
[rank: 0] Seed set to 4
[2024-06-06 14:34:43,646][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-06 14:34:43,647][train.py][INFO] - Instantiating model <src.methods.gru.lightningmodule.GRULightningModule>
[2024-06-06 14:34:43,648][train.py][INFO] - Instantiating callbacks...
[2024-06-06 14:34:43,649][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-06 14:34:43,651][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-06 14:34:43,651][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-06 14:34:43,652][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-06 14:34:43,652][train.py][INFO] - Instantiating loggers...
[2024-06-06 14:34:43,652][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-06 14:34:43,654][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-06 14:34:43,684][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-06-14-06-41/anything_regret/0.0/4/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃   ┃ Name          ┃ Type             ┃ Params ┃
┡━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0 │ loss          │ CrossEntropyLoss │      0 │
│ 1 │ model         │ GRU_base         │ 68.9 K │
│ 2 │ model.RNNCell │ GRU              │ 68.7 K │
│ 3 │ model.predict │ Linear           │    130 │
│ 4 │ model.act     │ Sigmoid          │      0 │
└───┴───────────────┴──────────────────┴────────┘
Trainable params: 68.9 K                                                        
Non-trainable params: 0                                                         
Total params: 68.9 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 40.10it/s v_num: 0.000      
                                                              val/auc: 0.444    
                                                              val/f1: 0.375     
                                                              val/precision:    
                                                              0.429 val/recall: 
                                                              0.333 train/auc:  
                                                              0.866 train/f1:   
                                                              0.874             
                                                              train/precision:  
                                                              0.825             
                                                              train/recall:     
                                                              0.929             
[2024-06-06 14:34:59,089][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-06-14-06-41/anything_regret/0.0/4>
[2024-06-06 14:34:59,089][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-06 14:34:59,092][HYDRA] 	#15 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=anything_regret data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=gru callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False logger=csv test=False seed=5
[rank: 0] Seed set to 5
[2024-06-06 14:34:59,267][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-06 14:34:59,269][train.py][INFO] - Instantiating model <src.methods.gru.lightningmodule.GRULightningModule>
[2024-06-06 14:34:59,270][train.py][INFO] - Instantiating callbacks...
[2024-06-06 14:34:59,270][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-06 14:34:59,272][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-06 14:34:59,272][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-06 14:34:59,273][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-06 14:34:59,275][train.py][INFO] - Instantiating loggers...
[2024-06-06 14:34:59,275][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-06 14:34:59,277][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-06 14:34:59,308][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-06-14-06-41/anything_regret/0.0/5/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃   ┃ Name          ┃ Type             ┃ Params ┃
┡━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0 │ loss          │ CrossEntropyLoss │      0 │
│ 1 │ model         │ GRU_base         │ 68.9 K │
│ 2 │ model.RNNCell │ GRU              │ 68.7 K │
│ 3 │ model.predict │ Linear           │    130 │
│ 4 │ model.act     │ Sigmoid          │      0 │
└───┴───────────────┴──────────────────┴────────┘
Trainable params: 68.9 K                                                        
Non-trainable params: 0                                                         
Total params: 68.9 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 39.53it/s v_num: 0.000      
                                                              val/auc: 0.444    
                                                              val/f1: 0.000     
                                                              val/precision:    
                                                              0.000 val/recall: 
                                                              0.000 train/auc:  
                                                              0.688 train/f1:   
                                                              0.755             
                                                              train/precision:  
                                                              0.621             
                                                              train/recall:     
                                                              0.964             
[2024-06-06 14:35:15,441][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-06-14-06-41/anything_regret/0.0/5>
[2024-06-06 14:35:15,441][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-06 14:35:15,444][HYDRA] 	#16 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=anything_regret data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=gru callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False logger=csv test=False seed=6
[rank: 0] Seed set to 6
[2024-06-06 14:35:15,614][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-06 14:35:15,615][train.py][INFO] - Instantiating model <src.methods.gru.lightningmodule.GRULightningModule>
[2024-06-06 14:35:15,616][train.py][INFO] - Instantiating callbacks...
[2024-06-06 14:35:15,616][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-06 14:35:15,618][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-06 14:35:15,619][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-06 14:35:15,619][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-06 14:35:15,620][train.py][INFO] - Instantiating loggers...
[2024-06-06 14:35:15,621][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-06 14:35:15,623][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-06 14:35:15,691][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-06-14-06-41/anything_regret/0.0/6/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃   ┃ Name          ┃ Type             ┃ Params ┃
┡━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0 │ loss          │ CrossEntropyLoss │      0 │
│ 1 │ model         │ GRU_base         │ 68.9 K │
│ 2 │ model.RNNCell │ GRU              │ 68.7 K │
│ 3 │ model.predict │ Linear           │    130 │
│ 4 │ model.act     │ Sigmoid          │      0 │
└───┴───────────────┴──────────────────┴────────┘
Trainable params: 68.9 K                                                        
Non-trainable params: 0                                                         
Total params: 68.9 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 40.04it/s v_num: 0.000      
                                                              val/auc: 0.556    
                                                              val/f1: 0.600     
                                                              val/precision:    
                                                              0.545 val/recall: 
                                                              0.667 train/auc:  
                                                              0.679 train/f1:   
                                                              0.757             
                                                              train/precision:  
                                                              0.609             
                                                              train/recall:     
                                                              1.000             
[2024-06-06 14:35:32,275][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-06-14-06-41/anything_regret/0.0/6>
[2024-06-06 14:35:32,275][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-06 14:35:32,280][HYDRA] 	#17 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=anything_regret data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=gru callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False logger=csv test=False seed=7
[rank: 0] Seed set to 7
[2024-06-06 14:35:32,466][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-06 14:35:32,467][train.py][INFO] - Instantiating model <src.methods.gru.lightningmodule.GRULightningModule>
[2024-06-06 14:35:32,468][train.py][INFO] - Instantiating callbacks...
[2024-06-06 14:35:32,469][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-06 14:35:32,471][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-06 14:35:32,471][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-06 14:35:32,472][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-06 14:35:32,472][train.py][INFO] - Instantiating loggers...
[2024-06-06 14:35:32,472][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-06 14:35:32,474][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-06 14:35:32,524][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-06-14-06-41/anything_regret/0.0/7/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃   ┃ Name          ┃ Type             ┃ Params ┃
┡━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0 │ loss          │ CrossEntropyLoss │      0 │
│ 1 │ model         │ GRU_base         │ 68.9 K │
│ 2 │ model.RNNCell │ GRU              │ 68.7 K │
│ 3 │ model.predict │ Linear           │    130 │
│ 4 │ model.act     │ Sigmoid          │      0 │
└───┴───────────────┴──────────────────┴────────┘
Trainable params: 68.9 K                                                        
Non-trainable params: 0                                                         
Total params: 68.9 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 37.91it/s v_num: 0.000      
                                                              val/auc: 0.556    
                                                              val/f1: 0.600     
                                                              val/precision:    
                                                              0.545 val/recall: 
                                                              0.667 train/auc:  
                                                              0.786 train/f1:   
                                                              0.821             
                                                              train/precision:  
                                                              0.705             
                                                              train/recall:     
                                                              0.982             
[2024-06-06 14:35:48,913][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-06-14-06-41/anything_regret/0.0/7>
[2024-06-06 14:35:48,913][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-06 14:35:48,915][HYDRA] 	#18 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=anything_regret data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=gru callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False logger=csv test=False seed=8
[rank: 0] Seed set to 8
[2024-06-06 14:35:49,085][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-06 14:35:49,087][train.py][INFO] - Instantiating model <src.methods.gru.lightningmodule.GRULightningModule>
[2024-06-06 14:35:49,088][train.py][INFO] - Instantiating callbacks...
[2024-06-06 14:35:49,088][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-06 14:35:49,090][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-06 14:35:49,090][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-06 14:35:49,091][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-06 14:35:49,091][train.py][INFO] - Instantiating loggers...
[2024-06-06 14:35:49,092][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-06 14:35:49,093][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-06 14:35:49,124][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-06-14-06-41/anything_regret/0.0/8/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃   ┃ Name          ┃ Type             ┃ Params ┃
┡━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0 │ loss          │ CrossEntropyLoss │      0 │
│ 1 │ model         │ GRU_base         │ 68.9 K │
│ 2 │ model.RNNCell │ GRU              │ 68.7 K │
│ 3 │ model.predict │ Linear           │    130 │
│ 4 │ model.act     │ Sigmoid          │      0 │
└───┴───────────────┴──────────────────┴────────┘
Trainable params: 68.9 K                                                        
Non-trainable params: 0                                                         
Total params: 68.9 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 37.83it/s v_num: 0.000      
                                                              val/auc: 0.556    
                                                              val/f1: 0.500     
                                                              val/precision:    
                                                              0.571 val/recall: 
                                                              0.444 train/auc:  
                                                              0.929 train/f1:   
                                                              0.933             
                                                              train/precision:  
                                                              0.875             
                                                              train/recall:     
                                                              1.000             
[2024-06-06 14:36:04,607][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-06-14-06-41/anything_regret/0.0/8>
[2024-06-06 14:36:04,607][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-06 14:36:04,610][HYDRA] 	#19 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=anything_regret data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=gru callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False logger=csv test=False seed=9
[rank: 0] Seed set to 9
[2024-06-06 14:36:04,783][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-06 14:36:04,784][train.py][INFO] - Instantiating model <src.methods.gru.lightningmodule.GRULightningModule>
[2024-06-06 14:36:04,785][train.py][INFO] - Instantiating callbacks...
[2024-06-06 14:36:04,785][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-06 14:36:04,788][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-06 14:36:04,788][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-06 14:36:04,788][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-06 14:36:04,791][train.py][INFO] - Instantiating loggers...
[2024-06-06 14:36:04,791][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-06 14:36:04,792][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-06 14:36:04,824][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-06-14-06-41/anything_regret/0.0/9/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃   ┃ Name          ┃ Type             ┃ Params ┃
┡━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0 │ loss          │ CrossEntropyLoss │      0 │
│ 1 │ model         │ GRU_base         │ 68.9 K │
│ 2 │ model.RNNCell │ GRU              │ 68.7 K │
│ 3 │ model.predict │ Linear           │    130 │
│ 4 │ model.act     │ Sigmoid          │      0 │
└───┴───────────────┴──────────────────┴────────┘
Trainable params: 68.9 K                                                        
Non-trainable params: 0                                                         
Total params: 68.9 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 42.46it/s v_num: 0.000      
                                                              val/auc: 0.611    
                                                              val/f1: 0.533     
                                                              val/precision:    
                                                              0.667 val/recall: 
                                                              0.444 train/auc:  
                                                              0.723 train/f1:   
                                                              0.780             
                                                              train/precision:  
                                                              0.647             
                                                              train/recall:     
                                                              0.982             
[2024-06-06 14:36:21,602][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-06-14-06-41/anything_regret/0.0/9>
[2024-06-06 14:36:21,603][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-06 14:36:21,606][HYDRA] 	#20 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=argued_someone data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=gru callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False logger=csv test=False seed=0
[2024-06-06 14:36:21,779][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-06 14:36:21,781][train.py][INFO] - Instantiating model <src.methods.gru.lightningmodule.GRULightningModule>
[2024-06-06 14:36:21,782][train.py][INFO] - Instantiating callbacks...
[2024-06-06 14:36:21,782][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-06 14:36:21,784][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-06 14:36:21,785][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-06 14:36:21,785][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-06 14:36:21,787][train.py][INFO] - Instantiating loggers...
[2024-06-06 14:36:21,787][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-06 14:36:21,788][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-06 14:36:21,854][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-06-14-06-41/argued_someone/0.0/0/csv_artifacts/
┏━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃   ┃ Name          ┃ Type             ┃ Params ┃
┡━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0 │ loss          │ CrossEntropyLoss │      0 │
│ 1 │ model         │ GRU_base         │ 68.9 K │
│ 2 │ model.RNNCell │ GRU              │ 68.7 K │
│ 3 │ model.predict │ Linear           │    130 │
│ 4 │ model.act     │ Sigmoid          │      0 │
└───┴───────────────┴──────────────────┴────────┘
Trainable params: 68.9 K                                                        
Non-trainable params: 0                                                         
Total params: 68.9 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 37.42it/s v_num: 0.000      
                                                              val/auc: 0.450    
                                                              val/f1: 0.267     
                                                              val/precision:    
                                                              0.400 val/recall: 
                                                              0.200 train/auc:  
                                                              0.960 train/f1:   
                                                              0.959             
                                                              train/precision:  
                                                              0.983             
                                                              train/recall:     
                                                              0.935             
[2024-06-06 14:36:38,570][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-06-14-06-41/argued_someone/0.0/0>
[2024-06-06 14:36:38,571][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-06 14:36:38,573][HYDRA] 	#21 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=argued_someone data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=gru callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False logger=csv test=False seed=1
[rank: 0] Seed set to 1
[2024-06-06 14:36:38,745][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-06 14:36:38,747][train.py][INFO] - Instantiating model <src.methods.gru.lightningmodule.GRULightningModule>
[2024-06-06 14:36:38,748][train.py][INFO] - Instantiating callbacks...
[2024-06-06 14:36:38,748][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-06 14:36:38,750][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-06 14:36:38,751][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-06 14:36:38,751][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-06 14:36:38,752][train.py][INFO] - Instantiating loggers...
[2024-06-06 14:36:38,752][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-06 14:36:38,753][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-06 14:36:38,784][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-06-14-06-41/argued_someone/0.0/1/csv_artifacts/
┏━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃   ┃ Name          ┃ Type             ┃ Params ┃
┡━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0 │ loss          │ CrossEntropyLoss │      0 │
│ 1 │ model         │ GRU_base         │ 68.9 K │
│ 2 │ model.RNNCell │ GRU              │ 68.7 K │
│ 3 │ model.predict │ Linear           │    130 │
│ 4 │ model.act     │ Sigmoid          │      0 │
└───┴───────────────┴──────────────────┴────────┘
Trainable params: 68.9 K                                                        
Non-trainable params: 0                                                         
Total params: 68.9 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 39.89it/s v_num: 0.000      
                                                              val/auc: 0.500    
                                                              val/f1: 0.286     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.200 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000             
[2024-06-06 14:36:55,508][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-06-14-06-41/argued_someone/0.0/1>
[2024-06-06 14:36:55,509][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-06 14:36:55,511][HYDRA] 	#22 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=argued_someone data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=gru callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False logger=csv test=False seed=2
[rank: 0] Seed set to 2
[2024-06-06 14:36:55,681][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-06 14:36:55,683][train.py][INFO] - Instantiating model <src.methods.gru.lightningmodule.GRULightningModule>
[2024-06-06 14:36:55,683][train.py][INFO] - Instantiating callbacks...
[2024-06-06 14:36:55,684][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-06 14:36:55,686][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-06 14:36:55,686][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-06 14:36:55,687][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-06 14:36:55,687][train.py][INFO] - Instantiating loggers...
[2024-06-06 14:36:55,687][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-06 14:36:55,689][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-06 14:36:55,717][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-06-14-06-41/argued_someone/0.0/2/csv_artifacts/
┏━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃   ┃ Name          ┃ Type             ┃ Params ┃
┡━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0 │ loss          │ CrossEntropyLoss │      0 │
│ 1 │ model         │ GRU_base         │ 68.9 K │
│ 2 │ model.RNNCell │ GRU              │ 68.7 K │
│ 3 │ model.predict │ Linear           │    130 │
│ 4 │ model.act     │ Sigmoid          │      0 │
└───┴───────────────┴──────────────────┴────────┘
Trainable params: 68.9 K                                                        
Non-trainable params: 0                                                         
Total params: 68.9 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 39.82it/s v_num: 0.000      
                                                              val/auc: 0.350    
                                                              val/f1: 0.133     
                                                              val/precision:    
                                                              0.200 val/recall: 
                                                              0.100 train/auc:  
                                                              0.984 train/f1:   
                                                              0.984             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              0.968             
[2024-06-06 14:37:11,977][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-06-14-06-41/argued_someone/0.0/2>
[2024-06-06 14:37:11,978][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-06 14:37:11,980][HYDRA] 	#23 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=argued_someone data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=gru callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False logger=csv test=False seed=3
[rank: 0] Seed set to 3
[2024-06-06 14:37:12,152][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-06 14:37:12,154][train.py][INFO] - Instantiating model <src.methods.gru.lightningmodule.GRULightningModule>
[2024-06-06 14:37:12,155][train.py][INFO] - Instantiating callbacks...
[2024-06-06 14:37:12,155][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-06 14:37:12,157][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-06 14:37:12,158][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-06 14:37:12,158][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-06 14:37:12,176][train.py][INFO] - Instantiating loggers...
[2024-06-06 14:37:12,176][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-06 14:37:12,178][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-06 14:37:12,210][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-06-14-06-41/argued_someone/0.0/3/csv_artifacts/
┏━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃   ┃ Name          ┃ Type             ┃ Params ┃
┡━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0 │ loss          │ CrossEntropyLoss │      0 │
│ 1 │ model         │ GRU_base         │ 68.9 K │
│ 2 │ model.RNNCell │ GRU              │ 68.7 K │
│ 3 │ model.predict │ Linear           │    130 │
│ 4 │ model.act     │ Sigmoid          │      0 │
└───┴───────────────┴──────────────────┴────────┘
Trainable params: 68.9 K                                                        
Non-trainable params: 0                                                         
Total params: 68.9 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 41.21it/s v_num: 0.000      
                                                              val/auc: 0.500    
                                                              val/f1: 0.286     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.200 train/auc:  
                                                              0.960 train/f1:   
                                                              0.961             
                                                              train/precision:  
                                                              0.925             
                                                              train/recall:     
                                                              1.000             
[2024-06-06 14:37:28,416][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-06-14-06-41/argued_someone/0.0/3>
[2024-06-06 14:37:28,417][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-06 14:37:28,419][HYDRA] 	#24 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=argued_someone data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=gru callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False logger=csv test=False seed=4
[rank: 0] Seed set to 4
[2024-06-06 14:37:28,585][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-06 14:37:28,586][train.py][INFO] - Instantiating model <src.methods.gru.lightningmodule.GRULightningModule>
[2024-06-06 14:37:28,587][train.py][INFO] - Instantiating callbacks...
[2024-06-06 14:37:28,587][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-06 14:37:28,589][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-06 14:37:28,590][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-06 14:37:28,590][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-06 14:37:28,592][train.py][INFO] - Instantiating loggers...
[2024-06-06 14:37:28,592][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-06 14:37:28,593][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-06 14:37:28,658][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-06-14-06-41/argued_someone/0.0/4/csv_artifacts/
┏━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃   ┃ Name          ┃ Type             ┃ Params ┃
┡━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0 │ loss          │ CrossEntropyLoss │      0 │
│ 1 │ model         │ GRU_base         │ 68.9 K │
│ 2 │ model.RNNCell │ GRU              │ 68.7 K │
│ 3 │ model.predict │ Linear           │    130 │
│ 4 │ model.act     │ Sigmoid          │      0 │
└───┴───────────────┴──────────────────┴────────┘
Trainable params: 68.9 K                                                        
Non-trainable params: 0                                                         
Total params: 68.9 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 41.00it/s v_num: 0.000      
                                                              val/auc: 0.450    
                                                              val/f1: 0.000     
                                                              val/precision:    
                                                              0.000 val/recall: 
                                                              0.000 train/auc:  
                                                              0.960 train/f1:   
                                                              0.959             
                                                              train/precision:  
                                                              0.983             
                                                              train/recall:     
                                                              0.935             
[2024-06-06 14:37:44,897][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-06-14-06-41/argued_someone/0.0/4>
[2024-06-06 14:37:44,897][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-06 14:37:44,900][HYDRA] 	#25 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=argued_someone data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=gru callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False logger=csv test=False seed=5
[rank: 0] Seed set to 5
[2024-06-06 14:37:45,072][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-06 14:37:45,074][train.py][INFO] - Instantiating model <src.methods.gru.lightningmodule.GRULightningModule>
[2024-06-06 14:37:45,075][train.py][INFO] - Instantiating callbacks...
[2024-06-06 14:37:45,075][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-06 14:37:45,077][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-06 14:37:45,077][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-06 14:37:45,078][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-06 14:37:45,078][train.py][INFO] - Instantiating loggers...
[2024-06-06 14:37:45,078][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-06 14:37:45,080][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-06 14:37:45,110][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-06-14-06-41/argued_someone/0.0/5/csv_artifacts/
┏━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃   ┃ Name          ┃ Type             ┃ Params ┃
┡━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0 │ loss          │ CrossEntropyLoss │      0 │
│ 1 │ model         │ GRU_base         │ 68.9 K │
│ 2 │ model.RNNCell │ GRU              │ 68.7 K │
│ 3 │ model.predict │ Linear           │    130 │
│ 4 │ model.act     │ Sigmoid          │      0 │
└───┴───────────────┴──────────────────┴────────┘
Trainable params: 68.9 K                                                        
Non-trainable params: 0                                                         
Total params: 68.9 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 39.12it/s v_num: 0.000      
                                                              val/auc: 0.450    
                                                              val/f1: 0.154     
                                                              val/precision:    
                                                              0.333 val/recall: 
                                                              0.100 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000             
[2024-06-06 14:38:01,215][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-06-14-06-41/argued_someone/0.0/5>
[2024-06-06 14:38:01,216][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-06 14:38:01,218][HYDRA] 	#26 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=argued_someone data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=gru callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False logger=csv test=False seed=6
[rank: 0] Seed set to 6
[2024-06-06 14:38:01,388][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-06 14:38:01,389][train.py][INFO] - Instantiating model <src.methods.gru.lightningmodule.GRULightningModule>
[2024-06-06 14:38:01,390][train.py][INFO] - Instantiating callbacks...
[2024-06-06 14:38:01,390][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-06 14:38:01,392][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-06 14:38:01,393][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-06 14:38:01,393][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-06 14:38:01,394][train.py][INFO] - Instantiating loggers...
[2024-06-06 14:38:01,394][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-06 14:38:01,395][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-06 14:38:01,425][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-06-14-06-41/argued_someone/0.0/6/csv_artifacts/
┏━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃   ┃ Name          ┃ Type             ┃ Params ┃
┡━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0 │ loss          │ CrossEntropyLoss │      0 │
│ 1 │ model         │ GRU_base         │ 68.9 K │
│ 2 │ model.RNNCell │ GRU              │ 68.7 K │
│ 3 │ model.predict │ Linear           │    130 │
│ 4 │ model.act     │ Sigmoid          │      0 │
└───┴───────────────┴──────────────────┴────────┘
Trainable params: 68.9 K                                                        
Non-trainable params: 0                                                         
Total params: 68.9 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 40.08it/s v_num: 0.000      
                                                              val/auc: 0.450    
                                                              val/f1: 0.353     
                                                              val/precision:    
                                                              0.429 val/recall: 
                                                              0.300 train/auc:  
                                                              0.952 train/f1:   
                                                              0.953             
                                                              train/precision:  
                                                              0.924             
                                                              train/recall:     
                                                              0.984             
[2024-06-06 14:38:18,120][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-06-14-06-41/argued_someone/0.0/6>
[2024-06-06 14:38:18,120][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-06 14:38:18,123][HYDRA] 	#27 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=argued_someone data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=gru callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False logger=csv test=False seed=7
[rank: 0] Seed set to 7
[2024-06-06 14:38:18,326][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-06 14:38:18,328][train.py][INFO] - Instantiating model <src.methods.gru.lightningmodule.GRULightningModule>
[2024-06-06 14:38:18,329][train.py][INFO] - Instantiating callbacks...
[2024-06-06 14:38:18,329][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-06 14:38:18,331][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-06 14:38:18,331][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-06 14:38:18,332][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-06 14:38:18,369][train.py][INFO] - Instantiating loggers...
[2024-06-06 14:38:18,370][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-06 14:38:18,371][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-06 14:38:18,426][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-06-14-06-41/argued_someone/0.0/7/csv_artifacts/
┏━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃   ┃ Name          ┃ Type             ┃ Params ┃
┡━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0 │ loss          │ CrossEntropyLoss │      0 │
│ 1 │ model         │ GRU_base         │ 68.9 K │
│ 2 │ model.RNNCell │ GRU              │ 68.7 K │
│ 3 │ model.predict │ Linear           │    130 │
│ 4 │ model.act     │ Sigmoid          │      0 │
└───┴───────────────┴──────────────────┴────────┘
Trainable params: 68.9 K                                                        
Non-trainable params: 0                                                         
Total params: 68.9 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 37.70it/s v_num: 0.000      
                                                              val/auc: 0.350    
                                                              val/f1: 0.235     
                                                              val/precision:    
                                                              0.286 val/recall: 
                                                              0.200 train/auc:  
                                                              0.944 train/f1:   
                                                              0.946             
                                                              train/precision:  
                                                              0.910             
                                                              train/recall:     
                                                              0.984             
[2024-06-06 14:38:35,657][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-06-14-06-41/argued_someone/0.0/7>
[2024-06-06 14:38:35,657][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-06 14:38:35,659][HYDRA] 	#28 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=argued_someone data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=gru callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False logger=csv test=False seed=8
[rank: 0] Seed set to 8
[2024-06-06 14:38:35,829][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-06 14:38:35,830][train.py][INFO] - Instantiating model <src.methods.gru.lightningmodule.GRULightningModule>
[2024-06-06 14:38:35,831][train.py][INFO] - Instantiating callbacks...
[2024-06-06 14:38:35,831][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-06 14:38:35,834][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-06 14:38:35,834][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-06 14:38:35,835][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-06 14:38:35,836][train.py][INFO] - Instantiating loggers...
[2024-06-06 14:38:35,836][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-06 14:38:35,837][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-06 14:38:35,901][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-06-14-06-41/argued_someone/0.0/8/csv_artifacts/
┏━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃   ┃ Name          ┃ Type             ┃ Params ┃
┡━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0 │ loss          │ CrossEntropyLoss │      0 │
│ 1 │ model         │ GRU_base         │ 68.9 K │
│ 2 │ model.RNNCell │ GRU              │ 68.7 K │
│ 3 │ model.predict │ Linear           │    130 │
│ 4 │ model.act     │ Sigmoid          │      0 │
└───┴───────────────┴──────────────────┴────────┘
Trainable params: 68.9 K                                                        
Non-trainable params: 0                                                         
Total params: 68.9 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 36.44it/s v_num: 0.000      
                                                              val/auc: 0.350    
                                                              val/f1: 0.133     
                                                              val/precision:    
                                                              0.200 val/recall: 
                                                              0.100 train/auc:  
                                                              0.992 train/f1:   
                                                              0.992             
                                                              train/precision:  
                                                              0.984             
                                                              train/recall:     
                                                              1.000             
[2024-06-06 14:38:52,206][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-06-14-06-41/argued_someone/0.0/8>
[2024-06-06 14:38:52,207][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-06 14:38:52,209][HYDRA] 	#29 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=argued_someone data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=gru callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False logger=csv test=False seed=9
[rank: 0] Seed set to 9
[2024-06-06 14:38:52,383][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-06 14:38:52,385][train.py][INFO] - Instantiating model <src.methods.gru.lightningmodule.GRULightningModule>
[2024-06-06 14:38:52,386][train.py][INFO] - Instantiating callbacks...
[2024-06-06 14:38:52,386][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-06 14:38:52,388][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-06 14:38:52,389][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-06 14:38:52,389][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-06 14:38:52,390][train.py][INFO] - Instantiating loggers...
[2024-06-06 14:38:52,390][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-06 14:38:52,391][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-06 14:38:52,422][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-06-14-06-41/argued_someone/0.0/9/csv_artifacts/
┏━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃   ┃ Name          ┃ Type             ┃ Params ┃
┡━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0 │ loss          │ CrossEntropyLoss │      0 │
│ 1 │ model         │ GRU_base         │ 68.9 K │
│ 2 │ model.RNNCell │ GRU              │ 68.7 K │
│ 3 │ model.predict │ Linear           │    130 │
│ 4 │ model.act     │ Sigmoid          │      0 │
└───┴───────────────┴──────────────────┴────────┘
Trainable params: 68.9 K                                                        
Non-trainable params: 0                                                         
Total params: 68.9 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 38.21it/s v_num: 0.000      
                                                              val/auc: 0.550    
                                                              val/f1: 0.526     
                                                              val/precision:    
                                                              0.556 val/recall: 
                                                              0.500 train/auc:  
                                                              0.968 train/f1:   
                                                              0.967             
                                                              train/precision:  
                                                              0.983             
                                                              train/recall:     
                                                              0.952             
[2024-06-06 14:39:08,654][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-06-14-06-41/argued_someone/0.0/9>
[2024-06-06 14:39:08,655][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-06 14:39:08,658][HYDRA] 	#30 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=controlling_temper data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=gru callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False logger=csv test=False seed=0
[2024-06-06 14:39:08,834][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-06 14:39:08,836][train.py][INFO] - Instantiating model <src.methods.gru.lightningmodule.GRULightningModule>
[2024-06-06 14:39:08,837][train.py][INFO] - Instantiating callbacks...
[2024-06-06 14:39:08,837][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-06 14:39:08,839][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-06 14:39:08,839][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-06 14:39:08,840][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-06 14:39:08,840][train.py][INFO] - Instantiating loggers...
[2024-06-06 14:39:08,841][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-06 14:39:08,842][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-06 14:39:08,873][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-06-14-06-41/controlling_temper/0.0/0/csv_artifacts/
┏━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃   ┃ Name          ┃ Type             ┃ Params ┃
┡━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0 │ loss          │ CrossEntropyLoss │      0 │
│ 1 │ model         │ GRU_base         │ 68.9 K │
│ 2 │ model.RNNCell │ GRU              │ 68.7 K │
│ 3 │ model.predict │ Linear           │    130 │
│ 4 │ model.act     │ Sigmoid          │      0 │
└───┴───────────────┴──────────────────┴────────┘
Trainable params: 68.9 K                                                        
Non-trainable params: 0                                                         
Total params: 68.9 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 39.54it/s v_num: 0.000      
                                                              val/auc: 0.350    
                                                              val/f1: 0.235     
                                                              val/precision:    
                                                              0.286 val/recall: 
                                                              0.200 train/auc:  
                                                              0.941 train/f1:   
                                                              0.943             
                                                              train/precision:  
                                                              0.906             
                                                              train/recall:     
                                                              0.983             
[2024-06-06 14:39:24,997][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-06-14-06-41/controlling_temper/0.0/0>
[2024-06-06 14:39:24,998][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-06 14:39:25,000][HYDRA] 	#31 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=controlling_temper data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=gru callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False logger=csv test=False seed=1
[rank: 0] Seed set to 1
[2024-06-06 14:39:25,177][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-06 14:39:25,179][train.py][INFO] - Instantiating model <src.methods.gru.lightningmodule.GRULightningModule>
[2024-06-06 14:39:25,179][train.py][INFO] - Instantiating callbacks...
[2024-06-06 14:39:25,180][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-06 14:39:25,182][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-06 14:39:25,182][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-06 14:39:25,183][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-06 14:39:25,185][train.py][INFO] - Instantiating loggers...
[2024-06-06 14:39:25,185][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-06 14:39:25,186][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-06 14:39:25,221][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-06-14-06-41/controlling_temper/0.0/1/csv_artifacts/
┏━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃   ┃ Name          ┃ Type             ┃ Params ┃
┡━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0 │ loss          │ CrossEntropyLoss │      0 │
│ 1 │ model         │ GRU_base         │ 68.9 K │
│ 2 │ model.RNNCell │ GRU              │ 68.7 K │
│ 3 │ model.predict │ Linear           │    130 │
│ 4 │ model.act     │ Sigmoid          │      0 │
└───┴───────────────┴──────────────────┴────────┘
Trainable params: 68.9 K                                                        
Non-trainable params: 0                                                         
Total params: 68.9 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 39.58it/s v_num: 0.000      
                                                              val/auc: 0.350    
                                                              val/f1: 0.000     
                                                              val/precision:    
                                                              0.000 val/recall: 
                                                              0.000 train/auc:  
                                                              0.941 train/f1:   
                                                              0.943             
                                                              train/precision:  
                                                              0.906             
                                                              train/recall:     
                                                              0.983             
[2024-06-06 14:39:40,822][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-06-14-06-41/controlling_temper/0.0/1>
[2024-06-06 14:39:40,822][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-06 14:39:40,824][HYDRA] 	#32 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=controlling_temper data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=gru callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False logger=csv test=False seed=2
[rank: 0] Seed set to 2
[2024-06-06 14:39:40,995][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-06 14:39:40,997][train.py][INFO] - Instantiating model <src.methods.gru.lightningmodule.GRULightningModule>
[2024-06-06 14:39:40,997][train.py][INFO] - Instantiating callbacks...
[2024-06-06 14:39:40,998][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-06 14:39:41,000][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-06 14:39:41,000][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-06 14:39:41,001][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-06 14:39:41,002][train.py][INFO] - Instantiating loggers...
[2024-06-06 14:39:41,002][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-06 14:39:41,003][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-06 14:39:41,076][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-06-14-06-41/controlling_temper/0.0/2/csv_artifacts/
┏━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃   ┃ Name          ┃ Type             ┃ Params ┃
┡━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0 │ loss          │ CrossEntropyLoss │      0 │
│ 1 │ model         │ GRU_base         │ 68.9 K │
│ 2 │ model.RNNCell │ GRU              │ 68.7 K │
│ 3 │ model.predict │ Linear           │    130 │
│ 4 │ model.act     │ Sigmoid          │      0 │
└───┴───────────────┴──────────────────┴────────┘
Trainable params: 68.9 K                                                        
Non-trainable params: 0                                                         
Total params: 68.9 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 44.77it/s v_num: 0.000      
                                                              val/auc: 0.450    
                                                              val/f1: 0.353     
                                                              val/precision:    
                                                              0.429 val/recall: 
                                                              0.300 train/auc:  
                                                              0.915 train/f1:   
                                                              0.921             
                                                              train/precision:  
                                                              0.866             
                                                              train/recall:     
                                                              0.983             
[2024-06-06 14:39:56,579][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-06-14-06-41/controlling_temper/0.0/2>
[2024-06-06 14:39:56,579][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-06 14:39:56,582][HYDRA] 	#33 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=controlling_temper data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=gru callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False logger=csv test=False seed=3
[rank: 0] Seed set to 3
[2024-06-06 14:39:56,756][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-06 14:39:56,758][train.py][INFO] - Instantiating model <src.methods.gru.lightningmodule.GRULightningModule>
[2024-06-06 14:39:56,759][train.py][INFO] - Instantiating callbacks...
[2024-06-06 14:39:56,759][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-06 14:39:56,761][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-06 14:39:56,762][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-06 14:39:56,762][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-06 14:39:56,763][train.py][INFO] - Instantiating loggers...
[2024-06-06 14:39:56,763][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-06 14:39:56,764][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-06 14:39:56,794][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-06-14-06-41/controlling_temper/0.0/3/csv_artifacts/
┏━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃   ┃ Name          ┃ Type             ┃ Params ┃
┡━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0 │ loss          │ CrossEntropyLoss │      0 │
│ 1 │ model         │ GRU_base         │ 68.9 K │
│ 2 │ model.RNNCell │ GRU              │ 68.7 K │
│ 3 │ model.predict │ Linear           │    130 │
│ 4 │ model.act     │ Sigmoid          │      0 │
└───┴───────────────┴──────────────────┴────────┘
Trainable params: 68.9 K                                                        
Non-trainable params: 0                                                         
Total params: 68.9 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 45.08it/s v_num: 0.000      
                                                              val/auc: 0.500    
                                                              val/f1: 0.375     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.300 train/auc:  
                                                              0.839 train/f1:   
                                                              0.846             
                                                              train/precision:  
                                                              0.812             
                                                              train/recall:     
                                                              0.881             
[2024-06-06 14:40:12,324][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-06-14-06-41/controlling_temper/0.0/3>
[2024-06-06 14:40:12,324][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-06 14:40:12,326][HYDRA] 	#34 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=controlling_temper data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=gru callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False logger=csv test=False seed=4
[rank: 0] Seed set to 4
[2024-06-06 14:40:12,496][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-06 14:40:12,497][train.py][INFO] - Instantiating model <src.methods.gru.lightningmodule.GRULightningModule>
[2024-06-06 14:40:12,498][train.py][INFO] - Instantiating callbacks...
[2024-06-06 14:40:12,498][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-06 14:40:12,500][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-06 14:40:12,501][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-06 14:40:12,501][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-06 14:40:12,502][train.py][INFO] - Instantiating loggers...
[2024-06-06 14:40:12,502][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-06 14:40:12,503][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-06 14:40:12,531][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-06-14-06-41/controlling_temper/0.0/4/csv_artifacts/
┏━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃   ┃ Name          ┃ Type             ┃ Params ┃
┡━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0 │ loss          │ CrossEntropyLoss │      0 │
│ 1 │ model         │ GRU_base         │ 68.9 K │
│ 2 │ model.RNNCell │ GRU              │ 68.7 K │
│ 3 │ model.predict │ Linear           │    130 │
│ 4 │ model.act     │ Sigmoid          │      0 │
└───┴───────────────┴──────────────────┴────────┘
Trainable params: 68.9 K                                                        
Non-trainable params: 0                                                         
Total params: 68.9 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 38.88it/s v_num: 0.000      
                                                              val/auc: 0.450    
                                                              val/f1: 0.353     
                                                              val/precision:    
                                                              0.429 val/recall: 
                                                              0.300 train/auc:  
                                                              0.924 train/f1:   
                                                              0.926             
                                                              train/precision:  
                                                              0.903             
                                                              train/recall:     
                                                              0.949             
[2024-06-06 14:40:28,710][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-06-14-06-41/controlling_temper/0.0/4>
[2024-06-06 14:40:28,711][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-06 14:40:28,713][HYDRA] 	#35 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=controlling_temper data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=gru callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False logger=csv test=False seed=5
[rank: 0] Seed set to 5
[2024-06-06 14:40:28,887][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-06 14:40:28,889][train.py][INFO] - Instantiating model <src.methods.gru.lightningmodule.GRULightningModule>
[2024-06-06 14:40:28,890][train.py][INFO] - Instantiating callbacks...
[2024-06-06 14:40:28,890][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-06 14:40:28,892][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-06 14:40:28,892][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-06 14:40:28,893][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-06 14:40:28,895][train.py][INFO] - Instantiating loggers...
[2024-06-06 14:40:28,895][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-06 14:40:28,897][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-06 14:40:28,928][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-06-14-06-41/controlling_temper/0.0/5/csv_artifacts/
┏━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃   ┃ Name          ┃ Type             ┃ Params ┃
┡━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0 │ loss          │ CrossEntropyLoss │      0 │
│ 1 │ model         │ GRU_base         │ 68.9 K │
│ 2 │ model.RNNCell │ GRU              │ 68.7 K │
│ 3 │ model.predict │ Linear           │    130 │
│ 4 │ model.act     │ Sigmoid          │      0 │
└───┴───────────────┴──────────────────┴────────┘
Trainable params: 68.9 K                                                        
Non-trainable params: 0                                                         
Total params: 68.9 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 41.85it/s v_num: 0.000      
                                                              val/auc: 0.500    
                                                              val/f1: 0.500     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.500 train/auc:  
                                                              0.864 train/f1:   
                                                              0.867             
                                                              train/precision:  
                                                              0.852             
                                                              train/recall:     
                                                              0.881             
[2024-06-06 14:40:44,788][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-06-14-06-41/controlling_temper/0.0/5>
[2024-06-06 14:40:44,789][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-06 14:40:44,791][HYDRA] 	#36 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=controlling_temper data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=gru callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False logger=csv test=False seed=6
[rank: 0] Seed set to 6
[2024-06-06 14:40:45,001][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-06 14:40:45,002][train.py][INFO] - Instantiating model <src.methods.gru.lightningmodule.GRULightningModule>
[2024-06-06 14:40:45,003][train.py][INFO] - Instantiating callbacks...
[2024-06-06 14:40:45,003][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-06 14:40:45,005][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-06 14:40:45,006][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-06 14:40:45,006][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-06 14:40:45,007][train.py][INFO] - Instantiating loggers...
[2024-06-06 14:40:45,008][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-06 14:40:45,009][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-06 14:40:45,074][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-06-14-06-41/controlling_temper/0.0/6/csv_artifacts/
┏━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃   ┃ Name          ┃ Type             ┃ Params ┃
┡━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0 │ loss          │ CrossEntropyLoss │      0 │
│ 1 │ model         │ GRU_base         │ 68.9 K │
│ 2 │ model.RNNCell │ GRU              │ 68.7 K │
│ 3 │ model.predict │ Linear           │    130 │
│ 4 │ model.act     │ Sigmoid          │      0 │
└───┴───────────────┴──────────────────┴────────┘
Trainable params: 68.9 K                                                        
Non-trainable params: 0                                                         
Total params: 68.9 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 39.11it/s v_num: 0.000      
                                                              val/auc: 0.450    
                                                              val/f1: 0.353     
                                                              val/precision:    
                                                              0.429 val/recall: 
                                                              0.300 train/auc:  
                                                              0.966 train/f1:   
                                                              0.966             
                                                              train/precision:  
                                                              0.966             
                                                              train/recall:     
                                                              0.966             
[2024-06-06 14:41:01,474][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-06-14-06-41/controlling_temper/0.0/6>
[2024-06-06 14:41:01,475][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-06 14:41:01,500][HYDRA] 	#37 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=controlling_temper data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=gru callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False logger=csv test=False seed=7
[rank: 0] Seed set to 7
[2024-06-06 14:41:01,678][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-06 14:41:01,680][train.py][INFO] - Instantiating model <src.methods.gru.lightningmodule.GRULightningModule>
[2024-06-06 14:41:01,681][train.py][INFO] - Instantiating callbacks...
[2024-06-06 14:41:01,681][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-06 14:41:01,683][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-06 14:41:01,684][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-06 14:41:01,684][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-06 14:41:01,685][train.py][INFO] - Instantiating loggers...
[2024-06-06 14:41:01,685][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-06 14:41:01,686][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-06 14:41:01,721][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-06-14-06-41/controlling_temper/0.0/7/csv_artifacts/
┏━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃   ┃ Name          ┃ Type             ┃ Params ┃
┡━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0 │ loss          │ CrossEntropyLoss │      0 │
│ 1 │ model         │ GRU_base         │ 68.9 K │
│ 2 │ model.RNNCell │ GRU              │ 68.7 K │
│ 3 │ model.predict │ Linear           │    130 │
│ 4 │ model.act     │ Sigmoid          │      0 │
└───┴───────────────┴──────────────────┴────────┘
Trainable params: 68.9 K                                                        
Non-trainable params: 0                                                         
Total params: 68.9 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 39.88it/s v_num: 0.000      
                                                              val/auc: 0.400    
                                                              val/f1: 0.143     
                                                              val/precision:    
                                                              0.250 val/recall: 
                                                              0.100 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000             
[2024-06-06 14:41:18,035][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-06-14-06-41/controlling_temper/0.0/7>
[2024-06-06 14:41:18,035][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-06 14:41:18,037][HYDRA] 	#38 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=controlling_temper data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=gru callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False logger=csv test=False seed=8
[rank: 0] Seed set to 8
[2024-06-06 14:41:18,204][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-06 14:41:18,205][train.py][INFO] - Instantiating model <src.methods.gru.lightningmodule.GRULightningModule>
[2024-06-06 14:41:18,206][train.py][INFO] - Instantiating callbacks...
[2024-06-06 14:41:18,206][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-06 14:41:18,208][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-06 14:41:18,209][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-06 14:41:18,209][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-06 14:41:18,210][train.py][INFO] - Instantiating loggers...
[2024-06-06 14:41:18,210][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-06 14:41:18,211][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-06 14:41:18,240][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-06-14-06-41/controlling_temper/0.0/8/csv_artifacts/
┏━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃   ┃ Name          ┃ Type             ┃ Params ┃
┡━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0 │ loss          │ CrossEntropyLoss │      0 │
│ 1 │ model         │ GRU_base         │ 68.9 K │
│ 2 │ model.RNNCell │ GRU              │ 68.7 K │
│ 3 │ model.predict │ Linear           │    130 │
│ 4 │ model.act     │ Sigmoid          │      0 │
└───┴───────────────┴──────────────────┴────────┘
Trainable params: 68.9 K                                                        
Non-trainable params: 0                                                         
Total params: 68.9 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 41.38it/s v_num: 0.000      
                                                              val/auc: 0.500    
                                                              val/f1: 0.286     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.200 train/auc:  
                                                              0.958 train/f1:   
                                                              0.958             
                                                              train/precision:  
                                                              0.950             
                                                              train/recall:     
                                                              0.966             
[2024-06-06 14:41:34,186][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-06-14-06-41/controlling_temper/0.0/8>
[2024-06-06 14:41:34,187][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-06 14:41:34,190][HYDRA] 	#39 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=controlling_temper data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=gru callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False logger=csv test=False seed=9
[rank: 0] Seed set to 9
[2024-06-06 14:41:34,563][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-06 14:41:34,564][train.py][INFO] - Instantiating model <src.methods.gru.lightningmodule.GRULightningModule>
[2024-06-06 14:41:34,565][train.py][INFO] - Instantiating callbacks...
[2024-06-06 14:41:34,566][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-06 14:41:34,568][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-06 14:41:34,568][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-06 14:41:34,569][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-06 14:41:34,571][train.py][INFO] - Instantiating loggers...
[2024-06-06 14:41:34,571][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-06 14:41:34,572][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-06 14:41:34,604][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-06-14-06-41/controlling_temper/0.0/9/csv_artifacts/
┏━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃   ┃ Name          ┃ Type             ┃ Params ┃
┡━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0 │ loss          │ CrossEntropyLoss │      0 │
│ 1 │ model         │ GRU_base         │ 68.9 K │
│ 2 │ model.RNNCell │ GRU              │ 68.7 K │
│ 3 │ model.predict │ Linear           │    130 │
│ 4 │ model.act     │ Sigmoid          │      0 │
└───┴───────────────┴──────────────────┴────────┘
Trainable params: 68.9 K                                                        
Non-trainable params: 0                                                         
Total params: 68.9 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 44.73it/s v_num: 0.000      
                                                              val/auc: 0.450    
                                                              val/f1: 0.154     
                                                              val/precision:    
                                                              0.333 val/recall: 
                                                              0.100 train/auc:  
                                                              0.975 train/f1:   
                                                              0.975             
                                                              train/precision:  
                                                              0.967             
                                                              train/recall:     
                                                              0.983             
[2024-06-06 14:41:49,896][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-06-14-06-41/controlling_temper/0.0/9>
[2024-06-06 14:41:49,897][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-06 14:41:49,899][HYDRA] 	#40 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_depression data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=gru callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False logger=csv test=False seed=0
[2024-06-06 14:41:50,079][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-06 14:41:50,080][train.py][INFO] - Instantiating model <src.methods.gru.lightningmodule.GRULightningModule>
[2024-06-06 14:41:50,081][train.py][INFO] - Instantiating callbacks...
[2024-06-06 14:41:50,081][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-06 14:41:50,083][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-06 14:41:50,084][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-06 14:41:50,084][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-06 14:41:50,085][train.py][INFO] - Instantiating loggers...
[2024-06-06 14:41:50,086][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-06 14:41:50,087][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-06 14:41:50,167][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-06-14-06-41/diagnosed_depression/0.0/0/csv_artifacts/
┏━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃   ┃ Name          ┃ Type             ┃ Params ┃
┡━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0 │ loss          │ CrossEntropyLoss │      0 │
│ 1 │ model         │ GRU_base         │ 68.9 K │
│ 2 │ model.RNNCell │ GRU              │ 68.7 K │
│ 3 │ model.predict │ Linear           │    130 │
│ 4 │ model.act     │ Sigmoid          │      0 │
└───┴───────────────┴──────────────────┴────────┘
Trainable params: 68.9 K                                                        
Non-trainable params: 0                                                         
Total params: 68.9 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 40.66it/s v_num: 0.000      
                                                              val/auc: 0.500    
                                                              val/f1: 0.000     
                                                              val/precision:    
                                                              0.000 val/recall: 
                                                              0.000 train/auc:  
                                                              0.532 train/f1:   
                                                              0.169             
                                                              train/precision:  
                                                              0.750             
                                                              train/recall:     
                                                              0.095             
[2024-06-06 14:42:05,906][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-06-14-06-41/diagnosed_depression/0.0/0>
[2024-06-06 14:42:05,907][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-06 14:42:05,910][HYDRA] 	#41 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_depression data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=gru callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False logger=csv test=False seed=1
[rank: 0] Seed set to 1
[2024-06-06 14:42:06,085][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-06 14:42:06,087][train.py][INFO] - Instantiating model <src.methods.gru.lightningmodule.GRULightningModule>
[2024-06-06 14:42:06,088][train.py][INFO] - Instantiating callbacks...
[2024-06-06 14:42:06,088][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-06 14:42:06,090][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-06 14:42:06,091][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-06 14:42:06,091][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-06 14:42:06,092][train.py][INFO] - Instantiating loggers...
[2024-06-06 14:42:06,092][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-06 14:42:06,093][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-06 14:42:06,122][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-06-14-06-41/diagnosed_depression/0.0/1/csv_artifacts/
┏━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃   ┃ Name          ┃ Type             ┃ Params ┃
┡━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0 │ loss          │ CrossEntropyLoss │      0 │
│ 1 │ model         │ GRU_base         │ 68.9 K │
│ 2 │ model.RNNCell │ GRU              │ 68.7 K │
│ 3 │ model.predict │ Linear           │    130 │
│ 4 │ model.act     │ Sigmoid          │      0 │
└───┴───────────────┴──────────────────┴────────┘
Trainable params: 68.9 K                                                        
Non-trainable params: 0                                                         
Total params: 68.9 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 41.81it/s v_num: 0.000      
                                                              val/auc: 0.500    
                                                              val/f1: 0.000     
                                                              val/precision:    
                                                              0.000 val/recall: 
                                                              0.000 train/auc:  
                                                              0.548 train/f1:   
                                                              0.174             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              0.095             
[2024-06-06 14:42:21,578][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-06-14-06-41/diagnosed_depression/0.0/1>
[2024-06-06 14:42:21,578][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-06 14:42:21,580][HYDRA] 	#42 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_depression data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=gru callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False logger=csv test=False seed=2
[rank: 0] Seed set to 2
[2024-06-06 14:42:21,749][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-06 14:42:21,750][train.py][INFO] - Instantiating model <src.methods.gru.lightningmodule.GRULightningModule>
[2024-06-06 14:42:21,751][train.py][INFO] - Instantiating callbacks...
[2024-06-06 14:42:21,751][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-06 14:42:21,753][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-06 14:42:21,754][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-06 14:42:21,754][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-06 14:42:21,755][train.py][INFO] - Instantiating loggers...
[2024-06-06 14:42:21,755][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-06 14:42:21,756][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-06 14:42:21,784][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-06-14-06-41/diagnosed_depression/0.0/2/csv_artifacts/
┏━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃   ┃ Name          ┃ Type             ┃ Params ┃
┡━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0 │ loss          │ CrossEntropyLoss │      0 │
│ 1 │ model         │ GRU_base         │ 68.9 K │
│ 2 │ model.RNNCell │ GRU              │ 68.7 K │
│ 3 │ model.predict │ Linear           │    130 │
│ 4 │ model.act     │ Sigmoid          │      0 │
└───┴───────────────┴──────────────────┴────────┘
Trainable params: 68.9 K                                                        
Non-trainable params: 0                                                         
Total params: 68.9 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 43.20it/s v_num: 0.000      
                                                              val/auc: 0.500    
                                                              val/f1: 0.000     
                                                              val/precision:    
                                                              0.000 val/recall: 
                                                              0.000 train/auc:  
                                                              0.548 train/f1:   
                                                              0.174             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              0.095             
[2024-06-06 14:42:37,050][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-06-14-06-41/diagnosed_depression/0.0/2>
[2024-06-06 14:42:37,051][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-06 14:42:37,053][HYDRA] 	#43 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_depression data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=gru callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False logger=csv test=False seed=3
[rank: 0] Seed set to 3
[2024-06-06 14:42:37,229][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-06 14:42:37,231][train.py][INFO] - Instantiating model <src.methods.gru.lightningmodule.GRULightningModule>
[2024-06-06 14:42:37,232][train.py][INFO] - Instantiating callbacks...
[2024-06-06 14:42:37,232][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-06 14:42:37,234][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-06 14:42:37,235][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-06 14:42:37,235][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-06 14:42:37,237][train.py][INFO] - Instantiating loggers...
[2024-06-06 14:42:37,237][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-06 14:42:37,239][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-06 14:42:37,271][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-06-14-06-41/diagnosed_depression/0.0/3/csv_artifacts/
┏━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃   ┃ Name          ┃ Type             ┃ Params ┃
┡━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0 │ loss          │ CrossEntropyLoss │      0 │
│ 1 │ model         │ GRU_base         │ 68.9 K │
│ 2 │ model.RNNCell │ GRU              │ 68.7 K │
│ 3 │ model.predict │ Linear           │    130 │
│ 4 │ model.act     │ Sigmoid          │      0 │
└───┴───────────────┴──────────────────┴────────┘
Trainable params: 68.9 K                                                        
Non-trainable params: 0                                                         
Total params: 68.9 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 44.05it/s v_num: 0.000      
                                                              val/auc: 0.500    
                                                              val/f1: 0.000     
                                                              val/precision:    
                                                              0.000 val/recall: 
                                                              0.000 train/auc:  
                                                              0.540 train/f1:   
                                                              0.171             
                                                              train/precision:  
                                                              0.857             
                                                              train/recall:     
                                                              0.095             
[2024-06-06 14:42:52,613][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-06-14-06-41/diagnosed_depression/0.0/3>
[2024-06-06 14:42:52,614][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-06 14:42:52,616][HYDRA] 	#44 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_depression data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=gru callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False logger=csv test=False seed=4
[rank: 0] Seed set to 4
[2024-06-06 14:42:52,784][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-06 14:42:52,786][train.py][INFO] - Instantiating model <src.methods.gru.lightningmodule.GRULightningModule>
[2024-06-06 14:42:52,787][train.py][INFO] - Instantiating callbacks...
[2024-06-06 14:42:52,787][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-06 14:42:52,789][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-06 14:42:52,790][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-06 14:42:52,790][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-06 14:42:52,791][train.py][INFO] - Instantiating loggers...
[2024-06-06 14:42:52,792][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-06 14:42:52,793][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-06 14:42:52,858][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-06-14-06-41/diagnosed_depression/0.0/4/csv_artifacts/
┏━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃   ┃ Name          ┃ Type             ┃ Params ┃
┡━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0 │ loss          │ CrossEntropyLoss │      0 │
│ 1 │ model         │ GRU_base         │ 68.9 K │
│ 2 │ model.RNNCell │ GRU              │ 68.7 K │
│ 3 │ model.predict │ Linear           │    130 │
│ 4 │ model.act     │ Sigmoid          │      0 │
└───┴───────────────┴──────────────────┴────────┘
Trainable params: 68.9 K                                                        
Non-trainable params: 0                                                         
Total params: 68.9 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 43.47it/s v_num: 0.000      
                                                              val/auc: 0.550    
                                                              val/f1: 0.526     
                                                              val/precision:    
                                                              0.357 val/recall: 
                                                              1.000 train/auc:  
                                                              0.587 train/f1:   
                                                              0.409             
                                                              train/precision:  
                                                              0.720             
                                                              train/recall:     
                                                              0.286             
[2024-06-06 14:43:08,296][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-06-14-06-41/diagnosed_depression/0.0/4>
[2024-06-06 14:43:08,296][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-06 14:43:08,298][HYDRA] 	#45 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_depression data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=gru callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False logger=csv test=False seed=5
[rank: 0] Seed set to 5
[2024-06-06 14:43:08,472][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-06 14:43:08,473][train.py][INFO] - Instantiating model <src.methods.gru.lightningmodule.GRULightningModule>
[2024-06-06 14:43:08,474][train.py][INFO] - Instantiating callbacks...
[2024-06-06 14:43:08,474][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-06 14:43:08,477][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-06 14:43:08,477][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-06 14:43:08,478][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-06 14:43:08,478][train.py][INFO] - Instantiating loggers...
[2024-06-06 14:43:08,478][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-06 14:43:08,480][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-06 14:43:08,512][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-06-14-06-41/diagnosed_depression/0.0/5/csv_artifacts/
┏━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃   ┃ Name          ┃ Type             ┃ Params ┃
┡━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0 │ loss          │ CrossEntropyLoss │      0 │
│ 1 │ model         │ GRU_base         │ 68.9 K │
│ 2 │ model.RNNCell │ GRU              │ 68.7 K │
│ 3 │ model.predict │ Linear           │    130 │
│ 4 │ model.act     │ Sigmoid          │      0 │
└───┴───────────────┴──────────────────┴────────┘
Trainable params: 68.9 K                                                        
Non-trainable params: 0                                                         
Total params: 68.9 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 43.24it/s v_num: 0.000      
                                                              val/auc: 0.500    
                                                              val/f1: 0.000     
                                                              val/precision:    
                                                              0.000 val/recall: 
                                                              0.000 train/auc:  
                                                              0.548 train/f1:   
                                                              0.174             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              0.095             
[2024-06-06 14:43:23,878][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-06-14-06-41/diagnosed_depression/0.0/5>
[2024-06-06 14:43:23,879][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-06 14:43:23,883][HYDRA] 	#46 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_depression data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=gru callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False logger=csv test=False seed=6
[rank: 0] Seed set to 6
[2024-06-06 14:43:24,053][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-06 14:43:24,055][train.py][INFO] - Instantiating model <src.methods.gru.lightningmodule.GRULightningModule>
[2024-06-06 14:43:24,056][train.py][INFO] - Instantiating callbacks...
[2024-06-06 14:43:24,056][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-06 14:43:24,058][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-06 14:43:24,058][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-06 14:43:24,059][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-06 14:43:24,059][train.py][INFO] - Instantiating loggers...
[2024-06-06 14:43:24,060][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-06 14:43:24,061][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-06 14:43:24,090][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-06-14-06-41/diagnosed_depression/0.0/6/csv_artifacts/
┏━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃   ┃ Name          ┃ Type             ┃ Params ┃
┡━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0 │ loss          │ CrossEntropyLoss │      0 │
│ 1 │ model         │ GRU_base         │ 68.9 K │
│ 2 │ model.RNNCell │ GRU              │ 68.7 K │
│ 3 │ model.predict │ Linear           │    130 │
│ 4 │ model.act     │ Sigmoid          │      0 │
└───┴───────────────┴──────────────────┴────────┘
Trainable params: 68.9 K                                                        
Non-trainable params: 0                                                         
Total params: 68.9 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 37.90it/s v_num: 0.000      
                                                              val/auc: 0.500    
                                                              val/f1: 0.000     
                                                              val/precision:    
                                                              0.000 val/recall: 
                                                              0.000 train/auc:  
                                                              0.563 train/f1:   
                                                              0.286             
                                                              train/precision:  
                                                              0.786             
                                                              train/recall:     
                                                              0.175             
[2024-06-06 14:43:40,248][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-06-14-06-41/diagnosed_depression/0.0/6>
[2024-06-06 14:43:40,249][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-06 14:43:40,251][HYDRA] 	#47 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_depression data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=gru callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False logger=csv test=False seed=7
[rank: 0] Seed set to 7
[2024-06-06 14:43:40,431][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-06 14:43:40,433][train.py][INFO] - Instantiating model <src.methods.gru.lightningmodule.GRULightningModule>
[2024-06-06 14:43:40,434][train.py][INFO] - Instantiating callbacks...
[2024-06-06 14:43:40,434][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-06 14:43:40,436][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-06 14:43:40,437][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-06 14:43:40,437][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-06 14:43:40,440][train.py][INFO] - Instantiating loggers...
[2024-06-06 14:43:40,440][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-06 14:43:40,441][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-06 14:43:40,474][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-06-14-06-41/diagnosed_depression/0.0/7/csv_artifacts/
┏━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃   ┃ Name          ┃ Type             ┃ Params ┃
┡━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0 │ loss          │ CrossEntropyLoss │      0 │
│ 1 │ model         │ GRU_base         │ 68.9 K │
│ 2 │ model.RNNCell │ GRU              │ 68.7 K │
│ 3 │ model.predict │ Linear           │    130 │
│ 4 │ model.act     │ Sigmoid          │      0 │
└───┴───────────────┴──────────────────┴────────┘
Trainable params: 68.9 K                                                        
Non-trainable params: 0                                                         
Total params: 68.9 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 37.43it/s v_num: 0.000      
                                                              val/auc: 0.500    
                                                              val/f1: 0.000     
                                                              val/precision:    
                                                              0.000 val/recall: 
                                                              0.000 train/auc:  
                                                              0.548 train/f1:   
                                                              0.174             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              0.095             
[2024-06-06 14:43:56,525][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-06-14-06-41/diagnosed_depression/0.0/7>
[2024-06-06 14:43:56,526][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-06 14:43:56,528][HYDRA] 	#48 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_depression data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=gru callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False logger=csv test=False seed=8
[rank: 0] Seed set to 8
[2024-06-06 14:43:56,697][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-06 14:43:56,698][train.py][INFO] - Instantiating model <src.methods.gru.lightningmodule.GRULightningModule>
[2024-06-06 14:43:56,699][train.py][INFO] - Instantiating callbacks...
[2024-06-06 14:43:56,700][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-06 14:43:56,702][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-06 14:43:56,702][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-06 14:43:56,703][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-06 14:43:56,704][train.py][INFO] - Instantiating loggers...
[2024-06-06 14:43:56,704][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-06 14:43:56,706][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-06 14:43:56,781][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-06-14-06-41/diagnosed_depression/0.0/8/csv_artifacts/
┏━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃   ┃ Name          ┃ Type             ┃ Params ┃
┡━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0 │ loss          │ CrossEntropyLoss │      0 │
│ 1 │ model         │ GRU_base         │ 68.9 K │
│ 2 │ model.RNNCell │ GRU              │ 68.7 K │
│ 3 │ model.predict │ Linear           │    130 │
│ 4 │ model.act     │ Sigmoid          │      0 │
└───┴───────────────┴──────────────────┴────────┘
Trainable params: 68.9 K                                                        
Non-trainable params: 0                                                         
Total params: 68.9 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 37.02it/s v_num: 0.000      
                                                              val/auc: 0.500    
                                                              val/f1: 0.000     
                                                              val/precision:    
                                                              0.000 val/recall: 
                                                              0.000 train/auc:  
                                                              0.524 train/f1:   
                                                              0.189             
                                                              train/precision:  
                                                              0.636             
                                                              train/recall:     
                                                              0.111             
[2024-06-06 14:44:12,906][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-06-14-06-41/diagnosed_depression/0.0/8>
[2024-06-06 14:44:12,907][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-06 14:44:12,910][HYDRA] 	#49 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_depression data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=gru callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False logger=csv test=False seed=9
[rank: 0] Seed set to 9
[2024-06-06 14:44:13,113][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-06 14:44:13,115][train.py][INFO] - Instantiating model <src.methods.gru.lightningmodule.GRULightningModule>
[2024-06-06 14:44:13,116][train.py][INFO] - Instantiating callbacks...
[2024-06-06 14:44:13,116][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-06 14:44:13,118][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-06 14:44:13,118][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-06 14:44:13,119][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-06 14:44:13,119][train.py][INFO] - Instantiating loggers...
[2024-06-06 14:44:13,120][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-06 14:44:13,121][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-06 14:44:13,153][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-06-14-06-41/diagnosed_depression/0.0/9/csv_artifacts/
┏━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃   ┃ Name          ┃ Type             ┃ Params ┃
┡━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0 │ loss          │ CrossEntropyLoss │      0 │
│ 1 │ model         │ GRU_base         │ 68.9 K │
│ 2 │ model.RNNCell │ GRU              │ 68.7 K │
│ 3 │ model.predict │ Linear           │    130 │
│ 4 │ model.act     │ Sigmoid          │      0 │
└───┴───────────────┴──────────────────┴────────┘
Trainable params: 68.9 K                                                        
Non-trainable params: 0                                                         
Total params: 68.9 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 35.26it/s v_num: 0.000      
                                                              val/auc: 0.500    
                                                              val/f1: 0.000     
                                                              val/precision:    
                                                              0.000 val/recall: 
                                                              0.000 train/auc:  
                                                              0.476 train/f1:   
                                                              0.441             
                                                              train/precision:  
                                                              0.473             
                                                              train/recall:     
                                                              0.413             
[2024-06-06 14:44:29,469][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-06-14-06-41/diagnosed_depression/0.0/9>
[2024-06-06 14:44:29,470][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-06 14:44:29,473][HYDRA] 	#50 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_p_t_s_d data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=gru callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False logger=csv test=False seed=0
[2024-06-06 14:44:29,647][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-06 14:44:29,648][train.py][INFO] - Instantiating model <src.methods.gru.lightningmodule.GRULightningModule>
[2024-06-06 14:44:29,649][train.py][INFO] - Instantiating callbacks...
[2024-06-06 14:44:29,650][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-06 14:44:29,652][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-06 14:44:29,652][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-06 14:44:29,653][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-06 14:44:29,653][train.py][INFO] - Instantiating loggers...
[2024-06-06 14:44:29,653][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-06 14:44:29,655][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-06 14:44:29,685][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-06-14-06-41/diagnosed_p_t_s_d/0.0/0/csv_artifacts/
┏━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃   ┃ Name          ┃ Type             ┃ Params ┃
┡━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0 │ loss          │ CrossEntropyLoss │      0 │
│ 1 │ model         │ GRU_base         │ 68.9 K │
│ 2 │ model.RNNCell │ GRU              │ 68.7 K │
│ 3 │ model.predict │ Linear           │    130 │
│ 4 │ model.act     │ Sigmoid          │      0 │
└───┴───────────────┴──────────────────┴────────┘
Trainable params: 68.9 K                                                        
Non-trainable params: 0                                                         
Total params: 68.9 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 43.19it/s v_num: 0.000      
                                                              val/auc: 0.587    
                                                              val/f1: 0.400     
                                                              val/precision:    
                                                              0.667 val/recall: 
                                                              0.286 train/auc:  
                                                              0.974 train/f1:   
                                                              0.974             
                                                              train/precision:  
                                                              0.966             
                                                              train/recall:     
                                                              0.982             
[2024-06-06 14:44:44,976][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-06-14-06-41/diagnosed_p_t_s_d/0.0/0>
[2024-06-06 14:44:44,977][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-06 14:44:44,979][HYDRA] 	#51 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_p_t_s_d data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=gru callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False logger=csv test=False seed=1
[rank: 0] Seed set to 1
[2024-06-06 14:44:45,151][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-06 14:44:45,153][train.py][INFO] - Instantiating model <src.methods.gru.lightningmodule.GRULightningModule>
[2024-06-06 14:44:45,154][train.py][INFO] - Instantiating callbacks...
[2024-06-06 14:44:45,154][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-06 14:44:45,156][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-06 14:44:45,157][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-06 14:44:45,157][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-06 14:44:45,159][train.py][INFO] - Instantiating loggers...
[2024-06-06 14:44:45,160][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-06 14:44:45,161][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-06 14:44:45,192][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-06-14-06-41/diagnosed_p_t_s_d/0.0/1/csv_artifacts/
┏━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃   ┃ Name          ┃ Type             ┃ Params ┃
┡━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0 │ loss          │ CrossEntropyLoss │      0 │
│ 1 │ model         │ GRU_base         │ 68.9 K │
│ 2 │ model.RNNCell │ GRU              │ 68.7 K │
│ 3 │ model.predict │ Linear           │    130 │
│ 4 │ model.act     │ Sigmoid          │      0 │
└───┴───────────────┴──────────────────┴────────┘
Trainable params: 68.9 K                                                        
Non-trainable params: 0                                                         
Total params: 68.9 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 43.55it/s v_num: 0.000      
                                                              val/auc: 0.516    
                                                              val/f1: 0.222     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.143 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000             
[2024-06-06 14:44:59,752][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-06-14-06-41/diagnosed_p_t_s_d/0.0/1>
[2024-06-06 14:44:59,752][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-06 14:44:59,755][HYDRA] 	#52 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_p_t_s_d data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=gru callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False logger=csv test=False seed=2
[rank: 0] Seed set to 2
[2024-06-06 14:44:59,922][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-06 14:44:59,923][train.py][INFO] - Instantiating model <src.methods.gru.lightningmodule.GRULightningModule>
[2024-06-06 14:44:59,924][train.py][INFO] - Instantiating callbacks...
[2024-06-06 14:44:59,925][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-06 14:44:59,927][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-06 14:44:59,927][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-06 14:44:59,928][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-06 14:44:59,929][train.py][INFO] - Instantiating loggers...
[2024-06-06 14:44:59,929][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-06 14:44:59,930][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-06 14:44:59,996][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-06-14-06-41/diagnosed_p_t_s_d/0.0/2/csv_artifacts/
┏━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃   ┃ Name          ┃ Type             ┃ Params ┃
┡━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0 │ loss          │ CrossEntropyLoss │      0 │
│ 1 │ model         │ GRU_base         │ 68.9 K │
│ 2 │ model.RNNCell │ GRU              │ 68.7 K │
│ 3 │ model.predict │ Linear           │    130 │
│ 4 │ model.act     │ Sigmoid          │      0 │
└───┴───────────────┴──────────────────┴────────┘
Trainable params: 68.9 K                                                        
Non-trainable params: 0                                                         
Total params: 68.9 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 44.88it/s v_num: 0.000      
                                                              val/auc: 0.516    
                                                              val/f1: 0.222     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.143 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000             
[2024-06-06 14:45:14,840][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-06-14-06-41/diagnosed_p_t_s_d/0.0/2>
[2024-06-06 14:45:14,841][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-06 14:45:14,843][HYDRA] 	#53 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_p_t_s_d data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=gru callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False logger=csv test=False seed=3
[rank: 0] Seed set to 3
[2024-06-06 14:45:15,016][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-06 14:45:15,018][train.py][INFO] - Instantiating model <src.methods.gru.lightningmodule.GRULightningModule>
[2024-06-06 14:45:15,019][train.py][INFO] - Instantiating callbacks...
[2024-06-06 14:45:15,019][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-06 14:45:15,021][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-06 14:45:15,021][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-06 14:45:15,022][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-06 14:45:15,022][train.py][INFO] - Instantiating loggers...
[2024-06-06 14:45:15,022][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-06 14:45:15,024][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-06 14:45:15,054][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-06-14-06-41/diagnosed_p_t_s_d/0.0/3/csv_artifacts/
┏━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃   ┃ Name          ┃ Type             ┃ Params ┃
┡━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0 │ loss          │ CrossEntropyLoss │      0 │
│ 1 │ model         │ GRU_base         │ 68.9 K │
│ 2 │ model.RNNCell │ GRU              │ 68.7 K │
│ 3 │ model.predict │ Linear           │    130 │
│ 4 │ model.act     │ Sigmoid          │      0 │
└───┴───────────────┴──────────────────┴────────┘
Trainable params: 68.9 K                                                        
Non-trainable params: 0                                                         
Total params: 68.9 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 44.23it/s v_num: 0.000      
                                                              val/auc: 0.516    
                                                              val/f1: 0.222     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.143 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000             
[2024-06-06 14:45:29,579][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-06-14-06-41/diagnosed_p_t_s_d/0.0/3>
[2024-06-06 14:45:29,580][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-06 14:45:29,582][HYDRA] 	#54 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_p_t_s_d data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=gru callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False logger=csv test=False seed=4
[rank: 0] Seed set to 4
[2024-06-06 14:45:29,750][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-06 14:45:29,751][train.py][INFO] - Instantiating model <src.methods.gru.lightningmodule.GRULightningModule>
[2024-06-06 14:45:29,752][train.py][INFO] - Instantiating callbacks...
[2024-06-06 14:45:29,752][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-06 14:45:29,754][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-06 14:45:29,755][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-06 14:45:29,755][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-06 14:45:29,756][train.py][INFO] - Instantiating loggers...
[2024-06-06 14:45:29,756][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-06 14:45:29,757][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-06 14:45:29,787][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-06-14-06-41/diagnosed_p_t_s_d/0.0/4/csv_artifacts/
┏━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃   ┃ Name          ┃ Type             ┃ Params ┃
┡━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0 │ loss          │ CrossEntropyLoss │      0 │
│ 1 │ model         │ GRU_base         │ 68.9 K │
│ 2 │ model.RNNCell │ GRU              │ 68.7 K │
│ 3 │ model.predict │ Linear           │    130 │
│ 4 │ model.act     │ Sigmoid          │      0 │
└───┴───────────────┴──────────────────┴────────┘
Trainable params: 68.9 K                                                        
Non-trainable params: 0                                                         
Total params: 68.9 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 44.28it/s v_num: 0.000      
                                                              val/auc: 0.516    
                                                              val/f1: 0.222     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.143 train/auc:  
                                                              0.982 train/f1:   
                                                              0.982             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              0.965             
[2024-06-06 14:45:44,355][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-06-14-06-41/diagnosed_p_t_s_d/0.0/4>
[2024-06-06 14:45:44,356][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-06 14:45:44,358][HYDRA] 	#55 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_p_t_s_d data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=gru callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False logger=csv test=False seed=5
[rank: 0] Seed set to 5
[2024-06-06 14:45:44,530][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-06 14:45:44,531][train.py][INFO] - Instantiating model <src.methods.gru.lightningmodule.GRULightningModule>
[2024-06-06 14:45:44,532][train.py][INFO] - Instantiating callbacks...
[2024-06-06 14:45:44,533][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-06 14:45:44,535][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-06 14:45:44,535][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-06 14:45:44,536][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-06 14:45:44,536][train.py][INFO] - Instantiating loggers...
[2024-06-06 14:45:44,536][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-06 14:45:44,537][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-06 14:45:44,567][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-06-14-06-41/diagnosed_p_t_s_d/0.0/5/csv_artifacts/
┏━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃   ┃ Name          ┃ Type             ┃ Params ┃
┡━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0 │ loss          │ CrossEntropyLoss │      0 │
│ 1 │ model         │ GRU_base         │ 68.9 K │
│ 2 │ model.RNNCell │ GRU              │ 68.7 K │
│ 3 │ model.predict │ Linear           │    130 │
│ 4 │ model.act     │ Sigmoid          │      0 │
└───┴───────────────┴──────────────────┴────────┘
Trainable params: 68.9 K                                                        
Non-trainable params: 0                                                         
Total params: 68.9 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 42.95it/s v_num: 0.000      
                                                              val/auc: 0.516    
                                                              val/f1: 0.222     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.143 train/auc:  
                                                              0.982 train/f1:   
                                                              0.982             
                                                              train/precision:  
                                                              0.982             
                                                              train/recall:     
                                                              0.982             
[2024-06-06 14:45:59,320][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-06-14-06-41/diagnosed_p_t_s_d/0.0/5>
[2024-06-06 14:45:59,321][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-06 14:45:59,323][HYDRA] 	#56 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_p_t_s_d data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=gru callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False logger=csv test=False seed=6
[rank: 0] Seed set to 6
[2024-06-06 14:45:59,497][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-06 14:45:59,499][train.py][INFO] - Instantiating model <src.methods.gru.lightningmodule.GRULightningModule>
[2024-06-06 14:45:59,500][train.py][INFO] - Instantiating callbacks...
[2024-06-06 14:45:59,500][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-06 14:45:59,502][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-06 14:45:59,503][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-06 14:45:59,503][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-06 14:45:59,505][train.py][INFO] - Instantiating loggers...
[2024-06-06 14:45:59,506][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-06 14:45:59,507][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-06 14:45:59,539][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-06-14-06-41/diagnosed_p_t_s_d/0.0/6/csv_artifacts/
┏━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃   ┃ Name          ┃ Type             ┃ Params ┃
┡━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0 │ loss          │ CrossEntropyLoss │      0 │
│ 1 │ model         │ GRU_base         │ 68.9 K │
│ 2 │ model.RNNCell │ GRU              │ 68.7 K │
│ 3 │ model.predict │ Linear           │    130 │
│ 4 │ model.act     │ Sigmoid          │      0 │
└───┴───────────────┴──────────────────┴────────┘
Trainable params: 68.9 K                                                        
Non-trainable params: 0                                                         
Total params: 68.9 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 42.73it/s v_num: 0.000      
                                                              val/auc: 0.500    
                                                              val/f1: 0.609     
                                                              val/precision:    
                                                              0.438 val/recall: 
                                                              1.000 train/auc:  
                                                              0.518 train/f1:   
                                                              0.675             
                                                              train/precision:  
                                                              0.509             
                                                              train/recall:     
                                                              1.000             
[2024-06-06 14:46:14,855][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-06-14-06-41/diagnosed_p_t_s_d/0.0/6>
[2024-06-06 14:46:14,856][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-06 14:46:14,859][HYDRA] 	#57 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_p_t_s_d data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=gru callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False logger=csv test=False seed=7
[rank: 0] Seed set to 7
[2024-06-06 14:46:15,047][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-06 14:46:15,049][train.py][INFO] - Instantiating model <src.methods.gru.lightningmodule.GRULightningModule>
[2024-06-06 14:46:15,050][train.py][INFO] - Instantiating callbacks...
[2024-06-06 14:46:15,051][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-06 14:46:15,053][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-06 14:46:15,053][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-06 14:46:15,053][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-06 14:46:15,055][train.py][INFO] - Instantiating loggers...
[2024-06-06 14:46:15,055][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-06 14:46:15,056][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-06 14:46:15,151][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-06-14-06-41/diagnosed_p_t_s_d/0.0/7/csv_artifacts/
┏━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃   ┃ Name          ┃ Type             ┃ Params ┃
┡━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0 │ loss          │ CrossEntropyLoss │      0 │
│ 1 │ model         │ GRU_base         │ 68.9 K │
│ 2 │ model.RNNCell │ GRU              │ 68.7 K │
│ 3 │ model.predict │ Linear           │    130 │
│ 4 │ model.act     │ Sigmoid          │      0 │
└───┴───────────────┴──────────────────┴────────┘
Trainable params: 68.9 K                                                        
Non-trainable params: 0                                                         
Total params: 68.9 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 41.64it/s v_num: 0.000      
                                                              val/auc: 0.516    
                                                              val/f1: 0.222     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.143 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000             
[2024-06-06 14:46:30,094][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-06-14-06-41/diagnosed_p_t_s_d/0.0/7>
[2024-06-06 14:46:30,095][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-06 14:46:30,097][HYDRA] 	#58 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_p_t_s_d data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=gru callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False logger=csv test=False seed=8
[rank: 0] Seed set to 8
[2024-06-06 14:46:30,272][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-06 14:46:30,273][train.py][INFO] - Instantiating model <src.methods.gru.lightningmodule.GRULightningModule>
[2024-06-06 14:46:30,274][train.py][INFO] - Instantiating callbacks...
[2024-06-06 14:46:30,274][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-06 14:46:30,276][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-06 14:46:30,277][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-06 14:46:30,277][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-06 14:46:30,278][train.py][INFO] - Instantiating loggers...
[2024-06-06 14:46:30,278][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-06 14:46:30,279][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-06 14:46:30,309][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-06-14-06-41/diagnosed_p_t_s_d/0.0/8/csv_artifacts/
┏━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃   ┃ Name          ┃ Type             ┃ Params ┃
┡━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0 │ loss          │ CrossEntropyLoss │      0 │
│ 1 │ model         │ GRU_base         │ 68.9 K │
│ 2 │ model.RNNCell │ GRU              │ 68.7 K │
│ 3 │ model.predict │ Linear           │    130 │
│ 4 │ model.act     │ Sigmoid          │      0 │
└───┴───────────────┴──────────────────┴────────┘
Trainable params: 68.9 K                                                        
Non-trainable params: 0                                                         
Total params: 68.9 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 44.89it/s v_num: 0.000      
                                                              val/auc: 0.516    
                                                              val/f1: 0.222     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.143 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000             
[2024-06-06 14:46:45,238][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-06-14-06-41/diagnosed_p_t_s_d/0.0/8>
[2024-06-06 14:46:45,239][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-06 14:46:45,241][HYDRA] 	#59 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_p_t_s_d data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=gru callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False logger=csv test=False seed=9
[rank: 0] Seed set to 9
[2024-06-06 14:46:45,411][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-06 14:46:45,413][train.py][INFO] - Instantiating model <src.methods.gru.lightningmodule.GRULightningModule>
[2024-06-06 14:46:45,414][train.py][INFO] - Instantiating callbacks...
[2024-06-06 14:46:45,414][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-06 14:46:45,416][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-06 14:46:45,417][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-06 14:46:45,417][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-06 14:46:45,418][train.py][INFO] - Instantiating loggers...
[2024-06-06 14:46:45,418][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-06 14:46:45,419][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-06 14:46:45,448][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-06-14-06-41/diagnosed_p_t_s_d/0.0/9/csv_artifacts/
┏━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃   ┃ Name          ┃ Type             ┃ Params ┃
┡━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0 │ loss          │ CrossEntropyLoss │      0 │
│ 1 │ model         │ GRU_base         │ 68.9 K │
│ 2 │ model.RNNCell │ GRU              │ 68.7 K │
│ 3 │ model.predict │ Linear           │    130 │
│ 4 │ model.act     │ Sigmoid          │      0 │
└───┴───────────────┴──────────────────┴────────┘
Trainable params: 68.9 K                                                        
Non-trainable params: 0                                                         
Total params: 68.9 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 40.78it/s v_num: 0.000      
                                                              val/auc: 0.516    
                                                              val/f1: 0.222     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.143 train/auc:  
                                                              0.991 train/f1:   
                                                              0.991             
                                                              train/precision:  
                                                              0.983             
                                                              train/recall:     
                                                              1.000             
[2024-06-06 14:47:00,238][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-06-14-06-41/diagnosed_p_t_s_d/0.0/9>
[2024-06-06 14:47:00,238][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-06 14:47:00,240][HYDRA] 	#60 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=doing_today data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=gru callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False logger=csv test=False seed=0
[2024-06-06 14:47:00,418][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-06 14:47:00,420][train.py][INFO] - Instantiating model <src.methods.gru.lightningmodule.GRULightningModule>
[2024-06-06 14:47:00,421][train.py][INFO] - Instantiating callbacks...
[2024-06-06 14:47:00,421][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-06 14:47:00,423][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-06 14:47:00,423][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-06 14:47:00,424][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-06 14:47:00,426][train.py][INFO] - Instantiating loggers...
[2024-06-06 14:47:00,426][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-06 14:47:00,427][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-06 14:47:00,463][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-06-14-06-41/doing_today/0.0/0/csv_artifacts/
┏━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃   ┃ Name          ┃ Type             ┃ Params ┃
┡━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0 │ loss          │ CrossEntropyLoss │      0 │
│ 1 │ model         │ GRU_base         │ 68.9 K │
│ 2 │ model.RNNCell │ GRU              │ 68.7 K │
│ 3 │ model.predict │ Linear           │    130 │
│ 4 │ model.act     │ Sigmoid          │      0 │
└───┴───────────────┴──────────────────┴────────┘
Trainable params: 68.9 K                                                        
Non-trainable params: 0                                                         
Total params: 68.9 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 35.60it/s v_num: 0.000      
                                                              val/auc: 0.400    
                                                              val/f1: 0.143     
                                                              val/precision:    
                                                              0.250 val/recall: 
                                                              0.100 train/auc:  
                                                              0.992 train/f1:   
                                                              0.992             
                                                              train/precision:  
                                                              0.985             
                                                              train/recall:     
                                                              1.000             
[2024-06-06 14:47:16,793][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-06-14-06-41/doing_today/0.0/0>
[2024-06-06 14:47:16,793][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-06 14:47:16,796][HYDRA] 	#61 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=doing_today data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=gru callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False logger=csv test=False seed=1
[rank: 0] Seed set to 1
[2024-06-06 14:47:16,972][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-06 14:47:16,974][train.py][INFO] - Instantiating model <src.methods.gru.lightningmodule.GRULightningModule>
[2024-06-06 14:47:16,975][train.py][INFO] - Instantiating callbacks...
[2024-06-06 14:47:16,975][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-06 14:47:16,977][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-06 14:47:16,977][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-06 14:47:16,978][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-06 14:47:16,979][train.py][INFO] - Instantiating loggers...
[2024-06-06 14:47:16,979][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-06 14:47:16,980][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-06 14:47:17,047][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-06-14-06-41/doing_today/0.0/1/csv_artifacts/
┏━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃   ┃ Name          ┃ Type             ┃ Params ┃
┡━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0 │ loss          │ CrossEntropyLoss │      0 │
│ 1 │ model         │ GRU_base         │ 68.9 K │
│ 2 │ model.RNNCell │ GRU              │ 68.7 K │
│ 3 │ model.predict │ Linear           │    130 │
│ 4 │ model.act     │ Sigmoid          │      0 │
└───┴───────────────┴──────────────────┴────────┘
Trainable params: 68.9 K                                                        
Non-trainable params: 0                                                         
Total params: 68.9 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 36.45it/s v_num: 0.000      
                                                              val/auc: 0.350    
                                                              val/f1: 0.133     
                                                              val/precision:    
                                                              0.200 val/recall: 
                                                              0.100 train/auc:  
                                                              0.992 train/f1:   
                                                              0.992             
                                                              train/precision:  
                                                              0.985             
                                                              train/recall:     
                                                              1.000             
[2024-06-06 14:47:33,728][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-06-14-06-41/doing_today/0.0/1>
[2024-06-06 14:47:33,728][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-06 14:47:33,730][HYDRA] 	#62 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=doing_today data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=gru callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False logger=csv test=False seed=2
[rank: 0] Seed set to 2
[2024-06-06 14:47:33,897][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-06 14:47:33,899][train.py][INFO] - Instantiating model <src.methods.gru.lightningmodule.GRULightningModule>
[2024-06-06 14:47:33,899][train.py][INFO] - Instantiating callbacks...
[2024-06-06 14:47:33,900][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-06 14:47:33,902][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-06 14:47:33,902][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-06 14:47:33,903][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-06 14:47:33,904][train.py][INFO] - Instantiating loggers...
[2024-06-06 14:47:33,904][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-06 14:47:33,905][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-06 14:47:33,934][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-06-14-06-41/doing_today/0.0/2/csv_artifacts/
┏━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃   ┃ Name          ┃ Type             ┃ Params ┃
┡━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0 │ loss          │ CrossEntropyLoss │      0 │
│ 1 │ model         │ GRU_base         │ 68.9 K │
│ 2 │ model.RNNCell │ GRU              │ 68.7 K │
│ 3 │ model.predict │ Linear           │    130 │
│ 4 │ model.act     │ Sigmoid          │      0 │
└───┴───────────────┴──────────────────┴────────┘
Trainable params: 68.9 K                                                        
Non-trainable params: 0                                                         
Total params: 68.9 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 37.00it/s v_num: 0.000      
                                                              val/auc: 0.500    
                                                              val/f1: 0.167     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.100 train/auc:  
                                                              0.992 train/f1:   
                                                              0.992             
                                                              train/precision:  
                                                              0.985             
                                                              train/recall:     
                                                              1.000             
[2024-06-06 14:47:51,023][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-06-14-06-41/doing_today/0.0/2>
[2024-06-06 14:47:51,024][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-06 14:47:51,026][HYDRA] 	#63 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=doing_today data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=gru callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False logger=csv test=False seed=3
[rank: 0] Seed set to 3
[2024-06-06 14:47:51,197][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-06 14:47:51,199][train.py][INFO] - Instantiating model <src.methods.gru.lightningmodule.GRULightningModule>
[2024-06-06 14:47:51,200][train.py][INFO] - Instantiating callbacks...
[2024-06-06 14:47:51,200][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-06 14:47:51,202][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-06 14:47:51,203][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-06 14:47:51,203][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-06 14:47:51,204][train.py][INFO] - Instantiating loggers...
[2024-06-06 14:47:51,204][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-06 14:47:51,205][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-06 14:47:51,235][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-06-14-06-41/doing_today/0.0/3/csv_artifacts/
┏━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃   ┃ Name          ┃ Type             ┃ Params ┃
┡━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0 │ loss          │ CrossEntropyLoss │      0 │
│ 1 │ model         │ GRU_base         │ 68.9 K │
│ 2 │ model.RNNCell │ GRU              │ 68.7 K │
│ 3 │ model.predict │ Linear           │    130 │
│ 4 │ model.act     │ Sigmoid          │      0 │
└───┴───────────────┴──────────────────┴────────┘
Trainable params: 68.9 K                                                        
Non-trainable params: 0                                                         
Total params: 68.9 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 34.90it/s v_num: 0.000      
                                                              val/auc: 0.400    
                                                              val/f1: 0.143     
                                                              val/precision:    
                                                              0.250 val/recall: 
                                                              0.100 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000             
[2024-06-06 14:48:08,273][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-06-14-06-41/doing_today/0.0/3>
[2024-06-06 14:48:08,274][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-06 14:48:08,276][HYDRA] 	#64 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=doing_today data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=gru callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False logger=csv test=False seed=4
[rank: 0] Seed set to 4
[2024-06-06 14:48:08,451][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-06 14:48:08,452][train.py][INFO] - Instantiating model <src.methods.gru.lightningmodule.GRULightningModule>
[2024-06-06 14:48:08,453][train.py][INFO] - Instantiating callbacks...
[2024-06-06 14:48:08,453][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-06 14:48:08,456][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-06 14:48:08,456][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-06 14:48:08,456][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-06 14:48:08,459][train.py][INFO] - Instantiating loggers...
[2024-06-06 14:48:08,459][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-06 14:48:08,460][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-06 14:48:08,495][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-06-14-06-41/doing_today/0.0/4/csv_artifacts/
┏━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃   ┃ Name          ┃ Type             ┃ Params ┃
┡━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0 │ loss          │ CrossEntropyLoss │      0 │
│ 1 │ model         │ GRU_base         │ 68.9 K │
│ 2 │ model.RNNCell │ GRU              │ 68.7 K │
│ 3 │ model.predict │ Linear           │    130 │
│ 4 │ model.act     │ Sigmoid          │      0 │
└───┴───────────────┴──────────────────┴────────┘
Trainable params: 68.9 K                                                        
Non-trainable params: 0                                                         
Total params: 68.9 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 35.08it/s v_num: 0.000      
                                                              val/auc: 0.450    
                                                              val/f1: 0.154     
                                                              val/precision:    
                                                              0.333 val/recall: 
                                                              0.100 train/auc:  
                                                              0.984 train/f1:   
                                                              0.985             
                                                              train/precision:  
                                                              0.970             
                                                              train/recall:     
                                                              1.000             
[2024-06-06 14:48:25,370][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-06-14-06-41/doing_today/0.0/4>
[2024-06-06 14:48:25,370][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-06 14:48:25,373][HYDRA] 	#65 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=doing_today data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=gru callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False logger=csv test=False seed=5
[rank: 0] Seed set to 5
[2024-06-06 14:48:25,550][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-06 14:48:25,552][train.py][INFO] - Instantiating model <src.methods.gru.lightningmodule.GRULightningModule>
[2024-06-06 14:48:25,553][train.py][INFO] - Instantiating callbacks...
[2024-06-06 14:48:25,553][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-06 14:48:25,556][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-06 14:48:25,556][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-06 14:48:25,556][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-06 14:48:25,558][train.py][INFO] - Instantiating loggers...
[2024-06-06 14:48:25,558][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-06 14:48:25,559][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-06 14:48:25,637][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-06-14-06-41/doing_today/0.0/5/csv_artifacts/
┏━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃   ┃ Name          ┃ Type             ┃ Params ┃
┡━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0 │ loss          │ CrossEntropyLoss │      0 │
│ 1 │ model         │ GRU_base         │ 68.9 K │
│ 2 │ model.RNNCell │ GRU              │ 68.7 K │
│ 3 │ model.predict │ Linear           │    130 │
│ 4 │ model.act     │ Sigmoid          │      0 │
└───┴───────────────┴──────────────────┴────────┘
Trainable params: 68.9 K                                                        
Non-trainable params: 0                                                         
Total params: 68.9 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 40.46it/s v_num: 0.000      
                                                              val/auc: 0.400    
                                                              val/f1: 0.143     
                                                              val/precision:    
                                                              0.250 val/recall: 
                                                              0.100 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000             
[2024-06-06 14:48:41,848][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-06-14-06-41/doing_today/0.0/5>
[2024-06-06 14:48:41,849][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-06 14:48:41,851][HYDRA] 	#66 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=doing_today data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=gru callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False logger=csv test=False seed=6
[rank: 0] Seed set to 6
[2024-06-06 14:48:42,028][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-06 14:48:42,030][train.py][INFO] - Instantiating model <src.methods.gru.lightningmodule.GRULightningModule>
[2024-06-06 14:48:42,030][train.py][INFO] - Instantiating callbacks...
[2024-06-06 14:48:42,031][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-06 14:48:42,033][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-06 14:48:42,033][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-06 14:48:42,034][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-06 14:48:42,035][train.py][INFO] - Instantiating loggers...
[2024-06-06 14:48:42,035][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-06 14:48:42,036][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-06 14:48:42,066][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-06-14-06-41/doing_today/0.0/6/csv_artifacts/
┏━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃   ┃ Name          ┃ Type             ┃ Params ┃
┡━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0 │ loss          │ CrossEntropyLoss │      0 │
│ 1 │ model         │ GRU_base         │ 68.9 K │
│ 2 │ model.RNNCell │ GRU              │ 68.7 K │
│ 3 │ model.predict │ Linear           │    130 │
│ 4 │ model.act     │ Sigmoid          │      0 │
└───┴───────────────┴──────────────────┴────────┘
Trainable params: 68.9 K                                                        
Non-trainable params: 0                                                         
Total params: 68.9 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 37.21it/s v_num: 0.000      
                                                              val/auc: 0.450    
                                                              val/f1: 0.154     
                                                              val/precision:    
                                                              0.333 val/recall: 
                                                              0.100 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000             
[2024-06-06 14:48:58,242][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-06-14-06-41/doing_today/0.0/6>
[2024-06-06 14:48:58,243][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-06 14:48:58,245][HYDRA] 	#67 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=doing_today data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=gru callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False logger=csv test=False seed=7
[rank: 0] Seed set to 7
[2024-06-06 14:48:58,415][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-06 14:48:58,417][train.py][INFO] - Instantiating model <src.methods.gru.lightningmodule.GRULightningModule>
[2024-06-06 14:48:58,418][train.py][INFO] - Instantiating callbacks...
[2024-06-06 14:48:58,418][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-06 14:48:58,420][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-06 14:48:58,421][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-06 14:48:58,421][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-06 14:48:58,422][train.py][INFO] - Instantiating loggers...
[2024-06-06 14:48:58,422][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-06 14:48:58,423][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-06 14:48:58,451][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-06-14-06-41/doing_today/0.0/7/csv_artifacts/
┏━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃   ┃ Name          ┃ Type             ┃ Params ┃
┡━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0 │ loss          │ CrossEntropyLoss │      0 │
│ 1 │ model         │ GRU_base         │ 68.9 K │
│ 2 │ model.RNNCell │ GRU              │ 68.7 K │
│ 3 │ model.predict │ Linear           │    130 │
│ 4 │ model.act     │ Sigmoid          │      0 │
└───┴───────────────┴──────────────────┴────────┘
Trainable params: 68.9 K                                                        
Non-trainable params: 0                                                         
Total params: 68.9 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 40.78it/s v_num: 0.000      
                                                              val/auc: 0.400    
                                                              val/f1: 0.143     
                                                              val/precision:    
                                                              0.250 val/recall: 
                                                              0.100 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000             
[2024-06-06 14:49:14,899][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-06-14-06-41/doing_today/0.0/7>
[2024-06-06 14:49:14,899][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-06 14:49:14,901][HYDRA] 	#68 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=doing_today data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=gru callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False logger=csv test=False seed=8
[rank: 0] Seed set to 8
[2024-06-06 14:49:15,077][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-06 14:49:15,078][train.py][INFO] - Instantiating model <src.methods.gru.lightningmodule.GRULightningModule>
[2024-06-06 14:49:15,079][train.py][INFO] - Instantiating callbacks...
[2024-06-06 14:49:15,079][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-06 14:49:15,081][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-06 14:49:15,082][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-06 14:49:15,082][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-06 14:49:15,084][train.py][INFO] - Instantiating loggers...
[2024-06-06 14:49:15,085][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-06 14:49:15,086][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-06 14:49:15,116][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-06-14-06-41/doing_today/0.0/8/csv_artifacts/
┏━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃   ┃ Name          ┃ Type             ┃ Params ┃
┡━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0 │ loss          │ CrossEntropyLoss │      0 │
│ 1 │ model         │ GRU_base         │ 68.9 K │
│ 2 │ model.RNNCell │ GRU              │ 68.7 K │
│ 3 │ model.predict │ Linear           │    130 │
│ 4 │ model.act     │ Sigmoid          │      0 │
└───┴───────────────┴──────────────────┴────────┘
Trainable params: 68.9 K                                                        
Non-trainable params: 0                                                         
Total params: 68.9 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 41.11it/s v_num: 0.000      
                                                              val/auc: 0.400    
                                                              val/f1: 0.143     
                                                              val/precision:    
                                                              0.250 val/recall: 
                                                              0.100 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000             
[2024-06-06 14:49:31,592][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-06-14-06-41/doing_today/0.0/8>
[2024-06-06 14:49:31,593][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-06 14:49:31,595][HYDRA] 	#69 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=doing_today data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=gru callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False logger=csv test=False seed=9
[rank: 0] Seed set to 9
[2024-06-06 14:49:31,772][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-06 14:49:31,774][train.py][INFO] - Instantiating model <src.methods.gru.lightningmodule.GRULightningModule>
[2024-06-06 14:49:31,775][train.py][INFO] - Instantiating callbacks...
[2024-06-06 14:49:31,775][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-06 14:49:31,777][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-06 14:49:31,778][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-06 14:49:31,778][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-06 14:49:31,780][train.py][INFO] - Instantiating loggers...
[2024-06-06 14:49:31,780][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-06 14:49:31,781][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-06 14:49:31,846][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-06-14-06-41/doing_today/0.0/9/csv_artifacts/
┏━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃   ┃ Name          ┃ Type             ┃ Params ┃
┡━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0 │ loss          │ CrossEntropyLoss │      0 │
│ 1 │ model         │ GRU_base         │ 68.9 K │
│ 2 │ model.RNNCell │ GRU              │ 68.7 K │
│ 3 │ model.predict │ Linear           │    130 │
│ 4 │ model.act     │ Sigmoid          │      0 │
└───┴───────────────┴──────────────────┴────────┘
Trainable params: 68.9 K                                                        
Non-trainable params: 0                                                         
Total params: 68.9 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 42.93it/s v_num: 0.000      
                                                              val/auc: 0.400    
                                                              val/f1: 0.250     
                                                              val/precision:    
                                                              0.333 val/recall: 
                                                              0.200 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000             
[2024-06-06 14:49:47,791][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-06-14-06-41/doing_today/0.0/9>
[2024-06-06 14:49:47,792][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
